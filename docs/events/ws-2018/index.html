<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Other Workshops and Events (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Other Workshops and Events (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-01>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-02>Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-03>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-05>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">31&nbsp;papers</span></li><li><a class=align-middle href=#w18-06>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-07>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-08>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-09>Proceedings of the Workshop on Figurative Language Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-10>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-11>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-12>Proceedings of the Second Workshop on Subword/Character LEvel Models</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-13>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-14>Proceedings of the First International Workshop on Spatial Language Understanding</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-15>Proceedings of the First Workshop on Storytelling</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-16>Proceedings of the Second Workshop on Stylistic Variation</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-17>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-18>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-19>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-20>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-21>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-22>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-23>Proceedings of the BioNLP 2018 workshop</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-24>Proceedings of the Seventh Named Entities Workshop</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-25>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-26>Proceedings of the Workshop on Machine Reading for Question Answering</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-27>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-28>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-29>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-30>Proceedings of The Third Workshop on Representation Learning for NLP</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w18-31>Proceedings of the First Workshop on Economics and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-32>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-33>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-34>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-35>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w18-36>Proceedings of the First Workshop on Multilingual Surface Realisation</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-37>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w18-38>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w18-39>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w18-40>Proceedings of the Third Workshop on Semantic Deep Learning</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-41>Proceedings of the First International Workshop on Language Cognition and Computational Models</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-42>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-43>Proceedings of the Workshop Events and Stories in the News 2018</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-44>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w18-45>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-46>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w18-47>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-48>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-49>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li><li><a class=align-middle href=#w18-50>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class=align-middle href=#w18-51>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-52>Proceedings of the 5th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#w18-53>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-54>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li><li><a class=align-middle href=#w18-55>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#w18-56>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w18-57>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w18-58>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-59>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-60>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-61>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w18-62>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">34&nbsp;papers</span></li><li><a class=align-middle href=#w18-63>Proceedings of the Third Conference on Machine Translation: Research Papers</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#w18-64>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a>
<span class="badge badge-info align-middle ml-1">51&nbsp;papers</span></li><li><a class=align-middle href=#w18-65>Proceedings of the 11th International Conference on Natural Language Generation</a>
<span class="badge badge-info align-middle ml-1">40&nbsp;papers</span></li><li><a class=align-middle href=#w18-66>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-67>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&NLG)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-69>Proceedings of the Workshop on NLG for Human–Robot Interaction</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-70>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-71>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li></ul></div></div><div id=w18-01><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-01.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-01/>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0100/>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (<span class=acl-fixed-case>CMCL</span> 2018)</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/m/marten-van-schijndel/>Marten van Schijndel</a></span></p></div><hr><div id=w18-02><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-02.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-02/>Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0200/>Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages</a></strong><br><a href=/people/t/tommi-a-pirinen/>Tommi A. Pirinen</a>
|
<a href=/people/m/michael-riessler/>Michael Rießler</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a>
|
<a href=/people/t/trond-trosterud/>Trond Trosterud</a>
|
<a href=/people/f/francis-tyers/>Francis M. Tyers</a></span></p></div><hr><div id=w18-03><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W18-03/>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0300/>Proceedings of the Society for Computation in Linguistics (<span class=acl-fixed-case>SC</span>i<span class=acl-fixed-case>L</span>) 2018</a></strong><br><a href=/people/g/gaja-jarosz/>Gaja Jarosz</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/j/joe-pater/>Joe Pater</a></span></p></div><hr><div id=w18-05><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-05.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-05/>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0500/>Proceedings of the Thirteenth Workshop on Innovative Use of <span class=acl-fixed-case>NLP</span> for Building Educational Applications</a></strong><br><a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0503/>Predicting misreadings from gaze in children with reading difficulties</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/s/sigrid-klerke/>Sigrid Klerke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0503><div class="card-body p-3 small">We present the first work on predicting reading mistakes in children with <a href=https://en.wikipedia.org/wiki/Reading_disability>reading difficulties</a> based on eye-tracking data from real-world reading teaching. Our approach employs several linguistic and gaze-based features to inform an ensemble of different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, including multi-task learning models that let us transfer knowledge about individual readers to attain better predictions. Notably, the <a href=https://en.wikipedia.org/wiki/Data>data</a> we use in this work stems from noisy readings in the wild, outside of controlled lab conditions. Our experiments show that despite the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and despite the small fraction of misreadings, gaze data improves the performance more than any other feature group and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve good performance. We further show that gaze patterns for misread words do not fully generalize across readers, but that we can transfer some knowledge between readers using <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> at least in some cases. Applications of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> include partial automation of reading assessment as well as personalized text simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0504/>Automatic Input Enrichment for Selecting Reading Material : An Online Study with English Teachers<span class=acl-fixed-case>E</span>nglish Teachers</a></strong><br><a href=/people/m/maria-chinkina/>Maria Chinkina</a>
|
<a href=/people/a/ankita-oswal/>Ankita Oswal</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0504><div class="card-body p-3 small">Input material at the appropriate level is crucial for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a>. Automating the search for such material can systematically and efficiently support teachers in their pedagogical practice. This is the goal of the computational linguistic task of automatic input enrichment (Chinkina & Meurers, 2016): It analyzes and re-ranks a collection of texts in order to prioritize those containing target linguistic forms. In the online study described in the paper, we collected 240 responses from English teachers in order to investigate whether they preferred automatic input enrichment over <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a> when selecting reading material for class. Participants demonstrated a general preference for the material provided by an automatic input enrichment system. It was also rated significantly higher than the texts retrieved by a standard <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engine</a> with regard to the representation of linguistic forms and equivalent with regard to the relevance of the content to the topic. We discuss the implications of the results for <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> and consider the potential strands of future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0506/>Second Language Acquisition Modeling</a></strong><br><a href=/people/b/burr-settles/>Burr Settles</a>
|
<a href=/people/c/chris-brust/>Chris Brust</a>
|
<a href=/people/e/erin-gustafson/>Erin Gustafson</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0506><div class="card-body p-3 small">We present the task of second language acquisition (SLA) modeling. Given a history of errors made by learners of a <a href=https://en.wikipedia.org/wiki/Second_language>second language</a>, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7 M words produced by more than 6k learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a> using <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a>, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>.<i>second language acquisition (SLA) modeling</i>. Given a history of errors made by learners of a\n second language, the task is to predict errors that they are\n likely to make at arbitrary points in the future. We describe a\n large corpus of more than 7M words produced by more than 6k\n learners of English, Spanish, and French using Duolingo, a\n popular online language-learning app. Then we report on the\n results of a shared task challenge aimed studying the SLA task\n via this corpus, which attracted 15 teams and synthesized work\n from various fields including cognitive science, linguistics,\n and machine learning.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0507/>A Report on the Complex Word Identification Shared Task 2018</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0507><div class="card-body p-3 small">We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT&#8217;2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks : English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks : <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and probabilistic classification. A total of 12 teams submitted their results in different task / track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0508/>Towards Single Word Lexical Complexity Prediction</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0508><div class="card-body p-3 small">In this paper we present work-in-progress where we investigate the usefulness of previously created word lists to the task of single-word lexical complexity analysis and prediction of the complexity level for learners of <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> as a second language. The word lists used map each word to a single CEFR level, and the task consists of predicting CEFR levels for unseen words. In contrast to previous work on word-level lexical complexity, we experiment with topics as additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and show that linking words to topics significantly increases <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0511/>Annotating Student Talk in Text-based Classroom Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/a/amanda-godley/>Amanda Godley</a>
|
<a href=/people/c/christopher-olshefski/>Christopher Olshefski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0511><div class="card-body p-3 small">Classroom discussions in English Language Arts have a positive effect on students&#8217; reading, writing and reasoning skills. Although prior work has largely focused on teacher talk and student-teacher interactions, we focus on three theoretically-motivated aspects of high-quality student talk : <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Knowledge_domain>knowledge domain</a>. We introduce an annotation scheme, then show that the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> can be used to produce reliable <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and that the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are predictive of discussion quality. We also highlight opportunities provided by our scheme for education and natural language processing research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0512/>Toward Automatically Measuring Learner Ability from Human-Machine Dialog Interactions using Novel Psychometric Models</a></strong><br><a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/m/michelle-lamar/>Michelle LaMar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0512><div class="card-body p-3 small">While dialog systems have been widely deployed for computer-assisted language learning (CALL) and formative assessment systems in recent years, relatively limited work has been done with respect to the psychometrics and validity of these technologies in evaluating and providing feedback regarding student learning and conversational ability. This paper formulates a Markov decision process based measurement model, and applies it to text chat data collected from crowdsourced native and non-native English language speakers interacting with an automated dialog agent. We investigate how well the model measures speaker conversational ability, and find that it effectively captures the differences in how native and non-native speakers of English accomplish the dialog task. Such models could have important implications for CALL systems of the future that effectively combine dialog management with measurement of learner conversational ability in real-time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0513/>Generating Feedback for English Foreign Language Exercises<span class=acl-fixed-case>E</span>nglish Foreign Language Exercises</a></strong><br><a href=/people/b/bjorn-rudzewitz/>Björn Rudzewitz</a>
|
<a href=/people/r/ramon-ziai/>Ramon Ziai</a>
|
<a href=/people/k/kordula-de-kuthy/>Kordula De Kuthy</a>
|
<a href=/people/v/verena-moller/>Verena Möller</a>
|
<a href=/people/f/florian-nuxoll/>Florian Nuxoll</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0513><div class="card-body p-3 small">While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a>. We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and content errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0523/>Grotoco@SLAM : Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models<span class=acl-fixed-case>SLAM</span>: Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models</a></strong><br><a href=/people/s/sigrid-klerke/>Sigrid Klerke</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0523><div class="card-body p-3 small">We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for the task, including user-derived measures, while examining how far we can get with a simple <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a>. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system : a task-wise (per exercise-format) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0524/>Context Based Approach for Second Language Acquisition</a></strong><br><a href=/people/n/nihal-v-nayak/>Nihal V. Nayak</a>
|
<a href=/people/a/arjun-r-rao/>Arjun R. Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0524><div class="card-body p-3 small">SLAM 2018 focuses on predicting a student&#8217;s mistake while using the Duolingo application. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> we developed for this shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our system uses a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a> to predict the likelihood of a student making a mistake while answering an exercise on <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a> in all three language tracks-English / Spanish (en / es), Spanish / English (es / en) and French / English (fr / en). We conduct an ablation study with several <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> during the development of this system and discover that context based features plays a major role in language acquisition modeling. Our model beats Duolingo&#8217;s baseline scores in all three language tracks (AUROC scores for en / es = 0.821, es / en = 0.790 and fr / en = 0.812). Our work makes a case for providing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>favourable textual context</a> for students while learning second language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0525/>Second Language Acquisition Modeling : An Ensemble Approach</a></strong><br><a href=/people/a/anton-osika/>Anton Osika</a>
|
<a href=/people/s/susanna-nilsson/>Susanna Nilsson</a>
|
<a href=/people/a/andrii-sydorchuk/>Andrii Sydorchuk</a>
|
<a href=/people/f/faruk-sahin/>Faruk Sahin</a>
|
<a href=/people/a/anders-huss/>Anders Huss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0525><div class="card-body p-3 small">Accurate prediction of students&#8217; knowledge is a fundamental building block of <a href=https://en.wikipedia.org/wiki/Personalized_learning>personalized learning systems</a>. Here, we propose an ensemble model to predict student knowledge gaps. Applying our approach to student trace data from the online educational platform Duolingo we achieved highest score on all three datasets in the 2018 Shared Task on Second Language Acquisition Modeling. We describe our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and discuss relevance of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to how it would be setup in a production environment for personalized education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0526/>Modeling Second-Language Learning from a Psychological Perspective</a></strong><br><a href=/people/a/alexander-rich/>Alexander Rich</a>
|
<a href=/people/p/pamela-osborn-popp/>Pamela Osborn Popp</a>
|
<a href=/people/d/david-halpern/>David Halpern</a>
|
<a href=/people/a/anselm-rothe/>Anselm Rothe</a>
|
<a href=/people/t/todd-gureckis/>Todd Gureckis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0526><div class="card-body p-3 small">Psychological research on learning and memory has tended to emphasize small-scale laboratory studies. However, large datasets of people using <a href=https://en.wikipedia.org/wiki/Educational_software>educational software</a> provide opportunities to explore these issues from a new perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well-known class of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> (gradient boosted decision trees), with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> partially informed by theories from the <a href=https://en.wikipedia.org/wiki/Psychology>psychological literature</a>. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0527/>A Memory-Sensitive Classification Model of Errors in Early Second Language Learning</a></strong><br><a href=/people/b/brendan-tomoschuk/>Brendan Tomoschuk</a>
|
<a href=/people/j/jarrett-lovelett/>Jarrett Lovelett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0527><div class="card-body p-3 small">In this paper, we explore a variety of linguistic and cognitive features to better understand <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language acquisition</a> in early users of the language learning app Duolingo. With these features, we trained a random forest classifier to predict errors in early learners of <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Of particular note was our finding that mean and variance in error for each user and token can be a memory efficient replacement for their respective dummy-encoded categorical variables. At test, these models improved over the baseline model with AUROC values of 0.803 for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 0.823 for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and 0.829 for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0529 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0529/>Language Model Based Grammatical Error Correction without Annotated Training Data</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0529><div class="card-body p-3 small">Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0530/>A Semantic Role-based Approach to Open-Domain Automatic Question Generation</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/brian-riordan/>Brian Riordan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0530><div class="card-body p-3 small">We present a novel rule-based system for automatic generation of factual questions from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a>, using semantic role labeling (SRL) as the main form of text analysis. The system is capable of generating both <a href=https://en.wikipedia.org/wiki/Questionnaire>wh-questions</a> and <a href=https://en.wikipedia.org/wiki/Yes&#8211;no_question>yes / no questions</a> from the same <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a>. We present an extensive evaluation of the <a href=https://en.wikipedia.org/wiki/System>system</a> and compare it to a recent neural network architecture for question generation. The SRL-based system outperforms the <a href=https://en.wikipedia.org/wiki/Nervous_system>neural system</a> in both average quality and variety of generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0531/>Automated Content Analysis : A Case Study of Computer Science Student Summaries</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/p/patricia-m-davies/>Patricia M. Davies</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0531><div class="card-body p-3 small">Technology is transforming Higher Education learning and teaching. This paper reports on a project to examine how and why automated content analysis could be used to assess precis writing by university students. We examine the case of one hundred and twenty-two summaries written by computer science freshmen. The texts, which had been hand scored using a teacher-designed rubric, were autoscored using the Natural Language Processing software, PyrEval. Pearson&#8217;s correlation coefficient and <a href=https://en.wikipedia.org/wiki/Spearman_rank_correlation>Spearman rank correlation</a> were used to analyze the relationship between the teacher score and the PyrEval score for each summary. Three content models automatically constructed by PyrEval from different sets of human reference summaries led to consistent correlations, showing that the approach is reliable. Also observed was that, in cases where the focus of student assessment centers on formative feedback, categorizing the PyrEval scores by examining the average and standard deviations could lead to novel interpretations of their relationships. It is suggested that this project has implications for the ways in which automated content analysis could be used to help university students improve their summarization skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0532/>Toward Data-Driven Tutorial Question Answering with Deep Learning Conversational Models</a></strong><br><a href=/people/m/mayank-kulkarni/>Mayank Kulkarni</a>
|
<a href=/people/k/kristy-boyer/>Kristy Boyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0532><div class="card-body p-3 small">There has been an increase in popularity of data-driven question answering systems given their recent success. This pa-per explores the possibility of building a tutorial question answering system for <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java programming</a> from data sampled from a community-based question answering forum. This paper reports on the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset. We investigate how retrieval-based and generative models perform on the given <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The work also investigates the usefulness of using hybrid approaches such as combining retrieval-based and generative models. The results indicate that building data-driven tutorial systems using community-based question answering forums holds significant promise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0533/>Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></strong><br><a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/n/neisarg-dave/>Neisarg Dave</a>
|
<a href=/people/d/drew-wham/>Drew Wham</a>
|
<a href=/people/b/bart-pursel/>Bart Pursel</a>
|
<a href=/people/c/c-lee-giles/>C. Lee Giles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0533><div class="card-body p-3 small">We investigate how <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, specifically <a href=https://en.wikipedia.org/wiki/Ranking>ranking models</a>, can be used to select useful distractors for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice questions</a>. Our proposed models can learn to select distractors that resemble those in actual exam questions, which is different from most existing unsupervised ontology-based and similarity-based methods. We empirically study feature-based and neural net (NN) based ranking models with experiments on the recently released SciQ dataset and our MCQL dataset. Experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outperform both the NN-based method and unsupervised baselines. These two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can also be used as benchmarks for distractor generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0534/>A Portuguese Native Language Identification Dataset<span class=acl-fixed-case>P</span>ortuguese Native Language Identification Dataset</a></strong><br><a href=/people/i/iria-del-rio-gayo/>Iria del Río Gayo</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0534><div class="card-body p-3 small">In this paper we present NLI-PT, the first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author&#8217;s first language based on their second language writing. The dataset includes 1,868 student essays written by learners of <a href=https://en.wikipedia.org/wiki/European_Portuguese>European Portuguese</a>, native speakers of the following L1s : <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/Tetum_language>Tetum</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. NLI-PT includes the original student text and four different types of annotation : POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>Second Language Acquisition</a> and educational NLP. We discuss possible applications of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and present the results obtained for the first lexical baseline system for Portuguese NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0536 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0536/>The Effect of Adding Authorship Knowledge in Automated Text Scoring</a></strong><br><a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/x/xie-chen/>Xie Chen</a>
|
<a href=/people/r/ronan-cummins/>Ronan Cummins</a>
|
<a href=/people/o/oistein-e-andersen/>Øistein E. Andersen</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0536><div class="card-body p-3 small">Some <a href=https://en.wikipedia.org/wiki/Test_(assessment)>language exams</a> have multiple writing tasks. When a learner writes multiple texts in a language exam, it is not surprising that the quality of these texts tends to be similar, and the existing automated text scoring (ATS) systems do not explicitly model this similarity. In this paper, we suggest that it could be useful to include the other texts written by this learner in the same exam as extra references in an ATS system. We propose various approaches of fusing information from multiple tasks and pass this authorship knowledge into our ATS model on six different datasets. We show that this can positively affect the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance at a global level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0537/>SB@GU at the Complex Word Identification 2018 Shared Task<span class=acl-fixed-case>SB</span>@<span class=acl-fixed-case>GU</span> at the Complex Word Identification 2018 Shared Task</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0537><div class="card-body p-3 small">In this paper, we describe our experiments for the Shared Task on Complex Word Identification (CWI) 2018 (Yimam et al., 2018), hosted by the 13th Workshop on Innovative Use of NLP for Building Educational Applications (BEA) at NAACL 2018. Our system for <a href=https://en.wikipedia.org/wiki/English_language>English</a> builds on previous work for <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> concerning the classification of words into proficiency levels. We investigate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and compare their usefulness using <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection methods</a>. For the German, Spanish and French data we use simple <a href=https://en.wikipedia.org/wiki/System>systems</a> based on character n-gram models and show that sometimes simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve comparable results to fully feature-engineered systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0539 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0539/>Deep Learning Architecture for Complex Word Identification</a></strong><br><a href=/people/d/dirk-de-hertog/>Dirk De Hertog</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0539><div class="card-body p-3 small">We describe a system for the CWI-task that includes information on 5 aspects of the (complex) lexical item, namely distributional information of the item itself, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological structure</a>, psychological measures, corpus-counts and topical information. We constructed a deep learning architecture that combines those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and apply it to the probabilistic and binary classification task for all <a href=https://en.wikipedia.org/wiki/English_language>English sets</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We achieved reasonable performance on all sets with best performances seen on the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>probabilistic task</a>, particularly on the English news set (MAE 0.054 and F1-score of 0.872). An analysis of the results shows that reasonable performance can be achieved with a single <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> without any domain-specific tweaking of the parameter settings and that distributional features capture almost all of the information also found in hand-crafted features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0540 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0540/>NILC at CWI 2018 : Exploring <a href=https://en.wikipedia.org/wiki/Feature_engineering>Feature Engineering</a> and Feature Learning<span class=acl-fixed-case>NILC</span> at <span class=acl-fixed-case>CWI</span> 2018: Exploring Feature Engineering and Feature Learning</a></strong><br><a href=/people/n/nathan-hartmann/>Nathan Hartmann</a>
|
<a href=/people/l/leandro-borges-dos-santos/>Leandro Borges dos Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0540><div class="card-body p-3 small">This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches : (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0541/>Complex Word Identification Using Character n-grams</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0541><div class="card-body p-3 small">This paper investigates the use of character n-gram frequencies for identifying complex words in English, German and Spanish texts. The approach is based on the assumption that complex words are likely to contain different <a href=https://en.wikipedia.org/wiki/Character_(computing)>character sequences</a> than simple words. The <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>multinomial Naive Bayes classifier</a> was used with n-grams of different lengths as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and the best results were obtained for the combination of 2-grams and 4-grams. This variant was submitted to the Complex Word Identification Shared Task 2018 for all texts and achieved F-scores between 70 % and 83 %. The system was ranked in the middle range for all English texts, as third of fourteen submissions for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and as tenth of seventeen submissions for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is not very convenient for the cross-language task, achieving only 59 % on the <a href=https://en.wikipedia.org/wiki/French_language>French text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0545/>Deep Factorization Machines for Knowledge Tracing</a></strong><br><a href=/people/j/jill-jenn-vie/>Jill-Jênn Vie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0545><div class="card-body p-3 small">This paper introduces our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We used deep factorization machines, a wide and deep learning model of pairwise relationships between users, items, skills, and other entities considered. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> (AUC 0.815) hopefully managed to beat the logistic regression baseline (AUC 0.774) but not the top performing <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (AUC 0.861) and reveals interesting strategies to build upon <a href=https://en.wikipedia.org/wiki/Item_response_theory>item response theory models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0546/>CLUF : a Neural Model for Second Language Acquisition Modeling<span class=acl-fixed-case>CLUF</span>: a Neural Model for Second Language Acquisition Modeling</a></strong><br><a href=/people/s/shuyao-xu/>Shuyao Xu</a>
|
<a href=/people/j/jin-chen/>Jin Chen</a>
|
<a href=/people/l/long-qin/>Long Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0546><div class="card-body p-3 small">Second Language Acquisition Modeling is the task to predict whether a <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learner</a> would respond correctly in future exercises based on their learning history. In this paper, we propose a neural network based system to utilize rich contextual, linguistic and user information. Our neural model consists of a Context encoder, a Linguistic feature encoder, a User information encoder and a Format information encoder (CLUF). Furthermore, a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is introduced to combine such encoded features and make final predictions. Our system ranked in first place in the English track and second place in the Spanish and French track with an AUROC score of 0.861, 0.835 and 0.854 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0547/>Neural sequence modelling for learner error prediction</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0547><div class="card-body p-3 small">This paper describes our use of two recurrent neural network sequence models : sequence labelling and sequence-to-sequence models, for the prediction of future learner errors in our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We show that these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capture complementary information as combining them improves performance. Furthermore, the same network architecture and group of features can be used directly to build competitive prediction models in all three language tracks, demonstrating that our approach generalises well across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0548 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0548/>Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a></a></strong><br><a href=/people/l/le-an-ha/>Le An Ha</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0548><div class="card-body p-3 small">Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process. In this paper we propose a fully automatic method for generating distractor suggestions for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice questions</a> used in high-stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval methods</a>. We frame the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> as a <a href=https://en.wikipedia.org/wiki/Prediction>prediction task</a> where we aim to predict the human-produced distractors used in large sets of <a href=https://en.wikipedia.org/wiki/Medical_research>medical questions</a>, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0549 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0549/>Co-Attention Based Neural Network for Source-Dependent Essay Scoring</a></strong><br><a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0549><div class="card-body p-3 small">This paper presents an investigation of using a co-attention based neural network for source-dependent essay scoring. We use a co-attention mechanism to help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn the importance of each part of the essay more accurately. Also, this paper shows that the co-attention based neural network model provides reliable score prediction of source-dependent responses. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two source-dependent response corpora. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> on both <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>. We also show that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is similar to the <a href=https://en.wikipedia.org/wiki/Expert_witness>expert opinions</a> with examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0550 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0550/>Cross-Lingual Content Scoring</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/s/sebastian-stennmanns/>Sebastian Stennmanns</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0550><div class="card-body p-3 small">We investigate the feasibility of cross-lingual content scoring, a scenario where training and test data in an automatic scoring task are from two different languages. Cross-lingual scoring can contribute to <a href=https://en.wikipedia.org/wiki/Educational_equality>educational equality</a> by allowing answers in multiple languages. Training a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in one language and applying it to another language might also help to overcome data sparsity issues by re-using trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> from other languages. As there is no suitable dataset available for this new task, we create a comparable bi-lingual corpus by extending the English ASAP dataset with German answers. Our experiments with cross-lingual scoring based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-translating</a> either training or test data show a considerable drop in scoring quality.</div></div></div><hr><div id=w18-06><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-06.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-06/>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0600/>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a></strong><br><a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0601/>What type of happiness are you looking for?-A closer look at detecting mental health from language</a></strong><br><a href=/people/a/alina-arseniev-koehler/>Alina Arseniev-Koehler</a>
|
<a href=/people/s/sharon-mozgai/>Sharon Mozgai</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0601><div class="card-body p-3 small">Computational models to detect <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental illnesses</a> from <a href=https://en.wikipedia.org/wiki/Writing>text</a> and <a href=https://en.wikipedia.org/wiki/Speech>speech</a> could enhance our understanding of <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> while offering opportunities for early detection and intervention. However, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are often disconnected from the lived experience of depression and the larger diagnostic debates in <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>. This article investigates these disconnects, primarily focusing on the labels used to diagnose <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>, how these labels are computationally represented, and the performance metrics used to evaluate <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. We also consider how <a href=https://en.wikipedia.org/wiki/Medical_device>medical instruments</a> used to measure <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>, such as the Patient Health Questionnaire (PHQ), contribute to these disconnects. To illustrate our points, we incorporate mixed-methods analyses of 698 interviews on <a href=https://en.wikipedia.org/wiki/Emotional_health>emotional health</a>, which are coupled with self-report PHQ screens for <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>. We propose possible strategies to bridge these gaps between modern <a href=https://en.wikipedia.org/wiki/Psychiatry>psychiatric understandings of depression</a>, <a href=https://en.wikipedia.org/wiki/Laity>lay experience of depression</a>, and <a href=https://en.wikipedia.org/wiki/Computational_psychology>computational representation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0603/>Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online Postings</a></strong><br><a href=/people/h/han-chin-shing/>Han-Chin Shing</a>
|
<a href=/people/s/suraj-nair/>Suraj Nair</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/m/meir-friedenberg/>Meir Friedenberg</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0603><div class="card-body p-3 small">We report on the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>assessment of suicide risk</a> via online postings in <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Evaluation of risk-level annotations by experts yields what is, to our knowledge, the first demonstration of reliability in <a href=https://en.wikipedia.org/wiki/Risk_assessment>risk assessment</a> by clinicians based on social media postings. We also introduce and demonstrate the value of a new, detailed rubric for assessing suicide risk, compare crowdsourced with expert performance, and present baseline predictive modeling experiments using the new dataset, which will be made available to researchers through the American Association of Suicidology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0604/>CLPsych 2018 Shared Task : Predicting Current and Future Psychological Health from Childhood Essays<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays</a></strong><br><a href=/people/v/veronica-lynn/>Veronica Lynn</a>
|
<a href=/people/a/alissa-goodman/>Alissa Goodman</a>
|
<a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0604><div class="card-body p-3 small">We describe the shared task for the CLPsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. Language-based predictions of a person&#8217;s current health have the potential to supplement traditional <a href=https://en.wikipedia.org/wiki/Psychological_evaluation>psychological assessment</a> such as <a href=https://en.wikipedia.org/wiki/Questionnaire>questionnaires</a>, improving intake risk measurement and monitoring. Predictions of future psychological health can aid with both early detection and the development of <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>preventative care</a>. Research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the NLP community. This shared task represents one of the first attempts to evaluate the use of early language to predict future health ; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>prevention</a> and <a href=https://en.wikipedia.org/wiki/Therapy>treatment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0606/>Using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for automatic triage of posts in a <a href=https://en.wikipedia.org/wiki/Internet_forum>peer-support forum</a></a></strong><br><a href=/people/e/edgar-altszyler/>Edgar Altszyler</a>
|
<a href=/people/a/ariel-j-berenstein/>Ariel J. Berenstein</a>
|
<a href=/people/d/david-n-milne/>David Milne</a>
|
<a href=/people/r/rafael-a-calvo/>Rafael A. Calvo</a>
|
<a href=/people/d/diego-fernandez-slezak/>Diego Fernandez Slezak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0606><div class="card-body p-3 small">Mental health forums are online spaces where people can share their experiences anonymously and get peer support. These <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>, require the supervision of moderators to provide support in delicate cases, such as posts expressing <a href=https://en.wikipedia.org/wiki/Suicide_ideation>suicide ideation</a>. The large increase in the number of forum users makes the task of the moderators unmanageable without the help of <a href=https://en.wikipedia.org/wiki/Triage_(medicine)>automatic triage systems</a>. In the present paper, we present a Machine Learning approach for the triage of posts. Most approaches in the literature focus on the content of the posts, but only a few authors take advantage of features extracted from the context in which they appear. Our approach consists of the development and implementation of a large variety of new <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> from both, the content and the context of posts, such as previous messages, interaction with other users and author&#8217;s history. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> has competed in the CLPsych 2017 Shared Task, obtaining the first place for several of the subtasks. Moreover, we also found that models that take advantage of post context improve significantly its performance in the detection of flagged posts (posts that require moderators attention), as well as those that focus on post content outperforms in the detection of most urgent events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0607/>Hierarchical neural model with <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for the classification of social media text related to mental health</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/g/george-gkotsis/>George Gkotsis</a>
|
<a href=/people/r/rina-dutta/>Rina Dutta</a>
|
<a href=/people/r/robert-stewart/>Robert Stewart</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0607><div class="card-body p-3 small">Mental health problems represent a major public health challenge. Automated analysis of text related to <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> is aimed to help <a href=https://en.wikipedia.org/wiki/Medical_decision-making>medical decision-making</a>, <a href=https://en.wikipedia.org/wiki/Public_health_policy>public health policies</a> and to improve <a href=https://en.wikipedia.org/wiki/Health_care>health care</a>. Such <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> may involve <a href=https://en.wikipedia.org/wiki/Categorization>text classification</a>. Traditionally, automated classification has been performed mainly using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> involving costly <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Recently, the performance of those <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> has been dramatically improved by neural methods. However, mainly Convolutional neural networks (CNNs) have been explored. In this paper, we apply a hierarchical Recurrent neural network (RNN) architecture with an attention mechanism on social media data related to <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>. We show that this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> improves overall classification results as compared to previously reported results on the same <a href=https://en.wikipedia.org/wiki/Data>data</a>. Benefitting from the attention mechanism, it can also efficiently select text elements crucial for classification decisions, which can also be used for in-depth analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0609/>Deep Learning for Depression Detection of Twitter Users<span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/p/prasadith-buddhitha/>Prasadith Buddhitha</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0609><div class="card-body p-3 small">Mental illness detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can be considered a complex task, mainly due to the complicated nature of <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental disorders</a>. In recent years, this research area has started to evolve with the continuous increase in popularity of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> that became an integral part of people&#8217;s life. This close relationship between social media platforms and their users has made these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> to reflect the users&#8217; personal life with different limitations. In such an environment, researchers are presented with a wealth of information regarding one&#8217;s life. In addition to the level of complexity in identifying <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental illnesses</a> through social media platforms, adopting supervised machine learning approaches such as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> have not been widely accepted due to the difficulties in obtaining sufficient amounts of annotated training data. Due to these reasons, we try to identify the most effective deep neural network architecture among a few of selected architectures that were successfully used in natural language processing tasks. The chosen architectures are used to detect users with signs of mental illnesses (depression in our case) given limited unstructured text data extracted from the <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social media platform</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0611/>Predicting Psychological Health from Childhood Essays with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for the CLPsych 2018 Shared Task (Team UKNLP)<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task (Team <span class=acl-fixed-case>UKNLP</span>)</a></strong><br><a href=/people/a/anthony-rios/>Anthony Rios</a>
|
<a href=/people/t/tung-tran/>Tung Tran</a>
|
<a href=/people/r/ramakanth-kavuluru/>Ramakanth Kavuluru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0611><div class="card-body p-3 small">This paper describes the systems we developed for tasks A and B of the 2018 CLPsych shared task. The first task (task A) focuses on predicting behavioral health scores at age 11 using childhood essays. The second task (task B) asks participants to predict future psychological distress at ages 23, 33, 42, and 50 using the age 11 essays. We propose two convolutional neural network based methods that map each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression problem</a>. Among seven teams we ranked third on task A with disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we ranked third on task B with an average DPC score of 0.3062.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0612/>A Psychologically Informed Approach to CLPsych Shared Task 2018<span class=acl-fixed-case>CLP</span>sych Shared Task 2018</a></strong><br><a href=/people/a/almog-simchon/>Almog Simchon</a>
|
<a href=/people/m/michael-gilead/>Michael Gilead</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0612><div class="card-body p-3 small">This paper describes our approach to the CLPsych 2018 Shared Task, in which we attempted to predict cross-sectional psychological health at age 11 and future psychological distress based on childhood essays. We attempted several modeling approaches and observed best cross-validated prediction accuracy with relatively simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> based on <a href=https://en.wikipedia.org/wiki/Psychology>psychological theory</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provided reasonable predictions in most outcomes. Notably, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was especially successful in predicting out-of-sample psychological distress (across people and across time) at age 50.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0613/>Predicting Psychological Health from Childhood Essays. The UGent-IDLab CLPsych 2018 Shared Task System.<span class=acl-fixed-case>UG</span>ent-<span class=acl-fixed-case>IDL</span>ab <span class=acl-fixed-case>CLP</span>sych 2018 Shared Task System.</a></strong><br><a href=/people/k/klim-zaporojets/>Klim Zaporojets</a>
|
<a href=/people/l/lucas-sterckx/>Lucas Sterckx</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0613><div class="card-body p-3 small">This paper describes the IDLab system submitted to Task A of the CLPsych 2018 shared task. The goal of this task is predicting psychological health of children based on language used in hand-written essays and socio-demographic control variables. Our entry uses word- and character-based features as well as lexicon-based features and <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> derived from the essays such as the quality of the language. We apply <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a>, <a href=https://en.wikipedia.org/wiki/Gradient_boosting>gradient boosting</a> as well as neural-network based regressors (feed-forward, CNNs and RNNs) to predict scores. We then make ensembles of our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0614/>Can adult mental health be predicted by childhood future-self narratives? Insights from the CLPsych 2018 Shared Task<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task</a></strong><br><a href=/people/k/kylie-radford/>Kylie Radford</a>
|
<a href=/people/l/louise-lavrencic/>Louise Lavrencic</a>
|
<a href=/people/r/ruth-peters/>Ruth Peters</a>
|
<a href=/people/k/kim-kiely/>Kim Kiely</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/s/scott-nowson/>Scott Nowson</a>
|
<a href=/people/w/will-radford/>Will Radford</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0614><div class="card-body p-3 small">The CLPsych 2018 Shared Task B explores how childhood essays can predict <a href=https://en.wikipedia.org/wiki/Distress_(medicine)>psychological distress</a> throughout the author&#8217;s life. Our main aim was to build tools to help our psychologists understand the data, propose <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and interpret predictions. We submitted two linear regression models : ModelA uses simple demographic and word-count features, while ModelB uses linguistic, entity, typographic, expert-gazetteer, and readability features. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and <a href=https://en.wikipedia.org/wiki/Clinical_psychology>clinical psychology</a> is a productive direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0615/>Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/j/jong-yoon/>Jong Yoon</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0615><div class="card-body p-3 small">Schizophrenia is a mental disorder which afflicts an estimated 0.7 % of adults world wide. It affects many areas of <a href=https://en.wikipedia.org/wiki/Cognition>mental function</a>, often evident from <a href=https://en.wikipedia.org/wiki/Speech_disorder>incoherent speech</a>. Diagnosing schizophrenia relies on <a href=https://en.wikipedia.org/wiki/Subjectivity>subjective judgments</a> resulting in disagreements even among trained clinicians. Recent studies have proposed the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for diagnosis by drawing on automatically-extracted linguistic features like discourse coherence and <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>. Here, we present the first benchmark comparison of previously proposed coherence models for detecting symptoms of schizophrenia and evaluate their performance on a new dataset of recorded interviews between subjects and clinicians. We also present two alternative coherence metrics based on modern sentence embedding techniques that outperform the previous methods on our dataset. Lastly, we propose a novel <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> for reference incoherence based on ambiguous pronoun usage and show that it is a highly predictive feature on our <a href=https://en.wikipedia.org/wiki/Data>data</a>. While the number of subjects is limited in this pilot study, our results suggest new directions for diagnosing common symptoms of schizophrenia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0616/>Oral-Motor and Lexical Diversity During Naturalistic Conversations in Adults with Autism Spectrum Disorder</a></strong><br><a href=/people/j/julia-parish-morris/>Julia Parish-Morris</a>
|
<a href=/people/e/evangelos-sariyanidi/>Evangelos Sariyanidi</a>
|
<a href=/people/c/casey-zampella/>Casey Zampella</a>
|
<a href=/people/g/g-keith-bartley/>G. Keith Bartley</a>
|
<a href=/people/e/emily-ferguson/>Emily Ferguson</a>
|
<a href=/people/a/ashley-a-pallathra/>Ashley A. Pallathra</a>
|
<a href=/people/l/leila-bateman/>Leila Bateman</a>
|
<a href=/people/s/samantha-plate/>Samantha Plate</a>
|
<a href=/people/m/meredith-cola/>Meredith Cola</a>
|
<a href=/people/j/juhi-pandey/>Juhi Pandey</a>
|
<a href=/people/e/edward-s-brodkin/>Edward S. Brodkin</a>
|
<a href=/people/r/robert-t-schultz/>Robert T. Schultz</a>
|
<a href=/people/b/birkan-tunc/>Birkan Tunç</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0616><div class="card-body p-3 small">Autism spectrum disorder (ASD) is a <a href=https://en.wikipedia.org/wiki/Neurodevelopmental_disorder>neurodevelopmental condition</a> characterized by impaired <a href=https://en.wikipedia.org/wiki/Communication>social communication</a> and the presence of restricted, repetitive patterns of behaviors and interests. Prior research suggests that restricted patterns of behavior in <a href=https://en.wikipedia.org/wiki/Autism_spectrum>ASD</a> may be cross-domain phenomena that are evident in a variety of modalities. Computational studies of language in ASD provide support for the existence of an underlying dimension of restriction that emerges during a conversation. Similar evidence exists for restricted patterns of facial movement. Using tools from <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, and <a href=https://en.wikipedia.org/wiki/Information_theory>information theory</a>, this study tests whether cognitive-motor restriction can be detected across multiple behavioral domains in adults with ASD during a naturalistic conversation. Our methods identify restricted behavioral patterns, as measured by <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> in <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>word use</a> and <a href=https://en.wikipedia.org/wiki/Human_mouth>mouth movement</a>. Results suggest that adults with ASD produce significantly less diverse mouth movements and words than neurotypical adults, with an increased reliance on repeated patterns in both domains. The diversity values of the two domains are not significantly correlated, suggesting that they provide complementary information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0617/>Dynamics of an idiostyle of a Russian suicidal blogger<span class=acl-fixed-case>R</span>ussian suicidal blogger</a></strong><br><a href=/people/t/tatiana-litvinova/>Tatiana Litvinova</a>
|
<a href=/people/o/olga-litvinova/>Olga Litvinova</a>
|
<a href=/people/p/pavel-seredin/>Pavel Seredin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0617><div class="card-body p-3 small">Over 800000 people die of suicide each year. It is es-timated that by the year 2020, this figure will have in-creased to 1.5 million. It is considered to be one of the major causes of mortality during adolescence. Thus there is a growing need for <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of identifying su-icidal individuals. Language analysis is known to be a valuable psychodiagnostic tool, however the material for such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> is not easy to obtain. Currently as the Internet communications are developing, there is an opportunity to study texts of suicidal individuals. Such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> can provide a useful insight into the peculiarities of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal thinking</a>, which can be used to further develop methods for diagnosing the risk of suicidal behavior. The paper analyzes the dynamics of a number of linguistic parameters of an idiostyle of a Russian-language blogger who died by suicide. For the first time such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> has been conducted using the material of <a href=https://en.wikipedia.org/wiki/Internet_in_Russia>Russian online texts</a>. For <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, the LIWC program is used. A <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation analysis</a> was performed to identify the relationship between LIWC variables and number of days prior to suicide. Data visualization, as well as comparison with the results of related studies was performed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0618/>RSDD-Time : Temporal Annotation of Self-Reported Mental Health Diagnoses<span class=acl-fixed-case>RSDD</span>-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses</a></strong><br><a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/b/bart-desmet/>Bart Desmet</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0618><div class="card-body p-3 small">Self-reported diagnosis statements have been widely employed in studying language related to mental health in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. However, existing research has largely ignored the temporality of mental health diagnoses. In this work, we introduce RSDD-Time : a new dataset of 598 manually annotated self-reported depression diagnosis posts from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> that include temporal information about the diagnosis. Annotations include whether a <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health condition</a> is present and how recently the diagnosis happened. Furthermore, we include exact temporal spans that relate to the date of diagnosis. This information is valuable for various computational methods to examine <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> because one&#8217;s mental health state is not static. We also test several baseline classification and extraction approaches, which suggest that extracting temporal information from self-reported diagnosis statements is challenging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0620/>Within and Between-Person Differences in Language Used Across Anxiety Support and Neutral Reddit Communities<span class=acl-fixed-case>R</span>eddit Communities</a></strong><br><a href=/people/m/molly-ireland/>Molly Ireland</a>
|
<a href=/people/m/micah-iserman/>Micah Iserman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0620><div class="card-body p-3 small">Although many studies have distinguished between the social media language use of people who do and do not have a <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health condition</a>, within-person context-sensitive comparisons (for example, analyzing individuals&#8217; language use when seeking support or discussing neutral topics) are less common. Two dictionary-based analyses of Reddit communities compared (1) anxious individuals&#8217; comments in <a href=https://en.wikipedia.org/wiki/Internet_forum>anxiety support communities</a> (e.g., /r / PanicParty) with the same users&#8217; comments in <a href=https://en.wikipedia.org/wiki/Internet_forum>neutral communities</a> (e.g., /r / todayilearned), and, (2) within popular neutral communities, comments by members of <a href=https://en.wikipedia.org/wiki/Internet_forum>anxiety subreddits</a> with comments by other users. Each comparison yielded theory-consistent effects as well as unexpected results that suggest novel hypotheses to be tested in the future. Results have relevance for improving researchers&#8217; and practitioners&#8217; ability to unobtrusively assess <a href=https://en.wikipedia.org/wiki/Anxiety>anxiety symptoms</a> in conversations that are not explicitly about <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0621/>Helping or Hurting? Predicting Changes in Users’ Risk of Self-Harm Through Online Community Interactions</a></strong><br><a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/t/timothy-walsh/>Timothy Walsh</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/j/julien-han/>Julien Han</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0621><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> have formed around <a href=https://en.wikipedia.org/wiki/Suicide>suicide</a> and <a href=https://en.wikipedia.org/wiki/Suicide_prevention>self-harm prevention</a>. While these <a href=https://en.wikipedia.org/wiki/Community>communities</a> offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a>. In this work, we focus on how interaction with others in such a <a href=https://en.wikipedia.org/wiki/Community>community</a> affects the <a href=https://en.wikipedia.org/wiki/Mental_state>mental state</a> of users who are seeking support. We first build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of conversation threads between users in a distressed state and community members offering support. We then show how to construct a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to predict whether distressed users are helped or harmed by the interactions in the thread, and we achieve a macro-F1 score of up to 0.69.</div></div></div><hr><div id=w18-07><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-07.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-07/>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0700/>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0701/>Anaphora Resolution for Twitter Conversations : An Exploratory Study<span class=acl-fixed-case>T</span>witter Conversations: An Exploratory Study</a></strong><br><a href=/people/b/berfin-aktas/>Berfin Aktaş</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0701><div class="card-body p-3 small">We present a corpus study of pronominal anaphora on Twitter conversations. After outlining the specific features of this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, with respect to reference resolution, we explain the construction of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and the annotation steps. From this we derive a list of phenomena that need to be considered when performing anaphora resolution on this type of <a href=https://en.wikipedia.org/wiki/Data>data</a>. Finally, we test the performance of an off-the-shelf resolution system, and provide some qualitative error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0702/>Anaphora Resolution with the ARRAU Corpus<span class=acl-fixed-case>ARRAU</span> Corpus</a></strong><br><a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Moosavi</a>
|
<a href=/people/i/ina-roesiger/>Ina Roesiger</a>
|
<a href=/people/a/adam-roussel/>Adam Roussel</a>
|
<a href=/people/f/fabian-simonjetz/>Fabian Simonjetz</a>
|
<a href=/people/a/alexandra-uma/>Alexandra Uma</a>
|
<a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/h/heike-zinsmeister/>Heike Zinsmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0702><div class="card-body p-3 small">The ARRAU corpus is an <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphorically annotated corpus of English</a> providing rich linguistic information about <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>. The most distinctive feature of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is the annotation of a wide range of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric relations</a>, including bridging references and <a href=https://en.wikipedia.org/wiki/Discourse_deixis>discourse deixis</a> in addition to identity (coreference). Other distinctive features include treating all NPs as markables, including non-referring NPs ; and the annotation of a variety of morphosyntactic and semantic mention and entity attributes, including the genericity status of the entities referred to by markables. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> however has not been extensively used for <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a> research so far. In this paper, we discuss three datasets extracted from the ARRAU corpus to support the three subtasks of the CRAC 2018 Shared Taskidentity anaphora resolution over ARRAU-style markables, bridging references resolution, and discourse deixis ; the evaluation scripts assessing system performance on those datasets ; and preliminary results on these three tasks that may serve as baseline for subsequent research in these phenomena.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0703/>Rule- and Learning-based Methods for Bridging Resolution in the ARRAU Corpus<span class=acl-fixed-case>ARRAU</span> Corpus</a></strong><br><a href=/people/i/ina-roesiger/>Ina Roesiger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0703><div class="card-body p-3 small">We present two systems for bridging resolution, which we submitted to the CRAC shared task on bridging anaphora resolution in the ARRAU corpus (track 2): a rule-based approach following Hou et al. 2014 and a learning-based approach. The re-implementation of Hou et al. 2014 achieves very poor performance when being applied to ARRAU. We found that the reasons for this lie in the different bridging annotations : whereas the rule-based system suggests many referential bridging pairs, ARRAU contains mostly lexical bridging. We describe the differences between these two types of bridging and adapt the rule-based approach to be able to handle lexical bridging. The modified rule-based approach achieves reasonable performance on all (sub)-tasks and outperforms a simple learning-based approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0704/>A <a href=https://en.wikipedia.org/wiki/Predictive_modelling>Predictive Model</a> for <a href=https://en.wikipedia.org/wiki/Anaphora_(rhetoric)>Notional Anaphora</a> in English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0704><div class="card-body p-3 small">Notional anaphors are pronouns which disagree with their antecedents&#8217; grammatical categories for notional reasons, such as plural to singular agreement in : the government... they. Since such cases are rare and conflict with evidence from strictly agreeing cases (the government... it), they present a substantial challenge to both <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Referring_expression_generation>referring expression generation</a>. Using the OntoNotes corpus, this paper takes an ensemble approach to predicting English notional anaphora in context on the basis of the largest empirical data to date. In addition to state of the art prediction accuracy, the results suggest that theoretical approaches positing a plural construal at the antecedent&#8217;s utterance are insufficient, and that circumstances at the anaphor&#8217;s utterance location, as well as global factors such as genre, have a strong effect on the choice of referring expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0706/>Towards Bridging Resolution in <a href=https://en.wikipedia.org/wiki/German_language>German</a> : <a href=https://en.wikipedia.org/wiki/Data_analysis>Data Analysis</a> and Rule-based Experiments<span class=acl-fixed-case>G</span>erman: Data Analysis and Rule-based Experiments</a></strong><br><a href=/people/j/janis-pagel/>Janis Pagel</a>
|
<a href=/people/i/ina-roesiger/>Ina Roesiger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0706><div class="card-body p-3 small">Bridging resolution is the task of recognising bridging anaphors and linking them to their antecedents. While there is some work on bridging resolution for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, there is only little work for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We present two datasets which contain bridging annotations, namely DIRNDL and <a href=https://en.wikipedia.org/wiki/GRAIN>GRAIN</a>, and compare the performance of a rule-based system with a simple baseline approach on these two corpora. The performance for full bridging resolution ranges between an <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>F1 score</a> of 13.6 % for <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>DIRNDL</a> and 11.8 % for <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>GRAIN</a>. An analysis using <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle lists</a> suggests that the <a href=https://en.wikipedia.org/wiki/System>system</a> could, to a certain extent, benefit from ranking and re-ranking antecedent candidates. Furthermore, we investigate the importance of single features and show that the <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> used in our work seem promising for future bridging resolution approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0707/>Detecting and Resolving Shell Nouns in German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/a/adam-roussel/>Adam Roussel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0707><div class="card-body p-3 small">This paper describes the design and evaluation of a <a href=https://en.wikipedia.org/wiki/System>system</a> for the automatic detection and resolution of shell nouns in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Shell nouns are general nouns, such as fact, question, or problem, whose full interpretation relies on a content phrase located elsewhere in a text, which these <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> simultaneously serve to characterize and encapsulate. To accomplish this, the system uses a series of lexico-syntactic patterns in order to extract shell noun candidates and their content in parallel. Each pattern has its own <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>, which makes the final decision as to whether or not a link is to be established and the shell noun resolved. Overall, about 26.2 % of the annotated shell noun instances were correctly identified by the <a href=https://en.wikipedia.org/wiki/System>system</a>, and of these cases, about 72.5 % are assigned the correct content phrase. Though it remains difficult to identify shell noun instances reliably (recall is accordingly low in this regard), this system usually assigns the right content to correctly classified cases. cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0709/>A Fine-grained Large-scale Analysis of Coreference Projection</a></strong><br><a href=/people/m/michal-novak/>Michal Novák</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0709><div class="card-body p-3 small">We perform a fine-grained large-scale analysis of coreference projection. By projecting gold coreference from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and vice versa on Prague Czech-English Dependency Treebank 2.0 Coref, we set an upper bound of a proposed projection approach for these two languages. We undertake a detailed thorough analysis that combines the analysis of projection&#8217;s subtasks with analysis of performance on individual mention types. The findings are accompanied with examples from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div></div><hr><div id=w18-08><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-08.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-08/>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0800/>Proceedings of the Second <span class=acl-fixed-case>ACL</span> Workshop on Ethics in Natural Language Processing</a></strong><br><a href=/people/m/mark-alfano/>Mark Alfano</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0801/>On the Utility of Lay Summaries and AI Safety Disclosures : Toward Robust, Open Research Oversight<span class=acl-fixed-case>AI</span> Safety Disclosures: Toward Robust, Open Research Oversight</a></strong><br><a href=/people/a/allen-schmaltz/>Allen Schmaltz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0801><div class="card-body p-3 small">In this position paper, we propose that the community consider encouraging researchers to include two riders, a Lay Summary and an AI Safety Disclosure, as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchersvia a relatively non-intrusive mechanismto consider the societal implications of technologies carrying (un)known and/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers ; the lay summary, at least, has significant precedence in the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> and other areas of science ; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).</div></div></div><hr><div id=w18-09><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-09.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-09/>Proceedings of the Workshop on Figurative Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0900/>Proceedings of the Workshop on Figurative Language Processing</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/p/patricia-lichtenstein/>Patricia Lichtenstein</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/c/chee-wee/>Chee Wee</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0902/>Linguistic Features of <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> and Metaphor Production Quality</a></strong><br><a href=/people/s/stephen-skalicky/>Stephen Skalicky</a>
|
<a href=/people/s/scott-crossley/>Scott Crossley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0902><div class="card-body p-3 small">Using <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to detect <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> has provided a deeper in-sight into <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. The purpose of this study is to assess whether <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> can help explain differences in quality of figurative language. In this study a large corpus of metaphors and sarcastic responses are collected from human subjects and rated for figurative language quality based on <a href=https://en.wikipedia.org/wiki/Metaphor>theoretical components of metaphor</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>. Using natural language processing tools, specific linguistic features related to lexical sophistication and semantic cohesion were used to predict the human ratings of figurative language quality. Results demonstrate <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> were able to predict small amounts of variance in metaphor and sarcasm production quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0905/>Catching Idiomatic Expressions in EFL Essays<span class=acl-fixed-case>EFL</span> Essays</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0905><div class="card-body p-3 small">This paper presents an exploratory study on large-scale detection of idiomatic expressions in <a href=https://en.wikipedia.org/wiki/Essay>essays</a> written by non-native speakers of English. We describe a computational search procedure for automatic detection of idiom-candidate phrases in essay texts. The study used a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of essays</a> written during a standardized examination of English language proficiency. Automatically-flagged candidate expressions were manually annotated for <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>. The study found that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> are widely used in <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>EFL essays</a>. The study also showed that a search algorithm that accommodates the syntactic and lexical exibility of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> can increase the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of idiom instances by 30 %, but it also increases the amount of false positives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0906/>Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0906><div class="card-body p-3 small">We propose a new annotated corpus for metaphor interpretation by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a>, and a novel DNN model for performing this task. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of 200 sets of 5 sentences, with each set containing one reference metaphorical sentence, and four ranked candidate paraphrases. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained for a binary classification of paraphrase candidates, and then used to predict graded paraphrase acceptability. It reaches an encouraging 75 % accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0907/>A Report on the 2018 VUA Metaphor Detection Shared Task<span class=acl-fixed-case>VUA</span> Metaphor Detection Shared Task</a></strong><br><a href=/people/c/chee-wee-leong/>Chee Wee (Ben) Leong</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0907><div class="card-body p-3 small">As the community working on computational approaches to <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative language</a> is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area. One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset. We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0908/>An LSTM-CRF Based Approach to Token-Level Metaphor Detection<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Based Approach to Token-Level Metaphor Detection</a></strong><br><a href=/people/m/malay-pramanick/>Malay Pramanick</a>
|
<a href=/people/a/ashim-gupta/>Ashim Gupta</a>
|
<a href=/people/p/pabitra-mitra/>Pabitra Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0908><div class="card-body p-3 small">Automatic processing of <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative languages</a> is gaining popularity in NLP community for their ubiquitous nature and increasing volume. In this era of <a href=https://en.wikipedia.org/wiki/Web_2.0>web 2.0</a>, automatic analysis of sarcasm and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> is important for their extensive usage. Metaphors are a part of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> that compares different concepts, often on a <a href=https://en.wikipedia.org/wiki/Cognition>cognitive level</a>. Many approaches have been proposed for automatic detection of metaphors, even using <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential models</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In this paper, we propose a method for detection of metaphors at the token level using a hybrid model of Bidirectional-LSTM and CRF. We used fewer <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, as compared to the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a>. On experimentation with VUAMC, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtained an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.674.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0911" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0911/>Bigrams and BiLSTMs Two <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Sequential Metaphor Detection<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>s Two Neural Networks for Sequential Metaphor Detection</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0911><div class="card-body p-3 small">We present and compare two alternative deep neural architectures to perform word-level metaphor detection on text : a bi-LSTM model and a new structure based on recursive feed-forward concatenation of the input. We discuss different versions of such models and the effect that input manipulation-specifically, reducing the length of sentences and introducing concreteness scores for words-have on their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0913/>Neural Metaphor Detecting with CNN-LSTM Model<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0913><div class="card-body p-3 small">Metaphors are <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative languages</a> widely used in daily life and literatures. It&#8217;s an important task to detect the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> evoked by texts. Thus, the metaphor shared task is aimed to extract <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> from plain texts at <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>word level</a>. We propose to use a CNN-LSTM model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information. In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task. We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieved 65.06 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in the all POS testing subtask and 67.15 % in the verbs testing subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0914/>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection<span class=acl-fixed-case>LSTM</span> Contrast : A Deep Neural Network for Metaphor Detection</a></strong><br><a href=/people/k/krishnkant-swarnkar/>Krishnkant Swarnkar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0914><div class="card-body p-3 small">The contrast between the contextual and general meaning of a word serves as an important clue for detecting its <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0915/>Conditional Random Fields for Metaphor Detection</a></strong><br><a href=/people/a/anna-mosolova/>Anna Mosolova</a>
|
<a href=/people/i/ivan-bondarenko/>Ivan Bondarenko</a>
|
<a href=/people/v/vadim-fomin/>Vadim Fomin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0915><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for detecting <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> in sentences which was used in Shared Task on Metaphor Detection by First Workshop on Figurative Language Processing. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0916/>Detecting Figurative Word Occurrences Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/m/malgorzata-marciniak/>Malgorzata Marciniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0916><div class="card-body p-3 small">The paper addresses detection of figurative usage of words in <a href=https://en.wikipedia.org/wiki/English_language>English text</a>. The chosen method was to use <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> fed by pretrained word embeddings. The obtained results show that simple <a href=https://en.wikipedia.org/wiki/Solution_concept>solutions</a>, based on <a href=https://en.wikipedia.org/wiki/Word_embedding>words embeddings</a> only, are comparable to complex solutions, using many sources of information which are not available for languages less-studied than <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0917/>Multi-Module Recurrent Neural Networks with Transfer Learning</a></strong><br><a href=/people/f/filip-skurniak/>Filip Skurniak</a>
|
<a href=/people/m/maria-janicka/>Maria Janicka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0917><div class="card-body p-3 small">This paper describes multiple solutions designed and tested for the problem of word-level metaphor detection. The proposed systems are all based on variants of recurrent neural network architectures. Specifically, we explore multiple sources of information : pre-trained word embeddings (Glove), a dictionary of language concreteness and a transfer learning scenario based on the states of an encoder network from neural network machine translation system. One of the architectures is based on combining all three systems : (1) Neural CRF (Conditional Random Fields), trained directly on the metaphor data set ; (2) Neural Machine Translation encoder of a transfer learning scenario ; (3) a neural network used to predict final labels, trained directly on the metaphor data set. Our results vary between test sets : Neural CRF standalone is the best one on submission data, while combined system scores the highest on a test subset randomly selected from training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0918/>Using Language Learner Data for Metaphor Detection</a></strong><br><a href=/people/e/egon-stemle/>Egon Stemle</a>
|
<a href=/people/a/alexander-onysko/>Alexander Onysko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0918><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in the shared task on metaphor detection on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing figurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018). The system combines a small assertion of trending techniques, which implement matured methods from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>ML</a> ; in particular, the system uses word embeddings from standard corpora and from <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> representing different proficiency levels of language learners in a LSTM BiRNN architecture. The <a href=https://en.wikipedia.org/wiki/System>system</a> is available under the APLv2 open-source license.</div></div></div><hr><div id=w18-10><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-10.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-10/>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1000/>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a></strong><br><a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1002/>Commonsense mining as knowledge base completion? A study on the impact of <a href=https://en.wikipedia.org/wiki/Novelty>novelty</a></a></strong><br><a href=/people/s/stanislaw-jastrzebski/>Stanislaw Jastrzębski</a>
|
<a href=/people/d/dzmitry-bahdanau/>Dzmitry Bahdanau</a>
|
<a href=/people/s/seyedarian-hosseini/>Seyedarian Hosseini</a>
|
<a href=/people/m/michael-noukhovitch/>Michael Noukhovitch</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1002><div class="card-body p-3 small">Commonsense knowledge bases such as <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> represent knowledge in the form of relational triples. Inspired by recent work by Li et al., we analyse if knowledge base completion models can be used to mine <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> from raw text. We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results. We critically analyse the difficulty of mining novel commonsense knowledge, and show that a simple baseline method that outperforms the previous state of the art on predicting more novel triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1003/>Deep learning evaluation using <a href=https://en.wikipedia.org/wiki/Deep_linguistic_processing>deep linguistic processing</a></a></strong><br><a href=/people/a/alexander-kuhnle/>Alexander Kuhnle</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1003><div class="card-body p-3 small">We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing &#8216;deep&#8217; linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail, as compared to a single performance value on a static and monolithic dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1004/>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Attention Models</a></strong><br><a href=/people/n/noah-weber/>Noah Weber</a>
|
<a href=/people/l/leena-shekhar/>Leena Shekhar</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1004><div class="card-body p-3 small">Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models typically do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution (Ettinger et al., 2017), and as such, are likely to fail on examples from the tail end of the distribution (such as inputs that are noisy (Belinkov and Bisk, 2018), or of different length (Bentivogli et al., 2016)). In this paper we look at a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s ability to generalize this structure beyond the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training distribution</a> depends greatly on the chosen random seed, even when performance on the test set remains the same. This finding suggests that <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to capture generalizable structure is highly sensitive, and more so, this sensitivity may not be apparent when evaluating the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on standard test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1005/>Extrapolation in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/p/pasquale-minervini/>Pasquale Minervini</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1005><div class="card-body p-3 small">We argue that <a href=https://en.wikipedia.org/wiki/Extrapolation>extrapolation</a> to unseen data will often be easier for <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that capture global structures, rather than just maximise their local fit to the training data. We show that this is true for two popular <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> : the Decomposable Attention Model and <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>.</div></div></div><hr><div id=w18-11><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-11.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-11/>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1100/>Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/c/claudia-wagner/>Claudia Wagner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1102/>Social and Emotional Correlates of Capitalization on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/sophia-chan/>Sophia Chan</a>
|
<a href=/people/a/alona-fyshe/>Alona Fyshe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1102><div class="card-body p-3 small">Social media text is replete with unusual <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization patterns</a>. We posit that capitalizing a token like THIS performs two expressive functions : it marks a person socially, and marks certain parts of an utterance as more salient than others. Focusing on gender and sentiment, we illustrate using a corpus of tweets that <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> appears in more negative than positive contexts, and is used more by females compared to males. Yet we find that both genders use <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> in a similar way when expressing sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1106/>The Social and the Neural Network : How to Make <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> about People again</a></strong><br><a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1106><div class="card-body p-3 small">Over the years, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> has increasingly focused on <a href=https://en.wikipedia.org/wiki/Problem_solving>tasks</a> that can be solved by <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a>, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, but have narrowed our focus and biased the tools demographically. However, with the increased availability of <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1107/>Observational Comparison of Geo-tagged and Randomly-drawn Tweets</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a>
|
<a href=/people/a/annabelle-carrell/>Annabelle Carrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1107><div class="card-body p-3 small">Twitter is a ubiquitous source of micro-blog social media data, providing the academic, industrial, and public sectors real-time access to actionable information. A particularly attractive property of some <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> is * geo-tagging *, where a user account has opted-in to attaching their current location to each message. Unfortunately (from a researcher&#8217;s perspective) only a fraction of Twitter accounts agree to this, and these <a href=https://en.wikipedia.org/wiki/User_(computing)>accounts</a> are likely to have systematic diffences with the general population. This work is an exploratory study of these differences across the full range of Twitter content, and complements previous studies that focus on the <a href=https://en.wikipedia.org/wiki/English_language>English-language subset</a>. Additionally, we compare methods for querying users by self-identified properties, finding that the constrained semantics of the description field provides cleaner, higher-volume results than more complex regular expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1110/>Understanding the Effect of Gender and Stance in Opinion Expression in Debates on Abortion</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1110><div class="card-body p-3 small">In this paper, we focus on understanding linguistic differences across groups with different self-identified gender and stance in expressing opinions about ABORTION. We provide a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of users&#8217; gender, stance on ABORTION as well as the debates in ABORTION drawn from debate.org. We use the gender and stance information to identify significant linguistic differences across individuals with different gender and stance. We show the importance of considering the stance information along with the gender since we observe significant linguistic differences across individuals with different stance even within the same gender group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1111/>Frustrated, Polite, or Formal : Quantifying Feelings and Tone in Email</a></strong><br><a href=/people/n/niyati-chhaya/>Niyati Chhaya</a>
|
<a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/p/projjal-chanda/>Projjal Chanda</a>
|
<a href=/people/j/jaya-singh/>Jaya Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1111><div class="card-body p-3 small">Email conversations are the primary mode of communication in enterprises. The email content expresses an individual&#8217;s needs, requirements and intentions. Affective information in the <a href=https://en.wikipedia.org/wiki/Email>email text</a> can be used to get an insight into the sender&#8217;s mood or emotion. We present a novel approach to model human frustration in text. We identify <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> that influence <a href=https://en.wikipedia.org/wiki/Frustration>human perception of frustration</a> and model it as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning task</a>. The paper provides a detailed comparison across traditional <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and word distribution-based models. We report a <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean-squared error (MSE)</a> of 0.018 against human-annotated frustration for the best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The approach establishes the importance of <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect features</a> in frustration prediction for <a href=https://en.wikipedia.org/wiki/Email>email data</a>. We further evaluate the efficacy of the proposed feature set and model in predicting other tone or affects in text, namely <a href=https://en.wikipedia.org/wiki/Formality>formality</a> and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> ; results demonstrate a comparable performance against the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1112/>Reddit : A Gold Mine for Personality Prediction<span class=acl-fixed-case>R</span>eddit: A Gold Mine for Personality Prediction</a></strong><br><a href=/people/m/matej-gjurkovic/>Matej Gjurković</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1112><div class="card-body p-3 small">Automated personality prediction from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is gaining increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and social sciences communities. However, due to high labeling costs and privacy issues, the few publicly available datasets are of limited size and low <a href=https://en.wikipedia.org/wiki/Diversity_(business)>topic diversity</a>. We address this problem by introducing a large-scale dataset derived from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, a source so far overlooked for personality prediction. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is labeled with <a href=https://en.wikipedia.org/wiki/Myers&#8211;Briggs_Type_Indicator>Myers-Briggs Type Indicators (MBTI)</a> and comes with a rich set of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for more than 9k users. We carry out a preliminary feature analysis, revealing marked differences between the MBTI dimensions and <a href=https://en.wikipedia.org/wiki/Zeros_and_poles>poles</a>. Furthermore, we use the dataset to train and evaluate benchmark personality prediction models, achieving macro F1-scores between 67 % and 82 % on the individual dimensions and 82 % accuracy for exact or one-off accurate type prediction. These results are encouraging and comparable with the reliability of standardized tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1113/>Predicting Authorship and Author Traits from Keystroke Dynamics</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1113><div class="card-body p-3 small">Written text transmits a good deal of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>nonverbal information</a> related to the author&#8217;s identity and social factors, such as <a href=https://en.wikipedia.org/wiki/Ageing>age</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Personality>personality</a>. However, it is less known to what extent behavioral biometric traces transmit such <a href=https://en.wikipedia.org/wiki/Information>information</a>. We use typist data to study the predictiveness of <a href=https://en.wikipedia.org/wiki/Author>authorship</a>, and present first experiments on predicting both <a href=https://en.wikipedia.org/wiki/Ageing>age</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a> from <a href=https://en.wikipedia.org/wiki/Keystroke_dynamics>keystroke dynamics</a>. Our results show that the model based on keystroke features, while being two orders of magnitude smaller, leads to significantly higher accuracies for authorship than the text-based system. For user attribute prediction, the best approach is to combine the two, suggesting that extralinguistic factors are disclosed to a larger degree in written text, while author identity is better transmitted in typing behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1114/>Predicting Twitter User Demographics from Names Alone<span class=acl-fixed-case>T</span>witter User Demographics from Names Alone</a></strong><br><a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1114><div class="card-body p-3 small">Social media analysis frequently requires tools that can automatically infer <a href=https://en.wikipedia.org/wiki/Demography>demographics</a> to contextualize trends. These tools often require hundreds of user-authored messages for each user, which may be prohibitive to obtain when analyzing millions of users. We explore character-level neural models that learn a representation of a user&#8217;s name and <a href=https://en.wikipedia.org/wiki/User_(computing)>screen name</a> to predict gender and ethnicity, allowing for demographic inference with minimal data. We release trained models1 which may enable new <a href=https://en.wikipedia.org/wiki/Demography>demographic analyses</a> that would otherwise require enormous amounts of data collection</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1115/>Modeling Personality Traits of Filipino Twitter Users<span class=acl-fixed-case>F</span>ilipino <span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/e/edward-tighe/>Edward Tighe</a>
|
<a href=/people/c/charibeth-cheng/>Charibeth Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1115><div class="card-body p-3 small">Recent studies in the field of text-based personality recognition experiment with different languages, <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction techniques</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to create better and more accurate models ; however, little focus is placed on exploring the language use of a group of individuals defined by nationality. Individuals of the same nationality share certain practices and communicate certain ideas that can become embedded into their <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Many nationals are also not limited to speaking just one language, such as how Filipinos speak <a href=https://en.wikipedia.org/wiki/Filipino_language>Filipino</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the two national languages of the Philippines. The addition of several <a href=https://en.wikipedia.org/wiki/Languages_of_the_Philippines>regional / indigenous languages</a>, along with the commonness of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, allow for a <a href=https://en.wikipedia.org/wiki/Filipino_language>Filipino</a> to have a rich vocabulary. This presents an opportunity to create a text-based personality model based on how Filipinos speak, regardless of the language they use. To do so, data was collected from 250 Filipino Twitter users. Different combinations of <a href=https://en.wikipedia.org/wiki/Data_processing>data processing techniques</a> were experimented upon to create <a href=https://en.wikipedia.org/wiki/Personality_type>personality models</a> for each of the <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five</a>. The results for both <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> show that <a href=https://en.wikipedia.org/wiki/Conscientiousness>Conscientiousness</a> is consistently the easiest trait to model, followed by <a href=https://en.wikipedia.org/wiki/Extraversion_and_introversion>Extraversion</a>. Classification models for <a href=https://en.wikipedia.org/wiki/Agreeableness>Agreeableness</a> and <a href=https://en.wikipedia.org/wiki/Neuroticism>Neuroticism</a> had subpar performances, but performed better than those of <a href=https://en.wikipedia.org/wiki/Openness>Openness</a>. An analysis on personality trait score representation showed that classifying extreme outliers generally produce better results for all traits except for <a href=https://en.wikipedia.org/wiki/Neuroticism>Neuroticism</a> and <a href=https://en.wikipedia.org/wiki/Openness>Openness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1116/>Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1116><div class="card-body p-3 small">The usage of part-of-day nouns, such as &#8216;night&#8217;, and their time-specific greetings (&#8216;good night&#8217;), varies across languages and cultures. We show the possibilities that <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> offers for studying the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of these terms and its variability between countries. We mine a worldwide sample of multilingual tweets with temporal greetings, and study how their frequencies vary in relation with local time. The results provide insights into the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of these temporal expressions and the <a href=https://en.wikipedia.org/wiki/Sociology>cultural and sociological factors</a> influencing their usage.</div></div></div><hr><div id=w18-12><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-12.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-12/>Proceedings of the Second Workshop on Subword/Character LEvel Models</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1200/>Proceedings of the Second Workshop on Subword/Character <span class=acl-fixed-case>LE</span>vel Models</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1201/>Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings<span class=acl-fixed-case>A</span>rabic Neural Machine Translation in Low-Resource Settings</a></strong><br><a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1201><div class="card-body p-3 small">Neural machine translation has achieved impressive results in the last few years, but its success has been limited to settings with large amounts of parallel data. One way to improve <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> for lower-resource settings is to initialize a word-based NMT model with pretrained word embeddings. However, rare words still suffer from lower quality word embeddings when trained with standard word-level objectives. We introduce <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> that utilize <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological resources</a>, and compare to purely <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised alternatives</a>. We work with <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphologically rich language with available linguistic resources, and perform Ar-to-En MT experiments on a small corpus of <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED subtitles</a>. We find that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> utilizing subword information consistently outperform standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on a word similarity task and as initialization of the source word embeddings in a low-resource NMT system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1202/>Entropy-Based Subword Mining with an Application to Word Embeddings</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/f/frank-f-xu/>Frank Xu</a>
|
<a href=/people/a/aston-zhang/>Aston Zhang</a>
|
<a href=/people/s/stephen-macke/>Stephen Macke</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1202><div class="card-body p-3 small">Recent literature has shown a wide variety of benefits to mapping traditional one-hot representations of words and phrases to lower-dimensional real-valued vectors known as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> by learning distinct <a href=https://en.wikipedia.org/wiki/Embedding>embedding vectors</a> for each word. Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as <a href=https://en.wikipedia.org/wiki/Prefix>prefixes</a>, <a href=https://en.wikipedia.org/wiki/Suffix>suffixes</a>, and <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>root-words</a> as well as <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound words</a>. Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures. This not only leads to poor embeddings for <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> that have <a href=https://en.wikipedia.org/wiki/Long-tail_distribution>long-tail distributions</a>, but also <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic methods</a> for handling out-of-vocabulary words. In this paper we propose SubwordMine, an entropy-based subword mining algorithm that is fast, unsupervised, and fully data-driven. We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1204/>Addressing Low-Resource Scenarios with Character-aware Embeddings</a></strong><br><a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1204><div class="card-body p-3 small">Most modern approaches to computing word embeddings assume the availability of <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> with billions of words. In this paper, we explore a setup where only <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> with millions of words are available, and many words in any new text are out of vocabulary. This setup is both of practical interests modeling the situation for specific domains and low-resource languages and of psycholinguistic interest, since it corresponds much more closely to the actual experiences and challenges of human language learning and use. We compare standard skip-gram word embeddings with character-based embeddings on word relatedness prediction. Skip-grams excel on large corpora, while character-based embeddings do well on small corpora generally and rare and complex words specifically. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can be combined easily.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1205/>Subword-level Composition Functions for Learning Word Embeddings</a></strong><br><a href=/people/b/bofang-li/>Bofang Li</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1205><div class="card-body p-3 small">Subword-level information is crucial for capturing the meaning and morphology of words, especially for out-of-vocabulary entries. We propose CNN- and RNN-based subword-level composition functions for learning word embeddings, and systematically compare them with popular word-level and subword-level models (Skip-Gram and FastText). Additionally, we propose a hybrid training scheme in which a pure subword-level model is trained jointly with a conventional word-level embedding model based on lookup-tables. This increases the fitness of all types of subword-level word embeddings ; the word-level embeddings can be discarded after training, leaving only compact subword-level representation with much smaller data volume. We evaluate these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> on a set of intrinsic and extrinsic tasks, showing that subword-level models have advantage on tasks related to <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and datasets with high OOV rate, and can be combined with other types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1206/>Discovering Phonesthemes with Sparse Regularization</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/g/gina-anne-levow/>Gina-Anne Levow</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1206><div class="card-body p-3 small">We introduce a simple method for extracting non-arbitrary form-meaning representations from a collection of semantic vectors. We treat the problem as one of <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained to predict word vectors from subword features. We apply this model to the problem of automatically discovering phonesthemes, which are submorphemic sound clusters that appear in words with similar meaning. Many of our model-predicted phonesthemes overlap with those proposed in the linguistics literature, and we validate our approach with human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/291466308 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-1209/>Incorporating Subword Information into Matrix Factorization Word Embeddings</a></strong><br><a href=/people/a/alexandre-salle/>Alexandre Salle</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1209><div class="card-body p-3 small">The positive effect of adding subword information to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> has been demonstrated for <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a>. In this paper we investigate whether similar benefits can also be derived from incorporating <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> into counting models. We evaluate the impact of different types of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1210/>A Multi-Context Character Prediction Model for a Brain-Computer Interface</a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/shaobin-xu/>Shaobin Xu</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1210><div class="card-body p-3 small">Brain-computer interfaces and other augmentative and alternative communication devices introduce language-modeing challenges distinct from other character-entry methods. In particular, the acquired signal of the EEG (electroencephalogram) signal is noisier, which, in turn, makes the user intent harder to decipher. In order to adapt to this condition, we propose to maintain ambiguous history for every time step, and to employ, apart from the character language model, word information to produce a more robust prediction system. We present preliminary results that compare this proposed Online-Context Language Model (OCLM) to current <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that are used in this type of setting. Evaluation on both perplexity and predictive accuracy demonstrates promising results when dealing with ambiguous histories in order to provide to the front end a distribution of the next character the user might type.</div></div></div><hr><div id=w18-13><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-13.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-13/>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1300/>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a></strong><br><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1301/>Using Hedge Detection to Improve Committed Belief Tagging</a></strong><br><a href=/people/m/morgan-ulinski/>Morgan Ulinski</a>
|
<a href=/people/s/seth-benjamin/>Seth Benjamin</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1301><div class="card-body p-3 small">We describe a novel method for identifying hedge terms using a set of manually constructed rules. We present experiments adding hedge features to a committed belief system to improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. We compare performance of this system (a) without <a href=https://en.wikipedia.org/wiki/Hedge_(finance)>hedging features</a>, (b) with dictionary-based features, and (c) with rule-based features. We find that using hedge features improves performance of the committed belief system, particularly in identifying instances of non-committed belief and reported belief.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1303/>Detecting Sarcasm is Extremely Easy ;-)</a></strong><br><a href=/people/n/natalie-parde/>Natalie Parde</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1303><div class="card-body p-3 small">Detecting sarcasm in text is a particularly challenging problem in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>, and its solution may vary across different types of text. We analyze the performance of a domain-general sarcasm detection system on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from two very different domains : <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon product reviews</a>. We categorize the errors that we identify with each, and make recommendations for addressing these issues in NLP systems in the future.</div></div></div><hr><div id=w18-14><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-14.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-14/>Proceedings of the First International Workshop on Spatial Language Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1400/>Proceedings of the First International Workshop on Spatial Language Understanding</a></strong><br><a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1406/>Points, Paths, and Playscapes : Large-scale Spatial Language Understanding Tasks Set in the Real World</a></strong><br><a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/t/tania-bedrax-weiss/>Tania Bedrax-Weiss</a>
|
<a href=/people/d/daphne-luong/>Daphne Luong</a>
|
<a href=/people/s/srini-narayanan/>Srini Narayanan</a>
|
<a href=/people/b/bo-pang/>Bo Pang</a>
|
<a href=/people/f/fernando-pereira/>Fernando Pereira</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a>
|
<a href=/people/m/michael-tseng/>Michael Tseng</a>
|
<a href=/people/y/yuan-zhang/>Yuan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1406><div class="card-body p-3 small">Spatial language understanding is important for practical applications and as a building block for better abstract language understanding. Much progress has been made through work on understanding spatial relations and values in <a href=https://en.wikipedia.org/wiki/Image>images</a> and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> as well as on giving and following navigation instructions in restricted domains. We argue that the next big advances in spatial language understanding can be best supported by creating large-scale datasets that focus on points and paths based in the real world, and then extending these to create online, persistent playscapes that mix human and bot players, where the bot players must learn, evolve, and survive according to their depth of understanding of scenes, navigation, and interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1408/>The Case for Systematically Derived Spatial Language Usage</a></strong><br><a href=/people/b/bonnie-dorr/>Bonnie Dorr</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1408><div class="card-body p-3 small">This position paper argues that, while prior work in spatial language understanding for tasks such as <a href=https://en.wikipedia.org/wiki/Robot_navigation>robot navigation</a> focuses on mapping <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> into deep conceptual or non-linguistic representations, it is possible to systematically derive regular patterns of spatial language usage from existing lexical-semantic resources. Furthermore, even with access to such resources, effective solutions to many application areas such as <a href=https://en.wikipedia.org/wiki/Robot_navigation>robot navigation</a> and <a href=https://en.wikipedia.org/wiki/Narrative>narrative generation</a> also require additional knowledge at the syntax-semantics interface to cover the wide range of spatial expressions observed and available to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language speakers</a>. We ground our insights in, and present our extensions to, an existing lexico-semantic resource, covering 500 semantic classes of verbs, of which 219 fall within a spatial subset. We demonstrate that these extensions enable systematic derivation of regular patterns of spatial language without requiring manual annotation.</div></div></div><hr><div id=w18-15><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-15.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-15/>Proceedings of the First Workshop on Storytelling</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1500/>Proceedings of the First Workshop on Storytelling</a></strong><br><a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao ‘Kenneth’ Huang</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/i/ishan-misra/>Ishan Misra</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1502/>Linguistic Features of Helpfulness in Automated Support for Creative Writing</a></strong><br><a href=/people/m/melissa-roemmele/>Melissa Roemmele</a>
|
<a href=/people/a/andrew-gordon/>Andrew Gordon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1502><div class="card-body p-3 small">We examine an emerging NLP application that supports <a href=https://en.wikipedia.org/wiki/Creative_writing>creative writing</a> by automatically suggesting continuing sentences in a story. The <a href=https://en.wikipedia.org/wiki/Application_software>application</a> tracks users&#8217; modifications to generated sentences, which can be used to quantify their helpfulness in advancing the story. We explore the task of predicting helpfulness based on automatically detected linguistic features of the suggestions. We illustrate this analysis on a set of user interactions with the <a href=https://en.wikipedia.org/wiki/Application_software>application</a> using an initial selection of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> relevant to <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1505/>Towards Controllable Story Generation</a></strong><br><a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1505><div class="card-body p-3 small">We present a general <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> of analyzing existing story corpora to generate controllable and creative new stories. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>storyline</a>. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>storylines</a>. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.</div></div></div><hr><div id=w18-16><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-16.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-16/>Proceedings of the Second Workshop on Stylistic Variation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1600/>Proceedings of the Second Workshop on Stylistic Variation</a></strong><br><a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/l/lucie-flekova/>Lucie Flekova</a>
|
<a href=/people/m/moshe-koppel/>Moshe Koppel</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1601/>Stylistic variation over 200 years of court proceedings according to gender and social class</a></strong><br><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1601><div class="card-body p-3 small">We present an approach to detect stylistic variation across social variables (here : gender and social class), considering also diachronic change in language use. For detection of stylistic variation, we use <a href=https://en.wikipedia.org/wiki/Relative_entropy>relative entropy</a>, measuring the difference between <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distributions</a> at different linguistic levels (here : <a href=https://en.wikipedia.org/wiki/Lexis_(linguistics)>lexis</a> and grammar). In addition, by <a href=https://en.wikipedia.org/wiki/Relative_entropy>relative entropy</a>, we can determine which linguistic units are related to stylistic variation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1602/>Stylistic Variation in Social Media Part-of-Speech Tagging</a></strong><br><a href=/people/m/murali-raghu-babu-balusu/>Murali Raghu Babu Balusu</a>
|
<a href=/people/t/taha-merghani/>Taha Merghani</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1602><div class="card-body p-3 small">Social media features substantial stylistic variation, raising new challenges for syntactic analysis of online writing. However, this variation is often aligned with author attributes such as age, gender, and <a href=https://en.wikipedia.org/wiki/Geography>geography</a>, as well as more readily-available social network metadata. In this paper, we report new evidence on the link between language and social networks in the task of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. We find that tagger error rates are correlated with <a href=https://en.wikipedia.org/wiki/Flow_network>network structure</a>, with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some parts of the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a>, and lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> elsewhere. As a result, tagger accuracy depends on training from a balanced sample of the network, rather than training on texts from a narrow subcommunity. We also describe our attempts to add robustness to stylistic variation, by building a mixture-of-experts model in which each expert is associated with a region of the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. While prior work found that similar approaches yield performance improvements in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, we were unable to obtain performance improvements in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, despite strong evidence for the link between part-of-speech error rates and social network structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1603/>Detecting Syntactic Features of Translated Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/h/hai-hu/>Hai Hu</a>
|
<a href=/people/w/wen-li/>Wen Li</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1603><div class="card-body p-3 small">We present a machine learning approach to distinguish texts translated to Chinese (by humans) from texts originally written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, with a focus on a wide range of syntactic features. Using Support Vector Machines (SVMs) as classifier on a genre-balanced corpus in translation studies of Chinese, we find that constituent parse trees and dependency triples as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> without lexical information perform very well on the task, with an F-measure above 90 %, close to the results of lexical n-gram features, without the risk of learning topic information rather than translation features. Thus, we claim syntactic features alone can accurately distinguish <a href=https://en.wikipedia.org/wiki/Translation>translated</a> from original Chinese. Translated Chinese exhibits an increased use of determiners, subject position pronouns, NP + as NP modifiers, multiple NPs or VPs conjoined by, among other structures. We also interpret the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic features</a> with reference to previous translation studies in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, particularly the usage of <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1604/>Evaluating Creative Language Generation : The Case of Rap Lyric Ghostwriting</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1604><div class="card-body p-3 small">Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>, <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>style</a>, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluations methods for one such <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation methodology</a> that addresses several complementary aspects of this task, and illustrate how such <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> can be used to meaning fully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1605/>Cross-corpus Native Language Identification via Statistical Embedding</a></strong><br><a href=/people/f/francisco-rangel/>Francisco Rangel</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/a/alexandra-l-uitdenbogerd/>Alexandra Uitdenbogerd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1605><div class="card-body p-3 small">In this paper, we approach the task of native language identification in a realistic cross-corpus scenario where a model is trained with available data and has to predict the native language from data of a different corpus. The motivation behind this study is to investigate native language identification in the Australian academic scenario where a majority of students come from <a href=https://en.wikipedia.org/wiki/China>China</a>, <a href=https://en.wikipedia.org/wiki/Indonesia>Indonesia</a>, and <a href=https://en.wikipedia.org/wiki/Arab_world>Arabic-speaking nations</a>. We have proposed a statistical embedding representation reporting a significant improvement over common single-layer approaches of the state of the art, identifying <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and Indonesian in a cross-corpus scenario. The proposed approach was shown to be competitive even when the data is scarce and imbalanced.</div></div></div><hr><div id=w18-17><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-17.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-17/>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1700/>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs-12)</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1703/>Multi-hop Inference for Sentence-level TextGraphs : How Challenging is Meaningfully Combining Information for Science Question Answering?<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs: How Challenging is Meaningfully Combining Information for Science Question Answering?</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1703><div class="card-body p-3 small">Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question. This multi-hop inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, or the tendency for long chains of facts to quickly drift off topic. This is a major barrier to current <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference models</a>, as even elementary science questions require an average of 4 to 6 facts to answer and explain. In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> built from three free-text corpora (including study guides and Simple Wikipedia). We demonstrate <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> tends to be high and aggregation quality low, at between 0.04 and 3, and highlight scenarios that maximize the likelihood of meaningfully combining information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1704/>Multi-Sentence Compression with Word Vertex-Labeled Graphs and Integer Linear Programming</a></strong><br><a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/s/stephane-huet/>Stéphane Huet</a>
|
<a href=/people/t/thiago-gouveia-da-silva/>Thiago Gouveia da Silva</a>
|
<a href=/people/a/andrea-carneiro-linhares/>Andréa Carneiro Linhares</a>
|
<a href=/people/j/juan-manuel-torres-moreno/>Juan-Manuel Torres-Moreno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1704><div class="card-body p-3 small">Multi-Sentence Compression (MSC) aims to generate a short sentence with key information from a cluster of closely related sentences. MSC enables <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and question-answering systems to generate outputs combining fully formed sentences from one or several documents. This paper describes a new Integer Linear Programming method for MSC using a vertex-labeled graph to select different keywords, and novel 3-gram scores to generate more informative sentences while maintaining their grammaticality. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is of good quality and outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for evaluations led on <a href=https://en.wikipedia.org/wiki/News_media>news dataset</a>. We led both automatic and manual evaluations to determine the informativeness and the grammaticality of compressions for each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Additional tests, which take advantage of the fact that the length of compressions can be modulated, still improve ROUGE scores with shorter output sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1705/>Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs</a></strong><br><a href=/people/k/khiem-pham/>Khiem Pham</a>
|
<a href=/people/g/guangliang-chen/>Guangliang Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1705><div class="card-body p-3 small">Spectral clustering has received a lot of attention due to its ability to separate nonconvex, non-intersecting manifolds, but its high <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> has significantly limited its applicability. Motivated by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach in which we first use the selected landmark set and the given data to form a bipartite graph and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering. We show that our proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks, thus capable of handling large data. Finally, we demonstrate the excellent performance of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> by comparing with the state-of-the-art scalable algorithms on several benchmark data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1706/>Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings</a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/a/amol-agrawal/>Amol Agrawal</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/a/anirudha-desai/>Anirudha Desai</a>
|
<a href=/people/v/vinayak-mathur/>Vinayak Mathur</a>
|
<a href=/people/a/alfred-hough/>Alfred Hough</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1706><div class="card-body p-3 small">Word sense induction (WSI), which addresses <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graph-based method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each polysemous word. By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expensive step of <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor search</a> that plagues other graph-based methods without sacrificing the quality of sense clusters. Experiments on three datasets show that our proposed method produces similar or better sense clusters and embeddings compared with previous state-of-the-art methods while being significantly more efficient.</div></div></div><hr><div id=w18-18><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-18.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-18/>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1800/>Proceedings of the 13th Conference of the Association for Machine Translation in the <span class=acl-fixed-case>A</span>mericas (Volume 1: Research Track)</a></strong><br><a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p></div><hr><div id=w18-19><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-19.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-19/>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1900/>Proceedings of the 13th Conference of the Association for Machine Translation in the <span class=acl-fixed-case>A</span>mericas (Volume 2: User Track)</a></strong><br><a href=/people/j/janice-campbell/>Janice Campbell</a>
|
<a href=/people/a/alex-yanishevsky/>Alex Yanishevsky</a>
|
<a href=/people/j/jennifer-doyon/>Jennifer Doyon</a>
|
<a href=/people/d/douglas-jones/>Doug Jones</a></span></p></div><hr><div id=w18-20><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-20.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-20/>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2000/>Proceedings of the <span class=acl-fixed-case>AMTA</span> 2018 Workshop on The Role of Authoritative Standards in the <span class=acl-fixed-case>MT</span> Environment</a></strong><br><a href=/people/j/jennifer-decamp/>Jennifer DeCamp</a></span></p></div><hr><div id=w18-21><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-21.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-21/>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2100/>Proceedings of the <span class=acl-fixed-case>AMTA</span> 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</a></strong><br><a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/j/joao-graca/>João Graça</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a></span></p></div><hr><div id=w18-22><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-22.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-22/>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2200/>Proceedings of the <span class=acl-fixed-case>AMTA</span> 2018 Workshop on Technologies for <span class=acl-fixed-case>MT</span> of Low Resource Languages (<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>MT</span> 2018)</a></strong><br><a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a></span></p></div><hr><div id=w18-23><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-23.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-23/>Proceedings of the BioNLP 2018 workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2300/>Proceedings of the <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span> 2018 workshop</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2301/>Embedding Transfer for Low-Resource Medical Named Entity Recognition : A Case Study on Patient Mobility</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2301><div class="card-body p-3 small">Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research. We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of free text electronic health records. We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction. As <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> focused on patient functioning are scarce, we explore domain adaptation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for use in a recurrent neural network NER system. We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall. Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2304/>Keyphrases Extraction from User-Generated Contents in Healthcare Domain Using Long Short-Term Memory Networks</a></strong><br><a href=/people/i/ilham-fathy-saputra/>Ilham Fathy Saputra</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a>
|
<a href=/people/a/alfan-farizki-wicaksono/>Alfan Farizki Wicaksono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2304><div class="card-body p-3 small">We propose keyphrases extraction technique to extract important terms from the healthcare user-generated contents. We employ deep learning architecture, i.e. Long Short-Term Memory, and leverage <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, medical concepts from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, and linguistic components as our <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 61.37 % F-1 score. Experimental results indicate that our proposed approach outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline methods</a>, i.e. RAKE and CRF, on the task of extracting keyphrases from Indonesian health forum posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2305/>Identifying Key Sentences for Precision Oncology Using <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-Supervised Learning</a></a></strong><br><a href=/people/j/jurica-seva/>Jurica Ševa</a>
|
<a href=/people/m/martin-wackerbauer/>Martin Wackerbauer</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2305><div class="card-body p-3 small">We present a machine learning pipeline that identifies key sentences in abstracts of oncological articles to aid <a href=https://en.wikipedia.org/wiki/Evidence-based_medicine>evidence-based medicine</a>. This problem is characterized by the lack of gold standard datasets, data imbalance and thematic differences between available silver standard corpora. Additionally, available training and target data differs with regard to their domain (professional summaries vs. sentences in abstracts). This makes <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> inapplicable. We propose the use of two semi-supervised machine learning approaches : To mitigate difficulties arising from heterogeneous data sources, overcome data imbalance and create reliable training data we propose using <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> from positive and unlabelled data (PU Learning). For obtaining a realistic <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>, we propose the use of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> summarised in relevant sentences as unlabelled examples through Self-Training. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 84 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 0.84 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2310/>A Neural Autoencoder Approach for Document Ranking and Query Refinement in Pharmacogenomic Information Retrieval</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/m/mathias-goschl/>Mathias Göschl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2310><div class="card-body p-3 small">In this study, we investigate learning-to-rank and query refinement approaches for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> in the <a href=https://en.wikipedia.org/wiki/Pharmacogenomics>pharmacogenomic domain</a>. The goal is to improve the information retrieval process of biomedical curators, who manually build <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> for <a href=https://en.wikipedia.org/wiki/Personalized_medicine>personalized medicine</a>. We study how to exploit the relationships between <a href=https://en.wikipedia.org/wiki/Gene>genes</a>, <a href=https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism>variants</a>, <a href=https://en.wikipedia.org/wiki/Drug>drugs</a>, <a href=https://en.wikipedia.org/wiki/Disease>diseases</a> and outcomes as features for document ranking and query refinement. For a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approach</a>, we are faced with a small amount of <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> and a large amount of unannotated data. Therefore, we explore ways to use a neural document auto-encoder in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised approach</a>. We show that a combination of established <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature-engineering</a> and a neural auto-encoder model yield promising results in this setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2311/>Biomedical Event Extraction Using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and Dependency Parsing</a></strong><br><a href=/people/j/jari-bjorne/>Jari Björne</a>
|
<a href=/people/t/tapio-salakoski/>Tapio Salakoski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2311><div class="card-body p-3 small">Event and relation extraction are central tasks in <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. Where <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> concerns the detection of semantic connections between pairs of entities, <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the <a href=https://en.wikipedia.org/wiki/Parse_graph>parse graph</a> into this <a href=https://en.wikipedia.org/wiki/Vector_space>linear space</a> using dependency path embeddings. We integrate our <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> into the open source Turku Event Extraction System (TEES) framework. Using this <a href=https://en.wikipedia.org/wiki/System>system</a>, our <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> can be easily applied to a large set of corpora from e.g. the <a href=https://en.wikipedia.org/wiki/BioNLP>BioNLP</a>, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2318/>SingleCite : Towards an improved Single Citation Search in PubMed<span class=acl-fixed-case>S</span>ingle<span class=acl-fixed-case>C</span>ite: Towards an improved Single Citation Search in <span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>M</span>ed</a></strong><br><a href=/people/l/lana-yeganova/>Lana Yeganova</a>
|
<a href=/people/d/donald-c-comeau/>Donald C Comeau</a>
|
<a href=/people/w/won-kim/>Won Kim</a>
|
<a href=/people/w/w-john-wilbur/>W John Wilbur</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2318><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> that is targeted at finding a specific document in databases is called a Single Citation search. Single citation searches are particularly important for scholarly databases, such as <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, because users are frequently searching for a specific publication. In this work we describe SingleCite, a single citation matching system designed to facilitate user&#8217;s search for a specific document. We report on the progress that has been achieved towards building that <a href=https://en.wikipedia.org/wiki/Function_(engineering)>functionality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2319/>A Framework for Developing and Evaluating Word Embeddings of Drug-named Entity</a></strong><br><a href=/people/m/mengnan-zhao/>Mengnan Zhao</a>
|
<a href=/people/a/aaron-j-masino/>Aaron J. Masino</a>
|
<a href=/people/c/christopher-c-yang/>Christopher C. Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2319><div class="card-body p-3 small">We investigate the quality of task specific word embeddings created with relatively small, targeted corpora. We present a comprehensive evaluation framework including both intrinsic and extrinsic evaluation that can be expanded to named entities beyond drug name. Intrinsic evaluation results tell that drug name embeddings created with a domain specific document corpus outperformed the previously published versions that derived from a very large general text corpus. Extrinsic evaluation uses <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> for the task of drug name recognition with Bi-LSTM model and the results demonstrate the advantage of using domain-specific word embeddings as the only input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> for drug name recognition with F1-score achieving 0.91. This work suggests that it may be advantageous to derive domain specific embeddings for certain tasks even when the domain specific corpus is of limited size.</div></div></div><hr><div id=w18-24><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-24.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-24/>Proceedings of the Seventh Named Entities Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2400/>Proceedings of the Seventh Named Entities Workshop</a></strong><br><a href=/people/n/nancy-chen/>Nancy Chen</a>
|
<a href=/people/r/rafael-e-banchs/>Rafael E. Banchs</a>
|
<a href=/people/x/xiangyu-duan/>Xiangyu Duan</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/haizhou-li/>Haizhou Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2401/>Automatic Extraction of Entities and Relation from Legal Documents</a></strong><br><a href=/people/j/judith-jeyafreeda-andrew/>Judith Jeyafreeda Andrew</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2401><div class="card-body p-3 small">In recent years, the journalists and <a href=https://en.wikipedia.org/wiki/Computer_science>computer sciences</a> speak to each other to identify useful technologies which would help them in extracting useful information. This is called <a href=https://en.wikipedia.org/wiki/Computational_journalism>computational Journalism</a>. In this paper, we present a method that will enable the journalists to automatically identifies and annotates <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a> such as names of people, <a href=https://en.wikipedia.org/wiki/Organization>organizations</a>, role and functions of people in <a href=https://en.wikipedia.org/wiki/Legal_instrument>legal documents</a> ; the relationship between these entities are also explored. The <a href=https://en.wikipedia.org/wiki/System>system</a> uses a combination of both statistical and rule based technique. The <a href=https://en.wikipedia.org/wiki/Statistics>statistical method</a> used is <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> and for the rule based technique, document and language specific regular expressions are used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2402/>Connecting Distant Entities with Induction through Conditional Random Fields for Named Entity Recognition : Precursor-Induced CRF<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/w/wangjin-lee/>Wangjin Lee</a>
|
<a href=/people/j/jinwook-choi/>Jinwook Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2402><div class="card-body p-3 small">This paper presents a method of designing specific high-order dependency factor on the linear chain conditional random fields (CRFs) for named entity recognition (NER). Named entities tend to be separated from each other by multiple outside tokens in a text, and thus the first-order CRF, as well as the second-order CRF, may innately lose transition information between distant named entities. The proposed design uses outside label in NER as a transmission medium of precedent entity information on the CRF. Then, empirical results apparently demonstrate that it is possible to exploit long-distance label dependency in the original first-order linear chain CRF structure upon NER while reducing computational loss rather than in the second-order CRF.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2404/>Attention-based Semantic Priming for Slot-filling</a></strong><br><a href=/people/j/jiewen-wu/>Jiewen Wu</a>
|
<a href=/people/r/rafael-e-banchs/>Rafael E. Banchs</a>
|
<a href=/people/l/luis-fernando-dharo/>Luis Fernando D’Haro</a>
|
<a href=/people/p/pavitra-krishnaswamy/>Pavitra Krishnaswamy</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2404><div class="card-body p-3 small">The problem of sequence labelling in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> would benefit from approaches inspired by semantic priming phenomena. We propose that an attention-based RNN architecture can be used to simulate <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a> for sequence labelling. Specifically, we employ pre-trained word embeddings to characterize the semantic relationship between utterances and labels. We validate the approach using varying sizes of the ATIS and MEDIA datasets, and show up to 1.4-1.9 % improvement in F1 score. The developed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can enable more explainable and generalizable spoken language understanding systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2405/>Named Entity Recognition for Hindi-English Code-Mixed Social Media Text<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Text</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2405><div class="card-body p-3 small">Named Entity Recognition (NER) is a major task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, and also is a sub-task of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. The challenge of NER for <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> lie in the insufficient information available in a tweet. There has been a significant amount of work done related to <a href=https://en.wikipedia.org/wiki/Entity_extraction>entity extraction</a>, but only for resource rich languages and domains such as <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it&#8217;s unstructured and incomplete information. We propose experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classification algorithms</a> with word, character and lexical features. The algorithms we experimented with are <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision tree</a>, <a href=https://en.wikipedia.org/wiki/Long-term_memory>Long Short-Term Memory (LSTM)</a>, and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Field (CRF)</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for NER in Hindi-English Code-Mixed along with extensive experiments on our <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> which achieved the best <a href=https://en.wikipedia.org/wiki/F-number>f1-score</a> of 0.95 with both CRF and <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2411/>A Deep Learning Based Approach to <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a></a></strong><br><a href=/people/s/soumyadeep-kundu/>Soumyadeep Kundu</a>
|
<a href=/people/s/sayantan-paul/>Sayantan Paul</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2411><div class="card-body p-3 small">In this paper, we propose different <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for language independent machine transliteration which is extremely important for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) applications</a>. Though a number of statistical models for <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> have already been proposed in the past few decades, we proposed some neural network based deep learning architectures for the <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration of named entities</a>. Our transliteration systems adapt two different neural machine translation (NMT) frameworks : <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> and convolutional sequence to sequence based NMT. It is shown that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides quite satisfactory results when it comes to multi lingual machine transliteration. Our submitted runs are an ensemble of different <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration systems</a> for all the language pairs. In the NEWS 2018 Shared Task on <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a>, our method achieves top performance for the EnPe and PeEn language pairs and comparable results for other cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2412/>Comparison of Assorted Models for <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a></a></strong><br><a href=/people/s/saeed-najafi/>Saeed Najafi</a>
|
<a href=/people/b/bradley-hauer/>Bradley Hauer</a>
|
<a href=/people/r/rashed-rubby-riyadh/>Rashed Rubby Riyadh</a>
|
<a href=/people/l/leyuan-yu/>Leyuan Yu</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2412><div class="card-body p-3 small">We report the results of our experiments in the context of the NEWS 2018 Shared Task on <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a>. We focus on the comparison of several diverse <a href=https://en.wikipedia.org/wiki/System>systems</a>, including three neural MT models. A combination of discriminative, generative, and neural models obtains the best results on the development sets. We also put forward ideas for improving the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2414/>Low-Resource Machine Transliteration Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> of Asian Languages<span class=acl-fixed-case>A</span>sian Languages</a></strong><br><a href=/people/n/ngoc-tan-le/>Ngoc Tan Le</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2414><div class="card-body p-3 small">Grapheme-to-phoneme models are key components in <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech systems</a>. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are based on initial <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network-based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pre-trained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. We participated in the NEWS 2018 shared task for the English-Vietnamese transliteration task.</div></div></div><hr><div id=w18-25><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-25.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-25/>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2500/>Proceedings of Workshop for <span class=acl-fixed-case>NLP</span> Open Source Software (<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>OSS</span>)</a></strong><br><a href=/people/e/eunjeong-l-park/>Eunjeong L. Park</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/d/dmitrijs-milajevs/>Dmitrijs Milajevs</a>
|
<a href=/people/l/liling-tan/>Liling Tan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2501/>AllenNLP : A Deep Semantic Natural Language Processing Platform<span class=acl-fixed-case>A</span>llen<span class=acl-fixed-case>NLP</span>: A Deep Semantic Natural Language Processing Platform</a></strong><br><a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/m/michael-schmitz/>Michael Schmitz</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2501><div class="card-body p-3 small">Modern natural language processing (NLP) research requires writing code. Ideally this <a href=https://en.wikipedia.org/wiki/Source_code>code</a> would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the <a href=https://en.wikipedia.org/wiki/Research>research</a>. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2502.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-2502/>Stop Word Lists in Free Open-source Software Packages</a></strong><br><a href=/people/j/joel-nothman/>Joel Nothman</a>
|
<a href=/people/h/hanmin-qin/>Hanmin Qin</a>
|
<a href=/people/r/roman-yurchak/>Roman Yurchak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2502><div class="card-body p-3 small">Open-source software packages for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. has n&#8217;t but not had n&#8217;t) and inclusions (computer), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2503/>Texar : A Modularized, Versatile, and Extensible Toolbox for Text Generation<span class=acl-fixed-case>T</span>exar: A Modularized, Versatile, and Extensible Toolbox for Text Generation</a></strong><br><a href=/people/z/zhiting-hu/>Zhiting Hu</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/h/haoran-shi/>Haoran Shi</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/d/devendra-singh-chaplot/>Devendra Singh Chaplot</a>
|
<a href=/people/b/bowen-tan/>Bowen Tan</a>
|
<a href=/people/x/xingjiang-yu/>Xingjiang Yu</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2503><div class="card-body p-3 small">We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing <a href=https://en.wikipedia.org/wiki/Toolkit>toolkits</a> that are specialized for specific <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> across different <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text generation applications</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-2504/>The ACL Anthology : Current State and Future Directions<span class=acl-fixed-case>ACL</span> <span class=acl-fixed-case>A</span>nthology: Current State and Future Directions</a></strong><br><a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a>
|
<a href=/people/m/martin-villalba/>Martín Villalba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2504><div class="card-body p-3 small">The Association of Computational Linguistic&#8217;s Anthology is the open source archive, and the main source for <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and natural language processing&#8217;s scientific literature. The <a href=https://en.wikipedia.org/wiki/ACL_Anthology>ACL Anthology</a> is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of <a href=https://en.wikipedia.org/wiki/Docker_(software)>Docker images</a> will improve the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>&#8217;s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2507/>OpenSeq2Seq : Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models</a></strong><br><a href=/people/o/oleksii-kuchaiev/>Oleksii Kuchaiev</a>
|
<a href=/people/b/boris-ginsburg/>Boris Ginsburg</a>
|
<a href=/people/i/igor-gitman/>Igor Gitman</a>
|
<a href=/people/v/vitaly-lavrukhin/>Vitaly Lavrukhin</a>
|
<a href=/people/c/carl-case/>Carl Case</a>
|
<a href=/people/p/paulius-micikevicius/>Paulius Micikevicius</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2507><div class="card-body p-3 small">We present OpenSeq2Seq an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source toolkit</a> for training sequence-to-sequence models. The main goal of our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is to allow researchers to most effectively explore different sequence-to-sequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a>. We plan to extend <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with other modalities in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2508/>Integrating Multiple NLP Technologies into an Open-source Platform for Multilingual Media Monitoring<span class=acl-fixed-case>NLP</span> Technologies into an Open-source Platform for Multilingual Media Monitoring</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/renars-liepins/>Renārs Liepins</a>
|
<a href=/people/d/didzis-gosko/>Didzis Gosko</a>
|
<a href=/people/g/guntis-barzdins/>Guntis Barzdins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2508><div class="card-body p-3 small">The open-source SUMMA Platform is a highly scalable <a href=https://en.wikipedia.org/wiki/Distributed_computing>distributed architecture</a> for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> with comparatively little effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2509/>The Annotated Transformer</a></strong><br><a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2509><div class="card-body p-3 small">A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for <a href=https://en.wikipedia.org/wiki/Replication_(statistics)>replication</a>, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.</div></div></div><hr><div id=w18-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-26/>Proceedings of the Workshop on Machine Reading for Question Answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2600/>Proceedings of the Workshop on Machine Reading for Question Answering</a></strong><br><a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2601/>Ruminating Reader : Reasoning with Gated Multi-hop Attention</a></strong><br><a href=/people/y/yichen-gong/>Yichen Gong</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2601><div class="card-body p-3 small">To answer the question in machine comprehension (MC) task, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> need to establish the interaction between the question and the context. To tackle the problem that the single-pass model can not reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by 2.1 <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> and 2.7 EM score. Our analysis shows that different hops of the attention have different responsibilities in selecting answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2603/>A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension</a></strong><br><a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/s/seohyun-back/>Seohyun Back</a>
|
<a href=/people/h/haejun-lee/>Haejun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2603><div class="card-body p-3 small">Reading Comprehension (RC) of text is one of the fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In recent years, several end-to-end neural network models have been proposed to solve RC tasks. However, most of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> suffer in reasoning over long documents. In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension. We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA. We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2606" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2606/>Robust and Scalable Differentiable Neural Computer for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a></a></strong><br><a href=/people/j/jorg-franke/>Jörg Franke</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2606><div class="card-body p-3 small">Deep learning models are often not easily adaptable to new <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>tasks</a> and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We analyze the DNC and identify possible improvements within the application of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> with passable results on the CNN RC task without adaptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2610/>Comparative Analysis of Neural QA models on SQuAD<span class=acl-fixed-case>QA</span> models on <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span></a></strong><br><a href=/people/s/soumya-wadhwa/>Soumya Wadhwa</a>
|
<a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2610><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> has gained prominence in the past few decades for testing the ability of machines to understand <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Large datasets for <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a> have led to the development of neural models that cater to deeper language understanding compared to <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval tasks</a>. Different <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2611/>Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task<span class=acl-fixed-case>ROUGE</span> and <span class=acl-fixed-case>BLEU</span> to Better Evaluate Machine Reading Comprehension Task</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2611><div class="card-body p-3 small">Current evaluation metrics to <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, bias may appear when these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to better correlate n-gram overlap with the <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> for answers to these two question types. Statistical analysis proves the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>. Our adaptations may provide positive guidance for the development of real-scene MRC systems.<tex-math>n</tex-math>-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.</div></div></div><hr><div id=w18-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-27/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2700/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2701/>Findings of the Second Workshop on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Minh-Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2701><div class="card-body p-3 small">This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop&#8217;s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2704/>Inducing Grammars with and for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/k/ke-m-tran/>Ke Tran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2704><div class="card-body p-3 small">Machine translation systems require <a href=https://en.wikipedia.org/wiki/Semantics>semantic knowledge</a> and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical understanding</a>. Neural machine translation (NMT) systems often assume this information is captured by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and a decoder that ensures <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Recent work has shown that incorporating explicit <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> alleviates the burden of modeling both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>. However, requiring <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> is expensive and does not explore the question of what syntax a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> needs during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. To address both of these issues we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2705" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2705/>Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2705><div class="card-body p-3 small">Supervised domain adaptationwhere a large generic corpus and a smaller in-domain corpus are both available for trainingis a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, then continue training the second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a> between the in-domain model&#8217;s output word distribution and that of the out-of-domain model to prevent the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> shows improvements over standard continued training by up to 1.5 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2706/>Controllable Abstractive Summarization</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2706><div class="card-body p-3 small">Current models for <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> can produce high quality summaries that follow <a href=https://en.wikipedia.org/wiki/Preference>user preferences</a>. Without user input, we set the <a href=https://en.wikipedia.org/wiki/Control_variable>control variables</a> automatically on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2709/>On the Impact of Various Types of <a href=https://en.wikipedia.org/wiki/Noise>Noise</a> on Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2709><div class="card-body p-3 small">We examine how various types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the parallel training data impact the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a>. We create five types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>artificial noise</a> and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> than <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a>. For one especially egregious type of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> they learn to just copy the input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2711/>Multi-Source Neural Machine Translation with Missing Data</a></strong><br><a href=/people/y/yuta-nishimura/>Yuta Nishimura</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2711><div class="card-body p-3 small">Multi-source translation is an <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> to exploit multiple inputs (e.g. in two different languages) to increase <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a>. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a>, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol NULL. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the NULL tokens achieved higher translation accuracies measured by <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> than those by any one-to-one NMT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2715/>OpenNMT System Description for WNMT 2018 : 800 words / sec on a single-core CPU<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>NMT</span> System Description for <span class=acl-fixed-case>WNMT</span> 2018: 800 words/sec on a single-core <span class=acl-fixed-case>CPU</span></a></strong><br><a href=/people/j/jean-senellart/>Jean Senellart</a>
|
<a href=/people/d/dakun-zhang/>Dakun Zhang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/g/guillaume-klein/>Guillaume Klein</a>
|
<a href=/people/j/jean-pierre-ramatchandirin/>Jean-Pierre Ramatchandirin</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2715><div class="card-body p-3 small">We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a <a href=https://en.wikipedia.org/wiki/Supercomputer>high-performance CPU system</a>. The final system uses a combination of four techniques, all of them lead to significant speed-ups in combination : (a) sequence distillation, (b) architecture modifications, (c) <a href=https://en.wikipedia.org/wiki/Precomputation>precomputation</a>, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and available to the community.</div></div></div><hr><div id=w18-28><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-28.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-28/>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2800/>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></strong><br><a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/thierry-poibeau/>Thierry Poibeau</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2801" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2801/>Predicting Brain Activation with WordNet Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Embeddings</a></strong><br><a href=/people/j/joao-rodrigues/>João António Rodrigues</a>
|
<a href=/people/r/ruben-branco/>Ruben Branco</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/c/chakaveh-saedi/>Chakaveh Saedi</a>
|
<a href=/people/a/antonio-branco/>António Branco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2801><div class="card-body p-3 small">The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of <a href=https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging>fMRI spatial patterns</a> was pioneered by Mitchell et al. That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to feature-based models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a <a href=https://en.wikipedia.org/wiki/Semantic_network>semantic network</a> to encode lexical semantics, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2803/>Language Production Dynamics with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/j/jesus-calvillo/>Jesús Calvillo</a>
|
<a href=/people/m/matthew-crocker/>Matthew Crocker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2803><div class="card-body p-3 small">We present an analysis of the internal mechanism of the recurrent neural model of sentence production presented by Calvillo et al. The results show clear patterns of computation related to each layer in the network allowing to infer an algorithmic account, where the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> activates the semantically related words, then each word generated at each time step activates syntactic and semantic constraints on possible continuations, while the <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence</a> preserves information through time. We propose that such insights could generalize to other models with similar architecture, including some used in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and image caption generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2805/>Predicting Japanese Word Order in Double Object Constructions<span class=acl-fixed-case>J</span>apanese Word Order in Double Object Constructions</a></strong><br><a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/s/satoshi-nambu/>Satoshi Nambu</a>
|
<a href=/people/s/shin-ichiro-sano/>Shin-Ichiro Sano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2805><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> to predict Japanese word order in the double object constructions. We employed a Bayesian linear mixed model with manually annotated predicate-argument structure data. The findings from the refined corpus analysis confirmed the effects of information status of an NP as &#8216;givennew ordering&#8217; in addition to the effects of &#8216;long-before-short&#8217; as a tendency of the general Japanese word order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2806/>Affordances in Grounded Language Learning</a></strong><br><a href=/people/s/stephen-mcgregor/>Stephen McGregor</a>
|
<a href=/people/k/kyungtae-lim/>KyungTae Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2806><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> involving mappings between different modes of semantic representation. We propose distributional semantic models as a mechanism for representing the kind of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> inherent in the system of abstract symbols characteristic of a sophisticated community of language users. Then, motivated by insight from <a href=https://en.wikipedia.org/wiki/Ecological_psychology>ecological psychology</a>, we describe a model approximating <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, by which we mean a <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learner</a>&#8217;s direct perception of opportunities for action in an environment. We present a preliminary experiment involving mapping between these two representational modalities, and propose that our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> can become the basis for a cognitively inspired model of grounded language learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2807/>Rating Distributions and <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian Inference</a> : Enhancing Cognitive Models of Spatial Language Use<span class=acl-fixed-case>B</span>ayesian Inference: Enhancing Cognitive Models of Spatial Language Use</a></strong><br><a href=/people/t/thomas-kluth/>Thomas Kluth</a>
|
<a href=/people/h/holger-schultheis/>Holger Schultheis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2807><div class="card-body p-3 small">We present two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that improve the assessment of <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a>. The first method is applicable to models computing average acceptability ratings. For these models, we propose an extension that simulates a full rating distribution (instead of average ratings) and allows generating individual ratings. Our second method enables <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a> for models generating individual data. To this end, we propose to use the cross-match test (Rosenbaum, 2005) as a <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood function</a>. We exemplarily present both methods using <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a> from the domain of spatial language use. For spatial language use, determining linguistic acceptability judgments of a spatial preposition for a depicted spatial relation is assumed to be a crucial process (Logan and Sadler, 1996). Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of this <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a> compute an average acceptability rating. We extend the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and based on existing data show that the extended <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> allow extracting more information from the <a href=https://en.wikipedia.org/wiki/Empirical_evidence>empirical data</a> and yield more readily interpretable information about model successes and failures. Applying <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a>, we find that model performance relies less on mechanisms of capturing geometrical aspects than on mapping the captured geometry to a rating interval.</div></div></div><hr><div id=w18-29><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-29.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-29/>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2900/>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/georgiana-dinu/>Georgiana Dinu</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a>
|
<a href=/people/s/samuel-bowman/>Sam Bowman</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a>
|
<a href=/people/a/anders-sogaard/>Anders Sogaard</a>
|
<a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2901/>Compositional Morpheme Embeddings with Affixes as Functions and Stems as Arguments</a></strong><br><a href=/people/d/daniel-edmiston/>Daniel Edmiston</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2901><div class="card-body p-3 small">This work introduces a novel, linguistically motivated architecture for composing <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> to derive <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The principal novelty in the work is to treat stems as vectors and <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> as functions over vectors. In this way, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s architecture more closely resembles the compositionality of <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Such a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> stands in opposition to models which treat morphemes uniformly, making no distinction between <a href=https://en.wikipedia.org/wiki/Word_stem>stem</a> and <a href=https://en.wikipedia.org/wiki/Affix>affix</a>. We run this new architecture on a dependency parsing task in Koreana language rich in derivational morphologyand compare it against a lexical baseline, along with other sub-word architectures. StAffNet, the name of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, shows competitive performance with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2902/>Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2902><div class="card-body p-3 small">Incorporating source syntactic information into neural machine translation (NMT) has recently proven successful (Eriguchi et al., 2016 ; Luong et al., 2016). However, this is generally done using an outside <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to syntactically annotate the training data, making this technique difficult to use for languages or domains for which a reliable <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is not available. In this paper, we introduce an unsupervised tree-to-sequence (tree2seq) model for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> ; this model is able to induce an unsupervised hierarchical structure on the source sentence based on the downstream task of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We adapt the Gumbel tree-LSTM of Choi et al. (2018) to NMT in order to create the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> against sequential and supervised parsing baselines on three low- and medium-resource language pairs. For low-resource cases, the unsupervised tree2seq encoder significantly outperforms the baselines ; no improvements are seen for medium-resource translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2905/>Subcharacter Information in Japanese Embeddings : When Is It Worth It?<span class=acl-fixed-case>J</span>apanese Embeddings: When Is It Worth It?</a></strong><br><a href=/people/m/marzena-karpinska/>Marzena Karpinska</a>
|
<a href=/people/b/bofang-li/>Bofang Li</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2905><div class="card-body p-3 small">Languages with logographic writing systems present a difficulty for traditional character-level models. Leveraging the subcharacter information was recently shown to be beneficial for a number of intrinsic and extrinsic tasks in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We examine whether the same strategies could be applied for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, and contribute a new analogy dataset for this <a href=https://en.wikipedia.org/wiki/Language>language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2906/>A neural parser as a direct classifier for head-final languages</a></strong><br><a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/m/masayasu-muraoka/>Masayasu Muraoka</a>
|
<a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2906><div class="card-body p-3 small">This paper demonstrates a neural parser implementation suitable for consistently head-final languages such as <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. Unlike the transition- and graph-based algorithms in most state-of-the-art parsers, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> directly selects the head word of a dependent from a limited number of candidates. This method drastically simplifies the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> so that we can easily interpret the output of the neural model. Moreover, by exploiting grammatical knowledge to restrict possible modification types, we can control the output of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to reduce specific errors without adding annotated corpora. The neural parser performed well both on conventional Japanese corpora and the Japanese version of Universal Dependency corpus, and the advantages of <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> were observed in the comparison with the non-neural conventional model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2907/>Syntactic Dependency Representations in Neural Relation Classification</a></strong><br><a href=/people/f/farhad-nooralahzadeh/>Farhad Nooralahzadeh</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2907><div class="card-body p-3 small">We investigate the use of different syntactic dependency representations in a neural relation classification task and compare the CoNLL, Stanford Basic and Universal Dependencies schemes. We further compare with a syntax-agnostic approach and perform an <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> in order to gain a better understanding of the results.</div></div></div><hr><div id=w18-30><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-30.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-30/>Proceedings of The Third Workshop on Representation Learning for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3000/>Proceedings of The Third Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/j/jamie-kiros/>Jamie Kiros</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/d/dipendra-misra/>Dipendra Misra</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3001/>Corpus Specificity in LSA and <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> : The Role of Out-of-Domain Documents<span class=acl-fixed-case>LSA</span> and Word2vec: The Role of Out-of-Domain Documents</a></strong><br><a href=/people/e/edgar-altszyler/>Edgar Altszyler</a>
|
<a href=/people/m/mariano-sigman/>Mariano Sigman</a>
|
<a href=/people/d/diego-fernandez-slezak/>Diego Fernández Slezak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3001><div class="card-body p-3 small">Despite the popularity of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the precise way by which they acquire semantic relations between words remain unclear. In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However, if <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus size</a> grows in topics which are not specific to the domain of interest, <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>signal to noise ratio</a> may weaken. Here we investigate the effect of corpus specificity and size in <a href=https://en.wikipedia.org/wiki/Word_embedding>word-embeddings</a>, and for this, we study two ways for progressive elimination of documents : the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of <a href=https://en.wikipedia.org/wiki/Dimensionality>dimensionality</a>, can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view, we point out that LSA&#8217;s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> does.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3002/>Hierarchical Convolutional Attention Networks for Text Classification</a></strong><br><a href=/people/s/shang-gao/>Shang Gao</a>
|
<a href=/people/a/arvind-ramanathan/>Arvind Ramanathan</a>
|
<a href=/people/g/georgia-tourassi/>Georgia Tourassi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3002><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has demonstrated that self-attention mechanisms can be used in place of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> to increase <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training speed</a> without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a>. We propose combining this approach with the benefits of convolutional filters and a hierarchical structure to create a document classification model that is both highly accurate and fast to train we name our method Hierarchical Convolutional Attention Networks. We demonstrate the effectiveness of this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> by surpassing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on several classification tasks while being twice as fast to train.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3003/>Extrofitting : Enriching Word Representation and its Vector Space with Semantic Lexicons<span class=acl-fixed-case>E</span>xtrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons</a></strong><br><a href=/people/h/hwiyeol-jo/>Hwiyeol Jo</a>
|
<a href=/people/s/stanley-jungkyu-choi/>Stanley Jungkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3003><div class="card-body p-3 small">We propose post-processing method for enriching not only word representation but also its <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using semantic lexicons, which we call extrofitting. The method consists of 3 steps as follows : (i) Expanding 1 or more <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>dimension(s)</a> on all the word vectors, filling with their representative value. (ii) Transferring semantic knowledge by averaging each representative values of synonyms and filling them in the expanded dimension(s). These two steps make representations of the synonyms close together. (iii) Projecting the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using Linear Discriminant Analysis, which eliminates the expanded dimension(s) with semantic knowledge. When experimenting with GloVe, we find that our method outperforms Faruqui&#8217;s retrofitting on some of word similarity task. We also report further analysis on our method in respect to word vector dimensions, <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> as well as other well-known pretrained word vectors (e.g., Word2Vec, Fasttext).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3005/>Text Completion using Context-Integrated Dependency Parsing</a></strong><br><a href=/people/a/amr-rekaby-salama/>Amr Rekaby Salama</a>
|
<a href=/people/o/ozge-alacam/>Özge Alaçam</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3005><div class="card-body p-3 small">Incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges that a successful <a href=https://en.wikipedia.org/wiki/Communication_system>communication system</a> has to deal with. In this paper, we study text completion with a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> composed of sentences with gaps where a successful completion can not be achieved through a uni-modal (language-based) approach. We present a solution based on a context-integrating dependency parser incorporating an additional non-linguistic modality. An incompleteness in one <a href=https://en.wikipedia.org/wiki/Communication_channel>channel</a> is compensated by information from another one and the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> learns the association between the two modalities from a multiple level knowledge representation. We examined several model variations by adjusting the degree of influence of different modalities in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a> on possible filler words and their exact reference to a non-linguistic context element. Our model is able to fill the gap with 95.4 % word and 95.2 % exact reference accuracy hence the successful prediction can be achieved not only on the word level (such as mug) but also with respect to the correct identification of its context reference (such as mug 2 among several mug instances).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3006/>Quantum-Inspired Complex Word Embedding</a></strong><br><a href=/people/q/qiuchi-li/>Qiuchi Li</a>
|
<a href=/people/s/sagar-uprety/>Sagar Uprety</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a>
|
<a href=/people/d/dawei-song/>Dawei Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3006><div class="card-body p-3 small">A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> will assign high probabilities to the words Penguin and Fly if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense-Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in <a href=https://en.wikipedia.org/wiki/Quantum_mechanics>Quantum Mechanics</a> where we subscribe a relative phase to each word, which is a <a href=https://en.wikipedia.org/wiki/Complex_number>complex number</a>, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve better performances than state-of-the-art non-quantum models on binary sentence classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3007/>Natural Language Inference with Definition Embedding Considering Context On the Fly</a></strong><br><a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/h/hisako-asano/>Hisako Asano</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3007><div class="card-body p-3 small">Natural language inference (NLI) is one of the most important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In this study, we propose a novel method using word dictionaries, which are pairs of a word and its definition, as external knowledge. Our neural definition embedding mechanism encodes input sentences with the definitions of each word of the sentences on the fly. It can encode the definition of words considering the context of input sentences by using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We evaluated our method using <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> as a dictionary and confirmed that our method performed better than baseline models when using the full or a subset of 100d GloVe as word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3008/>Comparison of Representations of Named Entities for Document Classification</a></strong><br><a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3008><div class="card-body p-3 small">We explore representations for multi-word names in text classification tasks, on Reuters (RCV1) topic and sector classification. We find that : the best way to treat <a href=https://en.wikipedia.org/wiki/Name>names</a> is to split them into tokens and use each token as a separate feature ; NEs have more impact on sector classification than topic classification ; replacing NEs with entity types is not an effective strategy ; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our <a href=https://en.wikipedia.org/wiki/Computer_simulation>CNN models</a> yield.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3011/>A Hybrid Learning Scheme for Chinese Word Embedding<span class=acl-fixed-case>C</span>hinese Word Embedding</a></strong><br><a href=/people/w/wenfan-chen/>Wenfan Chen</a>
|
<a href=/people/w/weiguo-sheng/>Weiguo Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3011><div class="card-body p-3 small">To improve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, subword information has been widely employed in state-of-the-art methods. These <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can be classified to either compositional or predictive models. In this paper, we propose a hybrid learning scheme, which integrates compositional and predictive model for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. Such a scheme can take advantage of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, thus effectively learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The proposed scheme has been applied to learn <a href=https://en.wikipedia.org/wiki/Linguistic_description>word representation</a> on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Our results show that the proposed scheme can significantly improve the performance of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> in terms of <a href=https://en.wikipedia.org/wiki/Analogy>analogical reasoning</a> and is robust to the size of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3012.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3012/>Unsupervised Random Walk Sentence Embeddings : A Strong but Simple Baseline</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3012><div class="card-body p-3 small">Using a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings : take a weighted average of word embeddings and modify with SVD. This simple <a href=https://en.wikipedia.org/wiki/Methodology>method</a> even outperforms far more complex approaches such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.&#8217;s model. We propose a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our <a href=https://en.wikipedia.org/wiki/Stiffness>approach</a> beats Arora et al.&#8217;s by up to 44.4 % on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.&#8217;s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3013/>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3013><div class="card-body p-3 small">Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> given a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The task we use is fine-grained name typing : given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, we can build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task that are complementary to the current embedding evaluation datasets in : they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3015/>Exploiting Common Characters in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to Learn Cross-Lingual Word Embeddings via Matrix Factorization<span class=acl-fixed-case>C</span>hinese and <span class=acl-fixed-case>J</span>apanese to Learn Cross-Lingual Word Embeddings via Matrix Factorization</a></strong><br><a href=/people/j/jilei-wang/>Jilei Wang</a>
|
<a href=/people/s/shiying-luo/>Shiying Luo</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tao-dai/>Tao Dai</a>
|
<a href=/people/s/shu-tao-xia/>Shu-Tao Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3015><div class="card-body p-3 small">Learning vector space representation of words (i.e., word embeddings) has recently attracted wide research interests, and has been extended to cross-lingual scenario. Currently most cross-lingual word embedding learning models are based on sentence alignment, which inevitably introduces much noise. In this paper, we show in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the acquisition of semantic relation among words can benefit from the large number of common characters shared by both languages ; inspired by this unique feature, we design a method named CJC targeting to generate cross-lingual context of words. We combine CJC with GloVe based on matrix factorization, and then propose an integrated model named CJ-Glo. Taking two sentence-aligned models and CJ-BOC (also exploits common characters but is based on CBOW) as baseline algorithms, we compare them with CJ-Glo on a series of NLP tasks including cross-lingual synonym, word analogy and sentence alignment. The result indicates CJ-Glo achieves the best performance among these methods, and is more stable in cross-lingual tasks ; moreover, compared with CJ-BOC, CJ-Glo is less sensitive to the alteration of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3018/>Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3018><div class="card-body p-3 small">Word vector space specialisation models offer a portable, light-weight approach to fine-tuning arbitrary distributional vector spaces to discern between <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a>. Their effectiveness is drawn from <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>external linguistic constraints</a> that specify the exact <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical relation</a> between words. In this work, we show that a careful selection of the <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>external constraints</a> can steer and improve the <a href=https://en.wikipedia.org/wiki/Specialization_(functional)>specialisation</a>. By simply selecting appropriate constraints, we report state-of-the-art results on a suite of tasks with well-defined benchmarks where modeling lexical contrast is crucial : 1) true semantic similarity, with highest reported scores on SimLex-999 and SimVerb-3500 to date ; 2) detecting antonyms ; and 3) distinguishing antonyms from synonyms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3019.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3019/>Characters or <a href=https://en.wikipedia.org/wiki/Morphemes>Morphemes</a> : How to Represent Words?</a></strong><br><a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/m/murathan-kurfali/>Murathan Kurfalı</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3019><div class="card-body p-3 small">In this paper, we investigate the effects of using subword information in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. We argue that using syntactic subword units effects the quality of the word representations positively. We introduce a morpheme-based model and compare it against to word-based, character-based, and character n-gram level models. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes a list of candidate segmentations of a word and learns the representation of the word based on different segmentations that are weighted by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We performed experiments on <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as a morphologically rich language and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with a comparably poorer morphology. The results show that morpheme-based models are better at learning word representations of morphologically complex languages compared to character-based and character n-gram level models since the morphemes help to incorporate more syntactic knowledge in learning, that makes morpheme-based models better at syntactic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3020/>Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences</a></strong><br><a href=/people/a/athul-paul-jacob/>Athul Paul Jacob</a>
|
<a href=/people/z/zhouhan-lin/>Zhouhan Lin</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3020><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Hierarchical_model>hierarchical model</a> for sequential data that learns a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> on-the-fly, i.e. while reading the sequence. In the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections that ease the learning of long-term dependencies. The <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> can either be inferred without <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervision</a> through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, or learned in a supervised manner. We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is created to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a well-known propositional logic and language modelling tasks. Experimental results have shown the potential of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3021/>Limitations of Cross-Lingual Learning from Image Search</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3021><div class="card-body p-3 small">Cross-lingual representation learning is an important step in making <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> scale to all the world&#8217;s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3022/>Learning Semantic Textual Similarity from Conversations</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/s/steve-yuan/>Steve Yuan</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/s/sheng-yi-kong/>Sheng-yi Kong</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/p/petr-pilar/>Petr Pilar</a>
|
<a href=/people/h/heming-ge/>Heming Ge</a>
|
<a href=/people/y/yun-hsuan-sung/>Yun-Hsuan Sung</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3022><div class="card-body p-3 small">We present a novel approach to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for sentence-level semantic similarity using conversational data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> trains an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> to predict conversational responses. The resulting <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017&#8217;s Community Question Answering (CQA) question similarity subtask. Performance is further improved by introducing multitask training, combining conversational response prediction and natural language inference. Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3023/>Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification</a></strong><br><a href=/people/k/katherine-yu/>Katherine Yu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3023><div class="card-body p-3 small">In this paper we continue experiments where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> to the <a href=https://en.wikipedia.org/wiki/Loss_function>learning objective</a> which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both. Furthermore, we notice that while our Reuters results are very competitive, our English results are not as competitive, showing room for improvement in the current cross-lingual state-of-the-art. Our results are based on a set of 6 <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3024/>LSTMs Exploit Linguistic Attributes of Data<span class=acl-fixed-case>LSTM</span>s Exploit Linguistic Attributes of Data</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3024><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a> affect an LSTM&#8217;s ability to learn a nonlinguistic task : recalling elements from its input. We find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on natural language data are able to recall tokens from much longer sequences than <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> on the learnability of LSTMs remains an open question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3026/>Jointly Embedding Entities and Text with Distant Supervision</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/albert-m-lai/>Albert M Lai</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3026><div class="card-body p-3 small">Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> between <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and <a href=https://en.wikipedia.org/wiki/Word>words</a> capture complementary information that can be effectively combined for downstream use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3027/>A Sequence-to-Sequence Model for Semantic Role Labeling</a></strong><br><a href=/people/a/angel-daza/>Angel Daza</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3027><div class="card-body p-3 small">We explore a novel approach for Semantic Role Labeling (SRL) by casting it as a sequence-to-sequence process. We employ an attention-based model enriched with a copying mechanism to ensure faithful regeneration of the input sequence, while enabling interleaved generation of argument role labels. We apply this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a monolingual setting, performing PropBank SRL on English language data. The constrained sequence generation set-up enforced with the copying mechanism allows us to analyze the performance and special properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on manually labeled data and benchmarking against state-of-the-art sequence labeling models. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> truly competitive. Our work represents the first step towards more advanced, generative SRL labeling setups.</div></div></div><hr><div id=w18-31><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-31.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-31/>Proceedings of the First Workshop on Economics and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3100/>Proceedings of the First Workshop on Economics and Natural Language Processing</a></strong><br><a href=/people/u/udo-hahn/>Udo Hahn</a>
|
<a href=/people/v/veronique-hoste/>Véronique Hoste</a>
|
<a href=/people/m/ming-feng-tsai/>Ming-Feng Tsai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3102/>Causality Analysis of Twitter Sentiments and Stock Market Returns<span class=acl-fixed-case>T</span>witter Sentiments and Stock Market Returns</a></strong><br><a href=/people/n/narges-tabari/>Narges Tabari</a>
|
<a href=/people/p/piyusha-biswas/>Piyusha Biswas</a>
|
<a href=/people/b/bhanu-praneeth/>Bhanu Praneeth</a>
|
<a href=/people/a/armin-seyeditabari/>Armin Seyeditabari</a>
|
<a href=/people/m/mirsad-hadzikadic/>Mirsad Hadzikadic</a>
|
<a href=/people/w/wlodek-zadrozny/>Wlodek Zadrozny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3102><div class="card-body p-3 small">Sentiment analysis is the process of identifying the opinion expressed in text. Recently, it has been used to study <a href=https://en.wikipedia.org/wiki/Behavioral_economics>behavioral finance</a>, and in particular the effect of <a href=https://en.wikipedia.org/wiki/Opinion>opinions</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> on economic or financial decisions. In this paper, we use a public dataset of labeled tweets that has been labeled by Amazon Mechanical Turk and then we propose a baseline classification model. Then, by using Granger causality of both sentiment datasets with the different stocks, we shows that there is causality between <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and stock market returns (in both directions) for many stocks. Finally, We evaluate this <a href=https://en.wikipedia.org/wiki/Causality>causality analysis</a> by showing that in the event of a specific news on certain dates, there are evidences of trending the same news on Twitter for that stock.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3103/>A Corpus of Corporate Annual and Social Responsibility Reports : 280 Million Tokens of Balanced Organizational Writing</a></strong><br><a href=/people/s/sebastian-g-m-handschke/>Sebastian G.M. Händschke</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/j/jan-goldenstein/>Jan Goldenstein</a>
|
<a href=/people/p/philipp-poschmann/>Philipp Poschmann</a>
|
<a href=/people/t/tinghui-duan/>Tinghui Duan</a>
|
<a href=/people/p/peter-walgenbach/>Peter Walgenbach</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3103><div class="card-body p-3 small">We introduce JOCo, a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP analytics</a> in the field of <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S&P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world&#8217;s most important economies. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> spans a time frame from 2000 up to 2015 and contains, in total, 282 M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, business and management research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3104/>Word Embeddings-Based Uncertainty Detection in Financial Disclosures</a></strong><br><a href=/people/c/christoph-kilian-theil/>Christoph Kilian Theil</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3104><div class="card-body p-3 small">In this paper, we use NLP techniques to detect <a href=https://en.wikipedia.org/wiki/Uncertainty>linguistic uncertainty</a> in <a href=https://en.wikipedia.org/wiki/Financial_statement>financial disclosures</a>. Leveraging general-domain and domain-specific word embedding models, we automatically expand an existing dictionary of uncertainty triggers. We furthermore examine how an expert filtering affects the quality of such an <a href=https://en.wikipedia.org/wiki/Expansion_factor>expansion</a>. We show that the dictionary expansions significantly improve regressions on stock return volatility. Lastly, we prove that the expansions significantly boost the automatic detection of uncertain sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3108/>Implicit and Explicit Aspect Extraction in Financial Microblogs</a></strong><br><a href=/people/t/thomas-gaillat/>Thomas Gaillat</a>
|
<a href=/people/b/bernardo-stearns/>Bernardo Stearns</a>
|
<a href=/people/g/gopal-sridhar/>Gopal Sridhar</a>
|
<a href=/people/r/ross-mcdermott/>Ross McDermott</a>
|
<a href=/people/m/manel-zarrouk/>Manel Zarrouk</a>
|
<a href=/people/b/brian-davis/>Brian Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3108><div class="card-body p-3 small">This paper focuses on <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> which is a sub-task of Aspect-based Sentiment Analysis. The goal is to report an <a href=https://en.wikipedia.org/wiki/Information_extraction>extraction method</a> of <a href=https://en.wikipedia.org/wiki/Finance>financial aspects</a> in <a href=https://en.wikipedia.org/wiki/Microblogging>microblog messages</a>. Our approach uses a stock-investment taxonomy for the identification of explicit and implicit aspects. We compare <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised and unsupervised methods</a> to assign <a href=https://en.wikipedia.org/wiki/Categorization>predefined categories</a> at message level. Results on 7 <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect classes</a> show 0.71 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while the 32 class classification gives 0.82 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for messages containing explicit aspects and 0.35 for implicit aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3109/>Unsupervised Word Influencer Networks from News Streams</a></strong><br><a href=/people/a/ananth-balashankar/>Ananth Balashankar</a>
|
<a href=/people/s/sunandan-chakraborty/>Sunandan Chakraborty</a>
|
<a href=/people/l/lakshminarayanan-subramanian/>Lakshminarayanan Subramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3109><div class="card-body p-3 small">In this paper, we propose a new unsupervised learning framework to use news events for predicting trends in stock prices. We present Word Influencer Networks (WIN), a graph framework to extract longitudinal temporal relationships between any pair of informative words from news streams. Using the temporal occurrence of words, WIN measures how the appearance of one word in a news stream influences the emergence of another set of words in the future. The latent word-word influencer relationships in WIN are the building blocks for <a href=https://en.wikipedia.org/wiki/Causal_reasoning>causal reasoning</a> and <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive modeling</a>. We demonstrate the efficacy of WIN by using it for unsupervised extraction of latent features for <a href=https://en.wikipedia.org/wiki/Stock_market_prediction>stock price prediction</a> and obtain 2 orders lower prediction error compared to a similar causal graph based method. WIN discovered influencer links from seemingly unrelated words from topics like <a href=https://en.wikipedia.org/wiki/Politics>politics</a> to <a href=https://en.wikipedia.org/wiki/Finance>finance</a>. WIN also validated 67 % of the causal evidence found manually in the text through a direct edge and the rest 33 % through a path of length 2.</div></div></div><hr><div id=w18-32><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-32.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-32/>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3200/>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/f/fahad-alghamdi/>Fahad AlGhamdi</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3201/>Joint Part-of-Speech and Language ID Tagging for Code-Switched Data<span class=acl-fixed-case>ID</span> Tagging for Code-Switched Data</a></strong><br><a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3201><div class="card-body p-3 small">Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a>. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on known monolingual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English Spanish conversation, focusing on two types of experiments : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>POS tagging</a> alone, for which we achieve 96.34 % accuracy, and joint part-of-speech and language ID tagging, which achieves similar <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>POS tagging accuracy</a> (96.39 %) and very high language ID accuracy (98.78 %). Finally, we show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform other state-of-the-art code-switched taggers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3202/>Phone Merging For Code-Switched Speech Recognition</a></strong><br><a href=/people/s/sunit-sivasankaran/>Sunit Sivasankaran</a>
|
<a href=/people/b/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3202><div class="card-body p-3 small">Speakers in multilingual communities often switch between or mix multiple languages in the same conversation. Automatic Speech Recognition (ASR) of code-switched speech faces many challenges including the influence of phones of different languages on each other. This paper shows evidence that phone sharing between languages improves the Acoustic Model performance for Hindi-English code-switched speech. We compare baseline system built with separate phones for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with systems where the phones were manually merged based on linguistic knowledge. Encouraged by the improved ASR performance after manually merging the phones, we further investigate multiple data-driven methods to identify phones to be merged across the languages. We show detailed analysis of automatic phone merging in this language pair and the impact it has on individual phone accuracies and WER. Though the best performance gain of 1.2 % <a href=https://en.wikipedia.org/wiki/Effective_radiated_power>WER</a> was observed with manually merged phones, we show experimentally that the manual phone merge is not optimal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3203/>Improving Neural Network Performance by Injecting Background Knowledge : Detecting Code-switching and Borrowing in Algerian texts<span class=acl-fixed-case>A</span>lgerian texts</a></strong><br><a href=/people/w/wafia-adouane/>Wafia Adouane</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/simon-dobnik/>Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3203><div class="card-body p-3 small">We explore the effect of injecting background knowledge to different deep neural network (DNN) configurations in order to mitigate the problem of the scarcity of annotated data when applying these models on datasets of low-resourced languages. The background knowledge is encoded in the form of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> and pre-trained sub-word embeddings. The DNN models are evaluated on the task of detecting code-switching and borrowing points in non-standardised user-generated Algerian texts. Overall results show that DNNs benefit from adding background knowledge. However, the gain varies between models and categories. The proposed DNN architectures are generic and could be applied to other low-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3204/>Code-Mixed Question Answering Challenge : Crowd-sourcing Data and Techniques</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/ekaterina-loginova/>Ekaterina Loginova</a>
|
<a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/g/gunter-neumann/>Günter Neumann</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3204><div class="card-body p-3 small">Code-Mixing (CM) is the phenomenon of alternating between two or more languages which is prevalent in bi- and multi-lingual communities. Most NLP applications today are still designed with the assumption of a single interaction language and are most likely to break given a CM utterance with multiple languages mixed at a morphological, phrase or sentence level. For example, popular commercial search engines do not yet fully understand the intents expressed in CM queries. As a first step towards fostering research which supports CM in NLP applications, we systematically crowd-sourced and curated an evaluation dataset for factoid question answering in three CM languages-Hinglish (Hindi+English), Tenglish (Telugu+English) and Tamlish (Tamil+English) which belong to two language families (Indo-Aryan and Dravidian). We share the details of our data collection process, techniques which were used to avoid inducing lexical bias amongst the crowd workers and other CM specific linguistic properties of the dataset. Our final dataset, which is available freely for research purposes, has 1,694 <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, 2,848 Tamlish and 1,391 Tenglish factoid questions and their answers. We discuss the <a href=https://en.wikipedia.org/wiki/List_of_art_media>techniques</a> used by the participants for the first edition of this ongoing challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3205/>Transliteration Better than <a href=https://en.wikipedia.org/wiki/Translation>Translation</a>? Answering Code-mixed Questions over a Knowledge Base</a></strong><br><a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3205><div class="card-body p-3 small">Humans can learn multiple languages. If they know a fact in one language, they can answer a question in another language they understand. They can also answer Code-mix (CM) questions : questions which contain both languages. This behavior is attributed to the unique learning ability of humans. Our task aims to study if machines can achieve this. We demonstrate how effectively a <a href=https://en.wikipedia.org/wiki/Machine>machine</a> can answer CM questions. In this work, we adopt a two phase approach : candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. We show experiments on the SimpleQuestions dataset. Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English. Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5 % & 35 % in accuracy respectively, highlighting the efficacy of our approach in a resource constrained setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3208/>Predicting the presence of a Matrix Language in <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a></a></strong><br><a href=/people/b/barbara-bullock/>Barbara Bullock</a>
|
<a href=/people/w/wally-guzman/>Wally Guzmán</a>
|
<a href=/people/j/jacqueline-serigos/>Jacqueline Serigos</a>
|
<a href=/people/v/vivek-sharath/>Vivek Sharath</a>
|
<a href=/people/a/almeida-jacqueline-toribio/>Almeida Jacqueline Toribio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3208><div class="card-body p-3 small">One language is often assumed to be dominant in <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> but this assumption has not been empirically tested. We operationalize the matrix language (ML) at the level of the sentence, using three common definitions from <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. We test whether these converge and then model this <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence</a> via a set of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that together quantify the nature of C-S. We conduct our experiment on four Spanish-English corpora. Our results demonstrate that our model can separate some corpora according to whether they have a dominant ML or not but that the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> span a range of mixing types that can not be sorted neatly into an insertional vs. alternational dichotomy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3210/>Accommodation of Conversational Code-Choice</a></strong><br><a href=/people/a/anshul-bawa/>Anshul Bawa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3210><div class="card-body p-3 small">Bilingual speakers often freely mix languages. However, in such bilingual conversations, are the language choices of the speakers coordinated? How much does one speaker&#8217;s choice of language affect other speakers? In this paper, we formulate code-choice as a <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>linguistic style</a>, and show that speakers are indeed sensitive to and accommodating of each other&#8217;s code-choice. We find that the saliency or markedness of a language in context directly affects the degree of accommodation observed. More importantly, we discover that accommodation of code-choices persists over several conversational turns. We also propose an alternative interpretation of conversational accommodation as a retrieval problem, and show that the differences in accommodation characteristics of code-choices are based on their markedness in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3211/>Language Informed Modeling of Code-Switched Text</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/s/sumeet-singh/>Sumeet Singh</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3211><div class="card-body p-3 small">Code-switching (CS), the practice of alternating between two or more languages in conversations, is pervasive in most <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual communities</a>. CS texts have a complex interplay between languages and occur in informal contexts that make them harder to collect and construct NLP tools for. We approach this problem through Language Modeling (LM) on a new Hindi-English mixed corpus containing 59,189 unique sentences collected from <a href=https://en.wikipedia.org/wiki/Blog>blogging websites</a>. We implement and discuss different <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> derived from a multi-layered LSTM architecture. We hypothesize that <a href=https://en.wikipedia.org/wiki/Code-switching>encoding language information</a> strengthens a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> by helping to learn code-switching points. We show that our highest performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves a test perplexity of 19.52 on the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>CS corpus</a> that we collected and processed. On this data we demonstrate that our performance is an improvement over AWD-LSTM LM (a recent state of the art on monolingual English).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3212/>GHHT at CALCS 2018 : Named Entity Recognition for Dialectal Arabic Using Neural Networks<span class=acl-fixed-case>GHHT</span> at <span class=acl-fixed-case>CALCS</span> 2018: Named Entity Recognition for Dialectal <span class=acl-fixed-case>A</span>rabic Using Neural Networks</a></strong><br><a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/w/wolfgang-maier/>Wolfgang Maier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3212><div class="card-body p-3 small">This paper describes our system submission to the CALCS 2018 shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> on code-switched data for the language variant pair of <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic</a> and <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian dialectal Arabic</a>. We build a a <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Network</a> that combines word and character-based representations in convolutional and recurrent networks with a CRF layer. The model is augmented with stacked layers of enriched information such pre-trained embeddings, Brown clusters and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity gazetteers</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is ranked second among those participating in the shared task achieving an FB1 average of 70.09 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3213/>Simple Features for Strong Performance on Named Entity Recognition in Code-Switched Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/d/devanshu-jain/>Devanshu Jain</a>
|
<a href=/people/m/maria-kustikova/>Maria Kustikova</a>
|
<a href=/people/m/mayank-darbari/>Mayank Darbari</a>
|
<a href=/people/r/rishabh-gupta/>Rishabh Gupta</a>
|
<a href=/people/s/stephen-mayhew/>Stephen Mayhew</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3213><div class="card-body p-3 small">In this work, we address the problem of Named Entity Recognition (NER) in code-switched tweets as a part of the Workshop on Computational Approaches to Linguistic Code-switching (CALCS) at ACL&#8217;18. Code-switching is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential code-switching, respectively. Processing such <a href=https://en.wikipedia.org/wiki/Data>data</a> is challenging using state of the art methods since such <a href=https://en.wikipedia.org/wiki/Technology>technology</a> is generally geared towards processing monolingual text. In this paper we explored ways to use <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to recognize named entities in such data, however, utilizing simple features (sans multi-lingual features) with Conditional Random Field (CRF) classifier achieved the best results. Our experiments were mainly aimed at the (ENG-SPA) English-Spanish dataset but we submitted a language-independent version of our system to the (MSA-EGY) Arabic-Egyptian dataset as well and achieved good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3214/>Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3214><div class="card-body p-3 small">We propose an LSTM-based model with hierarchical architecture on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> from code-switching Twitter data. Our model uses bilingual character representation and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to address out-of-vocabulary words. In order to mitigate <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>data noise</a>, we propose to use token replacement and normalization. In the 3rd Workshop on Computational Approaches to Linguistic Code-Switching Shared Task, we achieved second place with 62.76 % harmonic mean F1-score for English-Spanish language pair without using any <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteer</a> and knowledge-based information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3216/>The University of Texas System Submission for the Code-Switching Workshop Shared Task 2018<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>T</span>exas System Submission for the Code-Switching Workshop Shared Task 2018</a></strong><br><a href=/people/f/florian-janke/>Florian Janke</a>
|
<a href=/people/t/tongrui-li/>Tongrui Li</a>
|
<a href=/people/e/eric-rincon/>Eric Rincón</a>
|
<a href=/people/g/gualberto-a-guzman/>Gualberto Guzmán</a>
|
<a href=/people/b/barbara-bullock/>Barbara Bullock</a>
|
<a href=/people/a/almeida-jacqueline-toribio/>Almeida Jacqueline Toribio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3216><div class="card-body p-3 small">This paper describes the system for the Named Entity Recognition Shared Task of the Third Workshop on Computational Approaches to Linguistic Code-Switching (CALCS) submitted by the Bilingual Annotations Tasks (BATs) research group of the University of Texas. Our system uses several features to train a Conditional Random Field (CRF) model for classifying input words as Named Entities (NEs) using the Inside-Outside-Beginning (IOB) tagging scheme. We participated in the Modern Standard Arabic-Egyptian Arabic (MSA-EGY) and English-Spanish (ENG-SPA) tasks, achieving <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average F-scores</a> of 65.62 and 54.16 respectively. We also describe the performance of a deep neural network (NN) trained on a subset of the CRF features, which did not surpass CRF performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3217/>Tackling Code-Switched NER : Participation of CMU<span class=acl-fixed-case>NER</span>: Participation of <span class=acl-fixed-case>CMU</span></a></strong><br><a href=/people/p/parvathy-geetha/>Parvathy Geetha</a>
|
<a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3217><div class="card-body p-3 small">Named Entity Recognition plays a major role in several downstream applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Though this task has been heavily studied in formal monolingual texts and also <a href=https://en.wikipedia.org/wiki/Noisy_text>noisy texts</a> like Twitter data, it is still an emerging task in code-switched (CS) content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper describes our participation in the shared task of NER on code-switched data for Spanglish (Spanish + English) and Arabish (Arabic + English). In this paper we describe <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that intuitively developed from the <a href=https://en.wikipedia.org/wiki/Data>data</a> for the shared task Named Entity Recognition on Code-switched Data. Owing to the sparse and non-linear relationships between words in Twitter data, we explored neural architectures that are capable of non-linearities fairly well. In specific, we trained character level models and word level models based on Bidirectional LSTMs (Bi-LSTMs) to perform sequential tagging. We train multiple models to identify nominal mentions and subsequently use this information to predict the labels of named entity in a sequence. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a character level model along with word level pre-trained multilingual embeddings that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 56.72 in <a href=https://en.wikipedia.org/wiki/Spanglish>Spanglish</a> and a word level model that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 65.02 in <a href=https://en.wikipedia.org/wiki/Arabic>Arabish</a> on the test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3218/>Multilingual Named Entity Recognition on Spanish-English Code-switched Tweets using Support Vector Machines<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish Code-switched Tweets using Support Vector Machines</a></strong><br><a href=/people/d/daniel-claeser/>Daniel Claeser</a>
|
<a href=/people/s/samantha-kent/>Samantha Kent</a>
|
<a href=/people/d/dennis-felske/>Dennis Felske</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3218><div class="card-body p-3 small">This paper describes our system submission for the ACL 2018 shared task on named entity recognition (NER) in code-switched Twitter data. Our best result (F1 = 53.65) was obtained using a Support Vector Machine (SVM) with 14 <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> combined with rule-based post processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3220/>IIT (BHU) Submission for the ACL Shared Task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> on Code-switched Data<span class=acl-fixed-case>IIT</span> (<span class=acl-fixed-case>BHU</span>) Submission for the <span class=acl-fixed-case>ACL</span> Shared Task on Named Entity Recognition on Code-switched Data</a></strong><br><a href=/people/s/shashwat-trivedi/>Shashwat Trivedi</a>
|
<a href=/people/h/harsh-rangwani/>Harsh Rangwani</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3220><div class="card-body p-3 small">This paper describes the best performing system for the shared task on Named Entity Recognition (NER) on code-switched data for the language pair Spanish-English (ENG-SPA). We introduce a gated neural architecture for the NER task. Our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 63.76 %, outperforming the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3221/>Code-Switched Named Entity Recognition with Embedding Attention</a></strong><br><a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3221><div class="card-body p-3 small">We describe our work for the CALCS 2018 shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> on code-switched data. Our system ranked first place for MS Arabic-Egyptian named entity recognition and third place for English-Spanish.</div></div></div><hr><div id=w18-33><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-33.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-33/>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3300/>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-<span class=acl-fixed-case>HML</span>)</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3301/>Getting the subtext without the text : Scalable multimodal sentiment classification from visual and acoustic modalities</a></strong><br><a href=/people/n/nathaniel-blanchard/>Nathaniel Blanchard</a>
|
<a href=/people/d/daniel-moreira/>Daniel Moreira</a>
|
<a href=/people/a/aparna-bharati/>Aparna Bharati</a>
|
<a href=/people/w/walter-scheirer/>Walter Scheirer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3301><div class="card-body p-3 small">In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment. The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection. Multimodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers. In the detection of sentiment in these <a href=https://en.wikipedia.org/wiki/Video>videos</a>, acoustic and video features provide clarity to otherwise ambiguous transcripts. In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>spoken sentences</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>. We discard traditional transcription features in order to minimize human intervention and to maximize the deployability of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on at-scale real-world data. We select high-level features for our model that have been successful in non-affect domains in order to test their generalizability in the sentiment detection domain. We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, obtaining an F1 score of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out challenge test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3302/>Recognizing Emotions in Video Using Multimodal DNN Feature Fusion<span class=acl-fixed-case>DNN</span> Feature Fusion</a></strong><br><a href=/people/j/jennifer-williams/>Jennifer Williams</a>
|
<a href=/people/s/steven-kleinegesse/>Steven Kleinegesse</a>
|
<a href=/people/r/ramona-comanescu/>Ramona Comanescu</a>
|
<a href=/people/o/oana-radu/>Oana Radu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3302><div class="card-body p-3 small">We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language. Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs). We show that our fusion approach outperforms <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal predictors</a>. Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment. This leads to an overall binary accuracy of 90 %, overall 4-class accuracy of 89.2 % and an overall <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>mean-absolute-error (MAE)</a> of 0.12. Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities. The presented multimodal approach creates a simple and robust <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on this new Grand Challenge dataset. Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3304/>Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data</a></strong><br><a href=/people/w/woo-yong-choi/>Woo Yong Choi</a>
|
<a href=/people/k/kyu-ye-song/>Kyu Ye Song</a>
|
<a href=/people/c/chan-woo-lee/>Chan Woo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3304><div class="card-body p-3 small">Emotion recognition has become a popular topic of interest, especially in the field of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human computer interaction</a>. Previous works involve <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal analysis of emotion</a>, while recent efforts focus on <a href=https://en.wikipedia.org/wiki/Emotion_recognition>multimodal emotion recognition</a> from <a href=https://en.wikipedia.org/wiki/Visual_perception>vision</a> and <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3306/>Polarity and Intensity : the Two Aspects of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/l/leimin-tian/>Leimin Tian</a>
|
<a href=/people/c/catherine-lai/>Catherine Lai</a>
|
<a href=/people/j/johanna-d-moore/>Johanna Moore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3306><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> frames sentiment score prediction as a general Machine Learning task. However, what the sentiment score actually represents has often been overlooked. As a measurement of opinions and affective states, a sentiment score generally consists of two aspects : polarity and intensity. We decompose sentiment scores into these two aspects and study how they are conveyed through individual modalities and combined multimodal models in a naturalistic monologue setting. In particular, we build unimodal and multimodal multi-task learning models with sentiment score prediction as the main task and polarity and/or intensity classification as the auxiliary tasks. Our experiments show that <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, and individual modalities differ when conveying the polarity and intensity aspects of sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3308/>Seq2Seq2Sentiment : Multimodal Sequence to Sequence Models for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>entiment: Multimodal Sequence to Sequence Models for Sentiment Analysis</a></strong><br><a href=/people/h/hai-pham/>Hai Pham</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/b/barnabas-poczos/>Barnabás Poczós</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3308><div class="card-body p-3 small">Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods : a Seq2Seq Modality Translation Model and a Hierarchical Seq2Seq Modality Translation Model. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a>, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.</div></div></div><hr><div id=w18-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-34/>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3400/>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/gholamreza-haffari/>Reza Haffari</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3403.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3403/>Multi-task learning for historical text normalization : Size matters</a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3403><div class="card-body p-3 small">Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> across 10 different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, representing different languages and periods. Our main findingcontrary to what has been observed for other NLP tasksis that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> mainly works when target task data is very scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3404/>Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication</a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3404><div class="card-body p-3 small">Icon-based communication systems are widely used in the field of <a href=https://en.wikipedia.org/wiki/Augmentative_and_alternative_communication>Augmentative and Alternative Communication</a>. Typically, icon-based systems have lagged behind word- and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models. We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies. We propose a method to generate <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for corpus-less symbol-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3405/>Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3405><div class="card-body p-3 small">In this paper, we investigate the effectiveness of training a multimodal neural machine translation (MNMT) system with image features for a low-resource language pair, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, using synthetic data. A three-way parallel corpus which contains <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual texts</a> and corresponding images is required to train a MNMT system with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>image features</a>. However, such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development / test dataset for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> based on an existing English-image parallel corpus. We used these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a <a href=https://en.wikipedia.org/wiki/System>system</a> can benefit from image features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3406/>Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus</a></strong><br><a href=/people/f/fariz-ikhwantri/>Fariz Ikhwantri</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/b/bagas-abisena/>Bagas Abisena</a>
|
<a href=/people/v/valdi-rachman/>Valdi Rachman</a>
|
<a href=/people/a/alfan-farizki-wicaksono/>Alfan Farizki Wicaksono</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3406><div class="card-body p-3 small">Most Semantic Role Labeling (SRL) approaches are supervised methods which require a significant amount of annotated corpus, and the annotation requires linguistic expertise. In this paper, we propose a Multi-Task Active Learning framework for Semantic Role Labeling with Entity Recognition (ER) as the auxiliary task to alleviate the need for extensive data and use additional information from ER to help SRL. We evaluate our approach on Indonesian conversational dataset. Our experiments show that multi-task active learning can outperform single-task active learning method and standard <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. According to our results, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is more efficient by using 12 % less of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> compared to <a href=https://en.wikipedia.org/wiki/Passive_learning>passive learning</a> in both single-task and multi-task setting. We also introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for SRL in Indonesian conversational domain to encourage further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3407.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3407/>Domain Adapted Word Embeddings for Improved Sentiment Classification</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha Kameswara Sarma</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a>
|
<a href=/people/b/bill-sethares/>Bill Sethares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3407><div class="card-body p-3 small">Generic word embeddings are trained on large-scale generic corpora ; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings. The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by first aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA (KCCA) and then combining them via <a href=https://en.wikipedia.org/wiki/Convex_optimization>convex optimization</a>. Results from evaluation on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3408/>Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus</a></strong><br><a href=/people/k/kanako-komiya/>Kanako Komiya</a>
|
<a href=/people/h/hiroyuki-shinnou/>Hiroyuki Shinnou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3408><div class="card-body p-3 small">Fine-tuning is a popular <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> to achieve better performance when only a small target corpus is available. However, <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> requires tuning of a number of metaparameters and thus it might carry risk of adverse effect when inappropriate metaparameters are used. Therefore, we investigate effective parameters for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> when only a small target corpus is available. In the current study, we target at improving Japanese word embeddings created from a <a href=https://en.wikipedia.org/wiki/Text_corpus>huge corpus</a>. First, we demonstrate that even the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> created from the huge corpus are affected by domain shift. After that, we investigate effective <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> of the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> using a small target corpus. We used perplexity of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> obtained from a Long Short-Term Memory network to assess the word embeddings input into the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. The experiments revealed that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. In addition, we confirmed that effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is higher when size of a target corpus was larger.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3409.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3409/>Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification</a></strong><br><a href=/people/m/mingkuan-liu/>Mingkuan Liu</a>
|
<a href=/people/m/musen-wen/>Musen Wen</a>
|
<a href=/people/s/selcuk-kopru/>Selcuk Kopru</a>
|
<a href=/people/x/xianjing-liu/>Xianjing Liu</a>
|
<a href=/people/a/alan-lu/>Alan Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3409><div class="card-body p-3 small">The lack of high-quality labeled training data has been one of the critical challenges facing many industrial machine learning tasks. To tackle this challenge, in this paper, we propose a semi-supervised learning method to utilize unlabeled data and user feedback signals to improve the performance of ML models. The method employs a primary model Main and an auxiliary evaluation model Eval, where Main and Eval models are trained iteratively by automatically generating labeled data from unlabeled data and/or users&#8217; feedback signals. The proposed approach is applied to different text classification tasks. We report results on both the publicly available Yahoo ! Answers dataset and our e-commerce product classification dataset. The experimental results show that the proposed method reduces the classification error rate by 4 % and up to 15 % across various experimental setups and datasets. A detailed comparison with other semi-supervised learning approaches is also presented later in the paper. The results from various text classification tasks demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms those developed in previous related studies.</div></div></div><hr><div id=w18-35><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-35.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-35/>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3500/>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/c/cheng-te-li/>Cheng-Te Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3501/>Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students<span class=acl-fixed-case>W</span>hats<span class=acl-fixed-case>A</span>pp Chats in <span class=acl-fixed-case>S</span>panish among College Students</a></strong><br><a href=/people/a/alejandro-dorantes/>Alejandro Dorantes</a>
|
<a href=/people/g/gerardo-sierra/>Gerardo Sierra</a>
|
<a href=/people/t/tlauhlia-yamin-donohue-perez/>Tlauhlia Yamín Donohue Pérez</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/m/monica-jasso-rosales/>Mónica Jasso Rosales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3501><div class="card-body p-3 small">This work presents the Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students, a corpus of raw data for general use. Its purpose is to offer data for the study of of language and interactions via Instant Messaging (IM) among bachelors. Our paper consists of an overview of both the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>&#8217;s content and demographic metadata. Furthermore, it presents the current research being conducted with it namely <a href=https://en.wikipedia.org/wiki/Parenthetical_expression>parenthetical expressions</a>, <a href=https://en.wikipedia.org/wiki/Orality>orality traits</a>, and <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. This work also includes a brief outline of similar corpora and recent studies in the field of IM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3503/>A Twitter Corpus for Hindi-English Code Mixed POS Tagging<span class=acl-fixed-case>T</span>witter Corpus for <span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code Mixed <span class=acl-fixed-case>POS</span> Tagging</a></strong><br><a href=/people/k/kushagra-singh/>Kushagra Singh</a>
|
<a href=/people/i/indira-sen/>Indira Sen</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3503><div class="card-body p-3 small">Code-mixing is a linguistic phenomenon where multiple languages are used in the same occurrence that is increasingly common in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual societies</a>. Code-mixed content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is also on the rise, prompting the need for tools to automatically understand such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>. Automatic Parts-of-Speech (POS) tagging is an essential step in any Natural Language Processing (NLP) pipeline, but there is a lack of annotated data to train such models. In this work, we present a unique language tagged and POS-tagged dataset of code-mixed English-Hindi tweets related to five incidents in India that led to a lot of Twitter activity. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is unique in two dimensions : (i) it is larger than previous annotated datasets and (ii) it closely resembles typical real-world tweets. Additionally, we present a POS tagging model that is trained on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to provide an example of how this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also shows the efficacy of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in enabling the creation of code-mixed social media POS taggers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3504/>Detecting Offensive Tweets in Hindi-English Code-Switched Language<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Switched Language</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/d/debanjan-mahata/>Debanjan Mahata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3504><div class="card-body p-3 small">The exponential rise of social media websites like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> in linguistically diverse geographical regions has led to hybridization of popular native languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a> in an effort to ease communication. The paper focuses on the classification of offensive tweets written in <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish language</a>, which is a portmanteau of the Indic language Hindi with the <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a>. The paper introduces a novel tweet dataset, titled Hindi-English Offensive Tweet (HEOT) dataset, consisting of tweets in Hindi-English code switched language split into three classes : non-offensive, abusive and hate-speech. Further, we approach the problem of classification of the tweets in HEOT dataset using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> wherein the proposed model employing <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> is pre-trained on tweets in English followed by retraining on <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish tweets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3507/>EmotionX-AR : CNN-DCNN autoencoder based Emotion Classifier<span class=acl-fixed-case>E</span>motion<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>AR</span>: <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>DCNN</span> autoencoder based Emotion Classifier</a></strong><br><a href=/people/s/sopan-khosla/>Sopan Khosla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3507><div class="card-body p-3 small">In this paper, we model emotions in EmotionLines dataset using a convolutional-deconvolutional autoencoder (CNN-DCNN) framework. We show that adding a joint reconstruction loss improves performance. Quantitative evaluation with jointly trained network, augmented with linguistic features, reports best accuracies for emotion prediction ; namely <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Sadness>sadness</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, and neutral emotion in text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3508/>EmotionX-SmartDubai_NLP : Detecting User Emotions In Social Media Text<span class=acl-fixed-case>E</span>motion<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>S</span>mart<span class=acl-fixed-case>D</span>ubai_<span class=acl-fixed-case>NLP</span>: Detecting User Emotions In Social Media Text</a></strong><br><a href=/people/h/hessa-albalooshi/>Hessa AlBalooshi</a>
|
<a href=/people/s/shahram-rahmanian/>Shahram Rahmanian</a>
|
<a href=/people/r/rahul-venkatesh-kumar/>Rahul Venkatesh Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3508><div class="card-body p-3 small">This paper describes the working note on EmotionX shared task. It is hosted by SocialNLP 2018. The objective of this task is to detect the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>, based on each speaker&#8217;s utterances that are in English. Taking this as multiclass text classification problem, we have experimented to develop a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to classify the target class. The primary challenge in this task is to detect the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> in short messages, communicated through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper describes the participation of SmartDubai_NLP team in EmotionX shared task and our investigation to detect the emotions from utterance using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural networks</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural language understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3513/>Political discourse classification in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> using context sensitive convolutional neural networks</a></strong><br><a href=/people/a/aritz-bilbao-jayo/>Aritz Bilbao-Jayo</a>
|
<a href=/people/a/aitor-almeida/>Aitor Almeida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3513><div class="card-body p-3 small">In this study we propose a new approach to analyse the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>on-line social networks</a> such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. To do so, we have built a discourse classifier using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a>. Our model has been trained using <a href=https://en.wikipedia.org/wiki/Manifesto>election manifestos</a> annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E-RMP). Using this <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a>, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 <a href=https://en.wikipedia.org/wiki/2016_Spanish_general_election>Spanish general election</a> and providing a study of their discourse.</div></div></div><hr><div id=w18-36><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-36.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-36/>Proceedings of the First Workshop on Multilingual Surface Realisation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3600/>Proceedings of the First Workshop on Multilingual Surface Realisation</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3602/>BinLin : A Simple Method of Dependency Tree Linearization<span class=acl-fixed-case>B</span>in<span class=acl-fixed-case>L</span>in: A Simple Method of Dependency Tree Linearization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3602><div class="card-body p-3 small">Surface Realization Shared Task 2018 is a workshop on generating sentences from lemmatized sets of dependency triples. This paper describes the results of our participation in the challenge. We develop a data-driven pipeline system which first orders the lemmas and then conjugates the words to finish the surface realization process. Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results. We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3607/>AX Semantics’ Submission to the Surface Realization Shared Task 2018<span class=acl-fixed-case>AX</span> Semantics’ Submission to the Surface Realization Shared Task 2018</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/j/johanna-heininger/>Johanna Heininger</a>
|
<a href=/people/n/nyamsuren-davaasambuu/>Nyamsuren Davaasambuu</a>
|
<a href=/people/v/vitaliia-voronik/>Vitaliia Voronik</a>
|
<a href=/people/m/michael-kaufl/>Michael Käufl</a>
|
<a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3607><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> and experimental results on the development set of the Surface Realisation Shared Task. Our system is an entry for the Shallow-Task, with two different models based on deep-learning implementations for building the sentence combined with a rule-based morphology component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3608/>NILC-SWORNEMO at the Surface Realization Shared Task : Exploring Syntax-Based Word Ordering using Neural Models<span class=acl-fixed-case>NILC</span>-<span class=acl-fixed-case>SWORNEMO</span> at the Surface Realization Shared Task: Exploring Syntax-Based Word Ordering using Neural Models</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3608><div class="card-body p-3 small">This paper describes the submission by the NILC Computational Linguistics research group of the University of So Paulo / Brazil to the Track 1 of the Surface Realization Shared Task (SRST Track 1). We present a neural-based method that works at the syntactic level to order the words (which we refer by NILC-SWORNEMO, standing for Syntax-based Word ORdering using NEural MOdels). Additionally, we apply a <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up approach</a> to build the sentence and, using language-specific lexicons, we produce the proper word form of each lemma in the sentence. The results obtained by our method outperformed the average of the results for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> in the track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3609/>The DipInfo-UniTo system for SRST 2018<span class=acl-fixed-case>D</span>ip<span class=acl-fixed-case>I</span>nfo-<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>T</span>o system for <span class=acl-fixed-case>SRST</span> 2018</a></strong><br><a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3609><div class="card-body p-3 small">This paper describes the system developed by the DipInfo-UniTo team to participate to the shallow track of the Surface Realization Shared Task 2018. The system employs two separate <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with different architectures to predict the <a href=https://en.wikipedia.org/wiki/Word_order>word ordering</a> and the <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> independently from each other. The UniTO realizer is language independent, and its simple architecture allowed it to be scored in the central part of the final ranking of the shared task.</div></div></div><hr><div id=w18-37><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-37.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-37/>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3700/>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3703/>Feature Optimization for Predicting Readability of Arabic L1 and L2<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>L</span>1 and <span class=acl-fixed-case>L</span>2</a></strong><br><a href=/people/h/hind-saddiki/>Hind Saddiki</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/v/violetta-cavalli-sforza/>Violetta Cavalli-Sforza</a>
|
<a href=/people/m/muhamed-al-khalil/>Muhamed Al Khalil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3703><div class="card-body p-3 small">Advances in automatic readability assessment can impact the way people consume information in a number of domains. Arabic, being a low-resource and morphologically complex language, presents numerous challenges to the task of automatic readability assessment. In this paper, we present the largest and most in-depth computational readability study for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> to date. We study a large set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> with varying depths, from shallow words to syntactic trees, for both L1 and L2 readability tasks. Our best L1 readability accuracy result is 94.8 % (75 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> from a commonly used baseline). The comparable results for L2 are 72.4 % (45 % error reduction). We also demonstrate the added value of leveraging L1 features for L2 readability prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3704/>A Tutorial Markov Analysis of Effective Human Tutorial Sessions<span class=acl-fixed-case>M</span>arkov Analysis of Effective Human Tutorial Sessions</a></strong><br><a href=/people/n/nabin-maharjan/>Nabin Maharjan</a>
|
<a href=/people/v/vasile-rus/>Vasile Rus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3704><div class="card-body p-3 small">This paper investigates what differentiates effective tutorial sessions from less effective sessions. Towards this end, we characterize and explore human tutors&#8217; actions in tutorial dialogue sessions by mapping the tutor-tutee interactions, which are streams of dialogue utterances, into streams of actions, based on the language-as-action theory. Next, we use human expert judgment measures, evidence of learning (EL) and evidence of soundness (ES), to identify effective and ineffective sessions. We perform sub-sequence pattern mining to identify sub-sequences of dialogue modes that discriminate good sessions from bad sessions. We finally use the results of sub-sequence analysis method to generate a tutorial Markov process for effective tutorial sessions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3705/>Thank Goodness ! A Way to Measure Style in Student Essays</a></strong><br><a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3705><div class="card-body p-3 small">Essays have two major components for scoring-content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3706/>Overview of NLPTEA-2018 Share Task Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>NLPTEA</span>-2018 Share Task <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/g/gaoqi-rao/>Gaoqi Rao</a>
|
<a href=/people/q/qi-gong/>Qi Gong</a>
|
<a href=/people/b/baolin-zhang/>Baolin Zhang</a>
|
<a href=/people/e/endong-xun/>Endong Xun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3706><div class="card-body p-3 small">This paper presents the NLPTEA 2018 shared task for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Grammatical Error Diagnosis (CGED)</a> which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as foreign language. We describe the task definition, data preparation, <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a>, and evaluation results. Of the 20 teams registered for this shared task, 13 teams developed the <a href=https://en.wikipedia.org/wiki/System>system</a> and submitted a total of 32 runs. Progress in <a href=https://en.wikipedia.org/wiki/System>system</a> performances was obviously, reaching F1 of 36.12 % in position level and 25.27 % in correction level. All data sets with gold standards and scoring scripts are made publicly available to researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3708/>A Hybrid System for Chinese Grammatical Error Diagnosis and Correction<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis and Correction</a></strong><br><a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zuyi-bao/>Zuyi Bao</a>
|
<a href=/people/h/hengyou-liu/>Hengyou Liu</a>
|
<a href=/people/g/guangwei-xu/>Guangwei Xu</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3708><div class="card-body p-3 small">This paper introduces the DM_NLP team&#8217;s system for NLPTEA 2018 shared task of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Grammatical Error Diagnosis (CGED)</a>, which can be used to detect and correct grammatical errors in texts written by <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a Foreign Language (CFL) learners. This task aims at not only detecting four types of grammatical errors including redundant words (R), missing words (M), bad word selection (S) and disordered words (W), but also recommending corrections for errors of M and S types. We proposed a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> including four <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for this task with two stages : the detection stage and the correction stage. In the detection stage, we first used a BiLSTM-CRF model to tag potential errors by <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, along with some handcraft features. Then we designed three Grammatical Error Correction (GEC) models to generate corrections, which could help to tune the detection result. In the correction stage, candidates were generated by the three GEC models and then merged to output the final corrections for M and S types. Our system reached the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3709/>Ling@CASS Solution to the NLP-TEA CGED Shared Task 2018<span class=acl-fixed-case>CASS</span> Solution to the <span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>TEA</span> <span class=acl-fixed-case>CGED</span> Shared Task 2018</a></strong><br><a href=/people/q/qinan-hu/>Qinan Hu</a>
|
<a href=/people/y/yongwei-zhang/>Yongwei Zhang</a>
|
<a href=/people/f/fang-liu/>Fang Liu</a>
|
<a href=/people/y/yueguo-gu/>Yueguo Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3709><div class="card-body p-3 small">In this study, we employ the sequence to sequence learning to model the task of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammar error correction</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> takes potentially erroneous sentences as inputs, and outputs correct sentences. To breakthrough the bottlenecks of very limited size of manually labeled data, we adopt a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised approach</a>. Specifically, we adapt correct sentences written by native Chinese speakers to generate pseudo grammatical errors made by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a second language. We use the pseudo data to pre-train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and the CGED data to fine-tune it. Being aware of the significance of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> in a grammar error correction system in real scenarios, we use <a href=https://en.wikipedia.org/wiki/Assembly_language>ensembles</a> to boost <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. When using inputs as simple as <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a>, the ensembled system achieves a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> at 86.56 % in the detection of erroneous sentences, and a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> at 51.53 % in the correction of errors of Selection and Missing types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3710/>Chinese Grammatical Error Diagnosis Based on Policy Gradient LSTM Model<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Based on Policy Gradient <span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/changliang-li/>Changliang Li</a>
|
<a href=/people/j/ji-qi/>Ji Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3710><div class="card-body p-3 small">Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA2018 workshop held during ACL2018. The goal of this task is to diagnose Chinese sentences containing four kinds of <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> through the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and find out the sentence errors. Chinese grammatical error diagnosis system is a very important tool, which can help <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese learners</a> automatically diagnose <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> in many scenarios. However, due to the limitations of the Chinese language&#8217;s own characteristics and datasets, the traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> faces the problem of extreme imbalances in the positive and negative samples and the disappearance of gradients. In this paper, we propose a sequence labeling method based on the Policy Gradient LSTM model and apply it to this task to solve the above problems. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve higher precision scores in the case of lower False positive rate (FPR) and it is convenient to optimize the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on-line.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3714/>Joint learning of frequency and word embeddings for multilingual readability assessment</a></strong><br><a href=/people/d/dieu-thu-le/>Dieu-Thu Le</a>
|
<a href=/people/c/cam-tu-nguyen/>Cam-Tu Nguyen</a>
|
<a href=/people/x/xiaoliang-wang/>Xiaoliang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3714><div class="card-body p-3 small">This paper describes two models that employ word frequency embeddings to deal with the problem of readability assessment in multiple languages. The task is to determine the difficulty level of a given document, i.e., how hard it is for a reader to fully comprehend the text. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> show how <a href=https://en.wikipedia.org/wiki/Frequency>frequency information</a> can be integrated to improve the readability assessment. The experimental results testing on both English and Chinese datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> improve the results notably when comparing to those using only traditional <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3715/>MULLE : A grammar-based Latin language learning tool to supplement the classroom setting<span class=acl-fixed-case>MULLE</span>: A grammar-based <span class=acl-fixed-case>L</span>atin language learning tool to supplement the classroom setting</a></strong><br><a href=/people/h/herbert-lange/>Herbert Lange</a>
|
<a href=/people/p/peter-ljunglof/>Peter Ljunglöf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3715><div class="card-body p-3 small">MULLE is a tool for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a> that focuses on teaching Latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> distinct from other <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning tools</a> that provide standalone learning experience. It uses grammar-based lessons and embraces methods of <a href=https://en.wikipedia.org/wiki/Gamification>gamification</a> to improve the learner motivation. The main type of exercise provided by our <a href=https://en.wikipedia.org/wiki/Application_software>application</a> is to practice <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, but it is also possible to shift the focus to vocabulary or morphology training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3716/>Textual Features Indicative of Writing Proficiency in Elementary School Spanish Documents<span class=acl-fixed-case>S</span>panish Documents</a></strong><br><a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/d/diana-duenas-chavez/>Diana Dueñas Chávez</a>
|
<a href=/people/a/arturo-curiel/>Arturo Curiel Díaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3716><div class="card-body p-3 small">Childhood acquisition of written language is not straightforward. Writing skills evolve differently depending on external factors, such as the conditions in which children practice their productions and the quality of their instructors&#8217; guidance. This can be challenging in low-income areas, where schools may struggle to ensure ideal acquisition conditions. Developing computational tools to support the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a> may counterweight negative environmental influences ; however, few work exists on the use of <a href=https://en.wikipedia.org/wiki/Information_technology>information technologies</a> to improve childhood literacy. This work centers around the computational study of Spanish word and syllable structure in documents written by 2nd and 3rd year elementary school students. The studied texts were compared against a corpus of short stories aimed at the same age group, so as to observe whether the children tend to produce similar written patterns as the ones they are expected to interpret at their <a href=https://en.wikipedia.org/wiki/Literacy>literacy level</a>. The obtained results show some significant differences between the two kinds of texts, pointing towards possible strategies for the implementation of new <a href=https://en.wikipedia.org/wiki/Educational_software>education software</a> in support of written language acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3718 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3718/>A Short Answer Grading System in Chinese by Support Vector Approach<span class=acl-fixed-case>C</span>hinese by Support Vector Approach</a></strong><br><a href=/people/s/shih-hung-wu/>Shih-Hung Wu</a>
|
<a href=/people/w/wen-feng-shih/>Wen-Feng Shih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3718><div class="card-body p-3 small">In this paper, we report a short answer grading system in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We build a system based on standard machine learning approaches and test it with translated corpus from two publicly available corpus in English. The experiment results show similar results on two different corpus as in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3719 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3719/>From Fidelity to Fluency : <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> for Translator Training</a></strong><br><a href=/people/o/olivia-o-y-kwong/>Oi Yee Kwong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3719><div class="card-body p-3 small">This study explores the use of natural language processing techniques to enhance bilingual lexical access beyond simple equivalents, to enable translators to navigate along a wider cross-lingual lexical space and more examples showing different translation strategies, which is essential for them to learn to produce not only faithful but also fluent translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3720.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3720/>Countering Position Bias in Instructor Interventions in MOOC Discussion Forums<span class=acl-fixed-case>MOOC</span> Discussion Forums</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3720><div class="card-body p-3 small">We systematically confirm that instructors are strongly influenced by the user interface presentation of Massive Online Open Course (MOOC) discussion forums. In a large scale dataset, we conclusively show that instructor interventions exhibit strong position bias, as measured by the position where the thread appeared on the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> at the time of intervention. We measure and remove this <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, enabling unbiased statistical modelling and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. We show that our de-biased classifier improves predicting interventions over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> with sufficient number of <a href=https://en.wikipedia.org/wiki/Intervention_(counseling)>interventions</a> by 8.2 % in F1 and 24.4 % in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3722.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3722 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3722 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3722/>Learning to Automatically Generate Fill-In-The-Blank Quizzes</a></strong><br><a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/a/ai-nakajima/>Ai Nakajima</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a>
|
<a href=/people/o/ono-yuichi/>Ono Yuichi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3722><div class="card-body p-3 small">In this paper we formalize the problem automatic fill-in-the-blank question generation using two standard NLP machine learning schemes, proposing concrete deep learning models for each. We present an empirical study based on data obtained from a <a href=https://en.wikipedia.org/wiki/Machine_learning>language learning platform</a> showing that both of our proposed settings offer promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3723 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3723/>Multilingual Short Text Responses Clustering for Mobile Educational Activities : a Preliminary Exploration</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/y/yu-ta-chien/>Yu-Ta Chien</a>
|
<a href=/people/c/chun-yen-chang/>Chun-Yen Chang</a>
|
<a href=/people/t/tsung-yen-li/>Tsung-Yen Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3723><div class="card-body p-3 small">Text clustering is a powerful technique to detect topics from <a href=https://en.wikipedia.org/wiki/Text_corpus>document corpora</a>, so as to provide information browsing, <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a>, and <a href=https://en.wikipedia.org/wiki/Organization>organization</a>. On the other hand, the Instant Response System (IRS) has been widely used in recent years to enhance student engagement in class and thus improve their learning effectiveness. However, the lack of functions to process short text responses from the <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a> prevents the further application of <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a> in classes. Therefore, this study aims to propose a proper short text clustering module for the <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a>, and demonstrate our implemented techniques through real-world examples, so as to provide experiences and insights for further study. In particular, we have compared three <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering methods</a> and the result shows that theoretically better methods need not lead to better results, as there are various factors that may affect the final performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3724 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3724/>Chinese Grammatical Error Diagnosis Based on CRF and LSTM-CRF model<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Based on <span class=acl-fixed-case>CRF</span> and <span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> model</a></strong><br><a href=/people/y/yujie-zhou/>Yujie Zhou</a>
|
<a href=/people/y/yinan-shao/>Yinan Shao</a>
|
<a href=/people/y/yong-zhou/>Yong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3724><div class="card-body p-3 small">When learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language, the learners may have some <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> due to negative migration of their native languages. However, few grammar checking applications have been developed to support the learners. The goal of this paper is to develop a tool to automatically diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W) in Chinese sentences written by those foreign learners. In this paper, a conventional linear CRF model with specific <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and a LSTM-CRF model are used to solve the CGED (Chinese Grammatical Error Diagnosis) task. We make some improvement on both models and the submitted results have better performance on <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than the average of all runs from CGED2018 for all three evaluation levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3727 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3727/>Detecting Simultaneously Chinese Grammar Errors Based on a BiLSTM-CRF Model<span class=acl-fixed-case>C</span>hinese Grammar Errors Based on a <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model</a></strong><br><a href=/people/y/yajun-liu/>Yajun Liu</a>
|
<a href=/people/h/hongying-zan/>Hongying Zan</a>
|
<a href=/people/m/mengjie-zhong/>Mengjie Zhong</a>
|
<a href=/people/h/hongchao-ma/>Hongchao Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3727><div class="card-body p-3 small">In the process of learning and using <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, many learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese as foreign language(CFL)</a> may have <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammar errors</a> due to negative migration of their native languages. This paper introduces our system that can simultaneously diagnose four types of grammatical errors including redundant (R), missing (M), selection (S), disorder (W) in NLPTEA-5 shared task. We proposed a Bidirectional LSTM CRF neural network (BiLSTM-CRF) that combines BiLSTM and CRF without hand-craft features for Chinese Grammatical Error Diagnosis (CGED). Evaluation includes three levels, which are detection level, identification level and position level. At the detection level and identification level, our <a href=https://en.wikipedia.org/wiki/System>system</a> got the third recall scores, and achieved good <a href=https://en.wikipedia.org/wiki/F-number>F1 values</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3728/>A Hybrid Approach Combining Statistical Knowledge with <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> for Chinese Grammatical Error Detection<span class=acl-fixed-case>C</span>hinese Grammatical Error Detection</a></strong><br><a href=/people/y/yiyi-wang/>Yiyi Wang</a>
|
<a href=/people/c/chilin-shih/>Chilin Shih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3728><div class="card-body p-3 small">This paper presents a method of combining Conditional Random Fields (CRFs) model with a post-processing layer using Google n-grams statistical information tailored to detect word selection and word order errors made by learners of Chinese as Foreign Language (CFL). We describe the architecture of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and its performance in the shared task of the ACL 2018 Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA). This hybrid approach yields comparably high <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a> (FPR = 0.1274) and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> (Pd= 0.7519 ; Pi= 0.6311), but low <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> (Rd = 0.3035 ; Ri = 0.1696) in grammatical error detection and identification tasks. Additional <a href=https://en.wikipedia.org/wiki/Statistics>statistical information</a> and <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a> can be added to enhance the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3729.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3729 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3729 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3729/>CYUT-III Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2018 CGED Shared Task<span class=acl-fixed-case>CYUT</span>-<span class=acl-fixed-case>III</span> Team <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis System Report in <span class=acl-fixed-case>NLPTEA</span>-2018 <span class=acl-fixed-case>CGED</span> Shared Task</a></strong><br><a href=/people/s/shih-hung-wu/>Shih-Hung Wu</a>
|
<a href=/people/j/jun-wei-wang/>Jun-Wei Wang</a>
|
<a href=/people/l/liang-pu-chen/>Liang-Pu Chen</a>
|
<a href=/people/p/ping-che-yang/>Ping-Che Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3729><div class="card-body p-3 small">This paper reports how we build a Chinese Grammatical Error Diagnosis system in the NLPTEA-2018 CGED shared task. In 2018, we sent three runs with three different <a href=https://en.wikipedia.org/wiki/Pitch_(sports_field)>approaches</a>. The first one is a pattern-based approach by frequent error pattern matching. The second one is a sequential labelling approach by conditional random fields (CRF). The third one is a rewriting approach by sequence to sequence (seq2seq) model. The three approaches have different properties that aim to optimize different <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> and the formal run results show the differences as we expected.</div></div></div><hr><div id=w18-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-38/>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3800/>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a></strong><br><a href=/people/p/peter-machonis/>Peter Machonis</a>
|
<a href=/people/a/anabela-barreiro/>Anabela Barreiro</a>
|
<a href=/people/k/kristina-kocijan/>Kristina Kocijan</a>
|
<a href=/people/m/max-silberztein/>Max Silberztein</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3804/>Linguistic Resources for Phrasal Verb Identification</a></strong><br><a href=/people/p/peter-machonis/>Peter Machonis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3804><div class="card-body p-3 small">This paper shows how a Lexicon-Grammar dictionary of English phrasal verbs (PV) can be transformed into an electronic dictionary, and with the help of multiple grammars, dictionaries, and filters within the linguistic development environment, NooJ, how to accurately identify PV in large corpora. The NooJ program is an alternative to statistical methods commonly used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : all PV are listed in a dictionary and then located by means of a PV grammar in both continuous and discontinuous format. Results are then refined with a series of <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>, disambiguating grammars, and other linguistics recourses. The main advantage of such a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> is that all PV can be identified in any corpus. The only drawback is that PV not listed in the dictionary (e.g., archaic forms, recent neologisms) are not identified ; however, new PV can easily be added to the electronic dictionary, which is freely available to all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3805/>Designing a Croatian Aspectual Derivatives Dictionary : Preliminary Stages<span class=acl-fixed-case>C</span>roatian Aspectual Derivatives Dictionary: Preliminary Stages</a></strong><br><a href=/people/k/kristina-kocijan/>Kristina Kocijan</a>
|
<a href=/people/k/kresimir-sojat/>Krešimir Šojat</a>
|
<a href=/people/d/dario-poljak/>Dario Poljak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3805><div class="card-body p-3 small">The paper focusses on <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivationally connected verbs</a> in <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, i.e. on verbs that share the same lexical morpheme and are derived from other verbs via <a href=https://en.wikipedia.org/wiki/Prefix>prefixation</a>, <a href=https://en.wikipedia.org/wiki/Suffix>suffixation</a> and/or <a href=https://en.wikipedia.org/wiki/Alternation_(linguistics)>stem alternations</a>. As in other <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a> with rich derivational morphology, each verb is marked for <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, either perfective or imperfective. Some verbs, mostly of foreign origin, are marked as bi-aspectual verbs. The main objective of this paper is to detect and to describe major derivational processes and affixes used in the derivation of aspectually connected verbs with NooJ. Annotated chains are exported into a format adequate for web database system and further used to enhance the aspectual and derivational information for each verb.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3806/>A Rule-Based System for Disambiguating French Locative Verbs and Their Translation into Arabic<span class=acl-fixed-case>F</span>rench Locative Verbs and Their Translation into <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/s/safa-boudhina/>Safa Boudhina</a>
|
<a href=/people/h/hela-fehri/>Héla Fehri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3806><div class="card-body p-3 small">This paper presents a rule-based system for disambiguating frensh locative verbs and their translation to <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>. The disambiguation phase is based on the use of the French Verb dictionary (LVF) of Dubois and Dubois Charlier as a linguistic resource, from which a base of disambiguation rules is extracted. The extracted <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> thus take the form of <a href=https://en.wikipedia.org/wiki/Transducer>transducers</a> which will be subsequently applied to texts. The translation phase consists in translating the disambiguated locative verbs returned by the disambiguation phase. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> takes into account the verb&#8217;s tense used as well as the <a href=https://en.wikipedia.org/wiki/Inflection>inflected form</a> of the verb. This phase is based on <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionaries</a> that contain the different French locative verbs and their translation into the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>. The experimentation and the evaluation are done in the linguistic platform NooJ. The obtained results are satisfactory.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3807/>A Pedagogical Application of NooJ in Language Teaching : The Adjective in Spanish and Italian<span class=acl-fixed-case>N</span>oo<span class=acl-fixed-case>J</span> in Language Teaching: The Adjective in <span class=acl-fixed-case>S</span>panish and <span class=acl-fixed-case>I</span>talian</a></strong><br><a href=/people/a/andrea-rodrigo/>Andrea Rodrigo</a>
|
<a href=/people/m/mario-monteleone/>Mario Monteleone</a>
|
<a href=/people/s/silvia-reyes/>Silvia Reyes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3807><div class="card-body p-3 small">In this paper, a pedagogical application of NooJ to the teaching and learning of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> as a foreign language is presented, which is directed to a specific addressee : learners whose mother tongue is <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. The category &#8216;adjective&#8217; has been chosen on account of its lower frequency of occurrence in texts written in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and particularly in the Argentine Rioplatense variety, and with the aim of developing strategies to increase its use. In addition, the features that the <a href=https://en.wikipedia.org/wiki/Adjective>adjective</a> shares with other <a href=https://en.wikipedia.org/wiki/Grammatical_category>grammatical categories</a> render it extremely productive and provide elements that enrich the learners&#8217; proficiency. The reference corpus contains the front pages of the Argentinian newspaper Clarn related to an emblematic historical moment, whose starting point is 24 March 1976, when a military coup began, and covers a thirty year period until 24 March 2006. It can be seen how the term desaparecido emerges with all its cultural and social charge, providing a context which allows an approach to Rioplatense Spanish from a more comprehensive perspective. Finally, a pedagogical proposal accounting for the application of the NooJ platform in <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> is included.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3808/>STYLUS : A Resource for Systematically Derived Language Usage<span class=acl-fixed-case>STYLUS</span>: A Resource for Systematically Derived Language Usage</a></strong><br><a href=/people/b/bonnie-dorr/>Bonnie Dorr</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3808><div class="card-body p-3 small">We describe a resource derived through extraction of a set of argument realizations from an existing lexical-conceptual structure (LCS) Verb Database of 500 verb classes (containing a total of 9525 verb entries) to include information about realization of arguments for a range of different verb classes. We demonstrate that our extended resource, called STYLUS (SysTematicallY Derived Language USe), enables systematic derivation of regular patterns of language usage without requiring manual annotation. We posit that both spatially oriented applications such as robot navigation and more general applications such as narrative generation require a layered representation scheme where a set of primitives (often grounded in space / motion such as GO) is coupled with a representation of constraints at the syntax-semantics interface. We demonstrate that the resulting resource covers three cases of lexico-semantic operations applicable to both <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3813/>Using <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a> to Compare FrameNet Frames Across Languages<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Frames Across Languages</a></strong><br><a href=/people/j/jennifer-sikos/>Jennifer Sikos</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3813><div class="card-body p-3 small">Much interest in <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame Semantics</a> is fueled by the substantial extent of its applicability across languages. At the same time, lexicographic studies have found that the applicability of individual frames can be diminished by cross-lingual divergences regarding <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, <a href=https://en.wikipedia.org/wiki/Valency_(linguistics)>syntactic valency</a>, and <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a>. Due to the large effort involved in manual investigations, there are so far no broad-coverage resources with problematic frames for any language pair. Our study investigates to what extent multilingual vector representations of frames learned from manually annotated corpora can address this need by serving as a wide coverage source for such divergences. We present a case study for the language pair English German using the FrameNet and SALSA corpora and find that inferences can be made about cross-lingual frame applicability using a vector space model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3814/>Construction of a Multilingual Corpus Annotated with Translation Relations</a></strong><br><a href=/people/y/yuming-zhai/>Yuming Zhai</a>
|
<a href=/people/a/aurelien-max/>Aurélien Max</a>
|
<a href=/people/a/anne-vilnat/>Anne Vilnat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3814><div class="card-body p-3 small">Translation relations, which distinguish <a href=https://en.wikipedia.org/wiki/Literal_translation>literal translation</a> from other <a href=https://en.wikipedia.org/wiki/Translation>translation techniques</a>, constitute an important subject of study for <a href=https://en.wikipedia.org/wiki/Translation>human translators</a> (Chuquet and Paillard, 1989). However, automatic processing techniques based on interlingual relations, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> exploiting translational equivalence, have not exploited these relations explicitly until now. In this work, we present a categorisation of translation relations and annotate them in a parallel multilingual (English, French, Chinese) corpus of oral presentations, the <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED Talks</a>. Our long term objective will be to automatically detect these relations in order to integrate them as important characteristics for the search of monolingual segments in relation of equivalence (paraphrases) or of <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>. The annotated corpus resulting from our work will be made available to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3817/>Enabling Code-Mixed Translation : Parallel Corpus Creation and MT Augmentation Approach<span class=acl-fixed-case>MT</span> Augmentation Approach</a></strong><br><a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3817><div class="card-body p-3 small">Code-mixing, use of two or more languages in a single sentence, is ubiquitous ; generated by multi-lingual speakers across the world. The phenomenon presents itself prominently in social media discourse. Consequently, there is a growing need for translating code-mixed hybrid language into standard languages. However, due to the lack of gold parallel data, existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> fail to properly translate code-mixed text. In an effort to initiate the task of machine translation of code-mixed content, we present a newly created <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> of code-mixed English-Hindi and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We selected previously available English-Hindi code-mixed data as a starting point for the creation of our parallel corpus. We then chose 4 human translators, fluent in both English and Hindi, for translating the 6088 code-mixed English-Hindi sentences to English. With the help of the created parallel corpus, we analyzed the structure of English-Hindi code-mixed data and present a technique to augment run-of-the-mill machine translation (MT) approaches that can help achieve superior translations without the need for specially designed translation systems. We present an augmentation pipeline for existing MT approaches, like Phrase Based MT (Moses) and Neural MT, to improve the translation of code-mixed text.</div></div></div><hr><div id=w18-39><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-39.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-39/>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3900/>Proceedings of the Fifth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2018)</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3902/>Encoder-Decoder Methods for Text Normalization</a></strong><br><a href=/people/m/massimo-lusetti/>Massimo Lusetti</a>
|
<a href=/people/t/tatyana-ruzsics/>Tatyana Ruzsics</a>
|
<a href=/people/a/anne-gohring/>Anne Göhring</a>
|
<a href=/people/t/tanja-samardzic/>Tanja Samardžić</a>
|
<a href=/people/e/elisabeth-stark/>Elisabeth Stark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3902><div class="card-body p-3 small">Text normalization is the task of mapping non-canonical language, typical of <a href=https://en.wikipedia.org/wiki/Speech_transcription>speech transcription</a> and <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>computer-mediated communication</a>, to a standardized writing. It is an up-stream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss German</a>, with strong regional variation and no written standard. Text normalization has been addressed with a variety of methods, most successfully with character-level statistical machine translation (CSMT). In the meantime, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has changed and the new methods, known as neural encoder-decoder (ED) models, resulted in remarkable improvements. Text normalization, however, has not yet followed. A number of <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> have been tried, but <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>CSMT</a> remains the state-of-the-art. In this work, we normalize Swiss German WhatsApp messages using the ED framework. We exploit the flexibility of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, which allows us to learn from the same training data in different ways. In particular, we modify the decoding stage of a plain ED model to include target-side language models operating at different levels of granularity : <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words. Our systematic comparison shows that our approach results in an improvement over the CSMT state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3903.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3903" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3903/>A High Coverage Method for Automatic False Friends Detection for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and Portuguese<span class=acl-fixed-case>F</span>riends Detection for <span class=acl-fixed-case>S</span>panish and <span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/s/santiago-castro/>Santiago Castro</a>
|
<a href=/people/j/jairo-bonanata/>Jairo Bonanata</a>
|
<a href=/people/a/aiala-rosa/>Aiala Rosá</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3903><div class="card-body p-3 small">False friends are words in two languages that look or sound similar, but have different meanings. They are a common source of confusion among <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a>. Methods to detect them automatically do exist, however they make use of large aligned bilingual corpora, which are hard to find and expensive to build, or encounter problems dealing with infrequent words. In this work we propose a high coverage method that uses word vector representations to build a false friends classifier for any pair of languages, which we apply to the particular case of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>. The required resources are a large corpus for each language and a small bilingual lexicon for the pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3905/>Part of Speech Tagging in <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia</a> : A Bantu Macrolanguage<span class=acl-fixed-case>L</span>uyia: A <span class=acl-fixed-case>B</span>antu Macrolanguage</a></strong><br><a href=/people/k/kenneth-steimel/>Kenneth Steimel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3905><div class="card-body p-3 small">Luyia is a <a href=https://en.wikipedia.org/wiki/ISO_639_macrolanguage>macrolanguage</a> in central Kenya. The <a href=https://en.wikipedia.org/wiki/Luyia_languages>Luyia languages</a>, like other <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu languages</a>, have a complex morphological system. This system can be leveraged to aid in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tagging</a>. Bag-of-characters taggers trained on a source <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia language</a> can be applied directly to another <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia language</a> with some degree of success. In addition, mixing data from the target language with data from the source language does produce more accurate predictive models compared to <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on just the target language data when the training set size is small. However, for both of these tagging tasks, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> involving the more distantly related language, <a href=https://en.wikipedia.org/wiki/Tiriki_language>Tiriki</a>, are better at predicting <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tags</a> for Wanga data. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> incorporating <a href=https://en.wikipedia.org/wiki/Bukusu>Bukusu data</a> are not as successful despite the closer relationship between <a href=https://en.wikipedia.org/wiki/Bukusu>Bukusu</a> and <a href=https://en.wikipedia.org/wiki/Wanga_language>Wanga</a>. Overlapping vocabulary between the Wanga and Tiriki corpora as well as a bias towards open class words help <a href=https://en.wikipedia.org/wiki/Tiriki_language>Tiriki</a> outperform <a href=https://en.wikipedia.org/wiki/Bukusu_language>Bukusu</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3907/>Iterative Language Model Adaptation for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan Language Identification</a><span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan Language Identification</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3907><div class="card-body p-3 small">This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan Language Identification shared task of the VarDial 2018 Evaluation Campaign. The shared task was an open one, but we did not use any corpora other than what was distributed by the organizers. A total of eight teams provided results for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>. Our submission using a HeLI-method based language identifier with iterative language model adaptation obtained the best results in the shared task with a macro F1-score of 0.958.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3910" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3910/>Varying image description tasks : spoken versus written descriptions</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/r/ruud-koolen/>Ruud Koolen</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3910><div class="card-body p-3 small">Automatic image description systems are commonly trained and evaluated on written image descriptions. At the same time, these systems are often used to provide <a href=https://en.wikipedia.org/wiki/Linguistic_description>spoken descriptions</a> (e.g. for visually impaired users) through <a href=https://en.wikipedia.org/wiki/Mobile_app>apps</a> like TapTapSee or Seeing AI. This is not a problem, as long as spoken and written descriptions are very similar. However, linguistic research suggests that <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> often differs from <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. These differences are not regular, and vary from context to context. Therefore, this paper investigates whether there are differences between written and spoken image descriptions, even if they are elicited through similar tasks. We compare descriptions produced in two <a href=https://en.wikipedia.org/wiki/Language>languages</a> (English and Dutch), and in both <a href=https://en.wikipedia.org/wiki/Language>languages</a> observe substantial differences between spoken and written descriptions. Future research should see if users prefer the spoken over the written style and, if so, aim to emulate spoken descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3911/>Transfer Learning for British Sign Language Modelling<span class=acl-fixed-case>B</span>ritish <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Modelling</a></strong><br><a href=/people/b/boris-mocialov/>Boris Mocialov</a>
|
<a href=/people/h/helen-hastie/>Helen Hastie</a>
|
<a href=/people/g/graham-turner/>Graham Turner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3911><div class="card-body p-3 small">Automatic speech recognition and <a href=https://en.wikipedia.org/wiki/Speech_recognition>spoken dialogue systems</a> have made great advances through the use of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep machine learning methods</a>. This is partly due to greater computing power but also through the large amount of data available in <a href=https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers>common languages</a>, such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Conversely, research in <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a>, including <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> developed for one language is reused as the starting point for a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on a <a href=https://en.wikipedia.org/wiki/Second_language>second language</a>, which is less resourced. In this paper, we examine two transfer learning techniques of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and layer substitution for language modelling of <a href=https://en.wikipedia.org/wiki/British_Sign_Language>British Sign Language</a>. Our results show improvement in perplexity when using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3913/>Character Level Convolutional Neural Network for Arabic Dialect Identification<span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/m/mohamed-ali/>Mohamed Ali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3913><div class="card-body p-3 small">This submission is for the description paper for our <a href=https://en.wikipedia.org/wiki/System>system</a> in the ADI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3918/>Computationally efficient discrimination between <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> with large feature vectors and regularized classifiers</a></strong><br><a href=/people/a/adrien-barbaresi/>Adrien Barbaresi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3918><div class="card-body p-3 small">The present contribution revolves around efficient approaches to <a href=https://en.wikipedia.org/wiki/Language_classification>language classification</a> which have been field-tested in the Vardial evaluation campaign. The methods used in several language identification tasks comprising different language types are presented and their results are discussed, giving insights on real-world application of <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>, <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifiers</a> and corresponding <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. The use of a specially adapted Ridge classifier proved useful in 2 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> out of 3. The overall approach (XAC) has slightly outperformed most of the other systems on the DFS task (Dutch and Flemish) and on the ILI task (Indo-Aryan languages), while its comparative performance was poorer in on the GDI task (Swiss German dialects).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3922/>Exploring Classifier Combinations for Language Variety Identification</a></strong><br><a href=/people/t/tim-kreutz/>Tim Kreutz</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3922><div class="card-body p-3 small">This paper describes CLiPS&#8217;s submissions for the Discriminating between Dutch and Flemish in Subtitles (DFS) shared task at VarDial 2018. We explore different ways to combine <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature groups</a>. Our best system uses two Linear SVM classifiers ; one trained on lexical features (word n-grams) and one trained on syntactic features (PoS n-grams). The final prediction for a document to be in Flemish Dutch or Netherlandic Dutch is made by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that outputs the highest probability for one of the two labels. This confidence vote approach outperforms a meta-classifier on the <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> and on the <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3927.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3927 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3927 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3927/>Using Neural Transfer Learning for Morpho-syntactic Tagging of South-Slavic Languages Tweets<span class=acl-fixed-case>S</span>outh-<span class=acl-fixed-case>S</span>lavic Languages Tweets</a></strong><br><a href=/people/s/sara-meftah/>Sara Meftah</a>
|
<a href=/people/n/nasredine-semmar/>Nasredine Semmar</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a>
|
<a href=/people/s/stephan-raaijmakers/>Stephan Raaijmakers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3927><div class="card-body p-3 small">In this paper, we describe a morpho-syntactic tagger of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, an important component of the CEA List DeepLIMA tool which is a multilingual text analysis platform based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. This tagger is built for the Morpho-syntactic Tagging of Tweets (MTT) Shared task of the 2018 VarDial Evaluation Campaign. The MTT task focuses on morpho-syntactic annotation of non-canonical Twitter varieties of three <a href=https://en.wikipedia.org/wiki/South_Slavic_languages>South-Slavic languages</a> : <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, Croatian and <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a>. We propose to use a neural network model trained in an end-to-end manner for the three languages without any need for task or domain specific features engineering. The proposed approach combines both character and word level representations. Considering the lack of annotated data in the social media domain for South-Slavic languages, we have also implemented a cross-domain Transfer Learning (TL) approach to exploit any available related out-of-domain annotated data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3928 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3928/>When Simple n-gram Models Outperform Syntactic Approaches : Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and Flemish<span class=acl-fixed-case>D</span>utch and <span class=acl-fixed-case>F</span>lemish</a></strong><br><a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/m/masha-medvedeva/>Masha Medvedeva</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3928><div class="card-body p-3 small">In this paper we present the results of our participation in the Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and <a href=https://en.wikipedia.org/wiki/Flemish>Flemish</a> in Subtitles VarDial 2018 shared task. We try techniques proven to work well for discriminating between language varieties as well as explore the potential of using <a href=https://en.wikipedia.org/wiki/Syntax_(linguistics)>syntactic features</a>, i.e. hierarchical syntactic subtrees. We experiment with different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Discriminating between these two languages turned out to be a very hard task, not only for a machine : human performance is only around 0.51 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> ; our best system is still a simple Naive Bayes model with <a href=https://en.wikipedia.org/wiki/Unigram>word unigrams</a> and <a href=https://en.wikipedia.org/wiki/Bigram>bigrams</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an F1 score (macro) of 0.62, which ranked us 4th in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3930 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3930/>Deep Models for Arabic Dialect Identification on Benchmarked Data<span class=acl-fixed-case>A</span>rabic Dialect Identification on Benchmarked Data</a></strong><br><a href=/people/m/mohamed-elaraby/>Mohamed Elaraby</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3930><div class="card-body p-3 small">The Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011) is a large-scale repos-itory of <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> with manual labels for4varieties of the language. Existing dialect iden-tification models exploiting the dataset pre-date the recent boost <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> brought to NLPand hence the data are not benchmarked for use with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, nor is it clear how much neural networks can help tease the categories in the data apart. We treat these two limitations : We (1) benchmark the data, and (2) empirically test6different deep learning methods on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>thetask</a>, comparing peformance to several classical machine learning models under different condi-tions (i.e., both binary and multi-way classification). Our experimental results show that variantsof (attention-based) bidirectional recurrent neural networks achieve best accuracy (acc) on thetask, significantly outperforming all competitive baselines. On <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test data</a>, our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> reach87.65%acc on the binary task (MSA vs. dialects),87.4%acc on the 3-way dialect task (Egyptianvs. Gulf vs. Levantine), and82.45%acc on the 4-way variants task (MSA vs. Egyptian vs. Gulfvs. Levantine). We release our benchmark for future work on the dataset</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3931 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3931/>A Neural Approach to Language Variety Translation</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3931><div class="card-body p-3 small">In this paper we present the first neural-based machine translation system trained to translate between standard national varieties of the same language. We take the pair <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian-European Portuguese</a> as an example and compare the performance of this method to a phrase-based statistical machine translation system. We report a performance improvement of 0.9 BLEU points in translating from European to Brazilian Portuguese and 0.2 BLEU points when translating in the opposite direction. We also carried out a human evaluation experiment with native speakers of <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> which indicates that humans prefer the output produced by the neural-based system in comparison to the statistical system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3932/>Character Level Convolutional Neural Network for Indo-Aryan Language Identification<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan Language Identification</a></strong><br><a href=/people/m/mohamed-ali/>Mohamed Ali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3932><div class="card-body p-3 small">This submission is a description paper for our <a href=https://en.wikipedia.org/wiki/System>system</a> in ILI shared task</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3933/>German Dialect Identification Using Classifier Ensembles<span class=acl-fixed-case>G</span>erman Dialect Identification Using Classifier Ensembles</a></strong><br><a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3933><div class="card-body p-3 small">In this paper we present the GDI classification entry to the second German Dialect Identification (GDI) shared task organized within the scope of the VarDial Evaluation Campaign 2018. We present a system based on SVM classifier ensembles trained on <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words. The <a href=https://en.wikipedia.org/wiki/System>system</a> was trained on a collection of speech transcripts of five <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss-German dialects</a> provided by the organizers. The transcripts included in the dataset contained speakers from <a href=https://en.wikipedia.org/wiki/Canton_of_Basel-Stadt>Basel</a>, <a href=https://en.wikipedia.org/wiki/Canton_of_Bern>Bern</a>, Lucerne, and <a href=https://en.wikipedia.org/wiki/Canton_of_Z&#252;rich>Zurich</a>. Our entry in the <a href=https://en.wikipedia.org/wiki/Challenge_(competition)>challenge</a> reached 62.03 % F1 score and was ranked third out of eight teams.</div></div></div><hr><div id=w18-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-40.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-40/>Proceedings of the Third Workshop on Semantic Deep Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4000/>Proceedings of the Third Workshop on Semantic Deep Learning</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4002/>Word-Embedding based Content Features for Automated Oral Proficiency Scoring</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/c/chungmin-lee/>Chong Min Lee</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/x/xinhao-wang/>Xinhao Wang</a>
|
<a href=/people/i/ikkyu-choi/>Ikkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4002><div class="card-body p-3 small">In this study, we develop content features for an <a href=https://en.wikipedia.org/wiki/Score_(statistics)>automated scoring system</a> of non-native English speakers&#8217; spontaneous speech. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> calculate the <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure. We explore the impact of these new features on the automated scoring of two different question types : (a) providing opinions on familiar topics and (b) answering a question about a stimulus material. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> showed statistically significant correlations with the oral proficiency scores, and the combination of new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> with the speech-driven features achieved a small but significant further improvement for the latter question type. Further analyses suggested that the new <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were effective in assigning more accurate scores for responses with serious content issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4003/>Automatically Linking Lexical Resources with Word Sense Embedding Models</a></strong><br><a href=/people/l/luis-nieto-pina/>Luis Nieto-Piña</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4003><div class="card-body p-3 small">Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings. The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to formally defined <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>. We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts. We evaluate the applicability of these <a href=https://en.wikipedia.org/wiki/Hyperlink>links</a> in a task to retrieve instances of word sense unlisted in the lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4004/>Transferred Embeddings for Igbo Similarity, <a href=https://en.wikipedia.org/wiki/Analogy>Analogy</a>, and Diacritic Restoration Tasks<span class=acl-fixed-case>I</span>gbo Similarity, Analogy, and Diacritic Restoration Tasks</a></strong><br><a href=/people/i/ignatius-ezeani/>Ignatius Ezeani</a>
|
<a href=/people/i/ikechukwu-onyenwe/>Ikechukwu Onyenwe</a>
|
<a href=/people/m/mark-hepple/>Mark Hepple</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4004><div class="card-body p-3 small">Existing NLP models are mostly trained with data from well-resourced languages. Most <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a> face the challenge of lack of resources-data and technologies-for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>. Building these resources from scratch for each <a href=https://en.wikipedia.org/wiki/Minority_language>minority language</a> will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of <a href=https://en.wikipedia.org/wiki/Analogy>analogy</a>, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4007/>Knowledge Representation and Extraction at Scale</a></strong><br><a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4007><div class="card-body p-3 small">These days, most general knowledge question-answering systems rely on large-scale knowledge bases comprising billions of facts about millions of entities. Having a structured source of semantic knowledge means that we can answer questions involving single static facts (e.g. Who was the 8th president of the US?) or dynamically generated ones (e.g. How old is Donald Trump?). More importantly, we can answer questions involving multiple inference steps (Is the queen older than the president of the US?). In this talk, I&#8217;m going to be discussing some of the unique challenges that are involved with building and maintaining a consistent <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> for <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, extending <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with new facts and using <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to serve answers in multiple languages. I will focus on three recent projects from our group. First, a way of measuring the completeness of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, that is based on usage patterns. The definition of the usage of the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> is done in terms of the relation distribution of entities seen in question-answer logs. Instead of directly estimating the <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation distribution</a> of individual entities, it is generalized to the class signature of each entity. For example, users ask for baseball players&#8217; height, age, and batting average, so a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> is complete (with respect to baseball players) if every entity has facts for those three relations. Second, an investigation into fact extraction from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. I will present a method for creating distant (weak) supervision labels for training a large-scale relation extraction system. I will also discuss the effectiveness of neural network approaches by decoupling the model architecture from the feature design of a state-of-the-art neural network system. Surprisingly, a much simpler <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on similar <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> performs on par with the highly complex neural network system (at 75x reduction to the training time), suggesting that the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are a bigger contributor to the final performance. Finally, I will present the Fact Extraction and VERification (FEVER) dataset and challenge. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprises more than 185,000 human-generated claims extracted from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia pages</a>. False claims were generated by mutating true claims in a variety of ways, some of which were meaningaltering. During the verification step, annotators were required to label a claim for its validity and also supply full-sentence textual evidence from (potentially multiple) Wikipedia articles for the label. With <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a>, we aim to help create a new generation of transparent and interprable knowledge extraction systems.</div></div></div><hr><div id=w18-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-41/>Proceedings of the First International Workshop on Language Cognition and Computational Models</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4100/>Proceedings of the First International Workshop on Language Cognition and Computational Models</a></strong><br><a href=/people/m/manjira-sinha/>Manjira Sinha</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4102/>Detecting Linguistic Traces of Depression in Topic-Restricted Text : Attending to Self-Stigmatized Depression with <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a><span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/jt-wolohan/>JT Wolohan</a>
|
<a href=/people/m/misato-hiraga/>Misato Hiraga</a>
|
<a href=/people/a/atreyee-mukherjee/>Atreyee Mukherjee</a>
|
<a href=/people/z/zeeshan-ali-sayyed/>Zeeshan Ali Sayyed</a>
|
<a href=/people/m/matthew-millard/>Matthew Millard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4102><div class="card-body p-3 small">Natural language processing researchers have proven the ability of machine learning approaches to detect depression-related cues from language ; however, to date, these efforts have primarily assumed it was acceptable to leave depression-related texts in the data. Our concerns with this are twofold : first, that the models may be overfitting on depression-related signals, which may not be present in all depressed users (only those who talk about depression on social media) ; and second, that these models would under-perform for users who are sensitive to the public stigma of depression. This study demonstrates the validity to those concerns. We construct a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of texts</a> from 12,106 Reddit users and perform lexical and predictive analyses under two conditions : one where all text produced by the users is included and one where the depression data is withheld. We find significant differences in the language used by depressed users under the two conditions as well as a difference in the ability of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to correctly detect <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>. However, despite the lexical differences and reduced classification performanceeach of which suggests that users may be able to fool algorithms by avoiding direct discussion of depressiona still respectable overall performance suggests lexical models are reasonably robust and well suited for a role in a diagnostic or monitoring capacity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4103/>An OpenNMT Model to Arabic Broken Plurals<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>NMT</span> Model to <span class=acl-fixed-case>A</span>rabic Broken Plurals</a></strong><br><a href=/people/e/elsayed-issa/>Elsayed Issa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4103><div class="card-body p-3 small">Arabic Broken Plurals show an interesting phenomenon in Arabic morphology as they are formed by shifting the consonants of the syllables into different syllable patterns, and subsequently, the pattern of the word changes. The present paper, therefore, attempts to look at Arabic broken plurals from the perspective of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> by implementing an OpenNMT experiment to better understand and interpret the behavior of these plurals, especially when it comes to L2 acquisition. The results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is successful in predicting the <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic template</a>. However, it fails to predict certain <a href=https://en.wikipedia.org/wiki/Consonant>consonants</a> such as the <a href=https://en.wikipedia.org/wiki/Emphatic_consonant>emphatics</a> and the <a href=https://en.wikipedia.org/wiki/Guttural>gutturals</a>. This reinforces the fact that these <a href=https://en.wikipedia.org/wiki/Consonant>consonants</a> or sounds are the most difficult for L2 learners to acquire.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4104/>Enhancing Cohesion and Coherence of Fake Text to Improve Believability for Deceiving Cyber Attackers</a></strong><br><a href=/people/p/prakruthi-karuna/>Prakruthi Karuna</a>
|
<a href=/people/h/hemant-purohit/>Hemant Purohit</a>
|
<a href=/people/o/ozlem-uzuner/>Özlem Uzuner</a>
|
<a href=/people/s/sushil-jajodia/>Sushil Jajodia</a>
|
<a href=/people/r/rajesh-ganesan/>Rajesh Ganesan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4104><div class="card-body p-3 small">Ever increasing <a href=https://en.wikipedia.org/wiki/Ransomware>ransomware attacks</a> and thefts of intellectual property demand <a href=https://en.wikipedia.org/wiki/Computer_security>cybersecurity solutions</a> to protect critical documents. One emerging solution is to place fake text documents in the repository of critical documents for deceiving and catching cyber attackers. We can generate fake text documents by obscuring the salient information in legit text documents. However, the obscuring process can result in linguistic inconsistencies, such as broken co-references and illogical flow of ideas across the sentences, which can discern the fake document and render it unbelievable. In this paper, we propose a novel method to generate believable fake text documents by automatically improving the linguistic consistency of computer-generated fake text. Our method focuses on enhancing syntactic cohesion and semantic coherence across <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse segments</a>. We conduct experiments with <a href=https://en.wikipedia.org/wiki/Human_subject_research>human subjects</a> to evaluate the effect of believability improvements in distinguishing legit texts from fake texts. Results show that the probability to distinguish legit texts from believable fake texts is consistently lower than from fake texts that have not been improved in believability. This indicates the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in generating believable fake text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4106/>Finite State Reasoning for Presupposition Satisfaction</a></strong><br><a href=/people/j/jacob-collard/>Jacob Collard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4106><div class="card-body p-3 small">Sentences with presuppositions are often treated as uninterpretable or unvalued (neither true nor false) if their presuppositions are not satisfied. However, there is an open question as to how this satisfaction is calculated. In some cases, determining whether a <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition</a> is satisfied is not a trivial task (or even a decidable one), yet native speakers are able to quickly and confidently identify instances of <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition failure</a>. I propose that this can be accounted for with a form of possible world semantics that encapsulates some reasoning abilities, but is limited in its computational power, thus circumventing the need to solve computationally difficult problems. This can be modeled using a variant of the framework of finite state semantics proposed by Rooth (2017). A few modifications to this <a href=https://en.wikipedia.org/wiki/Formal_system>system</a> are necessary, including its extension into a <a href=https://en.wikipedia.org/wiki/Three-valued_logic>three-valued logic</a> to account for <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition</a>. Within this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, the logic necessary to calculate presupposition satisfaction is readily available, but there is no risk of needing exceptional computational power. This correctly predicts that certain presuppositions will not be calculated intuitively, while others can be easily evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4107/>Language-Based Automatic Assessment of Cognitive and Communicative Functions Related to Parkinson’s Disease<span class=acl-fixed-case>P</span>arkinson’s Disease</a></strong><br><a href=/people/l/lesley-jessiman/>Lesley Jessiman</a>
|
<a href=/people/g/gabriel-murray/>Gabriel Murray</a>
|
<a href=/people/m/mckenzie-braley/>McKenzie Braley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4107><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for detecting evidence of Parkinson&#8217;s disease from transcribed speech of subjects who are describing everyday tasks. Experiments reveal the difficulty of treating this as a binary classification task, and a multi-class approach yields superior results. We also show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can be used to predict <a href=https://en.wikipedia.org/wiki/Cognitive_skill>cognitive abilities</a> across all subjects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4109/>Word-word Relations in Dementia and Typical Aging</a></strong><br><a href=/people/n/natalia-arias-trejo/>Natalia Arias-Trejo</a>
|
<a href=/people/a/aline-minto-garcia/>Aline Minto-García</a>
|
<a href=/people/d/diana-i-luna-umanzor/>Diana I. Luna-Umanzor</a>
|
<a href=/people/a/alma-e-rios-ponce/>Alma E. Ríos-Ponce</a>
|
<a href=/people/b/balderas-pliego-mariana/>Balderas-Pliego Mariana</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4109><div class="card-body p-3 small">Older adults tend to suffer a decline in some of their cognitive capabilities, being language one of least affected processes. Word association norms (WAN) also known as free word associations reflect word-word relations, the participant reads or hears a word and is asked to write or say the first word that comes to mind. Free word associations show how the organization of <a href=https://en.wikipedia.org/wiki/Semantic_memory>semantic memory</a> remains almost unchanged with age. We have performed a WAN task with very small samples of older adults with Alzheimer&#8217;s disease (AD), vascular dementia (VaD) and mixed dementia (MxD), and also with a control group of typical aging adults, matched by age, sex and education. All of them are native speakers of <a href=https://en.wikipedia.org/wiki/Mexican_Spanish>Mexican Spanish</a>. The results show, as expected, that <a href=https://en.wikipedia.org/wiki/Alzheimer&#8217;s_disease>Alzheimer disease</a> has a very important impact in lexical retrieval, unlike vascular and mixed dementia. This suggests that linguistic tests elaborated from <a href=https://en.wikipedia.org/wiki/Wide_area_network>WAN</a> can be also used for detecting <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a> at early stages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4110/>Part-of-Speech Annotation of English-Assamese code-mixed texts : Two Approaches<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>A</span>ssamese code-mixed texts: Two Approaches</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/m/manas-jyoti-bora/>Manas Jyoti Bora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4110><div class="card-body p-3 small">In this paper, we discuss the development of a <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagger</a> for English-Assamese code-mixed texts. We provide a comparison of 2 approaches to annotating code-mixed data a) annotation of the texts from the two languages using monolingual resources from each language and b) annotation of the text through a different resource created specifically for code-mixed data. We present a comparative study of the efforts required in each <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> and the final performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. Based on this, we argue that it might be a better approach to develop new technologies using code-mixed data instead of monolingual, &#8216;clean&#8217; data, especially for those languages where we do not have significant tools and technologies available till now.</div></div></div><hr><div id=w18-42><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-42.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-42/>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4200/>Proceedings of the First Workshop on Natural Language Processing for <span class=acl-fixed-case>I</span>nternet Freedom</a></strong><br><a href=/people/c/chris-brew/>Chris Brew</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4203/>Creative Language Encoding under Censorship</a></strong><br><a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4203><div class="card-body p-3 small">People often create obfuscated language for online communication to avoid <a href=https://en.wikipedia.org/wiki/Internet_censorship>Internet censorship</a>, share sensitive information, express strong sentiment or emotion, plan for secret actions, trade illegal products, or simply hold interesting conversations. In this position paper we systematically categorize human-created obfuscated language on various levels, investigate their basic mechanisms, give an overview on automated techniques needed to simulate human encoding. These encoders have potential to frustrate and evade, co-evolve with dynamic human or automated decoders, and produce interesting and adoptable code words. We also summarize remaining challenges for future research on the interaction between Natural Language Processing (NLP) and <a href=https://en.wikipedia.org/wiki/Encryption>encryption</a>, and leveraging NLP techniques for encoding and decoding.</div></div></div><hr><div id=w18-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-43/>Proceedings of the Workshop Events and Stories in the News 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4300/>Proceedings of the Workshop Events and Stories in the News 2018</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/b/ben-miller/>Ben Miller</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/d/david-caswell/>David Caswell</a>
|
<a href=/people/s/susan-windisch-brown/>Susan W. Brown</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4301/>Every Object Tells a Story</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4301><div class="card-body p-3 small">Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with <a href=https://en.wikipedia.org/wiki/Verb>verbs</a> and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of <a href=https://en.wikipedia.org/wiki/Image>images</a>. By semantically grounding <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event descriptions</a> in their visualization, the importance of <a href=https://en.wikipedia.org/wiki/Object-oriented_programming>object-based attributes</a> becomes more apparent. In this position paper, we look at the narrative structure of objects : that is, how objects reference events through their intrinsic attributes, such as <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, purposes, and <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4302/>A Rich Annotation Scheme for Mental Events</a></strong><br><a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/p/pavlina-peskova/>Pavlína Pešková</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/s/sook-kyung-lee/>Sook-kyung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4302><div class="card-body p-3 small">We present a rich annotation scheme for the structure of mental events. Mental events are those in which the verb describes a mental state or process, usually oriented towards an external situation. While <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>physical events</a> have been described in detail and there are numerous studies of their <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, <a href=https://en.wikipedia.org/wiki/Mental_event>mental events</a> are less thoroughly studied. The annotation scheme proposed here is based on decompositional analyses in the semantic and typological linguistic literature. The scheme was applied to the <a href=https://en.wikipedia.org/wiki/Text_corpus>news corpus</a> from the 2016 Events workshop, and <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> of the test annotation provides suggestions for refinement and clarification of the annotation scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4304/>Identifying the Discourse Function of News Article Paragraphs</a></strong><br><a href=/people/w/w-victor-yarlott/>W. Victor Yarlott</a>
|
<a href=/people/c/cristina-cornelio/>Cristina Cornelio</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4304><div class="card-body p-3 small">Discourse structure is a key aspect of all forms of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, providing valuable information both to humans and machines. We applied the hierarchical theory of news discourse developed by van Dijk to examine how paragraphs operate as units of discourse structure within news articleswhat we refer to here as document-level discourse. This document-level discourse provides a characterization of the content of each paragraph that describes its relation to the events presented in the article (such as main events, backgrounds, and consequences) as well as to other components of the story (such as commentary and evaluation). The purpose of a news discourse section is of great utility to story understanding as it affects both the importance and temporal order of items introduced in the texttherefore, if we know the news discourse purpose for different sections, we should be able to better rank events for their importance and better construct timelines. We test two hypotheses : first, that people can reliably annotate news articles with van Dijk&#8217;s theory ; second, that we can reliably predict these labels using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We show that people have a high degree of agreement with each other when annotating the theory (F1 0.8, Cohen&#8217;s kappa 0.6), demonstrating that it can be both learned and reliably applied by human annotators. Additionally, we demonstrate first steps toward <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> of the <a href=https://en.wikipedia.org/wiki/Theory>theory</a>, achieving a performance of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> = 0.54, which is 65 % of human performance. Moreover, we have generated a gold-standard, adjudicated corpus of 50 documents for document-level discourse annotation based on the ACE Phase 2 corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4305/>An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines</a></strong><br><a href=/people/s/shi-yuan/>Shi Yuan</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4305><div class="card-body p-3 small">This study evaluates the performance of four information extraction tools (extractors) on identifying health claims in health news headlines. A <a href=https://en.wikipedia.org/wiki/Health_claim>health claim</a> is defined as a triplet : IV (what is being manipulated), DV (what is being measured) and their relation. Tools that can identify <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a> provide the foundation for evaluating the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of these <a href=https://en.wikipedia.org/wiki/Health_claim>claims</a> against authoritative resources. The evaluation result shows that 26 % headlines do not in-clude <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, and all extractors face difficulty separating them from the rest. For those with <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, OPENIE-5.0 performed the best with <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> at 0.6 level for ex-tracting IV-relation-DV. However, the characteristic linguistic structures in health news headlines, such as <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>incomplete sentences</a> and non-verb relations, pose particular challenge to existing tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4307/>Can You Spot the Semantic Predicate in this Video?</a></strong><br><a href=/people/c/christopher-reale/>Christopher Reale</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/h/heesung-kwon/>Heesung Kwon</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4307><div class="card-body p-3 small">We propose a method to improve human activity recognition in video by leveraging semantic information about the target activities from an expert-defined linguistic resource, <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>. Our hypothesis is that activities that share similar event semantics, as defined by the semantic predicates of <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>, will be more likely to share some visual components. We use a deep convolutional neural network approach as a baseline and incorporate linguistic information from <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We present results of experiments showing the added information has negligible impact on <a href=https://en.wikipedia.org/wiki/Computer_vision>recognition</a> performance. We discuss how this may be because the lexical semantic information defined by <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> is generally not visually salient given the video processing approach used here, and how we may handle this in future approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4309/>On Training Classifiers for Linking Event Templates</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/f/fredi-saric/>Fredi Šarić</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/m/martin-atkinson/>Martin Atkinson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4309><div class="card-body p-3 small">The paper reports on exploring various machine learning techniques and a range of textual and meta-data features to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> for linking related event templates automatically extracted from <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news</a>. With the best <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>textual features</a> only we achieved 94.7 % (92.9 %) <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> on GOLD (SILVER) dataset. These figures were further improved to 98.6 % (GOLD) and 97 % (SILVER) F1 score by adding meta-data features, mainly thanks to the strong discriminatory power of automatically extracted geographical information related to <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4310/>HEI : Hunter Events Interface A <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> based on <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the detection and reasoning about events<span class=acl-fixed-case>HEI</span>: Hunter Events Interface A platform based on services for the detection and reasoning about events</a></strong><br><a href=/people/a/antonio-sorgente/>Antonio Sorgente</a>
|
<a href=/people/a/antonio-calabrese/>Antonio Calabrese</a>
|
<a href=/people/g/gianluca-coda/>Gianluca Coda</a>
|
<a href=/people/p/paolo-vanacore/>Paolo Vanacore</a>
|
<a href=/people/f/francesco-mele/>Francesco Mele</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4310><div class="card-body p-3 small">In this paper we present the definition and implementation of the Hunter Events Interface (HEI) System. The HEI System is a system for events annotation and temporal reasoning in Natural Language Texts and media, mainly oriented to texts of historical and cultural contents available on the Web. In this work we assume that <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> are defined through various components : <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>actions</a>, participants, <a href=https://en.wikipedia.org/wiki/Location>locations</a>, and occurrence intervals. The HEI system, through independent services, locates (annotates) the various components, and successively associates them to a specific event. The objective of this work is to build a system integrating <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the identification of events, the discovery of their connections, and the evaluation of their consistency. We believe this interface is useful to develop applications that use the notion of story, to integrate data of digital cultural archives, and to build systems of fruition in the same field. The HEI system has been partially developed within the TrasTest project</div></div></div><hr><div id=w18-44><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-44.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-44/>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4400/>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (<span class=acl-fixed-case>TRAC</span>-2018)</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4401/>Benchmarking Aggression Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4401><div class="card-body p-3 small">In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-1) at COLING 2018. The task was to develop a <a href=https://en.wikipedia.org/wiki/Social_class>classifier</a> that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> (in both Roman and Devanagari script) and <a href=https://en.wikipedia.org/wiki/English_language>English</a> for training and validation. For testing, two different sets-one from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and another from a different social media-were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> respectively. The results presented in this report depict how challenging the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4402/>RiTUAL-UH at TRAC 2018 Shared Task : Aggression Identification<span class=acl-fixed-case>R</span>i<span class=acl-fixed-case>TUAL</span>-<span class=acl-fixed-case>UH</span> at <span class=acl-fixed-case>TRAC</span> 2018 Shared Task: Aggression Identification</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/d/deepthi-mave/>Deepthi Mave</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4402><div class="card-body p-3 small">This paper presents our <a href=https://en.wikipedia.org/wiki/System>system</a> for TRAC 2018 Shared Task on Aggression Identification. Our best systems for the English dataset use a combination of lexical and semantic features. However, for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi data</a> using only lexical features gave us the best results. We obtained weighted F1-measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4405/>Cyberbullying Intervention Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/j/jianhong-zhang/>Jianhong Zhang</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4405><div class="card-body p-3 small">This paper describes the process of building a cyberbullying intervention interface driven by a machine-learning based text-classification service. We make two main contributions. First, we show that <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> can be identified in real-time before it takes place, with available machine learning and natural language processing tools. Second, we present a mechanism that provides individuals with early feedback about how other people would feel about wording choices in their messages before they are sent out. This <a href=https://en.wikipedia.org/wiki/User_interface>interface</a> not only gives a chance for the user to revise the text, but also provides a system-level flagging / intervention in a situation related to <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4406/>LSTMs with Attention for Aggression Detection<span class=acl-fixed-case>LSTM</span>s with Attention for Aggression Detection</a></strong><br><a href=/people/n/nishant-nikhil/>Nishant Nikhil</a>
|
<a href=/people/r/ramit-pahwa/>Ramit Pahwa</a>
|
<a href=/people/m/mehul-kumar-nirala/>Mehul Kumar Nirala</a>
|
<a href=/people/r/rohan-khilnani/>Rohan Khilnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4406><div class="card-body p-3 small">In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted for the shared task on Aggression Identification in <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments by the team Nishnik. Previous works demonstrate that <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> ranks 17th and 10th in the corresponding English subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4407/>TRAC-1 Shared Task on Aggression Identification : IIT(ISM)@COLING’18<span class=acl-fixed-case>TRAC</span>-1 Shared Task on Aggression Identification: <span class=acl-fixed-case>IIT</span>(<span class=acl-fixed-case>ISM</span>)@<span class=acl-fixed-case>COLING</span>’18</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/g/guggilla-bhanodai/>Guggilla Bhanodai</a>
|
<a href=/people/r/rajendra-pamula/>Rajendra Pamula</a>
|
<a href=/people/m/maheshwar-reddy-chennuru/>Maheshwar Reddy Chennuru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4407><div class="card-body p-3 small">This paper describes the work that our team bhanodaig did at Indian Institute of Technology (ISM) towards TRAC-1 Shared Task on Aggression Identification in Social Media for COLING 2018. In this paper we label aggression identification into three categories : Overtly Aggressive, Covertly Aggressive and Non-aggressive. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. We participated in two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> named as English (Facebook) task and English (Social Media) task. For English (Facebook) task System 05 was our best run (i.e. 0.3572) above the Random Baseline (i.e. 0.3535). For English (Social Media) task our <a href=https://en.wikipedia.org/wiki/System>system</a> 02 got the value (i.e. 0.1960) below the Random Bseline (i.e. 0.3477). For all of our runs we used Long Short-Term Memory model. Overall, our performance is not satisfactory. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4408/>An Ensemble Approach for Aggression Identification in English and Hindi Text<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>H</span>indi Text</a></strong><br><a href=/people/a/arjun-roy/>Arjun Roy</a>
|
<a href=/people/p/prashant-kapil/>Prashant Kapil</a>
|
<a href=/people/k/kingshuk-basak/>Kingshuk Basak</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4408><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted in the shared task at COLING 2018 TRAC-1 : Aggression Identification. The objective of this task was to predict online aggression spread through online textual post or comment. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was released in two languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We submitted a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Both the systems are based on an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble architecture</a> where the individual models are based on <a href=https://en.wikipedia.org/wiki/Convoluted_neural_network>Convoluted Neural Network</a> and <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machine</a>. Evaluation shows promising results for both the languages. The total submission for <a href=https://en.wikipedia.org/wiki/English_language>English</a> was 30 and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> was 15. Our system on <a href=https://en.wikipedia.org/wiki/Facebook>English facebook</a> and social media obtained F1 score of 0.5151 and 0.5099 respectively where <a href=https://en.wikipedia.org/wiki/Facebook>Hindi facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> obtained F1 score of 0.5599 and 0.3790 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4409/>Aggression Identification and Multi Lingual Word Embeddings</a></strong><br><a href=/people/t/thiago-galery/>Thiago Galery</a>
|
<a href=/people/e/efstathios-charitos/>Efstathios Charitos</a>
|
<a href=/people/y/ye-tian/>Ye Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4409><div class="card-body p-3 small">The system presented here took part in the 2018 Trolling, Aggression and Cyberbullying shared task (Forest and Trees team) and uses a Gated Recurrent Neural Network architecture (Cho et al., 2014) in an attempt to assess whether combining pre-trained English and Hindi fastText (Mikolov et al., 2018) word embeddings as a representation of the sequence input would improve classification performance. The motivation for this comes from the fact that the shared task data for <a href=https://en.wikipedia.org/wiki/English_language>English</a> contained many Hindi tokens and therefore some users might be doing <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> : the alternation between two or more languages in communication. To test this hypothesis, we also aligned Hindi and English vectors using pre-computed SVD matrices that pulls representations from different languages into a common space (Smith et al., 2017). Two conditions were tested : (i) one with standard pre-trained fastText word embeddings where each <a href=https://en.wikipedia.org/wiki/Hindi>Hindi word</a> is treated as an OOV token, and (ii) another where word embeddings for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> are loaded in a common vector space, so <a href=https://en.wikipedia.org/wiki/Hindi>Hindi tokens</a> can be assigned a meaningful representation. We submitted the second (i.e., multilingual) <a href=https://en.wikipedia.org/wiki/System>system</a> and obtained the scores of 0.531 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-FB dataset and 0.438 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-TW dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4410/>A K-Competitive Autoencoder for Aggression Detection in Social Media Text</a></strong><br><a href=/people/p/promita-maitra/>Promita Maitra</a>
|
<a href=/people/r/ritesh-sarkhel/>Ritesh Sarkhel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4410><div class="card-body p-3 small">We present an approach to detect <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from social media text in this work. A winner-takes-all autoencoder, called Emoti-KATE is proposed for this purpose. Using a log-normalized, weighted word-count vector at input dimensions, the <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> simulates a competition between neurons in the hidden layer to minimize the reconstruction loss between the input and final output layers. We have evaluated the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> provided by the organizers of TRAC workshop, 2018. Using the <a href=https://en.wikipedia.org/wiki/Code>encoding</a> generated by Emoti-KATE, a 3-way classification is performed for every social media text in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Each data point is classified as &#8216;Overtly Aggressive&#8217;, &#8216;Covertly Aggressive&#8217; or &#8216;Non-aggressive&#8217;. Results show that our (team name : PMRS) proposed method is able to achieve promising results on some of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, we have described the effects of introducing an winner-takes-all autoencoder for the task of aggression detection, reported its performance on four different datasets, analyzed some of its limitations and how to improve its performance in future works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4413/>Degree based Classification of Harmful Speech using Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/s/sanjana-sharma/>Sanjana Sharma</a>
|
<a href=/people/s/saksham-agrawal/>Saksham Agrawal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4413><div class="card-body p-3 small">Harmful speech has various forms and it has been plaguing the <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in different ways. If we need to crackdown different degrees of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets we created based on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological classes</a> and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it&#8217;s automatic detection more robust and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4414/>Aggressive Language Identification Using Word Embeddings and Sentiment Features</a></strong><br><a href=/people/c/constantin-orasan/>Constantin Orăsan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4414><div class="card-body p-3 small">This paper describes our participation in the First Shared Task on Aggression Identification. The method proposed relies on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to identify social media texts which contain <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a>. The main <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> employed by our method are information extracted from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and the output of a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyser</a>. Several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> and different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were tried. The official submissions used <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a> and <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forests</a>. The official evaluation showed that for texts similar to the ones in the training dataset Random Forests work best, whilst for texts which are different SVMs are a better choice. The evaluation also showed that despite its simplicity the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well when compared with more elaborated <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4415/>Aggression Detection in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> using Deep Neural Networks</a></strong><br><a href=/people/s/sreekanth-madisetty/>Sreekanth Madisetty</a>
|
<a href=/people/m/maunendra-sankar-desarkar/>Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4415><div class="card-body p-3 small">With the rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> coupled with almost non-existent <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a> in many such systems, aggressive contents have been observed to rise in such <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>. In this paper, we work on the problem of aggression detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Aggression can sometimes be expressed directly or overtly or it can be hidden or covert in the text. On the other hand, most of the content in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is non-aggressive in nature. We propose an ensemble based system to classify an input post to into one of three classes, namely, Overtly Aggressive, Covertly Aggressive, and Non-aggressive. Our approach uses three deep learning methods, namely, Convolutional Neural Networks (CNN) with five layers (input, convolution, pooling, hidden, and output), Long Short Term Memory networks (LSTM), and Bi-directional Long Short Term Memory networks (Bi-LSTM). A majority voting based ensemble method is used to combine these classifiers (CNN, LSTM, and Bi-LSTM). We trained our method on Facebook comments dataset and tested on Facebook comments (in-domain) and other social media posts (cross-domain). Our system achieves the F1-score (weighted) of 0.604 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and 0.508 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>social media posts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4416/>Merging Datasets for Aggressive Text Identification</a></strong><br><a href=/people/p/paula-fortuna/>Paula Fortuna</a>
|
<a href=/people/j/jose-ferreira/>José Ferreira</a>
|
<a href=/people/l/luiz-pires/>Luiz Pires</a>
|
<a href=/people/g/guilherme-routar/>Guilherme Routar</a>
|
<a href=/people/s/sergio-nunes/>Sérgio Nunes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4416><div class="card-body p-3 small">This paper presents the approach of the team groutar to the shared task on Aggression Identification, considering the test sets in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, both from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and general Social Media. This experiment aims to test the effect of merging new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and apply <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> to data from different <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>, indicating that more generalized models can be an advantage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4417/>Cyberbullying Detection Task : the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1<span class=acl-fixed-case>EBSI</span>-<span class=acl-fixed-case>LIA</span>-<span class=acl-fixed-case>UNAM</span> System (<span class=acl-fixed-case>ELU</span>) at <span class=acl-fixed-case>COLING</span>’18 <span class=acl-fixed-case>TRAC</span>-1</a></strong><br><a href=/people/i/ignacio-arroyo-fernandez/>Ignacio Arroyo-Fernández</a>
|
<a href=/people/d/dominic-forest/>Dominic Forest</a>
|
<a href=/people/j/juan-manuel-torres-moreno/>Juan-Manuel Torres-Moreno</a>
|
<a href=/people/m/mauricio-carrasco-ruiz/>Mauricio Carrasco-Ruiz</a>
|
<a href=/people/t/thomas-legeleux/>Thomas Legeleux</a>
|
<a href=/people/k/karen-joannette/>Karen Joannette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4417><div class="card-body p-3 small">The phenomenon of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> has growing in worrying proportions with the development of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Forums and <a href=https://en.wikipedia.org/wiki/Chat_room>chat rooms</a> are spaces where serious damage can now be done to others, while the tools for avoiding on-line spills are still limited. This study aims to assess the ability that both classical and state-of-the-art vector space modeling methods provide to well known <a href=https://en.wikipedia.org/wiki/Machine_learning>learning machines</a> to identify aggression levels in social network cyberbullying (i.e. social network posts manually labeled as Overtly Aggressive, Covertly Aggressive and Non-aggressive). To this end, an exploratory stage was performed first in order to find relevant settings to test, i.e. by using training and development samples, we trained multiple learning machines using multiple vector space modeling methods and discarded the less informative configurations. Finally, we selected the two best settings and their <a href=https://en.wikipedia.org/wiki/Electoral_system>voting combination</a> to form three competing <a href=https://en.wikipedia.org/wiki/Electoral_system>systems</a>. These systems were submitted to the competition of the TRACK-1 task of the Workshop on Trolling, Aggression and Cyberbullying. Our voting combination system resulted second place in predicting Aggression levels on a test set of untagged social network posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4418/>Aggression Identification Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Data Augmentation</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4418><div class="card-body p-3 small">Social media platforms allow users to share and discuss their opinions online. However, a minority of user posts is aggressive, thereby hinders respectful discussion, and at an extreme level is liable to prosecution. The automatic identification of such harmful posts is important, because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can support the costly manual moderation of online discussions. Further, the <a href=https://en.wikipedia.org/wiki/Automation>automation</a> allows unprecedented analyses of discussion datasets that contain millions of posts. This system description paper presents our submission to the First Shared Task on Aggression Identification. We propose to augment the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to increase the number of labeled comments from 15,000 to 60,000. Thereby, we introduce <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>linguistic variety</a> into the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. As a consequence of the larger amount of training data, we are able to train a special <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural net</a>, which generalizes especially well to unseen data. To further boost the performance, we combine this <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural net</a> with three logistic regression classifiers trained on character and word n-grams, and hand-picked syntactic features. This <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> is more robust than the individual single models. Our team named Julian achieves an F1-score of 60 % on both English datasets, 63 % on the Hindi Facebook dataset, and 38 % on the Hindi Twitter dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4419 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4419/>Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text</a></strong><br><a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4419><div class="card-body p-3 small">In this paper, we propose a novel deep-learning architecture for text classification, named cross segment-and-concatenate multi-task learning (CSC-MTL). We use CSC-MTL to improve the performance of cyber-aggression detection from text. Our approach provides a robust shared feature representation for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> by detecting contrasts and similarities among polarity and neutral classes. We participated in the cyber-aggression shared task under the team name uOttawa. We report 59.74 % F1 performance for the Facebook test set and 56.9 % for the Twitter test set, for detecting <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4420/>Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4420><div class="card-body p-3 small">Comment sections of online news providers have enabled millions to share and discuss their opinions on news topics. Today, moderators ensure respectful and informative discussions by deleting not only insults, <a href=https://en.wikipedia.org/wiki/Defamation>defamation</a>, and hate speech, but also unverifiable facts. This process has to be transparent and comprehensive in order to keep the community engaged. Further, news providers have to make sure to not give the impression of <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> or dissemination of fake news. Yet <a href=https://en.wikipedia.org/wiki/Moderation_system>manual moderation</a> is very expensive and becomes more and more unfeasible with the increasing amount of comments. Hence, we propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles. For evaluation, we present experiments on a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 3 million news comments annotated by a team of professional moderators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4421/>Textual Aggression Detection through <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/a/antonela-tommasel/>Antonela Tommasel</a>
|
<a href=/people/j/juan-manuel-rodriguez/>Juan Manuel Rodriguez</a>
|
<a href=/people/d/daniela-godoy/>Daniela Godoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4421><div class="card-body p-3 small">Cyberbullying and cyberaggression are serious and widespread issues increasingly affecting Internet users. With the widespread of <a href=https://en.wikipedia.org/wiki/Social_media>social media networks</a>, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a>, once limited to particular places, can now occur anytime and anywhere. Cyberaggression refers to aggressive online behaviour that aims at harming other individuals, and involves rude, insulting, offensive, teasing or demoralising comments through <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Considering the dangerous consequences that cyberaggression has on its victims and its rapid spread amongst internet users (specially kids and teens), it is crucial to understand how <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> occurs to prevent <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> from escalating. Given the massive information overload on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, there is an imperious need to develop intelligent techniques to automatically detect harmful content, which would allow the large-scale social media monitoring and early detection of undesired situations. This paper presents the Isistanitos&#8217;s approach for detecting aggressive content in multiple social media sites. The approach is based on combining Support Vector Machines and Recurrent Neural Network models for analysing a wide-range of <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character</a>, <a href=https://en.wikipedia.org/wiki/Word>word</a>, word embeddings, sentiment and irony features. Results confirmed the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> (particularly for detecting covert aggressions), showing the limitations of traditionally used <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4422/>Combining Shallow and Deep Learning for Aggressive Text Detection</a></strong><br><a href=/people/v/viktor-golem/>Viktor Golem</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4422><div class="card-body p-3 small">We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Aggression manifests in a variety of ways. Unlike some forms of <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> that are impossible to prevent in day-to-day life, aggressive speech abounding on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages. The first step in achieving this is to detect such <a href=https://en.wikipedia.org/wiki/Message>messages</a>. The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is further complicated by the noisy nature of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a> on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Our system learns to distinguish between <a href=https://en.wikipedia.org/wiki/Aggression>open aggression</a>, <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>covert aggression</a>, and <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>non-aggression</a> in social media texts. We tried different machine learning approaches, including traditional (shallow) machine learning models, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, and a combination of both. We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4423 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4423/>Filtering Aggression from the Multilingual Social Media Feed</a></strong><br><a href=/people/s/sandip-modha/>Sandip Modha</a>
|
<a href=/people/p/prasenjit-majumder/>Prasenjit Majumder</a>
|
<a href=/people/t/thomas-mandl/>Thomas Mandl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4423><div class="card-body p-3 small">This paper describes the participation of team DA-LD-Hildesheim from the Information Retrieval Lab(IRLAB) at DA-IICT Gandhinagar, India in collaboration with the University of Hildesheim, Germany and LDRP-ITR, Gandhinagar, India in a shared task on Aggression Identification workshop in COLING 2018. The objective of the shared task is to identify the level of aggression from the User-Generated contents within <a href=https://en.wikipedia.org/wiki/Social_media>Social media</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Devanagari>Devnagiri Hindi</a> and Romanized Hindi. Aggression levels are categorized into three predefined classes namely : &#8216;Overtly Aggressive &#8216;, &#8216;Covertly Aggressive &#8216;and &#8216;Non-aggressive &#8216;. The participating teams are required to develop a multi-class classifier which classifies <a href=https://en.wikipedia.org/wiki/User-generated_content>User-generated content</a> into these pre-defined classes. Instead of relying on a <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a>, we have used pre-trained vectors for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. We have performed experiments with standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning classifiers</a>. In addition, we have developed various deep learning models for the multi-class classification problem. Using the <a href=https://en.wikipedia.org/wiki/Data_validation>validation data</a>, we found that validation accuracy of our deep learning models outperform all standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> and voting based ensemble techniques and results on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a> support these findings. We have also found that <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyper-parameters</a> of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> are the keys to improve the results.</div></div></div><hr><div id=w18-45><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-45.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-45/>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4500/>Proceedings of the Second Joint <span class=acl-fixed-case>SIGHUM</span> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4501/>Learning Diachronic Analogies to Analyze Concept Change</a></strong><br><a href=/people/m/matthias-orlikowski/>Matthias Orlikowski</a>
|
<a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4501><div class="card-body p-3 small">We propose to study the evolution of concepts by learning to complete diachronic analogies between lists of terms which relate to the same concept at different points in time. We present a number of models based on operations on word embedddings that correspond to different assumptions about the characteristics of diachronic analogies and change in concept vocabularies. These are tested in a quantitative evaluation for nine different <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> on a <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_the_Netherlands>corpus of Dutch newspapers</a> from the 1950s and 1980s. We show that a model which treats the concept terms as analogous and learns weights to compensate for diachronic changes (weighted linear combination) is able to more accurately predict the missing term than a learned transformation and two baselines for most of the evaluated concepts. We also find that all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> tend to be coherent in relation to the represented concept, but less discriminative in regard to other concepts. Additionally, we evaluate the effect of aligning the time-specific embedding spaces using orthogonal Procrustes, finding varying effects on performance, depending on the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, concept and evaluation metric. For the weighted linear combination, however, results improve with alignment in a majority of cases. All related code is released publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4502/>A Linked Coptic Dictionary Online<span class=acl-fixed-case>C</span>optic Dictionary Online</a></strong><br><a href=/people/f/frank-feder/>Frank Feder</a>
|
<a href=/people/m/maxim-kupreyev/>Maxim Kupreyev</a>
|
<a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/c/caroline-t-schroeder/>Caroline T. Schroeder</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4502><div class="card-body p-3 small">We describe a new project publishing a freely available <a href=https://en.wikipedia.org/wiki/Online_dictionary>online dictionary</a> for <a href=https://en.wikipedia.org/wiki/Coptic_language>Coptic</a>. The dictionary encompasses comprehensive cross-referencing mechanisms, including linking entries to an online scanned edition of Crum&#8217;s Coptic Dictionary, internal cross-references and etymological information, translated searchable definitions in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and linked corpus data which provides frequencies and corpus look-up for headwords and multiword expressions. Headwords are available for linking in external projects using a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST API</a>. We describe the challenges in encoding our <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> using <a href=https://en.wikipedia.org/wiki/Text_Encoding_Initiative>TEI XML</a> and implementing linking mechanisms to construct a Web interface querying frequency information, which draw on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a> to recognize inflected forms in context. We evaluate our <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>&#8217;s coverage using digital corpora of Coptic available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4505/>Analysis of Rhythmic Phrasing : <a href=https://en.wikipedia.org/wiki/Feature_engineering>Feature Engineering</a> vs. <a href=https://en.wikipedia.org/wiki/Representation_learning>Representation Learning</a> for Classifying Readout Poetry</a></strong><br><a href=/people/t/timo-baumann/>Timo Baumann</a>
|
<a href=/people/h/hussein-hussein/>Hussein Hussein</a>
|
<a href=/people/b/burkhard-meyer-sickendiek/>Burkhard Meyer-Sickendiek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4505><div class="card-body p-3 small">We show how to classify the phrasing of readout poems with the help of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> that use manually engineered features or automatically learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We investigate modern and postmodern poems from the webpage lyrikline, and focus on two exemplary rhythmical patterns in order to detect the rhythmic phrasing : The Parlando and the Variable Foot. These rhythmical patterns have been compared by using two important theoretical works : The Generative Theory of Tonal Music and the Rhythmic Phrasing in English Verse. Using both, we focus on a combination of four different features : The grouping structure, the <a href=https://en.wikipedia.org/wiki/Metre_(music)>metrical structure</a>, the time-span-variation, and the prolongation in order to detect the rhythmic phrasing in the two rhythmical types. We use manually engineered features based on <a href=https://en.wikipedia.org/wiki/Speech_recognition>text-speech alignment</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. We also train a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to learn its own <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> based on text, speech and audio during pauses. The <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> outperforms manual feature engineering, reaching an <a href=https://en.wikipedia.org/wiki/F-measure>f-measure</a> of 0.85.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4507/>The Historical Significance of Textual Distances</a></strong><br><a href=/people/t/ted-underwood/>Ted Underwood</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4507><div class="card-body p-3 small">Measuring similarity is a basic task in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, and now often a building-block for more complex arguments about <a href=https://en.wikipedia.org/wiki/Cultural_change>cultural change</a>. But do measures of textual similarity and <a href=https://en.wikipedia.org/wiki/Distance>distance</a> really correspond to evidence about cultural proximity and differentiation? To explore that question empirically, this paper compares textual and social measures of the similarities between genres of English-language fiction. Existing measures of textual similarity (cosine similarity on tf-idf vectors or topic vectors) are also compared to new strategies that strive to anchor textual measurement in a social context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4510/>Normalizing Early English Letters to Present-day English Spelling<span class=acl-fixed-case>E</span>nglish Letters to Present-day <span class=acl-fixed-case>E</span>nglish Spelling</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/t/tanja-saily/>Tanja Säily</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/e/eetu-makela/>Eetu Mäkelä</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4510><div class="card-body p-3 small">This paper presents multiple methods for normalizing the most deviant and infrequent historical spellings in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of <a href=https://en.wikipedia.org/wiki/Letter_(message)>personal correspondence</a> from the 15th to the 19th century. The methods include machine translation (neural and statistical), <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> and rule-based FST. Different <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization methods</a> are compared and evaluated. All of the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> have their own strengths in <a href=https://en.wikipedia.org/wiki/Word_normalization>word normalization</a>. This calls for finding ways of combining the results from these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to leverage their individual strengths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4513/>A Method for Human-Interpretable Paraphrasticality Prediction</a></strong><br><a href=/people/m/maria-moritz/>Maria Moritz</a>
|
<a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/s/sven-buechel/>Sven Büchel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4513><div class="card-body p-3 small">The detection of reused text is important in a wide range of disciplines. However, even as research in the field of <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a> is constantly improving, heavily modified or paraphrased text is still challenging for current <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a>. For historical texts, these problems are even more severe, since <a href=https://en.wikipedia.org/wiki/Source_text>text sources</a> were often subject to stronger and more frequent modifications. Despite the need for tools to automate <a href=https://en.wikipedia.org/wiki/Textual_criticism>text criticism</a>, e.g., tracing modifications in historical text, algorithmic support is still limited. While current techniques can tell if and how frequently a text has been modified, very little work has been done on determining the degree and kind of paraphrastic modificationdespite such information being of substantial interest to scholars. We present a human-interpretable, feature-based method to measure paraphrastic modification. Evaluating our technique on three data sets, we find that our approach performs competitive to text similarity scores borrowed from machine translation evaluation, being much harder to interpret.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4514/>Exploring <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and phonological similarity for the unsupervised correction of language learner errors</a></strong><br><a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4514><div class="card-body p-3 small">The presence of misspellings and other errors or non-standard word forms poses a considerable challenge for NLP systems. Although several <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approaches</a> have been proposed previously to normalize these, annotated training data is scarce for many languages. We investigate, therefore, an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> where correction candidates for Swedish language learners&#8217; errors are retrieved from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Furthermore, we compare the usefulness of combining <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> with orthographic and phonological similarity based on a neural grapheme-to-phoneme conversion system we train for this purpose. Although combinations of similarity measures have been explored for finding error correction candidates, it remains unclear how these <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> relate to each other and how much they contribute individually to identifying the correct alternative. We experiment with different combinations of these and find that integrating <a href=https://en.wikipedia.org/wiki/Phonology>phonological information</a> is especially useful when the majority of learner errors are related to <a href=https://en.wikipedia.org/wiki/Misspelling>misspellings</a>, but less so when errors are of a variety of types including, e.g. grammatical errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4515/>Towards Coreference for <a href=https://en.wikipedia.org/wiki/Literature>Literary Text</a> : Analyzing Domain-Specific Phenomena</a></strong><br><a href=/people/i/ina-roesiger/>Ina Roesiger</a>
|
<a href=/people/s/sarah-schulz/>Sarah Schulz</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4515><div class="card-body p-3 small">Coreference resolution is the task of grouping together references to the same discourse entity. Resolving <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> in literary texts could benefit a number of Digital Humanities (DH) tasks, such as analyzing the depiction of characters and/or their relations. Domain-dependent training data has shown to improve <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> for many domains, e.g. the biomedical domain, as its properties differ significantly from <a href=https://en.wikipedia.org/wiki/News>news text</a> or <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, on which <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> are typically trained. Literary texts could also benefit from corpora annotated with <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>. We therefore analyze the specific properties of coreference-related phenomena on a number of texts and give directions for the adaptation of annotation guidelines. As some of the adaptations have profound impact, we also present a new annotation tool for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, with a focus on enabling annotation of long texts with many discourse entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4516/>An Evaluation of Lexicon-based Sentiment Analysis Techniques for the Plays of Gotthold Ephraim Lessing<span class=acl-fixed-case>G</span>otthold <span class=acl-fixed-case>E</span>phraim <span class=acl-fixed-case>L</span>essing</a></strong><br><a href=/people/t/thomas-schmidt/>Thomas Schmidt</a>
|
<a href=/people/m/manuel-burghardt/>Manuel Burghardt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4516><div class="card-body p-3 small">We present results from a project in the research area of sentiment analysis of drama texts, more concretely the plays of Gotthold Ephraim Lessing. We conducted an annotation study to create a gold standard for a systematic evaluation. The <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> consists of 200 speeches of Lessing&#8217;s plays manually annotated with <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>. We explore the performance of different German sentiment lexicons and processing configurations like <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>, the extension of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> with historical linguistic variants or stop words elimination to explore the influence of these parameters and find best practices for our domain of application. The best performing configuration accomplishes an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 70 %. We discuss the problems and challenges for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in this area and describe our next steps toward further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4518/>Induction of a Large-Scale Knowledge Graph from the Regesta Imperii<span class=acl-fixed-case>R</span>egesta <span class=acl-fixed-case>I</span>mperii</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/l/leo-born/>Leo Born</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4518><div class="card-body p-3 small">We induce and visualize a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graph</a> over the Regesta Imperii (RI), an important large-scale resource for medieval history research. The RI comprise more than 150,000 digitized abstracts of medieval charters issued by the Roman-German kings and popes distributed over many European locations and a time span of more than 700 years. Our goal is to provide a resource for historians to visualize and query the RI, possibly aiding medieval history research. The resulting medieval graph and visualization tools are shared publicly.</div></div></div><hr><div id=w18-46><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-46.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-46/>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4600/>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</a></strong><br><a href=/people/l/leonor-becerra-bonache/>Leonor Becerra-Bonache</a>
|
<a href=/people/m/m-dolores-jimenez-lopez/>M. Dolores Jiménez-López</a>
|
<a href=/people/c/carlos-martin-vide/>Carlos Martín-Vide</a>
|
<a href=/people/a/adria-torrens-urrutia/>Adrià Torrens-Urrutia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4603/>Modeling Violations of Selectional Restrictions with Distributional Semantics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/adria-torrens-urrutia/>Adrià Torrens Urrutia</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4603><div class="card-body p-3 small">Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention : it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4606/>Investigating the importance of linguistic complexity features across different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> related to <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a></a></strong><br><a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4606><div class="card-body p-3 small">We present the results of our investigations aiming at identifying the most informative linguistic complexity features for classifying language learning levels in three different datasets. The <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> vary across two dimensions : the size of the instances (texts vs. sentences) and the language learning skill they involve (reading comprehension texts vs. texts written by learners themselves). We present a subset of the most predictive features for each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, taking into consideration significant differences in their per-class mean values and show that these subsets lead not only to simpler <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, but also to an improved <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. Furthermore, we pinpoint fourteen central features that are good predictors regardless of the size of the linguistic unit analyzed or the skills involved, which include both morpho-syntactic and lexical dimensions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4607/>An Approach to Measuring <a href=https://en.wikipedia.org/wiki/Complexity>Complexity</a> with a Fuzzy Grammar & Degrees of Grammaticality</a></strong><br><a href=/people/a/adria-torrens-urrutia/>Adrià Torrens Urrutia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4607><div class="card-body p-3 small">This paper presents an approach to evaluate <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of a given <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language input</a> by means of a Fuzzy Grammar with some <a href=https://en.wikipedia.org/wiki/Fuzzy_logic>fuzzy logic formulations</a>. Usually, the approaches in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> has described a natural language grammar by means of discrete terms. However, a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> can be explained in terms of degrees by following the concepts of linguistic gradience & fuzziness. Understanding a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> as a fuzzy or gradient object allows us to establish degrees of grammaticality for every linguistic input. This shall be meaningful for linguistic complexity considering that the less grammatical an input is the more complex its processing will be. In this regard, the degree of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of a linguistic input (which is a linguistic representation of a natural language expression) depends on the chosen <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>. The bases of the fuzzy grammar are shown here. Some of these are described by Fuzzy Type Theory. The linguistic inputs are characterized by constraints through a Property Grammar.</div></div></div><hr><div id=w18-47><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-47.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-47/>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4700/>Proceedings 14th Joint <span class=acl-fixed-case>ACL</span> - <span class=acl-fixed-case>ISO</span> Workshop on Interoperable Semantic Annotation</a></strong><br><a href=/people/h/harry-bunt/>Harry Bunt</a></span></p></div><hr><div id=w18-48><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-48.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-48/>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4800/>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a></strong><br><a href=/people/j/judith-l-klavans/>Judith L. Klavans</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4802/>A Neural Morphological Analyzer for Arapaho Verbs Learned from a <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite State Transducer</a><span class=acl-fixed-case>A</span>rapaho Verbs Learned from a Finite State Transducer</a></strong><br><a href=/people/s/sarah-moeller/>Sarah Moeller</a>
|
<a href=/people/g/ghazaleh-kazeminejad/>Ghazaleh Kazeminejad</a>
|
<a href=/people/a/andrew-cowell/>Andrew Cowell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4802><div class="card-body p-3 small">We experiment with training an encoder-decoder neural model for mimicking the behavior of an existing hand-written finite-state morphological grammar for <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho verbs</a>, a <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic language</a> with a highly complex verbal inflection system. After adjusting for ambiguous parses, we find that the <a href=https://en.wikipedia.org/wiki/System>system</a> is able to generalize to unseen forms with accuracies of 98.68 % (unambiguous verbs) and 92.90 % (all verbs).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4805/>Natural Language Generation for Polysynthetic Languages : Language Teaching and Learning Software for Kanyen’kha (Mohawk)<span class=acl-fixed-case>K</span>anyen’kéha (<span class=acl-fixed-case>M</span>ohawk)</a></strong><br><a href=/people/g/greg-lessard/>Greg Lessard</a>
|
<a href=/people/n/nathan-brinklow/>Nathan Brinklow</a>
|
<a href=/people/m/michael-levison/>Michael Levison</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4805><div class="card-body p-3 small">Kanyen&#8217;kha (in English, Mohawk) is an <a href=https://en.wikipedia.org/wiki/Iroquoian_languages>Iroquoian language</a> spoken primarily in Eastern Canada (Ontario, Qubec). Classified as endangered, <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> has only a small number of speakers and very few younger native speakers. Consequently, teachers and courses, teaching materials and <a href=https://en.wikipedia.org/wiki/Software>software</a> are urgently needed. In the case of <a href=https://en.wikipedia.org/wiki/Software>software</a>, the polysynthetic nature of Kanyen&#8217;kha means that the number of possible combinations grows exponentially and soon surpasses attempts to capture variant forms by hand. It is in this context that we describe an attempt to produce <a href=https://en.wikipedia.org/wiki/Language_pedagogy>language teaching materials</a> based on a generative approach. A natural language generation environment (ivi / Vinci) embedded in a web environment (VinciLingua) makes it possible to produce, by rule, variant forms of indefinite complexity. These may be used as models to explore, or as materials to which learners respond. Generated materials may take the form of written text, oral utterances, or images ; responses may be typed on a keyboard, gestural (using a mouse) or, to a limited extent, oral. The <a href=https://en.wikipedia.org/wiki/Software>software</a> also provides complex orthographic, morphological and syntactic analysis of learner productions. We describe the trajectory of development of materials for a suite of four courses on Kanyen&#8217;kha, the first of which will be taught in the fall of 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4808/>Lost in Translation : Analysis of Information Loss During <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> Between Polysynthetic and Fusional Languages</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/e/elisabeth-maier/>Elisabeth Mager</a>
|
<a href=/people/a/alfonso-medina-urrea/>Alfonso Medina-Urrea</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4808><div class="card-body p-3 small">Machine translation from polysynthetic to fusional languages is a challenging task, which gets further complicated by the limited amount of parallel text available. Thus, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance is far from the state of the art for high-resource and more intensively studied language pairs. To shed light on the phenomena which hamper automatic translation to and from <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a>, we study translations from three low-resource, polysynthetic languages (Nahuatl, Wixarika and Yorem Nokki) into <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and vice versa. Doing so, we find that in a morpheme-to-morpheme alignment an important amount of information contained in polysynthetic morphemes has no Spanish counterpart, and its translation is often omitted. We further conduct a qualitative analysis and, thus, identify morpheme types that are commonly hard to align or ignored in the translation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4809/>Automatic Glossing in a Low-Resource Setting for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a></a></strong><br><a href=/people/s/sarah-moeller/>Sarah Moeller</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4809><div class="card-body p-3 small">Morphological analysis of morphologically rich and low-resource languages is important to both <a href=https://en.wikipedia.org/wiki/Linguistic_description>descriptive linguistics</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such <a href=https://en.wikipedia.org/wiki/Data>data</a> is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a <a href=https://en.wikipedia.org/wiki/Nakh_languages>Nakh-Daghestanian language</a>, <a href=https://en.wikipedia.org/wiki/Lezgi_language>Lezgi</a>, from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3,000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich, endangered languages.</div></div></div><hr><div id=w18-49><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-49.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-49/>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4900/>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (<span class=acl-fixed-case>LAW</span>-<span class=acl-fixed-case>MWE</span>-<span class=acl-fixed-case>C</span>x<span class=acl-fixed-case>G</span>-2018)</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/m/melanie-andresen/>Melanie Andresen</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/m/miriam-r-l-petruck/>Miriam R. L. Petruck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4902/>From <a href=https://en.wikipedia.org/wiki/Lexical_functional_grammar>Lexical Functional Grammar</a> to Enhanced Universal Dependencies<span class=acl-fixed-case>L</span>exical <span class=acl-fixed-case>F</span>unctional <span class=acl-fixed-case>G</span>rammar to Enhanced <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/a/adam-przepiorkowski/>Adam Przepiórkowski</a>
|
<a href=/people/a/agnieszka-patejuk/>Agnieszka Patejuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4902><div class="card-body p-3 small">This is a summary of an invited talk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4906/>Discourse and Lexicons : Lexemes, MWEs, Grammatical Constructions and Compositional Word Combinations to Signal Discourse Relations<span class=acl-fixed-case>MWE</span>s, Grammatical Constructions and Compositional Word Combinations to Signal Discourse Relations</a></strong><br><a href=/people/l/laurence-danlos/>Laurence Danlos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4906><div class="card-body p-3 small">Lexicons generally record a list of <a href=https://en.wikipedia.org/wiki/Lexeme>lexemes</a> or non-compositional multiword expressions. We propose to build <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> for compositional word combinations, namely secondary discourse connectives. Secondary discourse connectives play the same function as primary discourse connectives but the latter are either lexemes or non-compositional multiword expressions. The paper defines primary and secondary connectives, and explains why it is possible to build a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> for the compositional ones and how <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> could be organized. It also puts forward the utility of such a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> in discourse annotation and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Finally, it opens the discussion on the constructions that signal a <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> between two spans of text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4907/>From Chinese Word Segmentation to Extraction of Constructions : Two Sides of the Same Algorithmic Coin<span class=acl-fixed-case>C</span>hinese Word Segmentation to Extraction of Constructions: Two Sides of the Same Algorithmic Coin</a></strong><br><a href=/people/j/jean-pierre-colson/>Jean-Pierre Colson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4907><div class="card-body p-3 small">This paper presents the results of two experiments carried out within the framework of computational construction grammar. Starting from the constructionist point of view that there are just constructions in language, including lexical ones, we tested the validity of a clustering algorithm that was primarily designed for MWE extraction, the cpr-score (Colson, 2017), on Chinese word segmentation. Our results indicate a striking recall rate of 75 percent without any special adaptation to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> or to the lexicon, which confirms that there is some similarity between extracting MWEs and CWS. Our second experiment also suggests that the same <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> might be used for extracting more schematic or abstract constructions, thereby providing evidence for the statistical foundation of <a href=https://en.wikipedia.org/wiki/Construction_grammar>construction grammar</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4908/>Fixed Similes : Measuring aspects of the relation between MWE idiomatic semantics and syntactic flexibility<span class=acl-fixed-case>MWE</span> idiomatic semantics and syntactic flexibility</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/p/panagiotis-kouris/>Panagiotis Kouris</a>
|
<a href=/people/y/yanis-maistros/>Yanis Maistros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4908><div class="card-body p-3 small">We shed light on aspects of the relation between the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and the syntactic flexibility of multiword expressions by investigating fixed adjective similes (FS), a predicative multiword expression class not studied in this respect before. We find that only a subset of the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> observed in the data are related with <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>. We identify and measure two aspects of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>, one of which seems to allow for predictions about FS syntactic flexibility. Our research draws on a resource developed with the semantic and detailed syntactic annotation of web-retrieved Modern Greek material, indicating frequency of use of the individual similes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4909/>Fine-Grained Termhood Prediction for German Compound Terms Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a><span class=acl-fixed-case>G</span>erman Compound Terms Using Neural Networks</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4909><div class="card-body p-3 small">Automatic term identification and investigating the understandability of terms in a specialized domain are often treated as two separate lines of research. We propose a combined approach for this matter, by defining fine-grained classes of termhood and framing a classification task. The <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> reflect tiers of a term&#8217;s association to a domain. The new setup is applied to German closed compounds as term candidates in the domain of cooking. For the prediction of the classes, we compare several neural network architectures and also take salient information about the compounds&#8217; components into account. We show that applying a similar class distinction to the compounds&#8217; components and propagating this information within the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> improves the compound class prediction results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4910/>Towards a Computational Lexicon for Moroccan Darija : <a href=https://en.wikipedia.org/wiki/Word>Words</a>, <a href=https://en.wikipedia.org/wiki/Idiom>Idioms</a>, and Constructions<span class=acl-fixed-case>M</span>oroccan <span class=acl-fixed-case>D</span>arija: Words, Idioms, and Constructions</a></strong><br><a href=/people/j/jamal-laoudi/>Jamal Laoudi</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/s/stephen-tratz/>Stephen Tratz</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4910><div class="card-body p-3 small">In this paper, we explore the challenges of building a computational lexicon for Moroccan Darija (MD), an <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialect</a> spoken by over 32 million people worldwide but which only recently has begun appearing frequently in written form in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We raise the question of what belongs in such a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> and start by describing our work building traditional word-level lexicon entries with their English translations. We then discuss challenges in translating idiomatic MD text that led to creating multi-word expression lexicon entries whose meanings could not be fully derived from the individual words. Finally, we provide a preliminary exploration of <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructions</a> to be considered for inclusion in an MD constructicon by translating examples of English constructions and examining their MD counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4911/>Verbal Multiword Expressions in <a href=https://en.wikipedia.org/wiki/Basque_language>Basque Corpora</a><span class=acl-fixed-case>B</span>asque Corpora</a></strong><br><a href=/people/u/uxoa-inurrieta/>Uxoa Iñurrieta</a>
|
<a href=/people/i/itziar-aduriz/>Itziar Aduriz</a>
|
<a href=/people/a/ainara-estarrona/>Ainara Estarrona</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a>
|
<a href=/people/a/antton-gurrutxaga/>Antton Gurrutxaga</a>
|
<a href=/people/r/ruben-urizar/>Ruben Urizar</a>
|
<a href=/people/i/inaki-alegria/>Iñaki Alegria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4911><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Basque_language>Basque corpus</a> where Verbal Multiword Expressions (VMWEs) were annotated following universal guidelines. Information on the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> is given, and some ideas for discussion upon the guidelines are also proposed. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is useful not only for NLP-related research, but also to draw conclusions on Basque phraseology in comparison with other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4912/>Annotation of Tense and Aspect Semantics for Sentential AMR<span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4912><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/English_grammar>English grammar</a> encodes a number of semantic contrasts with <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect marking</a>, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a> and applications that require sophisticated reasoning about time and <a href=https://en.wikipedia.org/wiki/Event_(computing)>event structure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4913/>A Syntax-Based Scheme for the Annotation and Segmentation of German Spoken Language Interactions<span class=acl-fixed-case>G</span>erman Spoken Language Interactions</a></strong><br><a href=/people/s/swantje-westpfahl/>Swantje Westpfahl</a>
|
<a href=/people/j/jan-gorisch/>Jan Gorisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4913><div class="card-body p-3 small">Unlike corpora of written language where segmentation can mainly be derived from orthographic punctuation marks, the basis for segmenting spoken language corpora is not predetermined by the primary data, but rather has to be established by the corpus compilers. This impedes consistent querying and visualization of such <a href=https://en.wikipedia.org/wiki/Data>data</a>. Several ways of <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segmenting</a> have been proposed, some of which are based on <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. In this study, we developed and evaluated annotation and segmentation guidelines in reference to the topological field model for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We can show that these <a href=https://en.wikipedia.org/wiki/Guideline>guidelines</a> are used consistently across annotators. We also investigated the influence of various interactional settings with a rather simple <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>, the word-count per segment and unit-type. We observed that the <a href=https://en.wikipedia.org/wiki/Word_count>word count</a> and the distribution of each unit type differ in varying interactional settings and that our developed segmentation and annotation guidelines are used consistently across annotators. In conclusion, our syntax-based segmentations reflect interactional properties that are intrinsic to the <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a> that participants are involved in. This can be used for further analysis of <a href=https://en.wikipedia.org/wiki/Social_relation>social interaction</a> and opens the possibility for automatic segmentation of transcripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4916/>A Treebank for the Healthcare Domain</a></strong><br><a href=/people/n/nganthoibi-oinam/>Nganthoibi Oinam</a>
|
<a href=/people/d/diwakar-mishra/>Diwakar Mishra</a>
|
<a href=/people/p/pinal-patel/>Pinal Patel</a>
|
<a href=/people/n/narayan-choudhary/>Narayan Choudhary</a>
|
<a href=/people/h/hitesh-desai/>Hitesh Desai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4916><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> for the healthcare domain developed at ezDI. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is created from a wide array of clinical health record documents across hospitals. The <a href=https://en.wikipedia.org/wiki/Data>data</a> has been de-identified and annotated for constituent syntactic structure. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> contains a total of 52053 sentences that have been sampled for subdomains as well as <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variations</a>. The paper outlines the sampling process followed to ensure a better domain representation in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, the annotation process and challenges, and <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus statistics</a>. The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> created was used to re-train the Berkeley parser and the Stanford parser. These <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> were also trained with the GENIA treebank for comparative quality assessment. Our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> yielded great-er accuracy on both <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. Berkeley parser performed better on our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> with an average F1 measure of 91 across 5-folds. This was a significant jump from the out-of-the-box F1 score of 70 on Berkeley parser&#8217;s default grammar.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4918/>All Roads Lead to UD : Converting Stanford and Penn Parses to English Universal Dependencies with Multilayer Annotations<span class=acl-fixed-case>UD</span>: Converting <span class=acl-fixed-case>S</span>tanford and <span class=acl-fixed-case>P</span>enn Parses to <span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies with Multilayer Annotations</a></strong><br><a href=/people/s/siyao-peng/>Siyao Peng</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4918><div class="card-body p-3 small">We describe and evaluate different approaches to the conversion of gold standard corpus data from Stanford Typed Dependencies (SD) and Penn-style constituent trees to the latest English Universal Dependencies representation (UD 2.2). Our results indicate that pure SD to UD conversion is highly accurate across multiple genres, resulting in around 1.5 % errors, but can be improved further to fewer than 0.5 % errors given access to annotations beyond the pure syntax tree, such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a> and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, which are necessary for correct generation of several UD relations. We show that constituent-based conversion using CoreNLP (with automatic NER) performs substantially worse in all genres, including when using gold constituent trees, primarily due to underspecification of phrasal grammatical functions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4921.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4921 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4921 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4921/>Constructing an Annotated Corpus of Verbal MWEs for English<span class=acl-fixed-case>MWE</span>s for <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/abigail-walsh/>Abigail Walsh</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/k/kristina-geeraert/>Kristina Geeraert</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/c/clarissa-somers/>Clarissa Somers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4921><div class="card-body p-3 small">This paper describes the construction and annotation of a corpus of verbal MWEs for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, as part of the PARSEME Shared Task 1.1 on automatic identification of verbal MWEs. The criteria for corpus selection, the categories of MWEs used, and the training process are discussed, along with the particular issues that led to revisions in edition 1.1 of the annotation guidelines. Finally, an overview of the characteristics of the final annotated corpus is presented, as well as some discussion on <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4922/>Cooperating Tools for MWE Lexicon Management and Corpus Annotation<span class=acl-fixed-case>MWE</span> Lexicon Management and Corpus Annotation</a></strong><br><a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/a/akihiko-kato/>Akihiko Kato</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/t/toshio-morita/>Toshio Morita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4922><div class="card-body p-3 small">We present tools for lexicon and corpus management that offer cooperating functionality in corpus annotation. The former, named Cradle, stores a set of words and expressions where multi-word expressions are defined with their own part-of-speech information and internal syntactic structures. The latter, named ChaKi, manages <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> with part-of-speech (POS) and syntactic dependency structure annotations. Those two tools cooperate so that the words and multi-word expressions stored in Cradle are directly referred to by ChaKi in conducting corpus annotation, and the words and expressions annotated in ChaKi can be output as a list of lexical entities that are to be stored in Cradle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4923 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4923/>Fingers in the Nose : Evaluating Speakers’ Identification of Multi-Word Expressions Using a Slightly Gamified Crowdsourcing Platform</a></strong><br><a href=/people/k/karen-fort/>Karën Fort</a>
|
<a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/m/matthieu-constant/>Matthieu Constant</a>
|
<a href=/people/n/nicolas-lefebvre/>Nicolas Lefèbvre</a>
|
<a href=/people/y/yann-alan-pilatte/>Yann-Alan Pilatte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4923><div class="card-body p-3 small">This article presents the results we obtained in crowdsourcing French speakers&#8217; intuition concerning multi-work expressions (MWEs). We developed a slightly gamified crowdsourcing platform, part of which is designed to test users&#8217; ability to identify MWEs with no prior training. The participants perform relatively well at the task, with a <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> reaching 65 % for MWEs that do not behave as <a href=https://en.wikipedia.org/wiki/Function_word>function words</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4925.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4925 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4925 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4925/>Edition 1.1 of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions<span class=acl-fixed-case>PARSEME</span> Shared Task on Automatic Identification of Verbal Multiword Expressions</a></strong><br><a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/s/silvio-cordeiro/>Silvio Ricardo Cordeiro</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/m/maja-buljan/>Maja Buljan</a>
|
<a href=/people/m/marie-candito/>Marie Candito</a>
|
<a href=/people/p/polona-gantar/>Polona Gantar</a>
|
<a href=/people/v/voula-giouli/>Voula Giouli</a>
|
<a href=/people/t/tunga-gungor/>Tunga Güngör</a>
|
<a href=/people/a/abdelati-hawwari/>Abdelati Hawwari</a>
|
<a href=/people/u/uxoa-inurrieta/>Uxoa Iñurrieta</a>
|
<a href=/people/j/jolanta-kovalevskaite/>Jolanta Kovalevskaitė</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/t/timm-lichte/>Timm Lichte</a>
|
<a href=/people/c/chaya-liebeskind/>Chaya Liebeskind</a>
|
<a href=/people/j/johanna-monti/>Johanna Monti</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra Escartín</a>
|
<a href=/people/b/behrang-qasemizadeh/>Behrang QasemiZadeh</a>
|
<a href=/people/r/renata-ramisch/>Renata Ramisch</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a>
|
<a href=/people/a/ashwini-vaidya/>Ashwini Vaidya</a>
|
<a href=/people/a/abigail-walsh/>Abigail Walsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4925><div class="card-body p-3 small">This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multiword expressions. We present the annotation methodology, focusing on changes from last year&#8217;s shared task. Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings. Corpora were created for 20 languages, which are also briefly discussed. We report organizational principles behind the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a> and the <a href=https://en.wikipedia.org/wiki/Performance_metric>evaluation metrics</a> employed for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>. The 17 participating <a href=https://en.wikipedia.org/wiki/System>systems</a>, their methods and obtained results are also presented and analysed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4926.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4926 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4926 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4926" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4926/>CRF-Seq and CRF-DepTree at PARSEME Shared Task 2018 : Detecting Verbal MWEs using Sequential and Dependency-Based Approaches<span class=acl-fixed-case>CRF</span>-Seq and <span class=acl-fixed-case>CRF</span>-<span class=acl-fixed-case>D</span>ep<span class=acl-fixed-case>T</span>ree at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Detecting Verbal <span class=acl-fixed-case>MWE</span>s using Sequential and Dependency-Based Approaches</a></strong><br><a href=/people/e/erwan-moreau/>Erwan Moreau</a>
|
<a href=/people/a/ashjan-alsulaimani/>Ashjan Alsulaimani</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4926><div class="card-body p-3 small">This paper describes two systems for detecting Verbal Multiword Expressions (VMWEs) which both competed in the closed track at the PARSEME VMWE Shared Task 2018. CRF-DepTree-categs implements an approach based on the dependency tree, intended to exploit the syntactic and semantic relations between tokens ; CRF-Seq-nocategs implements a robust sequential method which requires only lemmas and morphosyntactic tags. Both <a href=https://en.wikipedia.org/wiki/System>systems</a> ranked in the top half of the ranking, the latter ranking second for token-based evaluation. The code for both <a href=https://en.wikipedia.org/wiki/System>systems</a> is published under the GNU General Public License version 3.0 and is available at.<url>http://github.com/erwanm/adapt-vmwe18</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4927.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4927 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4927 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4927" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4927/>Deep-BGT at PARSEME Shared Task 2018 : Bidirectional LSTM-CRF Model for Verbal Multiword Expression Identification<span class=acl-fixed-case>BGT</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Bidirectional <span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model for Verbal Multiword Expression Identification</a></strong><br><a href=/people/g/gozde-berk/>Gözde Berk</a>
|
<a href=/people/b/berna-erden/>Berna Erden</a>
|
<a href=/people/t/tunga-gungor/>Tunga Güngör</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4927><div class="card-body p-3 small">This paper describes the Deep-BGT system that participated to the PARSEME shared task 2018 on automatic identification of verbal multiword expressions (VMWEs). Our system is language-independent and uses the bidirectional Long Short-Term Memory model with a Conditional Random Field layer on top (bidirectional LSTM-CRF). To the best of our knowledge, this paper is the first one that employs the bidirectional LSTM-CRF model for VMWE identification. Furthermore, the gappy 1-level tagging scheme is used for discontiguity and overlaps. Our system was evaluated on 10 languages in the open track and it was ranked the second in terms of the general ranking metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4929.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4929 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4929 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4929/>Mumpitz at PARSEME Shared Task 2018 : A Bidirectional LSTM for the Identification of Verbal Multiword Expressions<span class=acl-fixed-case>M</span>umpitz at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: A Bidirectional <span class=acl-fixed-case>LSTM</span> for the Identification of Verbal Multiword Expressions</a></strong><br><a href=/people/r/rafael-ehren/>Rafael Ehren</a>
|
<a href=/people/t/timm-lichte/>Timm Lichte</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4929><div class="card-body p-3 small">In this paper, we describe Mumpitz, the system we submitted to the PARSEME Shared task on automatic identification of verbal multiword expressions (VMWEs). Mumpitz consists of a Bidirectional Recurrent Neural Network (BRNN) with Long Short-Term Memory (LSTM) units and a <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> that leverages the dependency information provided in the PARSEME corpus data to differentiate VMWEs in a sentence. We submitted results for seven languages in the closed track of the task and for one language in the open track. For the open track we used the same <a href=https://en.wikipedia.org/wiki/System>system</a>, but with pretrained instead of randomly initialized word embeddings to improve the <a href=https://en.wikipedia.org/wiki/System>system</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4931 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4931/>TRAVERSAL at PARSEME Shared Task 2018 : Identification of Verbal Multiword Expressions Using a Discriminative Tree-Structured Model<span class=acl-fixed-case>TRAVERSAL</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Identification of Verbal Multiword Expressions Using a Discriminative Tree-Structured Model</a></strong><br><a href=/people/j/jakub-waszczuk/>Jakub Waszczuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4931><div class="card-body p-3 small">This paper describes a <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to the closed track of the PARSEME shared task (edition 1.1) on automatic identification of verbal multiword expressions (VMWEs). The system represents VMWE identification as a labeling task where one of two labels (MWE or not-MWE) must be predicted for each node in the dependency tree based on local context, including adjacent nodes and their labels. The <a href=https://en.wikipedia.org/wiki/System>system</a> relies on multiclass logistic regression to determine the globally optimal labeling of a tree. The system ranked 1st in the general cross-lingual ranking of the closed track systems, according to both official evaluation measures : MWE-based F1 and token-based F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4932/>VarIDE at PARSEME Shared Task 2018 : Are Variants Really as Alike as Two Peas in a Pod?<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>IDE</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Are Variants Really as Alike as Two Peas in a Pod?</a></strong><br><a href=/people/c/caroline-pasquer/>Caroline Pasquer</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/j/jean-yves-antoine/>Jean-Yves Antoine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4932><div class="card-body p-3 small">We describe the VarIDE system (standing for Variant IDEntification) which participated in the edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions (VMWEs). Our system focuses on the task of VMWE variant identification by using morphosyntactic information in the training data to predict if candidates extracted from the test corpus could be idiomatic, thanks to a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes classifier</a>. We report results for 19 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4933" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4933/>Veyn at PARSEME Shared Task 2018 : Recurrent Neural Networks for VMWE Identification<span class=acl-fixed-case>V</span>eyn at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Recurrent Neural Networks for <span class=acl-fixed-case>VMWE</span> Identification</a></strong><br><a href=/people/n/nicolas-zampieri/>Nicolas Zampieri</a>
|
<a href=/people/m/manon-scholivet/>Manon Scholivet</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4933><div class="card-body p-3 small">This paper describes the Veyn system, submitted to the closed track of the PARSEME Shared Task 2018 on automatic identification of verbal multiword expressions (VMWEs). Veyn is based on a sequence tagger using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We represent VMWEs using a variant of the begin-inside-outside encoding scheme combined with the VMWE category tag. In addition to the system description, we present development experiments to determine the best <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging scheme</a>. Veyn is freely available, covers 19 languages, and was ranked ninth (MWE-based) and eight (Token-based) among 13 submissions, considering macro-averaged F1 across languages.</div></div></div><hr><div id=w18-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-50/>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5000/>Proceedings of the 19th Annual <span class=acl-fixed-case>SIG</span>dial Meeting on Discourse and Dialogue</a></strong><br><a href=/people/k/kazunori-komatani/>Kazunori Komatani</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a>
|
<a href=/people/a/alexandros-papangelis/>Alex Papangelis</a>
|
<a href=/people/l/lawrence-cavedon/>Lawrence Cavedon</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5001/>Zero-Shot Dialog Generation with Cross-Domain Latent Actions</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5001><div class="card-body p-3 small">This paper introduces zero-shot dialog generation (ZSDG), as a step towards neural dialog systems that can instantly generalize to new situations with minimum data. ZSDG requires an end-to-end generative dialog system to generalize to a new domain for which only a domain description is provided and no training dialogs are available. Then a novel learning framework, Action Matching, is proposed. This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can learn a cross-domain embedding space that models the semantics of dialog responses which in turn, enables a neural dialog generation model to generalize to new domains. We evaluate our methods on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, a new synthetic dialog dataset, and an existing human-human multi-domain dialog dataset. Experimental results show that our method is able to achieve superior performance in learning dialog models that can rapidly adapt their behavior to new domains and suggests promising future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5002/>Changing the Level of Directness in <a href=https://en.wikipedia.org/wiki/Dialogue>Dialogue</a> using <a href=https://en.wikipedia.org/wiki/Dialogue>Dialogue Vector Models</a> and Recurrent Neural Networks</a></strong><br><a href=/people/l/louisa-pragst/>Louisa Pragst</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5002><div class="card-body p-3 small">In cooperative dialogues, identifying the intent of ones conversation partner and acting accordingly is of great importance. While this endeavour is facilitated by phrasing intentions as directly as possible, we can observe in human-human communication that a number of factors such as <a href=https://en.wikipedia.org/wiki/Social_norm>cultural norms</a> and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> may result in expressing one&#8217;s intent indirectly. Therefore, in <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer communication</a> we have to anticipate the possibility of users being indirect and be prepared to interpret their actual meaning. Furthermore, a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> should be able to conform to <a href=https://en.wikipedia.org/wiki/Expectation_(epistemic)>human expectations</a> by adjusting the degree of directness it uses to improve the <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a>. To reach those goals, we propose an approach to differentiate between direct and indirect utterances and find utterances of the opposite characteristic that express the same intent. In this endeavour, we employ dialogue vector models and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5003/>Modeling Linguistic and Personality Adaptation for Natural Language Generation</a></strong><br><a href=/people/z/zhichao-hu/>Zhichao Hu</a>
|
<a href=/people/j/jean-e-fox-tree/>Jean Fox Tree</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5003><div class="card-body p-3 small">Previous work has shown that conversants adapt to many aspects of their partners&#8217; language. Other work has shown that while every person is unique, they often share general patterns of behavior. Theories of personality aim to explain these shared patterns, and studies have shown that many <a href=https://en.wikipedia.org/wiki/Sensory_cue>linguistic cues</a> are correlated with <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a>. We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality theories</a> and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis. We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used to model it. We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5004/>Estimating User Interest from Open-Domain Dialogue</a></strong><br><a href=/people/m/michimasa-inaba/>Michimasa Inaba</a>
|
<a href=/people/k/kenichi-takahashi/>Kenichi Takahashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5004><div class="card-body p-3 small">Dialogue personalization is an important issue in the field of open-domain chat-oriented dialogue systems. If these <a href=https://en.wikipedia.org/wiki/System>systems</a> could consider their users&#8217; interests, <a href=https://en.wikipedia.org/wiki/User_engagement>user engagement</a> and satisfaction would be greatly improved. This paper proposes a neural network-based method for estimating users&#8217; interests from their utterances in chat dialogues to personalize dialogue systems&#8217; responses. We introduce a method for effectively extracting topics and user interests from utterances and also propose a pre-training approach that increases learning efficiency. Our experimental results indicate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can estimate user&#8217;s interest more accurately than baseline approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5007/>Neural User Simulation for Corpus-based Policy Optimisation of Spoken Dialogue Systems</a></strong><br><a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5007><div class="card-body p-3 small">User Simulators are one of the major tools that enable offline training of task-oriented dialogue systems. For this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> the Agenda-Based User Simulator (ABUS) is often used. The ABUS is based on hand-crafted rules and its output is in semantic form. Issues arise from both properties such as limited diversity and the inability to interface a text-level belief tracker. This paper introduces the Neural User Simulator (NUS) whose behaviour is learned from a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and which generates <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, hence needing a less labelled dataset than simulators generating a semantic output. In comparison to much of the past work on this topic, which evaluates user simulators on corpus-based metrics, we use the <a href=https://en.wikipedia.org/wiki/National_University_of_Singapore>NUS</a> to train the policy of a reinforcement learning based Spoken Dialogue System. The <a href=https://en.wikipedia.org/wiki/National_University_of_Singapore>NUS</a> is compared to the ABUS by evaluating the <a href=https://en.wikipedia.org/wiki/Policy>policies</a> that were trained using the <a href=https://en.wikipedia.org/wiki/Simulation>simulators</a>. Cross-model evaluation is performed i.e. training on one <a href=https://en.wikipedia.org/wiki/Simulation>simulator</a> and testing on the other. Furthermore, the trained <a href=https://en.wikipedia.org/wiki/Policy>policies</a> are tested on real users. In both evaluation tasks the <a href=https://en.wikipedia.org/wiki/NUS>NUS</a> outperformed the <a href=https://en.wikipedia.org/wiki/ABUS>ABUS</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5008/>Introduction method for <a href=https://en.wikipedia.org/wiki/Argumentative_dialogue>argumentative dialogue</a> using paired question-answering interchange about personality</a></strong><br><a href=/people/k/kazuki-sakai/>Kazuki Sakai</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/y/yuichiro-yoshikawa/>Yuichiro Yoshikawa</a>
|
<a href=/people/h/hiroshi-ishiguro/>Hiroshi Ishiguro</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5008><div class="card-body p-3 small">To provide a better discussion experience in current argumentative dialogue systems, it is necessary for the user to feel motivated to participate, even if the <a href=https://en.wikipedia.org/wiki/System>system</a> already responds appropriately. In this paper, we propose a method that can smoothly introduce <a href=https://en.wikipedia.org/wiki/Argumentative_dialogue>argumentative dialogue</a> by inserting an initial <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>, consisting of question-answer pairs concerning personality. The <a href=https://en.wikipedia.org/wiki/System>system</a> can induce interest of the users prior to agreement or disagreement during the main discourse. By disclosing their interests, the users will feel familiarity and motivation to further engage in the argumentative dialogue and understand the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s intent. To verify the effectiveness of a question-answer dialogue inserted before the argument, a subjective experiment was conducted using a <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text chat interface</a>. The results suggest that inserting the question-answer dialogue enhances familiarity and naturalness. Notably, the results suggest that women more than men regard the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as more natural and the argument as deepened, following an exchange concerning personality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5010/>A Situated Dialogue System for Learning Structural Concepts in Blocks World</a></strong><br><a href=/people/i/ian-perera/>Ian Perera</a>
|
<a href=/people/j/james-allen/>James Allen</a>
|
<a href=/people/c/choh-man-teng/>Choh Man Teng</a>
|
<a href=/people/l/lucian-galescu/>Lucian Galescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5010><div class="card-body p-3 small">We present a modular, end-to-end dialogue system for a situated agent to address a multimodal, natural language dialogue task in which the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> learns complex representations of block structure classes through assertions, demonstrations, and questioning. The concept to learn is provided to the user through a set of positive and negative visual examples, from which the user determines the underlying constraints to be provided to the <a href=https://en.wikipedia.org/wiki/System>system</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> in turn asks questions about demonstrated examples and simulates new examples to check its knowledge and verify the user&#8217;s description is complete. We find that this task is non-trivial for users and generates <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> that is varied yet understood by our deep language understanding architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5011.Attachment.mp4 data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5011/>Pardon the Interruption : Managing Turn-Taking through Overlap Resolution in Embodied Artificial Agents</a></strong><br><a href=/people/f/felix-gervits/>Felix Gervits</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5011><div class="card-body p-3 small">Speech overlap is a common phenomenon in natural conversation and in task-oriented interactions. As human-robot interaction (HRI) becomes more sophisticated, the need to effectively manage <a href=https://en.wikipedia.org/wiki/Turn-taking>turn-taking</a> and resolve overlap becomes more important. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> for speech overlap resolution in <a href=https://en.wikipedia.org/wiki/Embodied_agent>embodied artificial agents</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> identifies when overlap has occurred and uses timing information, dialogue history, and the agent&#8217;s goals to generate context-appropriate behavior. We implement this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in a <a href=https://en.wikipedia.org/wiki/Nao_(robot)>Nao robot</a> using the DIARC cognitive robotic architecture. The model is evaluated on a corpus of task-oriented human dialogue, and we find that the <a href=https://en.wikipedia.org/wiki/Robot>robot</a> can replicate many of the most common overlap resolution behaviors found in the human data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5013/>Turn-Taking Strategies for Human-Robot Peer-Learning Dialogue</a></strong><br><a href=/people/r/ranjini-das/>Ranjini Das</a>
|
<a href=/people/h/heather-pon-barry/>Heather Pon-Barry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5013><div class="card-body p-3 small">In this paper, we apply the contribution model of grounding to a corpus of human-human peer-mentoring dialogues. From this analysis, we propose effective turn-taking strategies for <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a> with a teachable robot. Specifically, we focus on (1) how <a href=https://en.wikipedia.org/wiki/Robot>robots</a> can encourage humans to present and (2) how robots can signal that they are going to begin a new presentation. We evaluate the strategies against a corpus of human-robot dialogues and offer three guidelines for teachable robots to follow to achieve more human-like collaborative dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5014 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5014/>Predicting Perceived Age : Both Language Ability and Appearance are Important</a></strong><br><a href=/people/s/sarah-plane/>Sarah Plane</a>
|
<a href=/people/a/ariel-marvasti/>Ariel Marvasti</a>
|
<a href=/people/t/tyler-egan/>Tyler Egan</a>
|
<a href=/people/c/casey-kennington/>Casey Kennington</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5014><div class="card-body p-3 small">When interacting with robots in a situated spoken dialogue setting, human dialogue partners tend to assign anthropomorphic and social characteristics to those <a href=https://en.wikipedia.org/wiki/Robot>robots</a>. In this paper, we explore the age and educational level that human dialogue partners assign to three different robotic systems, including an un-embodied spoken dialogue system. We found that how a robot speaks is as important to human perceptions as the way the robot looks. Using the data from our experiment, we derived prosodic, emotional, and linguistic features from the participants to train and evaluate a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that predicts perceived intelligence, age, and education level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5015/>Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog</a></strong><br><a href=/people/j/jiaping-zhang/>Jiaping Zhang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5015><div class="card-body p-3 small">Creating an intelligent conversational system that understands <a href=https://en.wikipedia.org/wiki/Visual_system>vision</a> and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972). Extensive research has focused on vision-to-language generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context. We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency. We also propose a new technique, state adaptation, to integrate <a href=https://en.wikipedia.org/wiki/Context_awareness>context awareness</a> in the dialog state representation. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and the state adaptation technique in an image guessing game and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5016 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5016/>Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments</a></strong><br><a href=/people/s/siddharth-patki/>Siddharth Patki</a>
|
<a href=/people/t/thomas-howard/>Thomas Howard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5016><div class="card-body p-3 small">The utility of collaborative manipulators for shared tasks is highly dependent on the speed and accuracy of communication between the human and the robot. The <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> of recently developed probabilistic inference models for situated symbol grounding of natural language instructions depends on the complexity of the representation of the environment in which they reason. As we move towards more complex bi-directional interactions, tasks, and environments, we need intelligent perception models that can selectively infer precise pose, <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, and affordances of the objects when inferring exhaustively detailed world models is inefficient and prohibits real-time interaction with these robots. In this paper we propose a model of language and perception for the problem of adapting the configuration of the robot perception pipeline for tasks where constructing exhaustively detailed models of the environment is inefficient and inconsequential for <a href=https://en.wikipedia.org/wiki/Symbol_grounding>symbol grounding</a>. We present experimental results from a synthetic corpus of natural language instructions for robot manipulation in example environments. The results demonstrate that by adapting <a href=https://en.wikipedia.org/wiki/Perception>perception</a> we get significant gains in terms of <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> for <a href=https://en.wikipedia.org/wiki/Perception>perception</a> and situated symbol grounding of the language instructions without a loss in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the latter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5017/>Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System</a></strong><br><a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5017><div class="card-body p-3 small">Positive emotion elicitation seeks to improve user&#8217;s emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user&#8217;s emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert&#8217;s action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert&#8217;s responses and use the resulting labels to train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. Our experiments and evaluation show that the proposed approach yields lower <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and generates a larger variety of responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5018/>Discovering User Groups for Natural Language Generation</a></strong><br><a href=/people/n/nikos-engonopoulos/>Nikos Engonopoulos</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5018><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> which predicts how individual users of a dialog system understand and produce utterances based on <a href=https://en.wikipedia.org/wiki/Users&#8217;_group>user groups</a>. In contrast to previous work, these <a href=https://en.wikipedia.org/wiki/User_group>user groups</a> are not specified beforehand, but learned in training. We evaluate on two referring expression (RE) generation tasks ; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5019/>Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators</a></strong><br><a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/l/lena-reed/>Lena Reed</a>
|
<a href=/people/s/shubhangi-tandon/>Shubhangi Tandon</a>
|
<a href=/people/s/sharath-t-s/>Sharath T.S.</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie Lukin</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5019><div class="card-body p-3 small">Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>style</a> has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals : this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5022/>Cost-Sensitive Active Learning for Dialogue State Tracking</a></strong><br><a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/l/liliang-ren/>Liliang Ren</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5022><div class="card-body p-3 small">Dialogue state tracking (DST), when formulated as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning problem</a>, relies on <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a>. Since dialogue state annotation usually requires labelling all turns of a single dialogue and utilizing context information, it is very expensive to annotate all available unlabelled data. In this paper, a novel cost-sensitive active learning framework is proposed based on a set of new dialogue-level query strategies. This is the first attempt to apply <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> for dialogue state tracking. Experiments on DSTC2 show that active learning with mixed data query strategies can effectively achieve the same DST performance with significantly less data annotation compared to traditional training approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5023.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5023/>Discourse Coherence in the Wild : A Dataset, Evaluation and Methods</a></strong><br><a href=/people/a/alice-lai/>Alice Lai</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5023><div class="card-body p-3 small">To date there has been very little work on assessing discourse coherence methods on real-world data. To address this, we present a new corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms. We show that neural models, including two that we introduce here (SentAvg and ParSeq), tend to perform best. We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5024/>Neural Dialogue Context Online End-of-Turn Detection</a></strong><br><a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/t/tomohiro-tanaka/>Tomohiro Tanaka</a>
|
<a href=/people/a/atsushi-ando/>Atsushi Ando</a>
|
<a href=/people/r/ryo-ishii/>Ryo Ishii</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/y/yushi-aono/>Yushi Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5024><div class="card-body p-3 small">This paper proposes a fully neural network based dialogue-context online end-of-turn detection method that can utilize long-range interactive information extracted from both speaker&#8217;s utterances and collocutor&#8217;s utterances. The proposed method combines multiple time-asynchronous long short-term memory recurrent neural networks, which can capture speaker&#8217;s and collocutor&#8217;s multiple sequential features, and their interactions. On the assumption of applying the proposed method to spoken dialogue systems, we introduce speaker&#8217;s acoustic sequential features and collocutor&#8217;s linguistic sequential features, each of which can be extracted in an online manner. Our evaluation confirms the effectiveness of taking dialogue context formed by the speaker&#8217;s utterances and collocutor&#8217;s utterances into consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5025/>Spoken Dialogue for Information Navigation</a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/p/panagiotis-papadakos/>Panagiotis Papadakos</a>
|
<a href=/people/y/yannis-stylianou/>Yannis Stylianou</a>
|
<a href=/people/y/yannis-tzitzikas/>Yannis Tzitzikas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5025><div class="card-body p-3 small">Aiming to expand the current research paradigm for training conversational AI agents that can address real-world challenges, we take a step away from traditional slot-filling goal-oriented spoken dialogue systems (SDS) and model the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> in a way that allows users to be more expressive in describing their needs. The goal is to help users make informed decisions rather than being fed matching items. To this end, we describe the Linked-Data SDS (LD-SDS), a system that exploits semantic knowledge bases that connect to <a href=https://en.wikipedia.org/wiki/Linked_data>linked data</a>, and supports complex constraints and preferences. We describe the required changes in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and state tracking, and the need for mined features, and we report the promising results (in terms of semantic errors, effort, etc) of a preliminary evaluation after training two statistical dialogue managers in various conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5026/>Improving User Impression in Spoken Dialog System with Gradual Speech Form Control</a></strong><br><a href=/people/y/yukiko-kageyama/>Yukiko Kageyama</a>
|
<a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/t/takashi-nose/>Takashi Nose</a>
|
<a href=/people/a/akinori-ito/>Akinori Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5026><div class="card-body p-3 small">This paper examines a method to improve the user impression of a spoken dialog system by introducing a mechanism that gradually changes form of utterances every time the user uses the <a href=https://en.wikipedia.org/wiki/System>system</a>. In some <a href=https://en.wikipedia.org/wiki/Language>languages</a>, including <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the form of utterances changes corresponding to social relationship between the talker and the listener. Thus, this mechanism can be effective to express the system&#8217;s intention to make <a href=https://en.wikipedia.org/wiki/Social_distance>social distance</a> to the user closer ; however, an actual effect of this method is not investigated enough when introduced to the dialog system. In this paper, we conduct dialog experiments and show that controlling the form of system utterances can improve the users&#8217; impression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5027/>A Bilingual Interactive Human Avatar Dialogue System</a></strong><br><a href=/people/d/dana-abu-ali/>Dana Abu Ali</a>
|
<a href=/people/m/muaz-ahmad/>Muaz Ahmad</a>
|
<a href=/people/h/hayat-al-hassan/>Hayat Al Hassan</a>
|
<a href=/people/p/paula-dozsa/>Paula Dozsa</a>
|
<a href=/people/m/ming-hu/>Ming Hu</a>
|
<a href=/people/j/jose-varias/>Jose Varias</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5027><div class="card-body p-3 small">This demonstration paper presents a bilingual (Arabic-English) interactive human avatar dialogue system. The system is named TOIA (time-offset interaction application), as it simulates <a href=https://en.wikipedia.org/wiki/Face-to-face_interaction>face-to-face conversations</a> between humans using <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>digital human avatars</a> recorded in the past. TOIA is a conversational agent, similar to a chat bot, except that it is based on an actual human being and can be used to preserve and tell stories. The system is designed to allow anybody, simply using a laptop, to create an avatar of themselves, thus facilitating cross-cultural and cross-generational sharing of narratives to wider audiences. The system currently supports monolingual and cross-lingual dialogues in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, but can be extended to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5029 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5029/>Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher Training</a></strong><br><a href=/people/d/david-pautler/>David Pautler</a>
|
<a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/k/kirby-cofino/>Kirby Cofino</a>
|
<a href=/people/p/patrick-l-lange/>Patrick Lange</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5029><div class="card-body p-3 small">We present a paradigm for interactive teacher training that leverages multimodal dialog technology to puppeteer custom-designed embodied conversational agents (ECAs) in student roles. We used the open-source multimodal dialog system HALEF to implement a small-group classroom math discussion involving <a href=https://en.wikipedia.org/wiki/Venn_diagram>Venn diagrams</a> where a human teacher candidate has to interact with two student ECAs whose actions are controlled by the dialog system. Such an automated paradigm has the potential to be extended and scaled to a wide range of interactive simulation scenarios in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>, and <a href=https://en.wikipedia.org/wiki/Business>business</a> where group interaction training is essential.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5032 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5032.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5032/>Addressing Objects and Their Relations : The Conversational Entity Dialogue Model</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/steve-young/>Steve Young</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5032><div class="card-body p-3 small">Statistical spoken dialogue systems usually rely on a single- or multi-domain dialogue model that is restricted in its capabilities of modelling complex dialogue structures, e.g., <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>relations</a>. In this work, we propose a novel dialogue model that is centred around entities and is able to model <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> as well as multiple entities of the same type. We demonstrate in a prototype implementation benefits of relation modelling on the dialogue level and show that a trained <a href=https://en.wikipedia.org/wiki/Policy>policy</a> using these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> outperforms the multi-domain baseline. Furthermore, we show that by modelling the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> on the dialogue level, the <a href=https://en.wikipedia.org/wiki/System>system</a> is capable of processing <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> present in the user input and even learns to address them in the system response.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5034 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5034/>Fine-Grained Discourse Structures in Continuation Semantics</a></strong><br><a href=/people/t/timothee-bernard/>Timothée Bernard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5034><div class="card-body p-3 small">In this work, we are interested in the computation of logical representations of discourse. We argue that all discourse connectives are <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphors</a> obeying different sets of constraints and show how this view allows one to account for the semantically parenthetical use of attitude verbs and verbs of report (e.g., think, say) and for sequences of <a href=https://en.wikipedia.org/wiki/Conjunction_(grammar)>conjunctions</a> (A CONJ_1 B CONJ_2 C). We implement this proposal in <a href=https://en.wikipedia.org/wiki/Event_(computing)>event semantics</a> using de Groote (2006)&#8217;s dynamic framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5037 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5037/>Identifying Explicit Discourse Connectives in German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5037><div class="card-body p-3 small">We are working on an end-to-end Shallow Discourse Parsing system for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and in this paper focus on the first subtask : the identification of explicit connectives. Starting with the feature set from an English system and a Random Forest classifier, we evaluate our approach on a (relatively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and experiment with including additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> obtained through annotation projection and achieve an <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> of 83.89.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5038/>Feudal Dialogue Management with Jointly Learned Feature Extractors</a></strong><br><a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5038><div class="card-body p-3 small">Reinforcement learning (RL) is a promising dialogue policy optimisation approach, but traditional RL algorithms fail to scale to large domains. Recently, Feudal Dialogue Management (FDM), has shown to increase the scalability to large domains by decomposing the dialogue management decision into two steps, making use of the domain ontology to abstract the dialogue state in each step. In order to abstract the <a href=https://en.wikipedia.org/wiki/State_space>state space</a>, however, previous work on <a href=https://en.wikipedia.org/wiki/Finite-state_machine>FDM</a> relies on handcrafted feature functions. In this work, we show that these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature functions</a> can be learned jointly with the <a href=https://en.wikipedia.org/wiki/Policy_model>policy model</a> while obtaining similar performance, even outperforming the handcrafted features in several environments and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5039/>Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5039><div class="card-body p-3 small">Cross-domain natural language generation (NLG) is still a difficult task within spoken dialogue modelling. Given a <a href=https://en.wikipedia.org/wiki/Semantics>semantic representation</a> provided by the <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a>, the language generator should generate sentences that convey desired information. Traditional template-based generators can produce sentences with all necessary information, but these sentences are not sufficiently diverse. With RNN-based models, the diversity of the generated sentences can be high, however, in the process some information is lost. In this work, we improve an RNN-based generator by considering latent information at the sentence level during generation using conditional variational auto-encoder architecture. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the original RNN-based generator, while yielding highly diverse sentences. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better when the training data is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5040 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5040/>Coherence Modeling Improves Implicit Discourse Relation Recognition</a></strong><br><a href=/people/n/noriki-nishida/>Noriki Nishida</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5040><div class="card-body p-3 small">The research described in this paper examines how to learn <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a> associated with <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> from unlabeled corpora. We introduce an unsupervised learning method on <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>text coherence</a> that could produce numerical representations that improve implicit discourse relation recognition in a semi-supervised manner. We also empirically examine two variants of coherence modeling : order-oriented and topic-oriented negative sampling, showing that, of the two, topic-oriented negative sampling tends to be more effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5041 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5041/>Adversarial Learning of Task-Oriented Neural Dialog Models</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5041><div class="card-body p-3 small">In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models. Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings. Such user ratings, however, may not always be consistent or available in practice. Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem. To address these challenges, we propose an adversarial learning method to learn dialog rewards directly from dialog samples. Such rewards are further used to optimize the dialog policy with policy gradient based RL. In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5042 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5042/>Constructing a Lexicon of English Discourse Connectives<span class=acl-fixed-case>E</span>nglish Discourse Connectives</a></strong><br><a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5042><div class="card-body p-3 small">We present a new lexicon of English discourse connectives called DiMLex-Eng, built by merging information from two annotated corpora and an additional list of relation signals from the literature. The format follows the German connective lexicon DiMLex, which provides a cross-linguistically applicable XML schema. DiMLex-Eng contains 149 English connectives, and gives information on <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a>, discourse semantics and non-connective uses (if any). We report on the development steps and discuss design decisions encountered in the lexicon expansion phase. The resource is freely available for use in studies of discourse structure and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5044 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5044.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5044/>An Analysis of the Effect of Emotional Speech Synthesis on Non-Task-Oriented Dialogue System</a></strong><br><a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/t/takashi-nose/>Takashi Nose</a>
|
<a href=/people/t/taketo-kase/>Taketo Kase</a>
|
<a href=/people/m/mai-yamanaka/>Mai Yamanaka</a>
|
<a href=/people/a/akinori-ito/>Akinori Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5044><div class="card-body p-3 small">This paper explores the effect of emotional speech synthesis on a spoken dialogue system when the dialogue is non-task-oriented. Although the use of emotional speech responses have been shown to be effective in a limited domain, e.g., scenario-based and counseling dialogue, the effect is still not clear in the non-task-oriented dialogue such as <a href=https://en.wikipedia.org/wiki/Voice_chat_in_online_gaming>voice chatting</a>. For this purpose, we constructed a simple <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> with example- and rule-based dialogue management. In the <a href=https://en.wikipedia.org/wiki/System>system</a>, two types of emotion labeling with emotion estimation are adopted, i.e., system-driven and user-cooperative emotion labeling. We conducted a dialogue experiment where subjects evaluate the subjective quality of the <a href=https://en.wikipedia.org/wiki/System>system</a> and the dialogue from the multiple aspects such as richness of the dialogue and impression of the agent. We then analyze and discuss the results and show the advantage of using appropriate <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> for the expressive speech responses in the non-task-oriented system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5045 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5045/>Multi-task Learning for Joint Language Understanding and Dialogue State Tracking</a></strong><br><a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/r/raghav-gupta/>Raghav Gupta</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5045><div class="card-body p-3 small">This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems. Multi-task training enables the sharing of the neural network layers responsible for encoding the user utterance for both LU and DST and improves performance while reducing the number of network parameters. In our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, DST operates on a set of candidate values for each slot that has been mentioned so far. These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training. Furthermore, to bridge the gap between <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5046 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5046/>Weighting Model Based on <a href=https://en.wikipedia.org/wiki/Group_dynamics>Group Dynamics</a> to Measure Convergence in Multi-party Dialogue</a></strong><br><a href=/people/z/zahra-rahimi/>Zahra Rahimi</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5046><div class="card-body p-3 small">This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering <a href=https://en.wikipedia.org/wiki/Group_dynamics>group dynamics</a> instead of simply <a href=https://en.wikipedia.org/wiki/Average>averaging</a>. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper <a href=https://en.wikipedia.org/wiki/Weighting>weighting</a> of the dyad-level measures performs better than non-weighted averaging in multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5047 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5047/>Concept Transfer Learning for Adaptive Language Understanding</a></strong><br><a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5047><div class="card-body p-3 small">Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between <a href=https://en.wikipedia.org/wiki/Concept>concepts</a>. A hierarchical semantic representation for <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> is proposed, where a semantic slot is represented as a composition of atomic concepts. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks : value set mismatch and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and evaluated on two LU benchmarks : ATIS and DSTC 2&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. In particular, we achieve state-of-the-art performance (F-score 96.08 %) on <a href=https://en.wikipedia.org/wiki/Automatic_terminal_information_service>ATIS</a> by only using lexicon features.<i>atomic concepts</i>. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance (F&#8321;-score 96.08%) on ATIS by only using lexicon features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5048 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5048/>Cogent : A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model<span class=acl-fixed-case>C</span>ogent: A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model</a></strong><br><a href=/people/l/lucian-galescu/>Lucian Galescu</a>
|
<a href=/people/c/choh-man-teng/>Choh Man Teng</a>
|
<a href=/people/j/james-allen/>James Allen</a>
|
<a href=/people/i/ian-perera/>Ian Perera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5048><div class="card-body p-3 small">The bulk of current research in <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> is focused on fairly simple task models, primarily state-based. Progress on developing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> for more complex tasks has been limited by the lack generic toolkits to build from. In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving. We implemented the model in a dialogue system shell (Cogent) that al-lows developers to plug in problem-solving agents to create dialogue systems in new domains. The Cogent shell has now been used by several independent teams of researchers to develop dialogue systems in different domains, with varied lexicons and interaction style, each with their own problem-solving back-end. We believe this to be the first practical demonstration of the feasibility of a CPS-based dialogue system shell.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5049 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5049/>Identifying Domain Independent Update Intents in Task Based Dialogs</a></strong><br><a href=/people/p/prakhar-biyani/>Prakhar Biyani</a>
|
<a href=/people/c/cem-akkaya/>Cem Akkaya</a>
|
<a href=/people/k/kostas-tsioutsiouliklis/>Kostas Tsioutsiouliklis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5049><div class="card-body p-3 small">One important problem in task-based conversations is that of effectively updating the belief estimates of user-mentioned slot-value pairs. Given a user utterance, the intent of a slot-value pair is captured using dialog acts (DA) expressed in that utterance. However, in certain cases, DA&#8217;s fail to capture the actual update intent of the user. In this paper, we describe such cases and propose a new type of <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic class</a> for <a href=https://en.wikipedia.org/wiki/User_intent>user intents</a>. This new <a href=https://en.wikipedia.org/wiki/Data_type>type</a>, Update Intents (UI), is directly related to the type of update a user intends to perform for a slot-value pair. We define five types of UI&#8217;s, which are independent of the domain of the conversation. We build a multi-class classification model using LSTM&#8217;s to identify the type of <a href=https://en.wikipedia.org/wiki/User_interface>UI</a> in user utterances in the Restaurant and Shopping domains. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve strong <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance in terms of F-1 score.</div></div></div><hr><div id=w18-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-51/>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5100/>Proceedings of the 2nd Workshop on Abusive Language Online (<span class=acl-fixed-case>ALW</span>2)</a></strong><br><a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/j/jacqueline-wernimont/>Jacqueline Wernimont</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5102/>Hate Speech Dataset from a White Supremacy Forum</a></strong><br><a href=/people/o/ona-de-gibert/>Ona de Gibert</a>
|
<a href=/people/n/naiara-perez/>Naiara Perez</a>
|
<a href=/people/a/aitor-garcia-pablos/>Aitor García-Pablos</a>
|
<a href=/people/m/montse-cuadros/>Montse Cuadros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5102><div class="card-body p-3 small">Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, colour, <a href=https://en.wikipedia.org/wiki/Ethnic_group>ethnicity</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Sexual_orientation>sexual orientation</a>, <a href=https://en.wikipedia.org/wiki/Nationality>nationality</a>, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, or other characteristic. Due to the massive rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated web content</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, the amount of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> is also steadily increasing. Over the past years, interest in online hate speech detection and, particularly, the automation of this task has continuously grown, along with the societal impact of the phenomenon. This paper describes a <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech dataset</a> composed of thousands of sentences manually labelled as containing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or not. The sentences have been extracted from <a href=https://en.wikipedia.org/wiki/Stormfront_(website)>Stormfront</a>, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and several baseline experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5104/>Predictive Embeddings for Hate Speech Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rohan-kshirsagar/>Rohan Kshirsagar</a>
|
<a href=/people/t/tyrus-cukuvac/>Tyrus Cukuvac</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a>
|
<a href=/people/s/susan-mcgregor/>Susan McGregor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5104><div class="card-body p-3 small">We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in particular. Using pre-trained word embeddings and max / mean pooling from simple, fully-connected transformations of these embeddings, we are able to predict the occurrence of hate speech on three commonly used publicly available datasets. Our models match or outperform state of the art F1 performance on all three datasets using significantly fewer parameters and minimal feature preprocessing compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5105/>Challenges for Toxic Comment Classification : An In-Depth Error Analysis</a></strong><br><a href=/people/b/betty-van-aken/>Betty van Aken</a>
|
<a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5105><div class="card-body p-3 small">Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task&#8217;s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> that outperforms all individual models. Further, we validate our findings on a second <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The results of the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5106/>Aggression Detection on Social Media Text Using Deep Neural Networks</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/a/aman-varshney/>Aman Varshney</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5106><div class="card-body p-3 small">In the past few years, bully and aggressive posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have grown significantly, causing serious consequences for victims / users of all demographics. Majority of the work in this field has been done for English only. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based classification system</a> for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments of Hindi-English Code-Mixed text to detect the aggressive behaviour of / towards users. Our work focuses on text from users majorly in the Indian Subcontinent. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we used for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is provided by <a href=https://en.wikipedia.org/wiki/TRAC-1>TRAC-1</a> in their shared task. Our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> assigns each <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook post / comment</a> to one of the three predefined categories : Overtly Aggressive, Covertly Aggressive and Non-Aggressive. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2 %.<b>TRAC-1</b>in their shared task. Our classification model assigns each Facebook post/comment to one of the three predefined categories: &#8220;Overtly Aggressive&#8221;, &#8220;Covertly Aggressive&#8221; and &#8220;Non-Aggressive&#8221;. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5107/>Creating a WhatsApp Dataset to Study Pre-teen Cyberbullying<span class=acl-fixed-case>W</span>hats<span class=acl-fixed-case>A</span>pp Dataset to Study Pre-teen Cyberbullying</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/f/filippo-oncini/>Filippo Oncini</a>
|
<a href=/people/e/enrico-piras/>Enrico Piras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5107><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/WhatsApp>WhatsApp</a> is used by teenagers as one major channel of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, such interactions remain invisible due to the <a href=https://en.wikipedia.org/wiki/Privacy_policy>app privacy policies</a> that do not allow ex-post data collection. Indeed, most of the information on these phenomena rely on surveys regarding <a href=https://en.wikipedia.org/wiki/Self-report_study>self-reported data</a>. In order to overcome this limitation, we describe in this paper the activities that led to the creation of a WhatsApp dataset to study <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> among Italian students aged 12-13. We present not only the collected chats with annotations about user role and type of offense, but also the living lab created in a collaboration between researchers and schools to monitor and analyse <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>. Finally, we discuss some open issues, dealing with ethical, operational and epistemic aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5109/>Aggressive language in an online hacking forum</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/s/sergio-pastrana/>Sergio Pastrana</a>
|
<a href=/people/a/alice-hutchings/>Alice Hutchings</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5109><div class="card-body p-3 small">We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classifier</a> for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5110/>The Effects of User Features on Twitter Hate Speech Detection<span class=acl-fixed-case>T</span>witter Hate Speech Detection</a></strong><br><a href=/people/e/elise-fehn-unsvag/>Elise Fehn Unsvåg</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5110><div class="card-body p-3 small">The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. While the incorporation of <a href=https://en.wikipedia.org/wiki/Software_feature>user features</a> resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5112/>Determining Code Words in Euphemistic Hate Speech Using Word Embedding Networks</a></strong><br><a href=/people/r/rijul-magu/>Rijul Magu</a>
|
<a href=/people/j/jiebo-luo/>Jiebo Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5112><div class="card-body p-3 small">While analysis of online explicit abusive language detection has lately seen an ever-increasing focus, implicit abuse detection remains a largely unexplored space. We carry out a study on a subcategory of implicit hate : euphemistic hate speech. We propose a method to assist in identifying unknown euphemisms (or code words) given a set of hateful tweets containing a known code word. Our approach leverages <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Network_theory>network analysis</a> (through centrality measures and community detection) in a manner that can be generalized to identify euphemisms across contexts- not just <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5117/>Cross-Domain Detection of Abusive Language Online</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5117><div class="card-body p-3 small">We investigate to what extent the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained to detect general abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, finding that the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in-domain training, especially when used to augment a smaller dataset with a larger one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5118/>Did you offend me? Classification of Offensive Tweets in Hinglish Language<span class=acl-fixed-case>H</span>inglish Language</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/m/meghna-ayyar/>Meghna Ayyar</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5118><div class="card-body p-3 small">The use of code-switched languages (e.g., <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, which is derived by the blending of <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> with the English language) is getting much popular on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> due to their ease of communication in <a href=https://en.wikipedia.org/wiki/First_language>native languages</a>. However, spelling variations and absence of <a href=https://en.wikipedia.org/wiki/Grammar>grammar rules</a> introduce <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.<i>e.g.</i>, Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5119/>Decipherment for Adversarial Offensive Language Detection</a></strong><br><a href=/people/z/zhelun-wu/>Zhelun Wu</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5119><div class="card-body p-3 small">Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We provide experimental results on three different datasets and show that <a href=https://en.wikipedia.org/wiki/Decipherment>decipherment</a> is an effective tool for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5120/>The Linguistic Ideologies of Deep Abusive Language Classification</a></strong><br><a href=/people/m/michael-castelle/>Michael Castelle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5120><div class="card-body p-3 small">This paper brings together theories from <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_anthropology>linguistic anthropology</a> to critically evaluate the so-called language ideologies the set of beliefs and ways of speaking about languagein the practices of abusive language classification in modern machine learning-based NLP. This argument is made at both a conceptual and empirical level, as we review approaches to <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language</a> from different fields, and use two neural network methods to analyze three datasets developed for <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language classification tasks</a> (drawn from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, and StackOverflow). By evaluating and comparing these results, we argue for the importance of incorporating theories of <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> and <a href=https://en.wikipedia.org/wiki/Metapragmatics>metapragmatics</a> into both the design of classification tasks as well as in ML architectures.</div></div></div><hr><div id=w18-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-52/>Proceedings of the 5th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5200/>Proceedings of the 5th Workshop on Argument Mining</a></strong><br><a href=/people/n/noam-slonim/>Noam Slonim</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5201/>Argumentative Link Prediction using <a href=https://en.wikipedia.org/wiki/Residual_network>Residual Networks</a> and Multi-Objective Learning</a></strong><br><a href=/people/a/andrea-galassi/>Andrea Galassi</a>
|
<a href=/people/m/marco-lippi/>Marco Lippi</a>
|
<a href=/people/p/paolo-torroni/>Paolo Torroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5201><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Residual_network>residual networks</a> for argumentation mining, with an emphasis on link prediction. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> we propose makes no assumptions on document or argument structure. We evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on a challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated comments</a> collected from an <a href=https://en.wikipedia.org/wiki/Online_platform>online platform</a>. Results show that our model outperforms an equivalent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a> and offers results comparable with state-of-the-art methods that rely on <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5202/>End-to-End Argument Mining for Discussion Threads Based on Parallel Constrained Pointer Architecture</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5202><div class="card-body p-3 small">Argument Mining (AM) is a relatively recent discipline, which concentrates on extracting claims or premises from discourses, and inferring their structures. However, many existing works do not consider micro-level AM studies on <a href=https://en.wikipedia.org/wiki/Conversation_threading>discussion threads</a> sufficiently. In this paper, we tackle <a href=https://en.wikipedia.org/wiki/Amplitude_modulation>AM</a> for <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion threads</a>. Our main contributions are follows : (1) A novel combination scheme focusing on micro-level inner- and inter- post schemes for a discussion thread. (2) Annotation of large-scale civic discussion threads with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. (3) Parallel constrained pointer architecture (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in terms of relations extraction, in comparison to existing state-of-the-art models.<i>Parallel constrained pointer architecture</i> (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed model shows better accuracy in terms of relations extraction, in comparison to existing state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5203/>ArguminSci : A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing<span class=acl-fixed-case>A</span>rgumin<span class=acl-fixed-case>S</span>ci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/k/kai-eckert/>Kai Eckert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5203><div class="card-body p-3 small">Argumentation is arguably one of the central features of scientific language. We present ArguminSci, an easy-to-use tool that analyzes <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> and other rhetorical aspects of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>, which we collectively dub scitorics. The main aspect we focus on is the fine-grained argumentative analysis of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific text</a> through identification of argument components. The functionality of ArguminSci is accessible via three interfaces : as a <a href=https://en.wikipedia.org/wiki/Command-line_interface>command line tool</a>, via a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>RESTful application programming interface</a>, and as a <a href=https://en.wikipedia.org/wiki/Web_application>web application</a>.<i>ArguminSci</i>, an easy-to-use tool that analyzes argumentation and other rhetorical aspects of scientific writing, which we collectively dub <i>scitorics</i>. The main aspect we focus on is the fine-grained argumentative analysis of scientific text through identification of argument components. The functionality of <i>ArguminSci</i> is accessible via three interfaces: as a command line tool, via a RESTful application programming interface, and as a web application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5204/>Evidence Type Classification in Randomized Controlled Trials</a></strong><br><a href=/people/t/tobias-mayer/>Tobias Mayer</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5204><div class="card-body p-3 small">Randomized Controlled Trials (RCT) are a common type of experimental studies in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a> for evidence-based decision making. The ability to automatically extract the arguments proposed therein can be of valuable support for clinicians and practitioners in their daily evidence-based decision making activities. Given the peculiarity of the medical domain and the required level of detail, standard approaches to argument component detection in argument(ation) mining are not fine-grained enough to support such activities. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> of the argument component identification task : evidence type classification. To address it, we propose a supervised approach and we test <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on a set of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>RCT abstracts</a> on different <a href=https://en.wikipedia.org/wiki/Medicine>medical topics</a>.<i>arguments</i> proposed therein can be of valuable support for clinicians and practitioners in their daily evidence-based decision making activities. Given the peculiarity of the medical domain and the required level of detail, standard approaches to argument component detection in <i>argument(ation) mining</i> are not fine-grained enough to support such activities. In this paper, we introduce a new sub-task of the argument component identification task: <i>evidence type classification</i>. To address it, we propose a supervised approach and we test it on a set of RCT abstracts on different medical topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5206/>An Argument-Annotated Corpus of Scientific Publications</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5206><div class="card-body p-3 small">Argumentation is an essential feature of scientific language. We present an annotation study resulting in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of scientific publications</a> annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus, already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> and other rhetorical aspects of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>, such as discourse roles and citation contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5207/>Annotating Claims in the Vaccination Debate</a></strong><br><a href=/people/b/benedetta-torsi/>Benedetta Torsi</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5207><div class="card-body p-3 small">In this paper we present <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> experiments with three different <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a> for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, which contains heterogeneous texts from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3 % IAA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5208/>Argument Component Classification for Classroom Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5208><div class="card-body p-3 small">This paper focuses on argument component classification for transcribed spoken classroom discussions, with the goal of automatically classifying student utterances into claims, evidence, and warrants. We show that an existing <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for argument component classification developed for another educationally-oriented domain performs poorly on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We then show that feature sets from prior work on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> for student essays and online dialogues can be used to improve performance considerably. We also provide a comparison between <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> when trained under different conditions to classify argument components in classroom discussions. While neural network models are not always able to outperform a logistic regression model, we were able to gain some useful insights : convolutional networks are more robust than recurrent networks both at the character and at the word level, and specificity information can help boost performance in multi-task training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5209/>Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence : Argument Mining in the Context of Legal Rules Governing Evidence Assessment</a></strong><br><a href=/people/v/vern-walker/>Vern R. Walker</a>
|
<a href=/people/d/dina-foerster/>Dina Foerster</a>
|
<a href=/people/j/julia-monica-ponce/>Julia Monica Ponce</a>
|
<a href=/people/m/matthew-rosen/>Matthew Rosen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5209><div class="card-body p-3 small">This paper reports on the results of an empirical study of adjudicatory decisions about veterans&#8217; claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or <a href=https://en.wikipedia.org/wiki/Soft_law>soft rules</a> that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with <a href=https://en.wikipedia.org/wiki/Legal_doctrine>legal rules</a> governing the <a href=https://en.wikipedia.org/wiki/Evidence_(law)>evidence assessment process</a> in the U.S. This approach should be transferable to other legal and non-legal domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5210/>Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument Templates</a></strong><br><a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5210><div class="card-body p-3 small">Most of the existing works on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> cast the problem of argumentative structure identification as classification tasks (e.g. attack-support relations, stance, explicit premise / claim). This paper goes a step further by addressing the task of automatically identifying reasoning patterns of arguments using predefined templates, which is called argument template (AT) instantiation. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> constrained by a feasible set of <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a>. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.<i>argument template (AT) instantiation</i>. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as structured prediction constrained by a feasible set of templates. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5211/>Frame- and Entity-Based Knowledge for Common-Sense Argumentative Reasoning</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5211><div class="card-body p-3 small">Common-sense argumentative reasoning is a challenging task that requires holistic understanding of the argumentation where external knowledge about the world is hypothesized to play a key role. We explore the idea of using event knowledge about prototypical situations from <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> and fact knowledge about concrete entities from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to solve the task. We find that both resources can contribute to an improvement over the non-enriched approach and point out two persisting challenges : first, integration of many <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> of the same type, and second, fusion of complementary annotations. After our explorations, we question the key role of external world knowledge with respect to the argumentative reasoning task and rather point towards a logic-based analysis of the chain of reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5213/>Proposed Method for Annotation of Scientific Arguments in Terms of Semantic Relations and Argument Schemes</a></strong><br><a href=/people/n/nancy-green/>Nancy Green</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5213><div class="card-body p-3 small">This paper presents a proposed method for annotation of scientific arguments in biological / biomedical journal articles. Semantic entities and <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> are used to represent the propositional content of arguments in instances of argument schemes. We describe an experiment in which we encoded the arguments in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>journal article</a> to identify issues in this approach. Our catalogue of argument schemes and a copy of the annotated article are now publically available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5215/>Dave the debater : a retrieval-based and generative argumentative dialogue agent</a></strong><br><a href=/people/d/dieu-thu-le/>Dieu Thu Le</a>
|
<a href=/people/c/cam-tu-nguyen/>Cam-Tu Nguyen</a>
|
<a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5215><div class="card-body p-3 small">In this paper, we explore the problem of developing an argumentative dialogue agent that can be able to discuss with human users on controversial topics. We describe two systems that use retrieval-based and generative models to make argumentative responses to the users. The experiments show promising results although they have been trained on a <a href=https://en.wikipedia.org/wiki/Data_set>small dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5216/>PD3 : Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection<span class=acl-fixed-case>PD</span>3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/a/andreas-ruckle/>Andreas Rücklé</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5216><div class="card-body p-3 small">We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumentation mining and standard POS tagging. We combine direct transfer using bilingual embeddings with annotation projection, which projects labels across unlabeled parallel data. We do so by either merging respective source and target language datasets or alternatively by using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our combination strategy considerably improves upon both direct transfer and <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a> with few available parallel sentences, the most realistic scenario for many low-resource target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5218/>More or less controlled elicitation of argumentative text : Enlarging a <a href=https://en.wikipedia.org/wiki/Microtext>microtext corpus</a> via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a></a></strong><br><a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/a/andreas-peldszus/>Andreas Peldszus</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5218><div class="card-body p-3 small">We present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. Our <a href=https://en.wikipedia.org/wiki/Extension_(semantics)>extension</a> more than doubles the size of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by means of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We report on the setup of this experiment and on the consequences that <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> had for assembling the <a href=https://en.wikipedia.org/wiki/Data>data</a>, and in particular for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, but had to make a few modifications in response to interesting phenomena in the data. Finally, we report on an experiment with the automatic prediction of this argumentation structure : We first replicated the approach of an earlier study on the original <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and compare the performance to various settings involving the extension.</div></div></div><hr><div id=w18-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-53/>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5300/>Proceedings of the 6th <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> Workshop A challenge on large-scale biomedical semantic indexing and question answering</a></strong><br><a href=/people/i/ioannis-kakadiaris/>Ioannis A. Kakadiaris</a>
|
<a href=/people/g/george-paliouras/>George Paliouras</a>
|
<a href=/people/a/anastasia-krithara/>Anastasia Krithara</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5301/>Results of the sixth edition of the BioASQ Challenge<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> Challenge</a></strong><br><a href=/people/a/anastasios-nentidis/>Anastasios Nentidis</a>
|
<a href=/people/a/anastasia-krithara/>Anastasia Krithara</a>
|
<a href=/people/k/konstantinos-bougiatiotis/>Konstantinos Bougiatiotis</a>
|
<a href=/people/g/georgios-paliouras/>Georgios Paliouras</a>
|
<a href=/people/i/ioannis-kakadiaris/>Ioannis Kakadiaris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5301><div class="card-body p-3 small">This paper presents the results of the sixth edition of the BioASQ challenge. The BioASQ challenge aims at the promotion of systems and methodologies through the organization of a challenge on two tasks : semantic indexing and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In total, 26 teams with more than 90 systems participated in this year&#8217;s challenge. As in previous years, the best systems were able to outperform the strong baselines. This suggests that state-of-the-art systems are continuously improving, pushing the frontier of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5306/>AttentionMeSH : Simple, Effective and Interpretable Automatic MeSH Indexer<span class=acl-fixed-case>A</span>ttention<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>SH</span>: Simple, Effective and Interpretable Automatic <span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>SH</span> Indexer</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5306><div class="card-body p-3 small">There are millions of articles in <a href=https://en.wikipedia.org/wiki/PubMed>PubMed database</a>. To facilitate <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, curators in the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article. MeSH is a hierarchically-organized vocabulary, containing about 28 K different concepts, covering the fields from <a href=https://en.wikipedia.org/wiki/Medicine>clinical medicine</a> to <a href=https://en.wikipedia.org/wiki/Information_science>information sciences</a>. Several automatic MeSH indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the NLM official tool Medical Text Indexer, and the winner of BioASQ Task5a challenge DeepMeSH. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are complex and not interpretable. We propose a novel <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a>, AttentionMeSH, which utilizes <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and attention mechanism to index <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MeSH terms</a> to biomedical text. The attention mechanism enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to associate textual evidence with annotations, thus providing <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> at the word level. The <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> also uses a novel masking mechanism to enhance <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>. In the final week of BioASQ Chanllenge Task6a, we ranked 2nd by average MiF using an on-construction model. After the contest, we achieve close to state-of-the-art MiF performance of 0.684 using our final model. Human evaluations show AttentionMeSH also provides high level of interpretability, retrieving about 90 % of all expert-labeled relevant words given an MeSH-article pair at 20 output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5308/>UNCC QA : Biomedical Question Answering system<span class=acl-fixed-case>UNCC</span> <span class=acl-fixed-case>QA</span>: Biomedical Question Answering system</a></strong><br><a href=/people/a/abhishek-bhandwaldar/>Abhishek Bhandwaldar</a>
|
<a href=/people/w/wlodek-zadrozny/>Wlodek Zadrozny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5308><div class="card-body p-3 small">In this paper, we detail our submission to the BioASQ competition&#8217;s Biomedical Semantic Question and Answering task. Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE-2 and Rogue-SU4 in all test batch sets. Our contributions are named-entity based method for answering factoid and list questions, and an extractive summarization techniques for building paragraph-sized summaries, based on lexical chains. Our system got highest ROUGE-2 and ROUGE-SU4 scores for ideal-type answers in all test batch sets. We also discuss the limitations of the described <a href=https://en.wikipedia.org/wiki/System>system</a>, such lack of the evaluation on other criteria (e.g. manual). Also, for factoid- and list -type question our system got low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> (which suggests that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> needs to improve in the ranking of entities).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5309/>An Adaption of BIOASQ Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.<span class=acl-fixed-case>BIOASQ</span> Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.</a></strong><br><a href=/people/s/sanjay-kamath/>Sanjay Kamath</a>
|
<a href=/people/b/brigitte-grau/>Brigitte Grau</a>
|
<a href=/people/y/yue-ma/>Yue Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5309><div class="card-body p-3 small">BIOASQ Task B Phase B challenge focuses on extracting answers from snippets for a given question. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provided by the organizers contains answers, but not all their variants. Henceforth a manual annotation was performed to extract all forms of correct answers. This article shows the impact of using all occurrences of correct answers for training on the evaluation scores which are improved significantly.</div></div></div><hr><div id=w18-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-54/>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5400/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>B</span>lackbox<span class=acl-fixed-case>NLP</span>: Analyzing and Interpreting Neural Networks for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5403/>Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures</a></strong><br><a href=/people/d/danilo-croce/>Danilo Croce</a>
|
<a href=/people/d/daniele-rossini/>Daniele Rossini</a>
|
<a href=/people/r/roberto-basili/>Roberto Basili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5403><div class="card-body p-3 small">Nonlinear methods such as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called Layerwise Relevance Propagation, over linguistically motivated neural architectures, namely Kernel-based Deep Architectures (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance : these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, i.e. question classification and <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>.<i>Layerwise Relevance Propagation</i>, over linguistically motivated neural architectures, namely <i>Kernel-based Deep Architectures</i> (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5405/>Evaluating Textual Representations through Image Generation</a></strong><br><a href=/people/g/graham-spinks/>Graham Spinks</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5405><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for determining the quality of textual representations through the ability to generate <a href=https://en.wikipedia.org/wiki/Image>images</a> from them. Continuous representations of textual input are ubiquitous in modern Natural Language Processing techniques either at the core of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> or as the by-product at any given layer of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. While current techniques to evaluate such representations focus on their performance on particular tasks, they do n&#8217;t provide a clear understanding of the level of informational detail that is stored within them, especially their ability to represent spatial information. The central premise of this paper is that visual inspection or analysis is the most convenient method to quickly and accurately determine information content. Through the use of text-to-image neural networks, we propose a new technique to compare the quality of textual representations by visualizing their information content. The method is illustrated on a <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical dataset</a> where the correct representation of spatial information and <a href=https://en.wikipedia.org/wiki/Shorthand>shorthands</a> are of particular importance. For four different well-known textual representations, we show with a quantitative analysis that some <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> are consistently able to deliver higher quality visualizations of the information content. Additionally, we show that the quantitative analysis technique correlates with the judgment of a human expert evaluator in terms of alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5409/>Linguistic representations in multi-task neural networks for ellipsis resolution</a></strong><br><a href=/people/o/ola-ronning/>Ola Rønning</a>
|
<a href=/people/d/daniel-hardt/>Daniel Hardt</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5409><div class="card-body p-3 small">Sluicing resolution is the task of identifying the antecedent to a question ellipsis. Antecedents are often <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>sentential constituents</a>, and previous work has therefore relied on <a href=https://en.wikipedia.org/wiki/Parsing>syntactic parsing</a>, together with complex linguistic features. A recent model instead used partial parsing as an auxiliary task in sequential neural network architectures to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. We explore the linguistic information being brought to bear by such <a href=https://en.wikipedia.org/wiki/Social_network>networks</a>, both by defining subsets of the data exhibiting relevant linguistic characteristics, and by examining the internal representations of the <a href=https://en.wikipedia.org/wiki/Social_network>network</a>. Both perspectives provide evidence for substantial <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic knowledge</a> being deployed by the <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5413/>Rearranging the Familiar : Testing Compositional Generalization in Recurrent Networks</a></strong><br><a href=/people/j/joao-loula/>João Loula</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a>
|
<a href=/people/b/brenden-lake/>Brenden Lake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5413><div class="card-body p-3 small">Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it&#8217;s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni&#8217;s main experiment required the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as around and right) in novel contexts. Our findings confirm and strengthen the earlier ones : seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of X around right to jump around right), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of around right from those of right and around).<i>around</i>&#8221; and &#8220;<i>right</i>&#8221;) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of &#8220;X <i>around right</i>&#8221; to &#8220;<i>jump around right</i>&#8221;), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of &#8220;<i>around right</i>&#8221; from those of &#8220;<i>right</i>&#8221; and &#8220;<i>around</i>&#8221;).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5415/>Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style</a></strong><br><a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/s/sugato-basu/>Sugato Basu</a>
|
<a href=/people/k/kazoo-sone/>Kazoo Sone</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5415><div class="card-body p-3 small">How much does free shipping ! help an advertisement&#8217;s ability to persuade? This paper presents two methods for <a href=https://en.wikipedia.org/wiki/Performance_attribution>performance attribution</a> : finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are based on interpreting the behaviors and parameters of trained <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residualization</a> to control for <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a> and performs <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>interpretation</a> by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A / B testing to discover.<i>performance attribution</i>: finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both algorithms are based on interpreting the behaviors and parameters of trained neural networks. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages residualization to control for confounds and performs interpretation by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed algorithms are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A/B testing to discover.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5418/>LISA : Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation<span class=acl-fixed-case>LISA</span>: Explaining Recurrent Neural Network Judgments via Layer-w<span class=acl-fixed-case>I</span>se Semantic Accumulation and Example to Pattern Transformation</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5418><div class="card-body p-3 small">Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a> via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We demonstrate (1) LISA : How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response (2) Example2pattern : How the saliency patterns look like for each category in the data according to the network in decision making. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a>. We employ two relation classification datasets : SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.<i>Layer-wIse-Semantic-Accumulation</i> (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) <i>LISA</i>: &#8220;How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response&#8221; (2) <i>Example2pattern</i>: &#8220;How the saliency patterns look like for each category in the data according to the network in decision making&#8221;. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the <i>LISA</i> and <i>example2pattern</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5420/>An Operation Sequence Model for Explainable Neural Machine Translation</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5420><div class="card-body p-3 small">We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5421/>Introspection for convolutional automatic speech recognition</a></strong><br><a href=/people/a/andreas-krug/>Andreas Krug</a>
|
<a href=/people/s/sebastian-stober/>Sebastian Stober</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5421><div class="card-body p-3 small">Artificial Neural Networks (ANNs) have experienced great success in the past few years. The increasing <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> leads to less understanding about their <a href=https://en.wikipedia.org/wiki/Decision-making>decision processes</a>. Therefore, introspection techniques have been proposed, mostly for <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> as input data. Patterns or relevant regions in <a href=https://en.wikipedia.org/wiki/Image>images</a> can be intuitively interpreted by a human observer. This is not the case for more complex data like speech recordings. In this work, we investigate the application of common introspection techniques from <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> to an Automatic Speech Recognition (ASR) task. To this end, we use a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to <a href=https://en.wikipedia.org/wiki/Image_classification>image classification</a>, which predicts <a href=https://en.wikipedia.org/wiki/Letter_(alphabet)>letters</a> from <a href=https://en.wikipedia.org/wiki/Optical_spectrometer>spectrograms</a>. We show difficulties in applying image introspection to ASR. To tackle these problems, we propose normalized averaging of aligned inputs (NAvAI): a data-driven method to reveal learned patterns for prediction of specific classes. Our method integrates information from many data examples through local introspection techniques for Convolutional Neural Networks (CNNs). We demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides better interpretability of letter-specific patterns than existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5422/>Learning and Evaluating Sparse Interpretable Sentence Embeddings</a></strong><br><a href=/people/v/valentin-trifonov/>Valentin Trifonov</a>
|
<a href=/people/o/octavian-eugen-ganea/>Octavian-Eugen Ganea</a>
|
<a href=/people/a/anna-potapenko/>Anna Potapenko</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5422><div class="card-body p-3 small">Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties : to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data. In this paper, we transfer this idea to <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> and explore several approaches to obtain a <a href=https://en.wikipedia.org/wiki/Sparse_representation>sparse representation</a>. We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods. We observe an increase in <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5425/>Closing Brackets with Recurrent Neural Networks</a></strong><br><a href=/people/n/natalia-skachkova/>Natalia Skachkova</a>
|
<a href=/people/t/thomas-alexander-trost/>Thomas Trost</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5425><div class="card-body p-3 small">Many natural and formal languages contain words or symbols that require a matching counterpart for making an expression well-formed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> is important for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. Currently, recurrent neural networks (RNNs) are extensively used for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of <a href=https://en.wikipedia.org/wiki/Bracket>brackets</a>. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>theoretical baseline</a> in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5426/>Under the Hood : Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information</a></strong><br><a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/j/jack-harding/>Jack Harding</a>
|
<a href=/people/f/florian-mohnert/>Florian Mohnert</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/w/willem-zuidema/>Willem Zuidema</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5426><div class="card-body p-3 small">How do neural language models keep track of <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> between subject and verb? We show that &#8216;diagnostic classifiers&#8217;, trained to predict number from the internal states of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where <a href=https://en.wikipedia.org/wiki/Number>number information</a> is corrupted in cases where the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> ends up making <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement errors</a>. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5427/>Iterative Recursive Attention Model for Interpretable Sequence Classification</a></strong><br><a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5427><div class="card-body p-3 small">Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference steps</a>. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5429/>Importance of Self-Attention for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/g/gael-letarte/>Gaël Letarte</a>
|
<a href=/people/f/frederik-paradis/>Frédérik Paradis</a>
|
<a href=/people/p/philippe-giguere/>Philippe Giguère</a>
|
<a href=/people/f/francois-laviolette/>François Laviolette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5429><div class="card-body p-3 small">Despite their superior performance, deep learning models often lack <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. In this paper, we explore the modeling of insightful relations between words, in order to understand and enhance predictions. To this effect, we propose the Self-Attention Network (SANet), a flexible and interpretable architecture for text classification. Experiments indicate that gains obtained by self-attention is task-dependent. For instance, experiments on sentiment analysis tasks showed an improvement of around 2 % when using self-attention compared to a baseline without <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, while topic classification showed no gain. Interpretability brought forward by our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> highlighted the importance of neighboring word interactions to extract <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5431/>An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5431><div class="card-body p-3 small">The attention mechanism is a successful technique in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, especially in tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The recently proposed <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> of the Transformer is based entirely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and achieves new state of the art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> while higher layers tend to encode more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>.<i>Transformer</i> is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5432/>Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar : A Case Study on Machine Translation<span class=acl-fixed-case>HPSG</span> Grammar: A Case Study on Machine Translation</a></strong><br><a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/k/khiem-pham/>Khiem Pham</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/b/brian-w-dillon/>Brian Dillon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5432><div class="card-body p-3 small">Sequence to sequence (seq2seq) models are often employed in settings where the target output is <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. However, the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic properties</a> of the language generated from these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are not well understood. We explore whether such output belongs to a formal and realistic grammar, by employing the English Resource Grammar (ERG), a broad coverage, linguistically precise HPSG-based grammar of English. From a French to English parallel corpus, we analyze the parseability and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical constructions</a> occurring in output from a seq2seq translation model. Over 93 % of the model translations are parseable, suggesting that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> learns to generate conforming to a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has trouble learning the distribution of rarer syntactic rules, and we pinpoint several constructions that differentiate translations between the references and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5434" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5434/>Learning Explanations from Language Data</a></strong><br><a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5434><div class="card-body p-3 small">PatternAttribution is a recent method, introduced in the <a href=https://en.wikipedia.org/wiki/Visual_system>vision domain</a>, that explains classifications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. We demonstrate that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also generates meaningful interpretations in the language domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5435 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5435/>How much should you ask? On the question structure in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>.<span class=acl-fixed-case>QA</span> systems.</a></strong><br><a href=/people/b/barbara-rychalska/>Barbara Rychalska</a>
|
<a href=/people/d/dominika-basaj/>Dominika Basaj</a>
|
<a href=/people/a/anna-wroblewska/>Anna Wróblewska</a>
|
<a href=/people/p/przemyslaw-biecek/>Przemyslaw Biecek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5435><div class="card-body p-3 small">Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner. However, users are still used to query-like systems where they type in <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to search for answer. In this study we validate which parts of questions are essential for obtaining valid answer. In order to conclude that, we take advantage of LIME-a framework that explains <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> by local approximation. We find that <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is disregarded by QA. State-of-the-art model can answer properly even if&#8217; asked&#8217; only with a few words with high coefficients calculated with <a href=https://en.wikipedia.org/wiki/LIME>LIME</a>. According to our knowledge, it is the first time that QA model is being explained by LIME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5438 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5438/>Language Models Learn POS First<span class=acl-fixed-case>POS</span> First</a></strong><br><a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5438><div class="card-body p-3 small">A glut of recent research shows that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> capture linguistic structure. Such work answers the question of whether a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents linguistic structure. But how and when are these <a href=https://en.wikipedia.org/wiki/List_of_nonbuilding_structure_types>structures</a> acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with <a href=https://en.wikipedia.org/wiki/Part_of_speech_tagging>part of speech tagging</a> acquired early and global topic information learned continuously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5439/>Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</a></strong><br><a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/j/jean-samuel-leboeuf/>Jean-Samuel Leboeuf</a>
|
<a href=/people/l/luc-lamontagne/>Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5439><div class="card-body p-3 small">We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> that predicts useful embeddings for OOV words based on their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>left context words</a>, the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>right context words</a> or the word&#8217;s characters, hence making the prediction more interpretable. The model is a drop-in module that is jointly trained with the downstream task&#8217;s <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, thus producing embeddings specialized for the task at hand. When the task is mostly syntactical, we observe that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> aims most of its attention on surface form characters. On the other hand, for tasks more semantical, the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> allocates more attention to the surrounding words. In all our tests, the <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> helps the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to achieve better performances in comparison to the use of simple random embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305194062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5441/>Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/j/j-edward-hu/>J. Edward Hu</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5441><div class="card-body p-3 small">We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence representation</a> encoded by a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> captures distinct types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. Our collection of diverse datasets is available at, and will grow over time as additional resources are recast and added from novel sources.<url>http://www.decomp.net/</url>, and will grow over time as additional resources are recast and added from novel sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5442/>Interpretable Word Embedding Contextualization</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a>
|
<a href=/people/s/sang-bum-kim/>Sang-Bum Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5442><div class="card-body p-3 small">In this paper, we propose a method of calibrating a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, so that the semantic it conveys becomes more relevant to the context. Our method is novel because the output shows clearly which senses that were originally presented in a target <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> become stronger or weaker. This is possible by utilizing the technique of using <a href=https://en.wikipedia.org/wiki/Sparse_coding>sparse coding</a> to recover senses that comprises a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5444/>Extracting Syntactic Trees from Transformer Encoder Self-Attentions</a></strong><br><a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5444><div class="card-body p-3 small">This is a work in progress about extracting the sentence tree structures from the encoder&#8217;s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing <a href=https://en.wikipedia.org/wiki/Syntax>syntactic theories</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5445/>Portable, layer-wise task performance monitoring for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a><span class=acl-fixed-case>NLP</span> models</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5445><div class="card-body p-3 small">There is a long-standing interest in understanding the internal behavior of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Deep neural architectures for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic). The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims. Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine. In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5447/>Explicitly modeling case improves neural dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5447><div class="card-body p-3 small">Neural dependency parsing models that compose word representations from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> can presumably exploit <a href=https://en.wikipedia.org/wiki/Morphosyntax>morphosyntax</a> when making attachment decisions. How much do they know about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? We investigate how well they handle <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological case</a>, which is important for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Our experiments on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> suggest that adding explicit morphological caseeither oracle or predictedimproves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5449/>Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/brian-murphy/>Brian Murphy</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5449><div class="card-body p-3 small">Performance in <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> has been significantly improved by training <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> on large corpora. This progress has come at the cost of interpretability and an understanding of how these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> function, making principled development of better <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> more difficult. We look inside a state-of-the-art neural language model to analyse how this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents high-level lexico-semantic information. In particular, we investigate how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents words by extracting activation patterns where they occur in the text, and compare these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> directly to human semantic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5450/>Interpretable Structure Induction via Sparse Attention</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5450><div class="card-body p-3 small">Neural network methods are experiencing wide adoption in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, thanks to their empirical performance on many tasks. Modern neural architectures go way beyond simple feedforward and recurrent models : they are complex pipelines that perform soft, differentiable computation instead of <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete logic</a>. The price of such <a href=https://en.wikipedia.org/wiki/Soft_computing>soft computing</a> is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction. Our recent work on sparse and structured latent computation presents a promising avenue for enhancing interpretability of such neural pipelines. Through this extended abstract, we aim to discuss and explore the potential and impact of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5451 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5451/>Debugging Sequence-to-Sequence Models with Seq2Seq-Vis<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Vis</a></strong><br><a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/m/michael-behrisch/>Michael Behrisch</a>
|
<a href=/people/a/adam-perer/>Adam Perer</a>
|
<a href=/people/h/hanspeter-pfister/>Hanspeter Pfister</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5451><div class="card-body p-3 small">Neural attention-based sequence-to-sequence models (seq2seq) (Sutskever et al., 2014 ; Bahdanau et al., 2014) have proven to be accurate and robust for many sequence prediction tasks. They have become the standard approach for automatic translation of text, at the cost of increased model complexity and <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>. End-to-end trained neural models act as a black box, which makes it difficult to examine model decisions and attribute errors to a specific part of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The highly connected and high-dimensional internal representations pose a challenge for analysis and visualization tools. The development of methods to understand seq2seq predictions is crucial for systems in production settings, as mistakes involving language are often very apparent to human readers. For instance, a widely publicized incident resulted from a translation system mistakenly translating good morning into attack them leading to a wrongful arrest (Hern, 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5453 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5453/>Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5453><div class="card-body p-3 small">Recent work has shown that neural models can be successfully trained on multiple languages simultaneously. We investigate whether such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5455/>End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/j/josiah-wang/>Josiah Wang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5455><div class="card-body p-3 small">We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn &#8216;distributional similarity&#8217; in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the &#8216;image&#8217; side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations ; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space ; (iii) cluster images with similar visual and linguistic information together. Our experiments all point to one fact : that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.</div></div></div><hr><div id=w18-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-55/>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5500/>Proceedings of the First Workshop on Fact Extraction and <span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>)</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5501/>The Fact Extraction and VERification (FEVER) Shared Task<span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>) Shared Task</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5501><div class="card-body p-3 small">We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a <a href=https://en.wikipedia.org/wiki/Fever>FEVER score</a> of 64.21 %. In this paper, we present the results of the shared task and a summary of the <a href=https://en.wikipedia.org/wiki/System>systems</a>, highlighting commonalities and innovations among participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5502/>The Data Challenge in Misinformation Detection : Source Reputation vs. Content Veracity</a></strong><br><a href=/people/f/fatemeh-torabi-asr/>Fatemeh Torabi Asr</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5502><div class="card-body p-3 small">Misinformation detection at the level of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>full news articles</a> is a text classification problem. Reliably labeled data in this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a> is rare. Previous work relied on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> collected from so-called reputable and suspicious websites and labeled accordingly. We leverage fact-checking websites to collect individually-labeled news articles with regard to the veracity of their content and use this data to test the cross-domain generalization of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on bigger text collections but labeled according to source reputation. Our results suggest that reputation-based classification is not sufficient for predicting the veracity level of the majority of news articles, and that the system performance on different test datasets depends on topic distribution. Therefore collecting well-balanced and carefully-assessed training data is a priority for developing robust misinformation detection systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5503/>Crowdsourcing Semantic Label Propagation in Relation Classification</a></strong><br><a href=/people/a/anca-dumitrache/>Anca Dumitrache</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/c/chris-welty/>Chris Welty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5503><div class="card-body p-3 small">Distant supervision is a popular method for performing <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> that is known to produce noisy labels. Most progress in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> has been made with crowdsourced corrections to distant-supervised labels, and there is evidence that indicates still more would be better. In this paper, we explore the problem of propagating human annotation signals gathered for open-domain relation classification through the CrowdTruth methodology for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, that captures ambiguity in annotations by measuring inter-annotator disagreement. Our approach propagates annotations to sentences that are similar in a low dimensional embedding space, expanding the number of labels by two orders of magnitude. Our experiments show significant improvement in a sentence-level multi-class relation classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5506/>Joint Modeling for <a href=https://en.wikipedia.org/wiki/Query_expansion>Query Expansion</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5506><div class="card-body p-3 small">Information extraction about an event can be improved by incorporating <a href=https://en.wikipedia.org/wiki/Empirical_evidence>external evidence</a>. In this study, we propose a joint model for pseudo-relevance feedback based query expansion and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates an event-specific query to effectively retrieve documents relevant to the event. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is comparable or has better performance than the previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in two publicly available datasets. Furthermore, we analyzed the influences of the retrieval effectiveness in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the <a href=https://en.wikipedia.org/wiki/Information_retrieval>extraction</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5508/>Belittling the Source : Trustworthiness Indicators to Obfuscate Fake News on the Web</a></strong><br><a href=/people/d/diego-esteves/>Diego Esteves</a>
|
<a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/p/piyush-chawla/>Piyush Chawla</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5508><div class="card-body p-3 small">With the growth of the <a href=https://en.wikipedia.org/wiki/Internet>internet</a>, the number of fake-news online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking algorithms</a> became a valuable asset. To this aim, an important step to detect <a href=https://en.wikipedia.org/wiki/Fake_news>fake-news</a> is to have access to a credibility score for a given <a href=https://en.wikipedia.org/wiki/Source_(journalism)>information source</a>. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore <a href=https://en.wikipedia.org/wiki/Confidentiality>confidential data</a> in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.<i>fake-news</i> online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, fact-checking algorithms became a valuable asset. To this aim, an important step to detect fake-news is to have access to a credibility score for a given information source. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5510/>Stance Detection in Fake News A Combined Feature Representation</a></strong><br><a href=/people/b/bilal-ghanem/>Bilal Ghanem</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/f/francisco-rangel/>Francisco Rangel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5510><div class="card-body p-3 small">With the uncontrolled increasing of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and rumors over the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and <a href=https://en.wikipedia.org/wiki/N-gram>n-gram features</a> to detect the stance in <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 % Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>feature representation</a>. Furthermore, we have investigated the importance of different <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> in the detection of the classification labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5513" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5513/>Where is Your Evidence : Improving <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact-checking</a> by Justification Modeling</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/s/savvas-petridis/>Savvas Petridis</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5513><div class="card-body p-3 small">Fact-checking is a <a href=https://en.wikipedia.org/wiki/Journalism>journalistic practice</a> that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a>. However, approaches based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5514/>Affordance Extraction and Inference based on Semantic Role Labeling</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/a/alipio-jorge/>Alípio Jorge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5514><div class="card-body p-3 small">Common-sense reasoning is becoming increasingly important for the advancement of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. While word embeddings have been very successful, they can not explain which aspects of &#8216;coffee&#8217; and &#8216;tea&#8217; make them similar, or how they could be related to &#8216;shop&#8217;. In this paper, we propose an explicit word representation that builds upon the Distributional Hypothesis to represent meaning from semantic roles, and allow inference of relations from their meshing, as supported by the affordance-based Indexical Hypothesis. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves the state-of-the-art on unsupervised word similarity tasks while allowing for direct inference of new relations from the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5515/>UCL Machine Reading Group : Four Factor Framework For Fact Finding (HexaF)<span class=acl-fixed-case>UCL</span> Machine Reading Group: Four Factor Framework For Fact Finding (<span class=acl-fixed-case>H</span>exa<span class=acl-fixed-case>F</span>)</a></strong><br><a href=/people/t/takuma-yoneda/>Takuma Yoneda</a>
|
<a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5515><div class="card-body p-3 small">In this paper we describe our 2nd place FEVER shared-task system that achieved a FEVER score of 62.52 % on the provisional test set (without additional human evaluation), and 65.41 % on the development set. Our system is a four stage model consisting of <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence retrieval</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and aggregation. Retrieval is performed leveraging task-specific features, and then a natural language inference model takes each of the retrieved sentences paired with the claimed fact. The resulting predictions are aggregated across retrieved sentences with a Multi-Layer Perceptron, and re-ranked corresponding to the final prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5516/>UKP-Athene : Multi-Sentence Textual Entailment for Claim Verification<span class=acl-fixed-case>UKP</span>-Athene: Multi-Sentence Textual Entailment for Claim Verification</a></strong><br><a href=/people/a/andreas-hanselowski/>Andreas Hanselowski</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/z/zile-li/>Zile Li</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/b/benjamin-schiller/>Benjamin Schiller</a>
|
<a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5516><div class="card-body p-3 small">The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>, in particular, <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, fact extraction, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>claim classification</a>. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5517/>Team Papelo : Transformer Networks at FEVER<span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5517><div class="card-body p-3 small">We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, to classify a broad set of potential evidence. The precision of the <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment classifier</a> allows us to enhance <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> by considering every statement from several articles to decide upon each claim. We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text. The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents). In preliminary evaluation, the system achieves.5736 FEVER score,.6108 label accuracy, and.6485 evidence F1 on the FEVER shared task test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5519/>SIRIUS-LTG : An Entity Linking Approach to Fact Extraction and Verification<span class=acl-fixed-case>SIRIUS</span>-<span class=acl-fixed-case>LTG</span>: An Entity Linking Approach to Fact Extraction and Verification</a></strong><br><a href=/people/f/farhad-nooralahzadeh/>Farhad Nooralahzadeh</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5519><div class="card-body p-3 small">This article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) Shared Task. It consists of three components : 1) Wikipedia Page Retrieval : First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a> 2) Sentence selection : We investigate various <a href=https://en.wikipedia.org/wiki/List_of_Internet_phenomena>techniques</a> i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) Textual Entailment : We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple <a href=https://en.wikipedia.org/wiki/Cosine_similarity>Cosine Similarity</a> using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence : 32.17, label accuracy : 59.61 and FEVER score : 0.3778). Furthermore, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.<i>Wikipedia Page Retrieval</i>: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2) <i>Sentence selection</i>: We investigate various techniques i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) <i>Textual Entailment</i>: We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence: 32.17, label accuracy: 59.61 and FEVER score: 0.3778). Furthermore, it obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5520/>Integrating Entity Linking and Evidence Ranking for Fact Extraction and Verification</a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5520><div class="card-body p-3 small">We describe here our <a href=https://en.wikipedia.org/wiki/System>system</a> and results on the FEVER shared task. We prepared a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> which composes of a document selection, a sentence retrieval, and a recognizing textual entailment (RTE) components. A simple entity linking approach with text match is used as the document selection component, this component identifies relevant documents for a given claim by using mentioned entities as clues. The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a>. Finally, the RTE component selects evidence sentences by ranking the sentences and classifies the claim simultaneously. The experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the <a href=https://en.wikipedia.org/wiki/FEVER>FEVER score</a> of 0.4016 and outperformed the official baseline system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5522/>DeFactoNLP : Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>F</span>acto<span class=acl-fixed-case>NLP</span>: Fact Verification using Entity Recognition, <span class=acl-fixed-case>TFIDF</span> Vector Comparison and Decomposable Attention</a></strong><br><a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/g/gil-rocha/>Gil Rocha</a>
|
<a href=/people/d/diego-esteves/>Diego Esteves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5522><div class="card-body p-3 small">In this paper, we describe DeFactoNLP, the <a href=https://en.wikipedia.org/wiki/System>system</a> we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> computed using these <a href=https://en.wikipedia.org/wiki/Probability>probabilities</a> are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5523/>An End-to-End Multi-task Learning Model for <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact Checking</a></a></strong><br><a href=/people/s/sizhen-li/>Sizhen Li</a>
|
<a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5523><div class="card-body p-3 small">With huge amount of information generated every day on the web, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> is an important and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Here we decompose this problem into two parts : an entity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment between the claim and selected pages. In this paper, we present an end-to-end multi-task learning with bi-direction attention (EMBA) model to classify the claim as supports, refutes or not enough info with respect to the pages retrieved and detect sentences as evidence at the same time. We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, a new public dataset for verification against textual sources. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5524/>Team GESIS Cologne : An all in all sentence-based approach for FEVER<span class=acl-fixed-case>GESIS</span> Cologne: An all in all sentence-based approach for <span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/w/wolfgang-otto/>Wolfgang Otto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5524><div class="card-body p-3 small">In this system description of our <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a>, we regarded single sentences as our processing unit. In our IR-Component, we searched in the set of all possible Wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. In the entailment module, we judged every sentence separately and combined the result of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5525/>Team SWEEPer : Joint Sentence Extraction and Fact Checking with Pointer Networks<span class=acl-fixed-case>SWEEP</span>er: Joint Sentence Extraction and Fact Checking with Pointer Networks</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5525><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> rely on information extracted from unreliable sources. These <a href=https://en.wikipedia.org/wiki/System>systems</a> would thus benefit from knowing whether a statement from an unreliable source is correct. We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and predicting whether a claim is supported by those sentences, refuted, or there is not enough information. Fact checking is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model <a href=https://en.wikipedia.org/wiki/Sentence_extraction>sentence extraction</a> and <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a> on the FEVER shared task. Among all participants, we ranked 5th on the <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test set</a> (prior to any additional human evaluation of the evidence).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5527/>Team UMBC-FEVER : Claim verification using Semantic Lexical Resources<span class=acl-fixed-case>UMBC</span>-<span class=acl-fixed-case>FEVER</span> : Claim verification using Semantic Lexical Resources</a></strong><br><a href=/people/a/ankur-padia/>Ankur Padia</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/tim-finin/>Tim Finin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5527><div class="card-body p-3 small">We describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used in the 2018 FEVER shared task. The system employed a frame-based information retrieval approach to select Wikipedia sentences providing evidence and used a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>two-layer multilayer perceptron</a> to classify a claim as correct or not. Our submission achieved a score of 0.3966 on the Evidence F1 metric with accuracy of 44.79 %, and FEVER score of 0.2628 F1 points.</div></div></div><hr><div id=w18-56><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-56.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-56/>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5600/>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5604/>Supervised Machine Learning for Extractive Query Based Summarisation of Biomedical Data</a></strong><br><a href=/people/m/mandeep-kaur/>Mandeep Kaur</a>
|
<a href=/people/d/diego-molla/>Diego Mollá</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5604><div class="card-body p-3 small">The automation of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarisation</a> of <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical publications</a> is a pressing need due to the plethora of information available online. This paper explores the impact of several supervised machine learning approaches for extracting multi-document summaries for given queries. In particular, we compare classification and regression approaches for query-based extractive summarisation using data provided by the BioASQ Challenge. We tackled the problem of annotating sentences for training classification systems and show that a simple annotation approach outperforms regression-based summarisation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5605/>Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition<span class=acl-fixed-case>CNN</span> and <span class=acl-fixed-case>LSTM</span> character-level embeddings in <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> models for chemical and disease named entity recognition</a></strong><br><a href=/people/z/zenan-zhai/>Zenan Zhai</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5605><div class="card-body p-3 small">We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> over word-based models by 25 % while the LSTM-based character-level word embeddings more than double the required <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5607/>Investigating the Challenges of Temporal Relation Extraction from Clinical Text</a></strong><br><a href=/people/d/diana-galvan/>Diana Galvan</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/koji-matsuda/>Koji Matsuda</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5607><div class="card-body p-3 small">Temporal reasoning remains as an unsolved task for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, particularly demonstrated in the <a href=https://en.wikipedia.org/wiki/Clinical_psychology>clinical domain</a>. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate : the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5608/>De-identifying Free Text of Japanese Dummy Electronic Health Records<span class=acl-fixed-case>J</span>apanese Dummy Electronic Health Records</a></strong><br><a href=/people/k/kohei-kajiyama/>Kohei Kajiyama</a>
|
<a href=/people/h/hiromasa-horiguchi/>Hiromasa Horiguchi</a>
|
<a href=/people/t/takashi-okumura/>Takashi Okumura</a>
|
<a href=/people/m/mizuki-morita/>Mizuki Morita</a>
|
<a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5608><div class="card-body p-3 small">A new law was established in Japan to promote utilization of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a> for research and developments, while <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> is required to use <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a>. However, studies of automatic de-identification in the healthcare domain is not active for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a>, no de-identification tool available in practical performance for Japanese medical domains, as far as we know. Previous work shows that rule-based methods are still effective, while <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> are reported to be better recently. In order to implement and evaluate a de-identification tool in a practical level, we implemented three methods, rule-based, CRF, and LSTM. We prepared three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of pseudo EHRs with de-identification tags manually annotated. These <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are derived from shared task data to compare with previous work, and our new data to increase training data. Our result shows that our LSTM-based method is better and robust, which leads to our future work that plans to apply our <a href=https://en.wikipedia.org/wiki/System>system</a> to actual de-identification tasks in hospitals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5609/>Unsupervised Identification of Study Descriptors in Toxicology Research : An Experimental Study</a></strong><br><a href=/people/d/drahomira-herrmannova/>Drahomira Herrmannova</a>
|
<a href=/people/s/steve-young/>Steven Young</a>
|
<a href=/people/r/robert-patton/>Robert Patton</a>
|
<a href=/people/c/christopher-stahl/>Christopher Stahl</a>
|
<a href=/people/n/nicole-kleinstreuer/>Nicole Kleinstreuer</a>
|
<a href=/people/m/mary-wolfe/>Mary Wolfe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5609><div class="card-body p-3 small">Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a>. Specifically, provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identify text segments (sentences) relevant to the criteria. A <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classifier</a> trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting the intuition that our method is able to accurately identify study descriptors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5613/>Iterative development of family history annotation guidelines using a synthetic corpus of clinical text</a></strong><br><a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/p/pal-brekke/>Pål Brekke</a>
|
<a href=/people/o/oystein-nytro/>Øystein Nytrø</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5613><div class="card-body p-3 small">In this article, we describe the development of annotation guidelines for <a href=https://en.wikipedia.org/wiki/Family_history_(medicine)>family history information</a> in <a href=https://en.wikipedia.org/wiki/Medical_literature>Norwegian clinical text</a>. We make use of incrementally developed synthetic clinical text describing patients&#8217; family history relating to cases of cardiac disease and present a general methodology which integrates the synthetically produced clinical statements and guideline development. We analyze <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> based on the developed guidelines and present results from experiments aimed at evaluating the validity and applicability of the <a href=https://en.wikipedia.org/wiki/Text_corpus>annotated corpus</a> using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a>. The resulting annotated corpus contains 477 sentences and 6030 tokens. Both the annotation guidelines and the annotated corpus are made freely available and as such constitutes the first publicly available resource of Norwegian clinical text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5615/>Analysis of Risk Factor Domains in Psychosis Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/n/nicholas-miller/>Nicholas Miller</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/m/marie-meteer/>Marie Meteer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei Hua-Hall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5615><div class="card-body p-3 small">Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary ; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long-term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> we will be incorporating in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5616/>Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks</a></strong><br><a href=/people/i/ivan-girardi/>Ivan Girardi</a>
|
<a href=/people/p/pengfei-ji/>Pengfei Ji</a>
|
<a href=/people/a/an-phi-nguyen/>An-phi Nguyen</a>
|
<a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/a/adam-ivankay/>Adam Ivankay</a>
|
<a href=/people/l/lorenz-kuhn/>Lorenz Kuhn</a>
|
<a href=/people/c/chiara-marchiori/>Chiara Marchiori</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5616><div class="card-body p-3 small">We present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best point of care and time to treat. We use an attention-based convolutional neural network architecture trained on 600,000 <a href=https://en.wikipedia.org/wiki/Medical_record>doctor notes</a> in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79 % and 66 % <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>, respectively, but on a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence threshold</a> of 0.6, <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> increases to 85 % and 75 %, respectively. In addition, a method to detect <a href=https://en.wikipedia.org/wiki/Atelectasis>warning symptoms</a> is implemented to render the classification task transparent from a medical perspective. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is based on the learning of attention scores and a method of <a href=https://en.wikipedia.org/wiki/Data_validation>automatic validation</a> using the same data.<i>point of care</i> and <i>time to treat</i>. We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79% and 66% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85% and 75%, respectively. In addition, a method to detect <i>warning symptoms</i> is implemented to render the classification task transparent from a medical perspective. The method is based on the learning of attention scores and a method of automatic validation using the same data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5617/>Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction</a></strong><br><a href=/people/j/joel-legrand/>Joël Legrand</a>
|
<a href=/people/y/yannick-toussaint/>Yannick Toussaint</a>
|
<a href=/people/c/chedy-raissi/>Chedy Raïssi</a>
|
<a href=/people/a/adrien-coulet/>Adrien Coulet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5617><div class="card-body p-3 small">Transfer learning (TL) proposes to enhance <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> performance on a <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, by reusing labeled data originally designed for a related problem. In particular, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> consists, for a specific <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, in reusing training data developed for the same <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> but a distinct domain. This is particularly relevant to the applications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains. In this paper, we experiment with TL for the task of Relation Extraction (RE) from biomedical texts, using the TreeLSTM model. We empirically show the impact of TreeLSTM alone and with domain adaptation by obtaining better performances than the state of the art on two biomedical RE tasks and equal performances for two others, for which few annotated data are available. Furthermore, we propose an analysis of the role that syntactic features may play in TL for RE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5618/>In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition</a></strong><br><a href=/people/g/golnar-sheikhshab/>Golnar Sheikhshabbafghi</a>
|
<a href=/people/i/inanc-birol/>Inanc Birol</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5618><div class="card-body p-3 small">Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, where texts are processed to annotate terms that are relevant for <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical studies</a>. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We show these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> improve <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5619/>Self-training improves <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> performance for Temporal Relation Extraction</a></strong><br><a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5619><div class="card-body p-3 small">Neural network models are oftentimes restricted by limited labeled instances and resort to advanced <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for cutting edge performance. We propose to build a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with multiple semantically heterogeneous embeddings within a self-training framework. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> makes use of labeled, unlabeled, and social media data, operates on basic features, and is scalable and generalizable. With this method, we establish the state-of-the-art result for both in- and cross-domain for a clinical temporal relation extraction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5620" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5620/>Listwise temporal ordering of events in clinical notes</a></strong><br><a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5620><div class="card-body p-3 small">We present metrics for listwise temporal ordering of events in clinical notes, as well as a baseline listwise temporal ranking model that generates a timeline of events that can be used in downstream medical natural language processing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5622/>Evaluation of a Sequence Tagging Tool for Biomedical Texts</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/m/matthieu-doutreligne/>Matthieu Doutreligne</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/n/nicolas-paris/>Nicolas Paris</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5622><div class="card-body p-3 small">Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> in a variety of text genres including articles from the biomedical literature in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.</div></div></div><hr><div id=w18-57><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-57.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-57/>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5700/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>SCAI</span>: The 2nd International Workshop on Search-Oriented Conversational <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/a/aleksandr-chuklin/>Aleksandr Chuklin</a>
|
<a href=/people/j/jeff-dalton/>Jeff Dalton</a>
|
<a href=/people/j/julia-kiseleva/>Julia Kiseleva</a>
|
<a href=/people/a/alexey-borisov/>Alexey Borisov</a>
|
<a href=/people/m/mikhail-burtsev/>Mikhail Burtsev</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5701" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5701/>Neural Response Ranking for Social Conversation : A Data-Efficient Approach</a></strong><br><a href=/people/i/igor-shalyminov/>Igor Shalyminov</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5701><div class="card-body p-3 small">The overall objective of &#8216;social&#8217; dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics, including social chit-chat. Apart from raw dialogue data, user-provided ratings are the most common signal used to train such systems to produce engaging responses. In this paper we show that social dialogue systems can be trained effectively from <a href=https://en.wikipedia.org/wiki/Raw_data>raw unannotated data</a>. Using a dataset of real conversations collected in the 2017 Alexa Prize challenge, we developed a neural ranker for selecting &#8216;good&#8217; system responses to <a href=https://en.wikipedia.org/wiki/User-generated_content>user utterances</a>, i.e. responses which are likely to lead to long and engaging conversations. We show that (1) our neural ranker consistently outperforms several strong baselines when trained to optimise for user ratings ; (2) when trained on larger amounts of data and only using conversation length as the objective, the <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> performs better than the one trained using ratings ultimately reaching a Precision@1 of 0.87. This advance will make <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> for social conversational agents simpler and less expensive in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5708/>Data Augmentation for Neural Online Chats Response Selection</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5708><div class="card-body p-3 small">Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5710/>Embedding Individual Table Columns for Resilient SQL Chatbots<span class=acl-fixed-case>SQL</span> Chatbots</a></strong><br><a href=/people/b/bojan-petrovski/>Bojan Petrovski</a>
|
<a href=/people/i/ignacio-aguado/>Ignacio Aguado</a>
|
<a href=/people/a/andreea-hossmann/>Andreea Hossmann</a>
|
<a href=/people/m/michael-baeriswyl/>Michael Baeriswyl</a>
|
<a href=/people/c/claudiu-musat/>Claudiu Musat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5710><div class="card-body p-3 small">Most of the world&#8217;s data is stored in <a href=https://en.wikipedia.org/wiki/Relational_database>relational databases</a>. Accessing these requires specialized knowledge of the Structured Query Language (SQL), putting them out of the reach of many people. A recent research thread in Natural Language Processing (NLP) aims to alleviate this problem by automatically translating natural language questions into SQL queries. While the proposed solutions are a great start, they lack robustness and do not easily generalize : the methods require high quality descriptions of the <a href=https://en.wikipedia.org/wiki/Column_(database)>database table columns</a>, and the most widely used training dataset, WikiSQL, is heavily biased towards using those descriptions as part of the questions. In this work, we propose solutions to both problems : we entirely eliminate the need for column descriptions, by relying solely on their contents, and we augment the WikiSQL dataset by paraphrasing column names to reduce bias. We show that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of existing methods drops when trained on our augmented, column-agnostic dataset, and that our own method reaches state of the art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while relying on column contents only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5711/>Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding</a></strong><br><a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5711><div class="card-body p-3 small">Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Most approaches for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> rely solely on the domain-specific datasets for training. We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup. Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. In particular, NER is effective when supervised at the lower layer of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. For low-resource scenarios, we found that MTL is effective for one dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5712 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5712/>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</a></strong><br><a href=/people/s/shaojie-jiang/>Shaojie Jiang</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5712><div class="card-body p-3 small">Diversity is a long-studied topic in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> matters : an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have <a href=https://en.wikipedia.org/wiki/Diversity_index>low diversity</a>. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence penalties</a> and label smoothing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5713/>Retrieve and Refine : Improved Sequence Generation Models For Dialogue</a></strong><br><a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/a/alexander-miller/>Alexander Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5713><div class="card-body p-3 small">Sequence generation models for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> are known to have several problems : they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the other hand can surface interesting responses, but are restricted to the given retrieval set leading to erroneous replies that can not be tuned to the specific context. In this work we develop a model that combines the two approaches to avoid both their deficiencies : first retrieve a response and then refine it the final sequence generator treating the retrieval as additional context. We show on the recent ConvAI2 challenge task our approach produces responses superior to both standard retrieval and generation models in human evaluations.</div></div></div><hr><div id=w18-58><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-58.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-58/>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5800/>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/s/sandra-kubler/>Sandra Kuebler</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5801/>Efficient Computation of Implicational Universals in Constraint-Based Phonology Through the <a href=https://en.wikipedia.org/wiki/Hyperplane_separation_theorem>Hyperplane Separation Theorem</a></a></strong><br><a href=/people/g/giorgio-magri/>Giorgio Magri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5801><div class="card-body p-3 small">This paper focuses on the most basic implicational universals in <a href=https://en.wikipedia.org/wiki/Phonology>phonological theory</a>, called T-orders after Anttila and Andrus (2006). It develops necessary and sufficient constraint characterizations of T-orders within Harmonic Grammar and <a href=https://en.wikipedia.org/wiki/Optimality_theory>Optimality Theory</a>. These conditions rest on the rich convex geometry underlying these frameworks. They are phonologically intuitive and have significant algorithmic implications.<i>implicational universals</i> in phonological theory, called <i>T-orders</i> after Anttila and Andrus (2006). It develops necessary and sufficient constraint characterizations of T-orders within <i>Harmonic Grammar</i> and <i>Optimality Theory</i>. These conditions rest on the rich convex geometry underlying these frameworks. They are phonologically intuitive and have significant algorithmic implications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5802/>Lexical Networks in ! Xung<span class=acl-fixed-case>X</span>ung</a></strong><br><a href=/people/s/syed-amad-hussain/>Syed-Amad Hussain</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/a/amanda-miller/>Amanda Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5802><div class="card-body p-3 small">We investigate the lexical network properties of the large phoneme inventory Southern African language Mangetti Dune ! Xung as it compares to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other commonly-studied languages. Lexical networks are graphs in which nodes (words) are linked to their minimal pairs ; global properties of these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> are believed to mediate <a href=https://en.wikipedia.org/wiki/Lexical_access>lexical access</a> in the minds of speakers. We show that the network properties of ! Xung are within the range found in previously-studied languages. By simulating data (pseudolexicons) with varying levels of phonotactic structure, we find that the lexical network properties of ! Xung diverge from previously-studied languages when fewer phonotactic constraints are retained. We conclude that lexical network properties are representative of an underlying cognitive structure which is necessary for efficient word retrieval and that the <a href=https://en.wikipedia.org/wiki/Phonotactics>phonotactics</a> of ! Xung may be shaped by a selective pressure which preserves network properties within this cognitively useful range.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5803/>Acoustic Word Disambiguation with Phonogical Features in Danish ASR<span class=acl-fixed-case>D</span>anish <span class=acl-fixed-case>ASR</span></a></strong><br><a href=/people/a/andreas-soeborg-kirkedal/>Andreas Søeborg Kirkedal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5803><div class="card-body p-3 small">Phonological features can indicate <a href=https://en.wikipedia.org/wiki/Word_class>word class</a> and we can use <a href=https://en.wikipedia.org/wiki/Word_class>word class information</a> to disambiguate both <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> and <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a> in automatic speech recognition (ASR). We show Danish std can be predicted from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> and used to improve <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR</a>. We discover which acoustic features contain the signal of <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a>, how to use these features to predict <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a> and how we can make use of <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a> and stdpredictive acoustic features to improve overall ASR accuracy and decoding speed. In the process, we discover acoustic features that are novel to the phonetic characterisation of std.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5804/>Adaptor Grammars for the Linguist : <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> Experiments for Very Low-Resource Languages<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for the Linguist: Word Segmentation Experiments for Very Low-Resource Languages</a></strong><br><a href=/people/p/pierre-godard/>Pierre Godard</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/m/martine-adda-decker/>Martine Adda-Decker</a>
|
<a href=/people/g/gilles-adda/>Gilles Adda</a>
|
<a href=/people/h/helene-bonneau-maynard/>Hélène Maynard</a>
|
<a href=/people/a/annie-rialland/>Annie Rialland</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5804><div class="card-body p-3 small">Computational Language Documentation attempts to make the most recent research in speech and language technologies available to linguists working on <a href=https://en.wikipedia.org/wiki/Language_preservation>language preservation</a> and documentation. In this paper, we pursue two main goals along these lines. The first is to improve upon a strong baseline for the unsupervised word discovery task on two very low-resource Bantu languages, taking advantage of the expertise of linguists on these particular languages. The second consists in exploring the Adaptor Grammar framework as a decision and prediction tool for linguists studying a new language. We experiment 162 grammar configurations for each language and show that using Adaptor Grammars for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> enables us to test hypotheses about a language. Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30 % token F-score from the results of a strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5805/>String Transduction with Target Language Models and Insertion Handling</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/s/saeed-najafi/>Saeed Najafi</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5805><div class="card-body p-3 small">Many character-level tasks can be framed as sequence-to-sequence transduction, where the target is a word from a <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We show that leveraging target language models derived from unannotated target corpora, combined with a precise alignment of the training data, yields state-of-the art results on cognate projection, inflection generation, and phoneme-to-grapheme conversion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5807/>Modeling Reduplication with 2-way Finite-State Transducers</a></strong><br><a href=/people/h/hossep-dolatian/>Hossep Dolatian</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5807><div class="card-body p-3 small">This article describes a novel approach to the computational modeling of reduplication. Reduplication is a well-studied <a href=https://en.wikipedia.org/wiki/Phenomenon>linguistic phenomenon</a>. However, it is often treated as a stumbling block within finite-state treatments of <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>. Most finite-state implementations of computational morphology can not adequately capture the productivity of unbounded copying in <a href=https://en.wikipedia.org/wiki/Reduplication>reduplication</a>, nor can they adequately capture bounded copying. We show that an understudied type of finite-state machines, two-way finite-state transducers (2-way FSTs), captures virtually all reduplicative processes, including total reduplication. 2-way FSTs can model reduplicative typology in a way which is convenient, easy to design and debug in practice, and linguistically-motivated. By virtue of being finite-state, 2-way FSTs are likewise incorporable into existing <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state systems</a> and <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>. A small but representative typology of reduplicative processes is described in this article, alongside their corresponding 2-way FST models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5809/>A Comparison of Entity Matching Methods between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Japanese Katakana<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>J</span>apanese Katakana</a></strong><br><a href=/people/m/michiharu-yamashita/>Michiharu Yamashita</a>
|
<a href=/people/h/hideki-awashima/>Hideki Awashima</a>
|
<a href=/people/h/hidekazu-oiwa/>Hidekazu Oiwa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5809><div class="card-body p-3 small">Japanese Katakana is one component of the <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese writing system</a> and is used to express English terms, <a href=https://en.wikipedia.org/wiki/Loanword>loanwords</a>, and <a href=https://en.wikipedia.org/wiki/Onomatopoeia>onomatopoeia</a> in <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese characters</a> based on the <a href=https://en.wikipedia.org/wiki/Japanese_phonology>phonemes</a>. The main purpose of this research is to find the best entity matching methods between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Katakana. We built two research questions to clarify which types of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching systems</a> works better than others. The first question is what <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> should be used for <a href=https://en.wikipedia.org/wiki/Conversion_(word_formation)>conversion</a>. We need to transliterate English or Katakana terms into the same form in order to compute the <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>. We consider five conversions that transliterate <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Katakana directly, <a href=https://en.wikipedia.org/wiki/Katakana>Katakana</a> to English directly, <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Katakana via phoneme, Katakana to <a href=https://en.wikipedia.org/wiki/English_language>English</a> via phoneme, and both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Katakana to phoneme. The second question is what should be used for the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measure</a> at <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching</a>. To investigate the problem, we choose six methods, which are <a href=https://en.wikipedia.org/wiki/Overlap_coefficient>Overlap Coefficient</a>, <a href=https://en.wikipedia.org/wiki/Cosine>Cosine</a>, <a href=https://en.wikipedia.org/wiki/Jaccard>Jaccard</a>, Jaro-Winkler, Levenshtein, and the similarity of the phoneme probability predicted by RNN. Our results show that 1) matching using <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> and conversion of Katakana to <a href=https://en.wikipedia.org/wiki/English_language>English</a> works better than other methods, and 2) the similarity of phonemes outperforms other methods while other similarity score is changed depending on data and models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5810/>Seq2Seq Models with Dropout can Learn Generalizable Reduplication<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Models with Dropout can Learn Generalizable Reduplication</a></strong><br><a href=/people/b/brandon-prickett/>Brandon Prickett</a>
|
<a href=/people/a/aaron-traylor/>Aaron Traylor</a>
|
<a href=/people/j/joe-pater/>Joe Pater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5810><div class="card-body p-3 small">Natural language reduplication can pose a challenge to neural models of language, and has been argued to require <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>variables</a> (Marcus et al., 1999). Sequence-to-sequence neural networks have been shown to perform well at a number of other morphological tasks (Cotterell et al., 2016), and produce results that highly correlate with human behavior (Kirov, 2017 ; Kirov & Cotterell, 2018) but do not include any explicit variables in their architecture. We find that they can learn a reduplicative pattern that generalizes to novel segments if they are trained with dropout (Srivastava et al., 2014). We argue that this matches the scope of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> observed in human reduplication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5811" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5811/>A Characterwise Windowed Approach to Hebrew Morphological Segmentation<span class=acl-fixed-case>H</span>ebrew Morphological Segmentation</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5811><div class="card-body p-3 small">This paper presents a novel approach to the segmentation of orthographic word forms in contemporary <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, focusing purely on splitting without carrying out <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> or <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>. Casting the analysis task as character-wise binary classification and using adjacent character and word-based lexicon-lookup features, this approach achieves over 98 % accuracy on the benchmark SPMRL shared task data for <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, and 97 % accuracy on a new out of domain Wikipedia dataset, an improvement of 4 % and 5 % over previous state of the art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5812/>Phonetic Vector Representations for Sound Sequence Alignment</a></strong><br><a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5812><div class="card-body p-3 small">This study explores a number of data-driven vector representations of the IPA-encoded sound segments for the purpose of sound sequence alignment. We test the alternative <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> based on the alignment accuracy in the context of computational historical linguistics. We show that the data-driven methods consistently do better than linguistically-motivated articulatory-acoustic features. The similarity scores obtained using the data-driven representations in a monolingual context, however, performs worse than the state-of-the-art distance (or similarity) scoring methods proposed in earlier studies of computational historical linguistics. We also show that adapting representations to the task at hand improves the results, yielding alignment accuracy comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5813/>Sounds Wilde. Phonetically Extended Embeddings for Author-Stylized Poetry Generation</a></strong><br><a href=/people/a/aleksey-tikhonov/>Aleksey Tikhonov</a>
|
<a href=/people/i/ivan-p-yamshchikov/>Ivan P. Yamshchikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5813><div class="card-body p-3 small">This paper addresses author-stylized text generation. Using a version of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with extended phonetic and semantic embeddings for poetry generation we show that <a href=https://en.wikipedia.org/wiki/Phonetics>phonetics</a> has comparable contribution to the overall model performance as the information on the target author. Phonetic information is shown to be important for <a href=https://en.wikipedia.org/wiki/English_language>English and Russian language</a>. Humans tend to attribute machine generated texts to the target author.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5814/>On Hapax Legomena and Morphological Productivity</a></strong><br><a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a>
|
<a href=/people/r/ramon-granell/>Ramon Granell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5814><div class="card-body p-3 small">Quantifying and predicting morphological productivity is a long-standing challenge in <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus linguistics</a> and <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>. The same challenge reappears in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> in the context of handling words that were not seen in the training set (out-of-vocabulary, or OOV, words). Prior research showed that a good indicator of the productivity of a <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> is the number of words involving it that occur exactly once (the hapax legomena). A technical connection was adduced between this result and <a href=https://en.wikipedia.org/wiki/Good-Turing_smoothing>Good-Turing smoothing</a>, which assigns <a href=https://en.wikipedia.org/wiki/Probability_mass_function>probability mass</a> to unseen events on the basis of the simplifying assumption that <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequencies</a> are stationary. In a large-scale study of 133 affixes in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.<i>hapax legomena</i>). A technical connection was adduced between this result and Good-Turing smoothing, which assigns probability mass to unseen events on the basis of the simplifying assumption that word frequencies are stationary. In a large-scale study of 133 affixes in Wikipedia, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5816 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5816/>An Arabic Morphological Analyzer and Generator with Copious Features<span class=acl-fixed-case>A</span>rabic Morphological Analyzer and Generator with Copious Features</a></strong><br><a href=/people/d/dima-taji/>Dima Taji</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/o/ossama-obeid/>Ossama Obeid</a>
|
<a href=/people/f/fadhl-eryani/>Fadhl Eryani</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5816><div class="card-body p-3 small">We introduce CALIMA-Star, a very rich Arabic morphological analyzer and generator that provides functional and form-based morphological features as well as built-in tokenization, phonological representation, lexical rationality and much more. This tool includes a fast engine that can be easily integrated into other <a href=https://en.wikipedia.org/wiki/System>systems</a>, as well as an easy-to-use API and a <a href=https://en.wikipedia.org/wiki/User_interface>web interface</a>. CALIMA-Star also supports morphological reinflection. We evaluate CALIMA-Star against four commonly used analyzers for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in terms of speed and morphological content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5817/>Sanskrit n-Retroflexion is Input-Output Tier-Based Strictly Local<span class=acl-fixed-case>S</span>anskrit n-Retroflexion is Input-Output Tier-Based Strictly Local</a></strong><br><a href=/people/t/thomas-graf/>Thomas Graf</a>
|
<a href=/people/c/connor-mayer/>Connor Mayer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5817><div class="card-body p-3 small">Sanskrit /n/-retroflexion is one of the most complex segmental processes in <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>. While <a href=https://en.wikipedia.org/wiki/It_(novel)>it</a> is still star-free, <a href=https://en.wikipedia.org/wiki/It_(novel)>it</a> does not fit in any of the subregular classes that are commonly entertained in the literature. We show that when construed as a phonotactic dependency, the process fits into a <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>class</a> we call input-output tier-based strictly local (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL&#8217;s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.<i>input-output tier-based strictly local</i> (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL&#8217;s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5818 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5818/>Phonological Features for Morphological Inflection</a></strong><br><a href=/people/a/adam-wiemerslage/>Adam Wiemerslage</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5818><div class="card-body p-3 small">Modeling morphological inflection is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. In contrast to earlier work that has largely used <a href=https://en.wikipedia.org/wiki/Orthographic_representation>orthographic representations</a>, we experiment with this task in a phonetic character space, representing inputs as either <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>IPA segments</a> or bundles of <a href=https://en.wikipedia.org/wiki/Distinctive_feature>phonological distinctive features</a>. We show that both of these inputs, somewhat counterintuitively, achieve similar accuracies on morphological inflection, slightly lower than orthographic models. We conclude that providing detailed phonological representations is largely redundant when compared to <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>IPA segments</a>, and that articulatory distinctions relevant for <a href=https://en.wikipedia.org/wiki/Inflection>word inflection</a> are already latently present in the distributional properties of many graphemic writing systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5819.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5819 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5819 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5819/>Extracting Morphophonology from Small Corpora</a></strong><br><a href=/people/m/marina-ermolaeva/>Marina Ermolaeva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5819><div class="card-body p-3 small">Probabilistic approaches have proven themselves well in learning <a href=https://en.wikipedia.org/wiki/Phonology>phonological structure</a>. In contrast, <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a> usually works with deterministic generalizations. The goal of this paper is to explore possible interactions between information-theoretic methods and deterministic linguistic knowledge and to examine some ways in which both can be used in tandem to extract phonological and morphophonological patterns from a small annotated dataset. Local and nonlocal processes in Mishar Tatar (Turkic / Kipchak) are examined as a case study.</div></div></div><hr><div id=w18-59><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-59.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-59/>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5900/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></strong><br><a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/m/michael-paul/>Michael Paul</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5901/>Football and Beer - a Social Media Analysis on <span class=acl-fixed-case>T</span>witter in Context of the <span class=acl-fixed-case>FIFA</span> Football World Cup 2018</a></strong><br><a href=/people/r/roland-roller/>Roland Roller</a>
|
<a href=/people/p/philippe-thomas/>Philippe Thomas</a>
|
<a href=/people/s/sven-schmeier/>Sven Schmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5901><div class="card-body p-3 small">In many societies alcohol is a legal and common recreational substance and socially accepted. Alcohol consumption often comes along with social events as it helps people to increase their sociability and to overcome their inhibitions. On the other hand we know that increased alcohol consumption can lead to serious health issues, such as cancer, cardiovascular diseases and diseases of the digestive system, to mention a few. This work examines alcohol consumption during the FIFA Football World Cup 2018, particularly the usage of alcohol related information on Twitter. For this we analyse the tweeting behaviour and show that the tournament strongly increases the interest in beer. Furthermore we show that countries who had to leave the tournament at early stage might have done something good to their fans as the interest in beer decreased again.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5902/>Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts</a></strong><br><a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/a/andreas-kerren/>Andreas Kerren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5902><div class="card-body p-3 small">The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5904/>Overview of the Third Social Media Mining for Health (SMM4H) Shared Tasks at EMNLP 2018<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>) Shared Tasks at <span class=acl-fixed-case>EMNLP</span> 2018</a></strong><br><a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5904><div class="card-body p-3 small">The goals of the SMM4H shared tasks are to release annotated social media based health related datasets to the research community, and to compare the performances of natural language processing and machine learning systems on tasks involving these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. The third execution of the SMM4H shared tasks, co-hosted with EMNLP-2018, comprised of four subtasks. These subtasks involve annotated user posts from Twitter (tweets) and focus on the (i) automatic classification of tweets mentioning a drug name, (ii) automatic classification of tweets containing reports of first-person medication intake, (iii) automatic classification of tweets presenting self-reports of adverse drug reaction (ADR) detection, and (iv) automatic classification of vaccine behavior mentions in tweets. A total of 14 teams participated and 78 system runs were submitted (23 for task 1, 20 for task 2, 18 for task 3, 17 for task 4).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5909" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5909/>Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi-Head Self-Attention</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5909><div class="card-body p-3 small">This paper describes our system for the first and third shared tasks of the third Social Media Mining for Health Applications (SMM4H) workshop, which aims to detect the tweets mentioning drug names and adverse drug reactions. In our system we propose a neural approach with hierarchical tweet representation and multi-head self-attention (HTR-MSA) for both tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the first place in both the first and third shared tasks of SMM4H with an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 91.83 % and 52.20 % respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5911/>Shot Or Not : Comparison of NLP Approaches for Vaccination Behaviour Detection<span class=acl-fixed-case>NLP</span> Approaches for Vaccination Behaviour Detection</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cécile Paris</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5911><div class="card-body p-3 small">Vaccination behaviour detection deals with predicting whether or not a person received / was about to receive a vaccine. We present our submission for vaccination behaviour detection shared task at the SMM4H workshop. Our findings are based on three prevalent text classification approaches : rule-based, statistical and deep learning-based. Our final submissions are : (1) an ensemble of statistical classifiers with task-specific features derived using lexicons, language processing tools and word embeddings ; and, (2) a LSTM classifier with pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5913/>IRISA at SMM4H 2018 : <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> and Bagging for Tweet Classification<span class=acl-fixed-case>IRISA</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2018: Neural Network and Bagging for Tweet Classification</a></strong><br><a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/c/christian-raymond/>Christian Raymond</a>
|
<a href=/people/v/vincent-claveau/>Vincent Claveau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5913><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> developed by IRISA to participate to the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> of the SMM4H 2018 challenge. For these tweet classification tasks, we adopt a common approach based on recurrent neural networks (BiLSTM). Our main contributions are the use of certain <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, the use of Bagging in order to deal with unbalanced datasets, and on the automatic selection of difficult examples. These techniques allow us to reach 91.4, 46.5, 47.8, 85.0 as F1-scores for Tasks 1 to 4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5914/>Drug-Use Identification from Tweets with Word and Character N-Grams</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5914><div class="card-body p-3 small">This paper describes our systems in <a href=https://en.wikipedia.org/wiki/Social_media_mining>social media mining</a> for health applications (SMM4H) shared task. We participated in all four tracks of the shared task using <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a> with a combination of character and word n-gram features. We did not use any external data or domain specific information. The resulting <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved above-average scores among other participating systems, with F1-scores of 91.22, 46.8, 42.4, and 85.53 on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> 1, 2, 3, and 4 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5917/>Deep Learning for Social Media Health Text Classification</a></strong><br><a href=/people/s/santosh-tokala/>Santosh Tokala</a>
|
<a href=/people/v/vaibhav-gambhir/>Vaibhav Gambhir</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5917><div class="card-body p-3 small">This paper describes the systems developed for 1st and 2nd tasks of the 3rd Social Media Mining for Health Applications Shared Task at EMNLP 2018. The first <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on automatic detection of posts mentioning a drug name or <a href=https://en.wikipedia.org/wiki/Dietary_supplement>dietary supplement</a>, a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>. The second <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is about distinguishing the tweets that present <a href=https://en.wikipedia.org/wiki/Medication>personal medication intake</a>, possible <a href=https://en.wikipedia.org/wiki/Medication>medication intake</a> and <a href=https://en.wikipedia.org/wiki/Drug>non-intake</a>. We performed extensive experiments with various classifiers like Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDT) and deep learning architectures such as Long Short-Term Memory Networks (LSTM), jointed Convolutional Neural Networks (CNN) and LSTM architecture, and attention based LSTM architecture both at word and character level. We have also explored using various pre-trained embeddings like Global Vectors for Word Representation (GloVe), Word2Vec and task-specific embeddings learned using CNN-LSTM and LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5919.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5919 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5919 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5919/>Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets</a></strong><br><a href=/people/r/rupsa-saha/>Rupsa Saha</a>
|
<a href=/people/a/abir-naskar/>Abir Naskar</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a>
|
<a href=/people/l/lipika-dey/>Lipika Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5919><div class="card-body p-3 small">In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> into <a href=https://en.wikipedia.org/wiki/Adverse_drug_reaction>Adverse Drug Events</a> or not. We have evaluated our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of 78.5 %, 88 % and 82.9 % respectively for Task1 and 33.2 %, 54.7 % and 41.3 % for Task3.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5920 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5920/>CLaC at SMM4H Task 1, 2, and 4<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> Task 1, 2, and 4</a></strong><br><a href=/people/p/parsa-bagherzadeh/>Parsa Bagherzadeh</a>
|
<a href=/people/n/nadia-sheikh/>Nadia Sheikh</a>
|
<a href=/people/s/sabine-bergler/>Sabine Bergler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5920><div class="card-body p-3 small">CLaC Labs participated in Tasks 1, 2, and 4 using the same base architecture for all tasks with various parameter variations. This was our first exploration of this data and the SMM4H Tasks, thus a unified system was useful to compare the behavior of our architecture over the different datasets and how they interact with different linguistic features.</div></div></div><hr><div id=w18-60><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-60.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-60/>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6000/>Proceedings of the Second Workshop on Universal Dependencies (<span class=acl-fixed-case>UDW</span> 2018)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6001/>Assessing the Impact of Incremental Error Detection and Correction. A Case Study on the Italian Universal Dependency Treebank<span class=acl-fixed-case>I</span>talian <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebank</a></strong><br><a href=/people/c/chiara-alzetta/>Chiara Alzetta</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a>
|
<a href=/people/g/giulia-venturi/>Giulia Venturi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6001><div class="card-body p-3 small">Detection and correction of errors and inconsistencies in gold treebanks are becoming more and more central topics of corpus annotation. The paper illustrates a new incremental method for enhancing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>, with particular emphasis on the extension of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error patterns</a> across different textual genres and registers. Impact and role of corrections have been assessed in a dependency parsing experiment carried out with four different <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, whose results are promising. For both evaluation datasets, the performance of parsers increases, in terms of the standard LAS and UAS measures and of a more focused measure taking into account only relations involved in error patterns, and at the level of individual dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6002/>Using Universal Dependencies in cross-linguistic complexity research<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies in cross-linguistic complexity research</a></strong><br><a href=/people/a/aleksandrs-berdicevskis/>Aleksandrs Berdicevskis</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/k/katharina-ehret/>Katharina Ehret</a>
|
<a href=/people/k/kilu-von-prince/>Kilu von Prince</a>
|
<a href=/people/d/daniel-ross/>Daniel Ross</a>
|
<a href=/people/b/bill-thompson/>Bill Thompson</a>
|
<a href=/people/c/chunxiao-yan/>Chunxiao Yan</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/g/gary-lupyan/>Gary Lupyan</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/c/christian-bentz/>Christian Bentz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6002><div class="card-body p-3 small">We evaluate corpus-based measures of linguistic complexity obtained using Universal Dependencies (UD) treebanks. We propose a method of estimating <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> values obtained using a given <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> and a given <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. The results indicate that measures of syntactic complexity might be on average less robust than those of morphological complexity. We also estimate the validity of <a href=https://en.wikipedia.org/wiki/Complexity_measure>complexity measures</a> by comparing the results for very similar languages and checking for unexpected differences. We show that some of those differences that arise can be diminished by using parallel treebanks and, more importantly from the practical point of view, by harmonizing the language-specific solutions in the UD annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6003/>Expletives in Universal Dependency Treebanks<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks</a></strong><br><a href=/people/g/gosse-bouma/>Gosse Bouma</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/d/dag-haug/>Dag Haug</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/per-erik-solberg/>Per Erik Solberg</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6003><div class="card-body p-3 small">Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a cross-linguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>core argument</a> (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> that are applicable across languages and compatible with the goals of UD, give an overview of <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> as found in current UD treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6007/>Integration complexity and the order of cosisters</a></strong><br><a href=/people/w/william-dyer/>William Dyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6007><div class="card-body p-3 small">The cost of integrating dependent constituents to their heads is thought to involve the distance between dependent and head and the complexity of the integration (Gibson, 1998). The former has been convincingly addressed by Dependency Distance Minimization (DDM) (cf. Liu et al., 2017). The current study addresses the latter by proposing a novel theory of integration complexity derived from the entropy of the probability distribution of a dependent&#8217;s heads. An analysis of Universal Dependency corpora provides empirical evidence regarding the preferred order of isomorphic cosisterssister constituents of the same syntactic form on the same side of their headsuch as the adjectives in pretty blue fish. Integration complexity, alongside DDM, allows for a general theory of <a href=https://en.wikipedia.org/wiki/Constituent_order>constituent order</a> based on integration cost.<i>pretty blue fish</i>. Integration complexity, alongside DDM, allows for a general theory of constituent order based on integration cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6008/>SUD or Surface-Syntactic Universal Dependencies : An annotation scheme near-isomorphic to UD<span class=acl-fixed-case>SUD</span> or Surface-Syntactic <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies: An annotation scheme near-isomorphic to <span class=acl-fixed-case>UD</span></a></strong><br><a href=/people/k/kim-gerdes/>Kim Gerdes</a>
|
<a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/s/sylvain-kahane/>Sylvain Kahane</a>
|
<a href=/people/g/guy-perrier/>Guy Perrier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6008><div class="card-body p-3 small">This article proposes a surface-syntactic annotation scheme called SUD that is near-isomorphic to the Universal Dependencies (UD) annotation scheme while following distributional criteria for defining the dependency tree structure and the naming of the syntactic functions. Rule-based graph transformation grammars allow for a bi-directional transformation of UD into SUD. The back-and-forth transformation can serve as an error-mining tool to assure the intra-language and inter-language coherence of the UD treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6011/>Marrying Universal Dependencies and Universal Morphology<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies and <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>M</span>orphology</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6011><div class="card-body p-3 small">The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languagesUD at the token level and UniMorph at the type level. As each <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project&#8217;s annotations could be used to validate the other&#8217;s. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6012/>Enhancing Universal Dependency Treebanks : A Case Study<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks: A Case Study</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/paola-marongiu/>Paola Marongiu</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6012><div class="card-body p-3 small">We evaluate two cross-lingual techniques for adding enhanced dependencies to existing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> in Universal Dependencies. We apply a rule-based system developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a data-driven system trained on <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. We find that both systems are accurate enough to bootstrap enhanced dependencies in existing UD treebanks. In the case of <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, results are even on par with those of a prototype language-specific system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6017/>Multi-source synthetic treebank creation for improved cross-lingual dependency parsing</a></strong><br><a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/m/mariya-sheyanova/>Mariya Sheyanova</a>
|
<a href=/people/a/aleksandra-martynova/>Aleksandra Martynova</a>
|
<a href=/people/p/pavel-stepachev/>Pavel Stepachev</a>
|
<a href=/people/k/konstantin-vinogorodskiy/>Konstantin Vinogorodskiy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6017><div class="card-body p-3 small">This paper describes a method of creating synthetic treebanks for cross-lingual dependency parsing using a combination of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (including pivot translation), annotation projection and the spanning tree algorithm. Sentences are first automatically translated from a lesser-resourced language to a number of related highly-resourced languages, parsed and then the annotations are projected back to the lesser-resourced language, leading to multiple <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> for each sentence from the lesser-resourced language. The final treebank is created by merging the possible <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and running the spanning tree algorithm to vote for the best <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> for each sentence. We present experiments aimed at parsing <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> using a combination of <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a>. In a similar experimental setup to the CoNLL 2018 shared task on dependency parsing we report state-of-the-art results on dependency parsing for <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> using an off-the-shelf parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6018/>Toward <a href=https://en.wikipedia.org/wiki/United_Nations_geoscheme>Universal Dependencies</a> for Shipibo-Konibo<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for <span class=acl-fixed-case>S</span>hipibo-Konibo</a></strong><br><a href=/people/a/alonso-vasquez/>Alonso Vasquez</a>
|
<a href=/people/r/renzo-ego-aguirre/>Renzo Ego Aguirre</a>
|
<a href=/people/c/candy-angulo/>Candy Angulo</a>
|
<a href=/people/j/john-miller/>John Miller</a>
|
<a href=/people/c/claudia-villanueva/>Claudia Villanueva</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/r/roberto-zariquiey/>Roberto Zariquiey</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6018><div class="card-body p-3 small">We present an initial version of the Universal Dependencies (UD) treebank for <a href=https://en.wikipedia.org/wiki/Shipibo-Konibo_language>Shipibo-Konibo</a>, the first South American, Amazonian, Panoan and Peruvian language with a resource built under UD. We describe the linguistic aspects of how the tagset was defined and the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> was annotated ; in addition we present our specific treatment of linguistic units called <a href=https://en.wikipedia.org/wiki/Clitic>clitics</a>. Although the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is still under development, it allowed us to perform a typological comparison against <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, the predominant language in <a href=https://en.wikipedia.org/wiki/Peru>Peru</a>, and dependency syntax parsing experiments in both monolingual and cross-lingual approaches.<i>clitics</i>. Although the treebank is still under development, it allowed us to perform a typological comparison against Spanish, the predominant language in Peru, and dependency syntax parsing experiments in both monolingual and cross-lingual approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6019/>Transition-based Parsing with Lighter Feed-Forward Networks</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6019><div class="card-body p-3 small">We explore whether it is possible to build lighter <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, that are statistically equivalent to their corresponding standard version, for a wide set of <a href=https://en.wikipedia.org/wiki/Language>languages</a> showing different structures and morphologies. As testbed, we use the Universal Dependencies and transition-based dependency parsers trained on feed-forward networks. For these, most existing research assumes de facto standard embedded features and relies on pre-computation tricks to obtain speed-ups. We explore how these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> and their size can be reduced and whether this translates into <a href=https://en.wikipedia.org/wiki/Speedup>speed-ups</a> with a negligible impact on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The experiments show that grand-daughter features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can be notably reduced.<i>de facto standard</i> embedded features and relies on pre-computation tricks to obtain speed-ups. We explore how these features and their size can be reduced and whether this translates into speed-ups with a negligible impact on accuracy. The experiments show that <i>grand-daughter</i> features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the embeddings can be notably reduced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6020/>Extended and Enhanced Polish Dependency Bank in Universal Dependencies Format<span class=acl-fixed-case>P</span>olish Dependency Bank in <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies Format</a></strong><br><a href=/people/a/alina-wroblewska/>Alina Wróblewska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6020><div class="card-body p-3 small">The paper presents the largest Polish Dependency Bank in Universal Dependencies format PDBUD with 22 K trees and 352 K tokens. PDBUD builds on its previous version, i.e. the Polish UD treebank (PL-SZ), and contains all 8 K PL-SZ trees. The PL-SZ trees are checked and possibly corrected in the current edition of PDBUD. Further 14 K trees are automatically converted from a new version of Polish Dependency Bank. The PDBUD trees are expanded with the enhanced edges encoding the shared dependents and the shared governors of the coordinated conjuncts and with the semantic roles of some dependents. The conducted evaluation experiments show that PDBUD is large enough for training a high-quality graph-based dependency parser for <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6021/>Approximate Dynamic Oracle for Dependency Parsing with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6021><div class="card-body p-3 small">We present a general approach with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive. We treat <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parsing</a> as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning (DQN) techniques to train the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle</a> with gold trees as features. The combination of a priori knowledge and <a href=https://en.wikipedia.org/wiki/Data-driven_programming>data-driven methods</a> enables an efficient dynamic oracle, which improves the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> performance over static oracles in several <a href=https://en.wikipedia.org/wiki/Transition_system>transition systems</a>.</div></div></div><hr><div id=w18-61><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-61.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-61/>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6100/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop W-<span class=acl-fixed-case>NUT</span>: The 4th Workshop on Noisy User-generated Text</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6101/>Inducing a lexicon of sociolinguistic variables from code-mixed text</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6101><div class="card-body p-3 small">Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin&#8217;) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text : that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (footballBritish, fitbaScottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>English dialects</a>, using <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> as the code-mixed text. Our system achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of over 70 % for two of these three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and produces useful results even without extensive parameter tuning. Our success in adapting this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> from <a href=https://en.wikipedia.org/wiki/Gender>gender</a> to <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a> suggests that it could be used to discover other types of analogous pairs as well.<i>nothing</i> vs. <i>nothin&#8217;</i>) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (<i>football</i>, <i>fitba</i>) along with their linguistic code (<i>football</i>&#8594;British, <i>fitba</i>&#8594;Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6102/>Twitter Geolocation using Knowledge-Based Methods<span class=acl-fixed-case>T</span>witter Geolocation using Knowledge-Based Methods</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6102><div class="card-body p-3 small">Automatic geolocation of microblog posts from their text content is particularly difficult because many location-indicative terms are rare terms, notably entity names such as locations, people or local organisations. Their low frequency means that key terms observed in testing are often unseen in training, such that standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> are unable to learn weights for them. We propose a method for reasoning over such terms using a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, through exploiting their relations with other entities. Our technique uses a <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> over the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, which we couple with a text representation to learn a geolocation classifier, trained end-to-end. We show that our method improves over purely text-based methods, which we ascribe to more robust treatment of low-count and out-of-vocabulary entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6104/>Assigning people to tasks identified in email : The EPA dataset for addressee tagging for detected task intent<span class=acl-fixed-case>EPA</span> dataset for addressee tagging for detected task intent</a></strong><br><a href=/people/r/revanth-rameshkumar/>Revanth Rameshkumar</a>
|
<a href=/people/p/peter-bailey/>Peter Bailey</a>
|
<a href=/people/a/abhishek-jha/>Abhishek Jha</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6104><div class="card-body p-3 small">We describe the Enron People Assignment (EPA) dataset, in which <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that are described in emails are associated with the person(s) responsible for carrying out these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We identify <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and the responsible people in the Enron email dataset. We define evaluation methods for this challenge and report scores for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and nave baselines. The resulting model enables a <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> operating within a commercial email service : given a person and a task, it determines if the person should be notified of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6109/>Paraphrase Detection on Noisy Subtitles in Six Languages</a></strong><br><a href=/people/e/eetu-sjoblom/>Eetu Sjöblom</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6109><div class="card-body p-3 small">We perform automatic paraphrase detection on subtitle data from the Opusparcus corpus comprising six European languages : <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We train two types of supervised sentence embedding models : a word-averaging (WA) model and a gated recurrent averaging network (GRAN) model. We find out that <a href=https://en.wikipedia.org/wiki/GRAN>GRAN</a> outperforms WA and is more robust to noisy training data. Better results are obtained with more and noisier data than less and cleaner data. Additionally, we experiment on other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, without reaching the same level of performance, because of domain mismatch between training and test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6110/>Distantly Supervised Attribute Detection from Reviews</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/p/pablo-barrio/>Pablo Barrio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6110><div class="card-body p-3 small">This work aims to detect specific <a href=https://en.wikipedia.org/wiki/Attribute_(philosophy)>attributes</a> of a place (e.g., if it has a romantic atmosphere, or if it offers outdoor seating) from its user reviews via distant supervision : without direct annotation of the review text, we use the crowdsourced attribute labels of the place as labels of the review text. We then use review-level attention to pay more attention to those reviews related to the <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a>. The experimental results show that our attention-based model predicts attributes for places from reviews with over 98 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The attention weights assigned to each review provide explanation of capturing relevant reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6111/>Using Wikipedia Edits in Low Resource Grammatical Error Correction<span class=acl-fixed-case>W</span>ikipedia Edits in Low Resource Grammatical Error Correction</a></strong><br><a href=/people/a/adriane-boyd/>Adriane Boyd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6111><div class="card-body p-3 small">We develop a grammatical error correction (GEC) system for <a href=https://en.wikipedia.org/wiki/German_language>German</a> using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and Junczys-Dowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach (Chollampatt and Ng, 2018), we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6112/>Empirical Evaluation of Character-Based Model on Neural Named-Entity Recognition in Indonesian Conversational Texts<span class=acl-fixed-case>I</span>ndonesian Conversational Texts</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6112><div class="card-body p-3 small">Despite the long history of named-entity recognition (NER) task in the natural language processing community, previous work rarely studied the task on conversational texts. Such <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> are challenging because they contain a lot of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>word variations</a> which increase the number of out-of-vocabulary (OOV) words. The high number of OOV words poses a difficulty for word-based neural models. Meanwhile, there is plenty of evidence to the effectiveness of character-based neural models in mitigating this OOV problem. We report an empirical evaluation of neural sequence labeling models with character embedding to tackle NER task in Indonesian conversational texts. Our experiments show that (1) character models outperform word embedding-only models by up to 4 F1 points, (2) character models perform better in OOV cases with an improvement of as high as 15 F1 points, and (3) character models are robust against a very high OOV rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6113/>Orthogonal Matching Pursuit for Text Classification</a></strong><br><a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/n/nikolaos-tziortziotis/>Nikolaos Tziortziotis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6113><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, the problem of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> arises due to the high dimensionality, making <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> essential. Although classic <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a> provide sparsity, they fail to return highly accurate models. On the contrary, state-of-the-art group-lasso regularizers provide better results at the expense of low sparsity. In this paper, we apply a greedy variable selection algorithm, called Orthogonal Matching Pursuit, for the text classification task. We also extend standard group OMP by introducing overlapping Group OMP to handle overlapping groups of features. Empirical analysis verifies that both OMP and overlapping GOMP constitute powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a>, able to produce effective and very sparse models. Code and data are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6114/>Training and Prediction Data Discrepancies : Challenges of Text Classification with Noisy, Historical Data</a></strong><br><a href=/people/r/r-andrew-kreek/>R. Andrew Kreek</a>
|
<a href=/people/e/emilia-apostolova/>Emilia Apostolova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6114><div class="card-body p-3 small">Industry datasets used for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> are rarely created for that purpose. In most cases, the data and target predictions are a by-product of accumulated historical data, typically fraught with <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, present in both the text-based document, as well as in the targeted labels. In this work, we address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future machine learning model input. The results demonstrate the utility of dirty training datasets used to build prediction models for cleaner (and different) prediction inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6115/>Detecting Code-Switching between Turkish-English Language Pair<span class=acl-fixed-case>T</span>urkish-<span class=acl-fixed-case>E</span>nglish Language Pair</a></strong><br><a href=/people/z/zeynep-yirmibesoglu/>Zeynep Yirmibeşoğlu</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6115><div class="card-body p-3 small">Code-switching (usage of different languages within a single conversation context in an alternative manner) is a highly increasing phenomenon in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial usage</a> which poses different challenges for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. This paper introduces the first study for the detection of Turkish-English code-switching and also a small test data collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in order to smooth the way for further studies. The proposed system using character level n-grams and conditional random fields (CRFs) obtains 95.6 % micro-averaged F1-score on the introduced test data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6116/>Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture</a></strong><br><a href=/people/s/soumil-mandal/>Soumil Mandal</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6116><div class="card-body p-3 small">An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there&#8217;s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28 % and 93.32 % is achieved on our two testing sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6117/>Modeling Student Response Times : Towards Efficient One-on-one Tutoring Dialogues</a></strong><br><a href=/people/l/luciana-benotti/>Luciana Benotti</a>
|
<a href=/people/j/jayadev-bhaskaran/>Jayadev Bhaskaran</a>
|
<a href=/people/s/sigtryggur-kjartansson/>Sigtryggur Kjartansson</a>
|
<a href=/people/d/david-lang/>David Lang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6117><div class="card-body p-3 small">In this paper we investigate the task of modeling how long it would take a student to respond to a tutor question during a tutoring dialogue. Solving such a task has applications in educational settings such as <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, as well as in platforms that help busy human tutors to keep students engaged. Knowing how long it would normally take a student to respond to different types of questions could help tutors optimize their own time while answering multiple dialogues concurrently, as well as deciding when to prompt a student again. We study this problem using data from a service that offers tutor support for <a href=https://en.wikipedia.org/wiki/Mathematics>math</a>, <a href=https://en.wikipedia.org/wiki/Chemistry>chemistry</a> and <a href=https://en.wikipedia.org/wiki/Physics>physics</a> through an instant messaging platform. We create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 240 K questions. We explore several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and compare them with human performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6118/>Content Extraction and <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Analysis</a> from Customer-Agent Interactions</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/a/anca-bucur/>Anca Bucur</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6118><div class="card-body p-3 small">In this paper, we provide a lexical comparative analysis of the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> used by customers and agents in an Enterprise Resource Planning (ERP) environment and a potential solution to clean the data and extract relevant content for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. As a result, we demonstrate that the actual vocabulary for the <a href=https://en.wikipedia.org/wiki/Language>language</a> that prevails in the ERP conversations is highly divergent from the standardized dictionary and further different from general language usage as extracted from the Common Crawl corpus. Moreover, in specific business communication circumstances, where it is expected to observe a high usage of <a href=https://en.wikipedia.org/wiki/Standard_language>standardized language</a>, <a href=https://en.wikipedia.org/wiki/Code_switching>code switching</a> and non-standard expression are predominant, emphasizing once more the discrepancy between the day-to-day use of language and the standardized one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6120/>Word-like character n-gram embedding</a></strong><br><a href=/people/g/geewook-kim/>Geewook Kim</a>
|
<a href=/people/k/kazuki-fukui/>Kazuki Fukui</a>
|
<a href=/people/h/hidetoshi-shimodaira/>Hidetoshi Shimodaira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6120><div class="card-body p-3 small">We propose a new word embedding method called word-like character n-gram embedding, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus. However, its <a href=https://en.wikipedia.org/wiki/N-gram>n-gram vocabulary</a> tends to contain too many non-word n-grams. We solved this problem by introducing an idea of expected word frequency. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and informal words (e.g., Chinese SNS dataset). Our experimental results on <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> (a Chinese microblog service) and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> show that the proposed method can embed more words and improve the performance of downstream tasks.<i>word-like character</i> n<i>-gram embedding</i>, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed <i>segmentation-free word embedding</i>, which directly embeds frequent character n-grams from a raw corpus. However, its n-gram vocabulary tends to contain too many non-word n-grams. We solved this problem by introducing an idea of <i>expected word frequency</i>. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset). Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6121 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6121/>Classification of Tweets about Reported Events using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/k/kiminobu-makino/>Kiminobu Makino</a>
|
<a href=/people/y/yuka-takei/>Yuka Takei</a>
|
<a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/j/jun-goto/>Jun Goto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6121><div class="card-body p-3 small">We developed a system that automatically extracts Event-describing Tweets which include incidents or accidents information for creating news reports. Event-describing Tweets can be classified into Reported-event Tweets and New-information Tweets. Reported-event Tweets cite <a href=https://en.wikipedia.org/wiki/News_agency>news agencies</a> or user generated content sites, and New-information Tweets are other Event-describing Tweets. A <a href=https://en.wikipedia.org/wiki/System>system</a> is needed to classify them so that creators of factual TV programs can use <a href=https://en.wikipedia.org/wiki/Them_(band)>them</a> in their productions. Proposing this Tweet classification task is one of the contributions of this paper, because no prior papers have used the same task even though program creators and other events information collectors have to do it to extract required information from <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking sites</a>. To classify Tweets in this task, this paper proposes a method to input and concatenate character and word sequences in Japanese Tweets by using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. This proposed method is another contribution of this paper. For comparison, character or word input methods and other <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are also used. Results show that a <a href=https://en.wikipedia.org/wiki/System>system</a> using the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> can classify Tweets with an F1 score of 88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6122/>Learning to Define Terms in the Software Domain</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/r/rose-catherine-kanjirathinkal/>Rose Catherine Kanjirathinkal</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6122><div class="card-body p-3 small">One way to test a person&#8217;s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and incorporate additional domain-specific information like <a href=https://en.wikipedia.org/wiki/Co-occurrence>word co-occurrence</a>, and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological category information</a>. Our approach improves previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by 2 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BLEU points</a> for the definition generation task. Our experiments also show the additional challenges associated with the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and the short-comings of language-model based architectures for definition generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6123 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6123/>FrameIt : <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>Ontology Discovery</a> for Noisy User-Generated Text<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>I</span>t: Ontology Discovery for Noisy User-Generated Text</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6123><div class="card-body p-3 small">A common need of NLP applications is to extract <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> in order to perform <a href=https://en.wikipedia.org/wiki/Analytics>analytics</a> or trigger an appropriate action. The <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to model a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and (2) learning an SRL model that extracts the instances of the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6125 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6125/>Low-resource named entity recognition via multi-source projection : Not quite there yet?</a></strong><br><a href=/people/j/jan-vium-enghoff/>Jan Vium Enghoff</a>
|
<a href=/people/s/soren-harrison/>Søren Harrison</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6125><div class="card-body p-3 small">Projecting linguistic annotations through word alignments is one of the most prevalent approaches to cross-lingual transfer learning. Conventional wisdom suggests that annotation projection just works regardless of the task at hand. We carefully consider multi-source projection for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our experiment with 17 languages shows that to detect <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> in true low-resource languages, annotation projection may not be the right way to move forward. On a more positive note, we also uncover the conditions that do favor named entity projection from multiple sources. We argue these are infeasible under noisy low-resource constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6126 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6126/>A Case Study on Learning a Unified Encoder of Relations</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6126><div class="card-body p-3 small">Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> may often be biased or overfitted to the corpus. We hypothesize that we can learn a better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. The additional corpora feeding the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6129/>Combining Human and Machine Transcriptions on the Zooniverse Platform</a></strong><br><a href=/people/d/daniel-hanson/>Daniel Hanson</a>
|
<a href=/people/a/andrea-simenstad/>Andrea Simenstad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6129><div class="card-body p-3 small">Transcribing handwritten documents to create fully searchable texts is an essential part of the <a href=https://en.wikipedia.org/wiki/Archival_science>archival process</a>. Traditional text recognition methods, such as <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition (OCR)</a>, do not work on handwritten documents due to their frequent noisiness and OCR&#8217;s need for individually segmented letters. Crowdsourcing and improved machine models are two modern methods for transcribing handwritten documents.</div></div></div><hr><div id=w18-62><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-62.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-62/>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6200/>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6201/>Identifying Affective Events and the Reasons for their Polarity</a></strong><br><a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6201><div class="card-body p-3 small">Many events have a positive or negative impact on our lives (e.g., I bought a house is typically good news, but My house burned down is bad news). Recognizing events that have affective polarity is essential for narrative text understanding, conversational dialogue, and applications such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and sarcasm detection. We will discuss our recent work on identifying <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective events</a> and categorizing them based on the underlying reasons for their <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective polarity</a>. First, we will describe a weakly supervised learning method to induce a large set of <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>affective events</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> by optimizing for semantic consistency. Second, we will present models to classify affective events based on Human Need Categories, which often explain people&#8217;s motivations and desires. Our best results use a co-training model that consists of event expression and event context classifiers and exploits both labeled and unlabeled texts. We will conclude with a discussion of interesting directions for future work in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6202" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6202/>Deep contextualized word representations for detecting sarcasm and irony</a></strong><br><a href=/people/s/suzana-ilic/>Suzana Ilić</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6202><div class="card-body p-3 small">Predicting context-dependent and non-literal utterances like sarcastic and ironic expressions still remains a challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, as it goes beyond linguistic patterns, encompassing <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a> and shared knowledge as crucial components. To capture complex morpho-syntactic features that can usually serve as indicators for <a href=https://en.wikipedia.org/wiki/Irony>irony</a> or <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> across dynamic contexts, we propose a model that uses character-level vector representations of words, based on ELMo. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 7 different datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them, and otherwise offering competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6203/>Implicit Subjective and Sentimental Usages in Multi-sense Word Embeddings</a></strong><br><a href=/people/y/yuqi-sun/>Yuqi Sun</a>
|
<a href=/people/h/haoyue-shi/>Haoyue Shi</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6203><div class="card-body p-3 small">In multi-sense word embeddings, contextual variations in corpus may cause a univocal word to be embedded into different sense vectors. Shi et al. (2016) show that this kind of pseudo multi-senses can be eliminated by <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a>. In this paper, we show that pseudo multi-senses may come from a uniform and meaningful phenomenon such as subjective and sentimental usage, though they are seemingly redundant. In this paper, we present an unsupervised algorithm to find a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> which can minimize the transformed distance of a group of sense pairs. The major shrinking direction of this transformation is found to be related with subjective shift. Therefore, we can not only eliminate pseudo multi-senses in multisense embeddings, but also identify these subjective senses and tag the subjective and sentimental usage of words in the corpus automatically.<i>pseudo multi-senses</i> can be eliminated by linear transformations. In this paper, we show that <i>pseudo multi-senses</i> may come from a uniform and meaningful phenomenon such as subjective and sentimental usage, though they are seemingly redundant. In this paper, we present an unsupervised algorithm to find a linear transformation which can minimize the transformed distance of a group of sense pairs. The major shrinking direction of this transformation is found to be related with subjective shift. Therefore, we can not only eliminate <i>pseudo multi-senses</i> in multisense embeddings, but also identify these subjective senses and tag the subjective and sentimental usage of words in the corpus automatically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6207/>Amobee at IEST 2018 : Transfer Learning from Language Models<span class=acl-fixed-case>A</span>mobee at <span class=acl-fixed-case>IEST</span> 2018: Transfer Learning from Language Models</a></strong><br><a href=/people/a/alon-rozental/>Alon Rozental</a>
|
<a href=/people/d/daniel-fleischer/>Daniel Fleischer</a>
|
<a href=/people/z/zohar-kelrich/>Zohar Kelrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6207><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> developed at Amobee for the WASSA 2018 implicit emotions shared task (IEST). The goal of this task was to predict the <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> expressed by missing words in tweets without an explicit mention of those words. We developed an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble system</a> consisting of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> together with LSTM-based networks containing a CNN attention mechanism. Our approach represents a novel use of language modelsspecifically trained on a large Twitter datasetto predict and classify <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> reached 1st place with a macro F1 score of 0.7145.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6208/>IIIDYT at IEST 2018 : Implicit Emotion Classification With Deep Contextualized Word Representations<span class=acl-fixed-case>IIIDYT</span> at <span class=acl-fixed-case>IEST</span> 2018: Implicit Emotion Classification With Deep Contextualized Word Representations</a></strong><br><a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6208><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> designed for the WASSA 2018 Implicit Emotion Shared Task (IEST), which obtained 2nd place out of 30 teams with a test macro F1 score of 0.710. The system is composed of a single pre-trained ELMo layer for encoding words, a Bidirectional Long-Short Memory Network BiLSTM for enriching word representations with context, a max-pooling operation for creating sentence representations from them, and a Dense Layer for projecting the sentence representations into label space. Our official submission was obtained by ensembling 6 of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> initialized with different <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a>. The code for replicating this paper is available at.<url>https://github.com/jabalazs/implicit_emotion</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6211/>Not Just Depressed : Bipolar Disorder Prediction on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/i/ivan-sekulic/>Ivan Sekulic</a>
|
<a href=/people/m/matej-gjurkovic/>Matej Gjurković</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6211><div class="card-body p-3 small">Bipolar disorder, an illness characterized by <a href=https://en.wikipedia.org/wiki/Bipolar_disorder>manic and depressive episodes</a>, affects more than 60 million people worldwide. We present a preliminary study on bipolar disorder prediction from <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a> on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, which relies on users&#8217; self-reported labels. Our benchmark classifiers for bipolar disorder prediction outperform the baselines and reach <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and F1-scores of above 86 %. Feature analysis shows interesting differences in language use between users with <a href=https://en.wikipedia.org/wiki/Bipolar_disorder>bipolar disorders</a> and the <a href=https://en.wikipedia.org/wiki/Treatment_and_control_groups>control group</a>, including differences in the use of emotion-expressive words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6212/>Topic-Specific Sentiment Analysis Can Help Identify Political Ideology</a></strong><br><a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/d/deepak-p/>Deepak P</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6212><div class="card-body p-3 small">Ideological leanings of an individual can often be gauged by the sentiment one expresses about different issues. We propose a simple <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> that represents a <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a> as a distribution of sentiment polarities towards a set of topics. This <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation</a> can then be used to detect ideological leanings of documents (speeches, <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, etc.) based on the sentiments expressed towards different topics. Experiments performed using a widely used dataset show the promise of our proposed approach that achieves comparable performance to other methods despite being much simpler and more interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6213/>Saying no but meaning yes : negation and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in Basque<span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/j/jon-alkorta/>Jon Alkorta</a>
|
<a href=/people/k/koldo-gojenola/>Koldo Gojenola</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6213><div class="card-body p-3 small">In this work, we have analyzed the effects of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> on the semantic orientation in <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. The analysis shows that negation markers can strengthen, weaken or have no effect on sentiment orientation of a word or a group of words. Using the Constraint Grammar formalism, we have designed and evaluated a set of <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a> to formalize these three <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a>. The results show that two <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a>, strengthening and no change, have been identified accurately and the third one, weakening, with acceptable results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6214/>Leveraging Writing Systems Change for Deep Learning Based Chinese Emotion Analysis<span class=acl-fixed-case>C</span>hinese Emotion Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/d/dan-xiong/>Dan Xiong</a>
|
<a href=/people/i/i-hsuan-chen/>I-Hsuan Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6214><div class="card-body p-3 small">Social media text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese communities</a> contains mixed scripts including major text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, an ideograph-based writing system, and some minor text using <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a>, an alphabet-based writing system. This phenomenon is called writing systems changes (WSCs). Past studies have shown that WSCs can be used to express emotions, particularly where the social and political environment is more conservative. However, because WSCs can break the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the major text, it poses more challenges in Natural Language Processing (NLP) tasks like <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. In this work, we present a novel deep learning based method to include WSCs as an effective <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> for emotion analysis. The method first identifies all WSCs points. Then representation of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> before <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6215/>Ternary Twitter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings<span class=acl-fixed-case>T</span>witter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings</a></strong><br><a href=/people/m/mats-byrkjeland/>Mats Byrkjeland</a>
|
<a href=/people/f/frederik-gorvell-de-lichtenberg/>Frederik Gørvell de Lichtenberg</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6215><div class="card-body p-3 small">The paper proposes the Ternary Sentiment Embedding Model, a new model for creating sentiment embeddings based on the Hybrid Ranking Model of Tang et al. (2016), but trained on ternary-labeled data instead of binary-labeled, utilizing sentiment embeddings from datasets made with different distant supervision methods. The model is used as part of a complete Twitter Sentiment Analysis system and empirically compared to existing systems, showing that it outperforms Hybrid Ranking and that the quality of the distant-supervised dataset has a great impact on the quality of the produced sentiment embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6216/>Linking News Sentiment to <a href=https://en.wikipedia.org/wiki/Microblogging>Microblogs</a> : A Distributional Semantics Approach to Enhance Microblog Sentiment Classification</a></strong><br><a href=/people/t/tobias-daudert/>Tobias Daudert</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6216><div class="card-body p-3 small">Social media&#8217;s popularity in society and research is gaining momentum and simultaneously increasing the importance of short textual content such as <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a>. Microblogs are affected by many factors including the <a href=https://en.wikipedia.org/wiki/News_media>news media</a>, therefore, we exploit sentiments conveyed from news to detect and classify sentiment in <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a>. Given that texts can deal with the same entity but might not be vastly related when it comes to <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, it becomes necessary to introduce further measures ensuring the relatedness of texts while leveraging the contained sentiments. This paper describes ongoing research introducing <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> to improve the exploitation of news-contained sentiment to enhance microblog sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6217/>Aspect Based Sentiment Analysis into the Wild</a></strong><br><a href=/people/c/caroline-brun/>Caroline Brun</a>
|
<a href=/people/v/vassilina-nikoulina/>Vassilina Nikoulina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6217><div class="card-body p-3 small">In this paper, we test state-of-the-art Aspect Based Sentiment Analysis (ABSA) systems trained on a widely used dataset on actual data. We created a new manually annotated dataset of user generated data from the same domain as the training dataset, but from other sources and analyse the differences between the new and the standard ABSA dataset. We then analyse the results in performance of different versions of the same <a href=https://en.wikipedia.org/wiki/System>system</a> on both datasets. We also propose light adaptation methods to increase system robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6219" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6219/>Self-Attention : A Better Building Block for Sentiment Analysis Neural Network Classifiers</a></strong><br><a href=/people/a/artaches-ambartsoumian/>Artaches Ambartsoumian</a>
|
<a href=/people/f/fred-popowich/>Fred Popowich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6219><div class="card-body p-3 small">Sentiment Analysis has seen much progress in the past two decades. For the past few years, neural network approaches, primarily RNNs and CNNs, have been the most successful for this task. Recently, a new category of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, self-attention networks (SANs), have been created which utilizes the <a href=https://en.wikipedia.org/wiki/Attention>attention mechanism</a> as the basic building block. Self-attention networks have been shown to be effective for sequence modeling tasks, while having no <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence</a> or <a href=https://en.wikipedia.org/wiki/Convolution>convolutions</a>. In this work we explore the effectiveness of the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>SANs</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We demonstrate that SANs are superior in performance to their RNN and CNN counterparts by comparing their classification accuracy on six datasets as well as their model characteristics such as training speed and memory consumption. Finally, we explore the effects of various SAN modifications such as multi-head attention as well as two methods of incorporating sequence position information into SANs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6220/>Dual Memory Network Model for Biased Product Review Classification</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-ma/>Mingyu Ma</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6220><div class="card-body p-3 small">In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle <a href=https://en.wikipedia.org/wiki/User_profile>user profile</a> and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> are used jointly for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. The use of separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> aims to capture <a href=https://en.wikipedia.org/wiki/User_profile>user profiles</a> and <a href=https://en.wikipedia.org/wiki/Product_information>product information</a> more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, <a href=https://en.wikipedia.org/wiki/Internet_Movie_Database>IMDB</a>, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6 %, 1.2 %, and 0.9 %, respectively. The improvements are also deemed very significant measured by <a href=https://en.wikipedia.org/wiki/P-value>p-values</a>.<i>p-values</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6221/>Measuring Issue Ownership using Word Embeddings</a></strong><br><a href=/people/a/amaru-cuba-gyllensten/>Amaru Cuba Gyllensten</a>
|
<a href=/people/m/magnus-sahlgren/>Magnus Sahlgren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6221><div class="card-body p-3 small">Sentiment and topic analysis are common methods used for <a href=https://en.wikipedia.org/wiki/Social_media_monitoring>social media monitoring</a>. Essentially, these methods answers questions such as, what is being talked about, regarding X, and what do people feel, regarding X. In this paper, we investigate another venue for <a href=https://en.wikipedia.org/wiki/Social_media_monitoring>social media monitoring</a>, namely issue ownership and <a href=https://en.wikipedia.org/wiki/Agenda_setting>agenda setting</a>, which are concepts from <a href=https://en.wikipedia.org/wiki/Political_science>political science</a> that have been used to explain voter choice and electoral outcomes. We argue that issue alignment and <a href=https://en.wikipedia.org/wiki/Agenda_setting>agenda setting</a> can be seen as a kind of semantic source similarity of the kind how similar is source A to issue owner P, when talking about issue X, and as such can be measured using word / document embedding techniques. We present work in progress towards measuring that kind of conditioned similarity, and introduce a new notion of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> for predictive embeddings. We then test this method by measuring the similarity between politically aligned media and political parties, conditioned on bloc-specific issues.<i>X</i>&#8221;, and &#8220;what do people feel, regarding <i>X</i>&#8221;. In this paper, we investigate another venue for social media monitoring, namely <i>issue ownership</i> and <i>agenda setting</i>, which are concepts from political science that have been used to explain voter choice and electoral outcomes. We argue that issue alignment and agenda setting can be seen as a kind of semantic source similarity of the kind &#8220;how similar is source <i>A</i> to issue owner <i>P</i>, when talking about issue <i>X</i>&#8221;, and as such can be measured using word/document embedding techniques. We present work in progress towards measuring that kind of conditioned similarity, and introduce a new notion of similarity for predictive embeddings. We then test this method by measuring the similarity between politically aligned media and political parties, conditioned on bloc-specific issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6222/>Sentiment Expression Boundaries in Sentiment Polarity Classification</a></strong><br><a href=/people/r/rasoul-kaljahi/>Rasoul Kaljahi</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6222><div class="card-body p-3 small">We investigate the effect of using sentiment expression boundaries in predicting sentiment polarity in aspect-level sentiment analysis. We manually annotate a freely available English sentiment polarity dataset with these boundaries and carry out a series of experiments which demonstrate that high quality sentiment expressions can boost the performance of polarity classification. Our experiments with neural architectures also show that CNN networks outperform <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on this task and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6223 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6223/>Exploring and Learning Suicidal Ideation Connotations on Social Media with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/p/prachi-manchanda/>Prachi Manchanda</a>
|
<a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a>
|
<a href=/people/r/raj-singh/>Raj Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6223><div class="card-body p-3 small">The increasing suicide rates amongst youth and its high correlation with suicidal ideation expression on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> warrants a deeper investigation into models for the detection of suicidal intent in text such as <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> to enable <a href=https://en.wikipedia.org/wiki/Suicide_prevention>prevention</a>. However, the complexity of the natural language constructs makes this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> very challenging. Deep Learning architectures such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a>, CNNs, and RNNs show promise in sentence level classification problems. This work investigates the ability of deep learning architectures to build an accurate and robust model for suicidal ideation detection and compares their performance with standard baselines in text classification problems. The experimental results reveal the merit in C-LSTM based models as compared to other deep learning and machine learning based classification models for suicidal ideation detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6226/>NLP at IEST 2018 : BiLSTM-Attention and LSTM-Attention via Soft Voting in Emotion Classification<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IEST</span> 2018: <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-Attention and <span class=acl-fixed-case>LSTM</span>-Attention via Soft Voting in Emotion Classification</a></strong><br><a href=/people/q/qimin-zhou/>Qimin Zhou</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6226><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that competed at WASSA2018 Implicit Emotion Shared Task. The goal of this task is to classify the emotions of excluded words in tweets into six different classes : <a href=https://en.wikipedia.org/wiki/Sadness>sad</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a> and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of these methods to give the final prediction which improves the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance significantly compared with the baseline model. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves 7th position out of 30 teams and outperforms the baseline method by 12.5 % in terms of macro F1.<i>Implicit Emotion Shared Task</i>. The goal of this task is to classify the emotions of excluded words in tweets into six different classes: sad, joy, disgust, surprise, anger and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an ensemble of these methods to give the final prediction which improves the model performance significantly compared with the baseline model. The proposed method achieves 7th position out of 30 teams and outperforms the baseline method by 12.5% in terms of <i>macro F1</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6227 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6227/>SINAI at IEST 2018 : Neural Encoding of Emotional External Knowledge for Emotion Classification<span class=acl-fixed-case>SINAI</span> at <span class=acl-fixed-case>IEST</span> 2018: Neural Encoding of Emotional External Knowledge for Emotion Classification</a></strong><br><a href=/people/f/flor-miriam-plaza-del-arco/>Flor Miriam Plaza-del-Arco</a>
|
<a href=/people/e/eugenio-martinez-camara/>Eugenio Martínez-Cámara</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Maite Martin</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Ureña- López</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6227><div class="card-body p-3 small">In this paper, we describe our participation in WASSA 2018 Implicit Emotion Shared Task (IEST 2018). We claim that the use of emotional external knowledge may enhance the performance and the capacity of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> of an <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification system</a> based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Accordingly, we submitted four <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a> grounded in a sequence encoding layer. They mainly differ in the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector space</a> and the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> used in the sequence encoding layer. The official results show that the <a href=https://en.wikipedia.org/wiki/System>systems</a> that used emotional external knowledge have a higher capacity of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, hence our claim holds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6228 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6228/>EmoNLP at IEST 2018 : An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IEST</span> 2018: An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets</a></strong><br><a href=/people/m/man-liu/>Man Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6228><div class="card-body p-3 small">This paper describes our system submitted to IEST 2018, a shared task (Klinger et al., 2018) to predict the emotion types. Six emotion types are involved : <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a> and <a href=https://en.wikipedia.org/wiki/Sadness>sad</a>. We perform three different approaches : feed forward neural network (FFNN), convolutional BLSTM (ConBLSTM) and Gradient Boosting Regression Tree Method (GBM). Word embeddings used in convolutional BLSTM are pre-trained on 470 million tweets which are filtered using the emotional words and emojis. In addition, broad sets of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> (i.e. syntactic features, lexicon features, cluster features) are adopted to train GBM and FFNN. The three approaches are finally ensembled by the <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average of predicted probabilities</a> of each emotion label.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6229 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6229/>HGSGNLP at IEST 2018 : An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets<span class=acl-fixed-case>HGSGNLP</span> at <span class=acl-fixed-case>IEST</span> 2018: An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets</a></strong><br><a href=/people/w/wenting-wang/>Wenting Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6229><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> designed for the WASSA-2018 Implicit Emotion Shared Task (IEST). The task is to predict the emotion category expressed in a tweet by removing the terms angry, afraid, happy, sad, surprised, disgusted and their synonyms. Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> approaches the problem from essentially different directions. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the <a href=https://en.wikipedia.org/wiki/Macro_(computer_science)>macro F1 score</a> of 65.8 %, which is a 5.9 % performance improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and is ranked 12 out of 30 participating teams.<i>angry</i>, <i>afraid</i>, <i>happy</i>, <i>sad</i>, <i>surprised</i>, <i>disgusted</i> and their synonyms. Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each model approaches the problem from essentially different directions. Our system achieves the macro F1 score of 65.8%, which is a 5.9% performance improvement over the baseline and is ranked 12 out of 30 participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6232 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6232/>UWB at IEST 2018 : Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network<span class=acl-fixed-case>UWB</span> at <span class=acl-fixed-case>IEST</span> 2018: Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network</a></strong><br><a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/j/jiri-martinek/>Jiří Martínek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6232><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> created for the WASSA 2018 Implicit Emotion Shared Task. The goal of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to predict the emotion of a given tweet, from which a certain emotion word is removed. The removed word can be sad, happy, disgusted, angry, afraid or a synonym of one of them. Our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning methods</a>. We use Bidirectional Long Short-Term Memory (BiLSTM) with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as an input. Pre-trained DeepMoji model and pre-trained emoji2vec emoji embeddings are also used as additional inputs. Our <a href=https://en.wikipedia.org/wiki/System>System</a> achieves 0.657 macro F1 score and our rank is 13th out of 30.<i>sad</i>, <i>happy</i>, <i>disgusted</i>, <i>angry</i>, <i>afraid</i> or a synonym of one of them. Our proposed system is based on deep-learning methods. We use Bidirectional Long Short-Term Memory (BiLSTM) with word embeddings as an input. Pre-trained DeepMoji model and pre-trained emoji2vec emoji embeddings are also used as additional inputs. Our System achieves 0.657 macro F1 score and our rank is 13th out of 30.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6234 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6234/>EmotiKLUE at IEST 2018 : Topic-Informed Classification of Implicit Emotions<span class=acl-fixed-case>E</span>moti<span class=acl-fixed-case>KLUE</span> at <span class=acl-fixed-case>IEST</span> 2018: Topic-Informed Classification of Implicit Emotions</a></strong><br><a href=/people/t/thomas-proisl/>Thomas Proisl</a>
|
<a href=/people/p/philipp-heinrich/>Philipp Heinrich</a>
|
<a href=/people/b/besim-kabashi/>Besim Kabashi</a>
|
<a href=/people/s/stefan-evert/>Stefan Evert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6234><div class="card-body p-3 small">EmotiKLUE is a submission to the Implicit Emotion Shared Task. It is a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning system</a> that combines independent representations of the left and right contexts of the emotion word with the topic distribution of an LDA topic model. EmotiKLUE achieves a macro average Fscore of 67.13 %, significantly outperforming the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> produced by a simple ML classifier. Further enhancements after the evaluation period lead to an improved Fscore of 68.10 %.<i>F&#8321;</i>score of 67.13%, significantly outperforming the baseline produced by a simple ML classifier. Further enhancements after the evaluation period lead to an improved <i>F&#8321;</i>score of 68.10%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6236 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6236/>Disney at IEST 2018 : Predicting Emotions using an Ensemble<span class=acl-fixed-case>IEST</span> 2018: Predicting Emotions using an Ensemble</a></strong><br><a href=/people/w/wojciech-witon/>Wojciech Witon</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/m/mubbasir-kapadia/>Mubbasir Kapadia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6236><div class="card-body p-3 small">This paper describes our participating <a href=https://en.wikipedia.org/wiki/System>system</a> in the WASSA 2018 shared task on emotion prediction. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on implicit emotion prediction in a tweet. In this task, keywords corresponding to the six emotion labels used (anger, fear, disgust, joy, sad, and surprise) have been removed from the tweet text, making emotion prediction implicit and the task challenging. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble of classifiers</a> for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Each <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> uses a sequence of Convolutional Neural Network (CNN) architecture blocks and uses ELMo (Embeddings from Language Model) as an input. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a 66.2 % <a href=https://en.wikipedia.org/wiki/Grading_in_education>F1 score</a> on the test set. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> in the shared task has reported a 71.4 % <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6238 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6238/>Fast Approach to Build an Automatic Sentiment Annotator for Legal Domain using Transfer Learning</a></strong><br><a href=/people/v/viraj-salaka/>Viraj Salaka</a>
|
<a href=/people/m/menuka-warushavithana/>Menuka Warushavithana</a>
|
<a href=/people/n/nisansa-de-silva/>Nisansa de Silva</a>
|
<a href=/people/a/amal-shehan-perera/>Amal Shehan Perera</a>
|
<a href=/people/g/gathika-ratnayaka/>Gathika Ratnayaka</a>
|
<a href=/people/t/thejan-rupasinghe/>Thejan Rupasinghe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6238><div class="card-body p-3 small">This study proposes a novel way of identifying the sentiment of the phrases used in the <a href=https://en.wikipedia.org/wiki/Legal_term>legal domain</a>. The added <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the language used in <a href=https://en.wikipedia.org/wiki/Law>law</a>, and the inability of the existing <a href=https://en.wikipedia.org/wiki/System>systems</a> to accurately predict the sentiments of words in law are the main motivations behind this study. This is a transfer learning approach which can be used for other domain adaptation tasks as well. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> achieves an improvement of over 6 % compared to the source model&#8217;s accuracy in the <a href=https://en.wikipedia.org/wiki/Legal_term>legal domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6239 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6239/>What Makes You Stressed? Finding Reasons From Tweets</a></strong><br><a href=/people/r/reshmi-gopalakrishna-pillai/>Reshmi Gopalakrishna Pillai</a>
|
<a href=/people/m/mike-thelwall/>Mike Thelwall</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6239><div class="card-body p-3 small">Detecting stress from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> gives a non-intrusive and inexpensive alternative to traditional tools such as <a href=https://en.wikipedia.org/wiki/Questionnaire>questionnaires</a> or physiological sensors for monitoring mental state of individuals. This paper introduces a novel framework for finding reasons for stress from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, analyzing multiple categories for the first time. Three word-vector based methods are evaluated on collections of tweets about <a href=https://en.wikipedia.org/wiki/Politics>politics</a> or <a href=https://en.wikipedia.org/wiki/Airline>airlines</a> and are found to be more accurate than standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6240 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6240/>EmojiGAN : learning emojis distributions with a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a><span class=acl-fixed-case>E</span>moji<span class=acl-fixed-case>GAN</span>: learning emojis distributions with a generative model</a></strong><br><a href=/people/b/bogdan-mazoure/>Bogdan Mazoure</a>
|
<a href=/people/t/thang-doan/>Thang Doan</a>
|
<a href=/people/s/saibal-ray/>Saibal Ray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6240><div class="card-body p-3 small">Generative models have recently experienced a surge in popularity due to the development of more efficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithms</a> and increasing <a href=https://en.wikipedia.org/wiki/Computational_power>computational power</a>. Models such as adversarial generative networks (GANs) have been successfully used in various areas such as <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical imaging</a>, style transfer and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Adversarial nets were recently shown to yield results in the image-to-text task, where given a set of images, one has to provide their corresponding text description. In this paper, we take a similar approach and propose a image-to-emoji architecture, which is trained on data from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> and can be used to score a given picture using <a href=https://en.wikipedia.org/wiki/Ideogram>ideograms</a>. We show empirical results of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on data obtained from the most influential Instagram accounts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6242 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6242/>Homonym Detection For Humor Recognition In Short Text</a></strong><br><a href=/people/s/sven-van-den-beukel/>Sven van den Beukel</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6242><div class="card-body p-3 small">In this paper, automatic homophone- and homograph detection are suggested as new useful <a href=https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)>features</a> for humor recognition systems. The <a href=https://en.wikipedia.org/wiki/System>system</a> combines style-features from previous studies on humor recognition in short text with ambiguity-based features. The performance of two potentially useful homograph detection methods is evaluated using crowdsourced annotations as ground truth. Adding <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> and <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a> as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> to the classifier results in a small but significant improvement over the style-features alone. For the task of humor recognition, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> appears to be a more important quality measure than <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. Although the <a href=https://en.wikipedia.org/wiki/System>system</a> was designed for humor recognition in <a href=https://en.wikipedia.org/wiki/Oneliner>oneliners</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also performs well at the classification of longer humorous texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6243 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6243/>Emo2Vec : Learning Generalized Emotion Representation by Multi-task Training<span class=acl-fixed-case>E</span>mo2<span class=acl-fixed-case>V</span>ec: Learning Generalized Emotion Representation by Multi-task Training</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6243><div class="card-body p-3 small">In this paper, we propose Emo2Vec which encodes emotional semantics into vectors. We train Emo2Vec by multi-task learning six different emotion-related tasks, including emotion / sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6245 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6245/>Super Characters : A Conversion from Sentiment Classification to Image Classification</a></strong><br><a href=/people/b/baohua-sun/>Baohua Sun</a>
|
<a href=/people/l/lin-yang/>Lin Yang</a>
|
<a href=/people/p/patrick-dong/>Patrick Dong</a>
|
<a href=/people/w/wenhan-zhang/>Wenhan Zhang</a>
|
<a href=/people/j/jason-dong/>Jason Dong</a>
|
<a href=/people/c/charles-young/>Charles Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6245><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> named Super Characters for sentiment classification. This method converts the sentiment classification problem into image classification problem by projecting texts into images and then applying CNN models for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Text features are extracted automatically from the generated Super Characters images, hence there is no need of any explicit step of embedding the words or characters into numerical vector representations. Experimental results on large social media corpus show that the Super Characters method consistently outperforms other methods for sentiment classification and topic classification tasks on ten large social media datasets of millions of contents in four different languages, including <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6246 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6246/>Learning Comment Controversy Prediction in Web Discussions Using Incidentally Supervised Multi-Task CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/n/nils-rethmeier/>Nils Rethmeier</a>
|
<a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6246><div class="card-body p-3 small">Comments on <a href=https://en.wikipedia.org/wiki/Online_newspaper>web news</a> contain <a href=https://en.wikipedia.org/wiki/Controversy>controversies</a> that manifest as inter-group agreement-conflicts. Tracking such rapidly evolving controversy could ease <a href=https://en.wikipedia.org/wiki/Conflict_resolution>conflict resolution</a> or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret comment-controversy model decisions we frame prediction as <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genre-controversy keywords to find that the models learn plausible controversy features using only incidentally supervised signals.<i>rapidly evolving controversy</i> could ease conflict resolution or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret comment-controversy model decisions we frame prediction as binary classification and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genre-controversy keywords &#8211; to find that the models learn plausible controversy features using only incidentally supervised signals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6248 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6248/>Predicting Adolescents’ Educational Track from Chat Messages on Dutch Social Media<span class=acl-fixed-case>D</span>utch Social Media</a></strong><br><a href=/people/l/lisa-hilte/>Lisa Hilte</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/r/reinhild-vandekerckhove/>Reinhild Vandekerckhove</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6248><div class="card-body p-3 small">We aim to predict Flemish adolescents&#8217; educational track based on their Dutch social media writing. We distinguish between the three main types of Belgian secondary education : General (theory-oriented), Vocational (practice-oriented), and Technical Secondary Education (hybrid). The best results are obtained with a Naive Bayes model, i.e. an F-score of 0.68 (std. dev. 0.05) in <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>10-fold cross-validation</a> experiments on the training data and an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.60 on unseen data. Many of the most informative features are character n-grams containing specific occurrences of chatspeak phenomena such as <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a>. While the detection of the most theory- and practice-oriented educational tracks seems to be a relatively easy task, the hybrid Technical level appears to be much harder to capture based on online writing style, as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6249 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6249/>Arabizi sentiment analysis based on <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> and automatic corpus annotation<span class=acl-fixed-case>A</span>rabizi sentiment analysis based on transliteration and automatic corpus annotation</a></strong><br><a href=/people/i/imane-guellil/>Imane Guellil</a>
|
<a href=/people/a/ahsan-adeel/>Ahsan Adeel</a>
|
<a href=/people/f/faical-azouaou/>Faical Azouaou</a>
|
<a href=/people/f/fodil-benali/>Fodil Benali</a>
|
<a href=/people/a/ala-eddine-hachani/>Ala-eddine Hachani</a>
|
<a href=/people/a/amir-hussain/>Amir Hussain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6249><div class="card-body p-3 small">Arabizi is a form of writing Arabic text which relies on <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a>, <a href=https://en.wikipedia.org/wiki/Arabic_numerals>numerals</a> and <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> rather than <a href=https://en.wikipedia.org/wiki/Arabic_alphabet>Arabic letters</a>. In the literature, the difficulties associated with <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi sentiment analysis</a> have been underestimated, principally due to the complexity of <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi</a>. In this paper, we present an approach to automatically classify <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiments</a> of Arabizi messages into positives or negatives. In the proposed approach, Arabizi messages are first transliterated into <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Afterwards, we automatically classify the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> of the transliterated corpus using an automatically annotated corpus. For <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus validation</a>, shallow machine learning algorithms such as Support Vectors Machine (SVM) and Naive Bays (NB) are used. Simulations results demonstrate the outperformance of NB algorithm over all others. The highest achieved F1-score is up to 78 % and 76 % for manually and automatically transliterated dataset respectively. Ongoing work is aimed at improving the transliterator module and annotated sentiment dataset.</div></div></div><hr><div id=w18-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-63/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6300/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6301/>Scaling Neural Machine Translation</a></strong><br><a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6301><div class="card-body p-3 small">Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT&#8217;14 English-German translation, we match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of Vaswani et al. (2017) in under 5 hours when training on 8 <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and we obtain a new state of the art of 29.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT&#8217;14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6302/>Character-level Chinese-English Translation through ASCII Encoding<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Translation through <span class=acl-fixed-case>ASCII</span> Encoding</a></strong><br><a href=/people/n/nikola-i-nikolov/>Nikola I. Nikolov</a>
|
<a href=/people/y/yuhuang-hu/>Yuhuang Hu</a>
|
<a href=/people/m/mi-xue-tan/>Mi Xue Tan</a>
|
<a href=/people/r/richard-h-r-hahnloser/>Richard H.R. Hahnloser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6302><div class="card-body p-3 small">Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They mainly do well for <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language pairs</a>, where the languages share the same <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a>. However, for <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the gap between the two different <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> poses a major challenge because of a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, by breaking down <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> into linguistic units similar to that of <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>. We use the Wubi encoding scheme, which preserves the original shape and semantic information of the characters, while also being reversible. We show promising results from training Wubi-based models on the character- and subword-level with recurrent as well as convolutional models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6304/>An Analysis of Attention Mechanisms : The Case of <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> in Neural Machine Translation</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6304><div class="card-body p-3 small">Recent work has shown that the encoder-decoder attention mechanisms in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> are different from the <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> pay more attention to <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context tokens</a> when translating <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>. We explore the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution patterns</a> when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is not the main mechanism used by NMT models to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for WSD. The experimental results suggest that NMT models learn to encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to align source and target tokens and the last few layers learn to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the related but unaligned context tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6305/>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>roatian Human and Machine Translation</a></strong><br><a href=/people/m/margita-sostaric/>Margita Šoštarić</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6305><div class="card-body p-3 small">We present an analysis of a number of <a href=https://en.wikipedia.org/wiki/Coreference>coreference phenomena</a> in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> produced by <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6306/>Coreference and Coherence in Neural Machine Translation : A Study Using Oracle Experiments</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6306><div class="card-body p-3 small">Cross-sentence context can provide valuable information in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence</a>. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, of up to 7.02 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and 1.89 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6307/>A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6307><div class="card-body p-3 small">The translation of pronouns presents a special challenge to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to this day, since <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> often requires context outside the current sentence. Recent work on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is needed to assess how well <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> translate inter-sentential phenomena such as <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6309/>A neural interlingua for multilingual machine translation</a></strong><br><a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a>
|
<a href=/people/s/shaonan-zhang/>Shaonan Zhang</a>
|
<a href=/people/j/jason-sun/>Jason Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6309><div class="card-body p-3 small">We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6310/>Improving Neural Language Models with Weight Norm Initialization and Regularization</a></strong><br><a href=/people/c/christian-herold/>Christian Herold</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6310><div class="card-body p-3 small">Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrices</a> in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a <a href=https://en.wikipedia.org/wiki/Grid_search>grid search</a>. With this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank (PTB)</a> and from 61.45 to 60.13 on <a href=https://en.wikipedia.org/wiki/WikiText>WikiText-2 (WT2)</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6311/>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6311><div class="card-body p-3 small">Recent works in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> extracted from <a href=https://en.wikipedia.org/wiki/Europarl>Europarl v7</a> and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6313/>Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6313><div class="card-body p-3 small">To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, and each embedding space) and consider each component&#8217;s contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6314/>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection</a></strong><br><a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a>
|
<a href=/people/t/tetsuji-nakagawa/>Tetsuji Nakagawa</a>
|
<a href=/people/c/ciprian-chelba/>Ciprian Chelba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6314><div class="card-body p-3 small">Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6315/>Using Monolingual Data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : a Systematic Study</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6315><div class="card-body p-3 small">Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-6319.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-6319/>A Call for Clarity in Reporting BLEU Scores<span class=acl-fixed-case>BLEU</span> Scores</a></strong><br><a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6319><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to the <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> are often not reported or are hard to find, and consequently, BLEU scores between papers can not be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6320/>Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for gisting</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6320><div class="card-body p-3 small">A popular application of machine translation (MT) is gisting : MT is consumed as is to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses reading comprehension questionnaires (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, gap-filling (GF), a form of cloze testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are : (a) both <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a> and GF clearly identify MT to be useful ; (b) global RCQ and GF rankings for the MT systems are mostly in agreement ; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a>, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of <a href=https://en.wikipedia.org/wiki/Glucosamine>GF</a> as a cheaper alternative to <a href=https://en.wikipedia.org/wiki/Carboxylic_acid>RCQ</a>.<i>gisting</i>: MT is consumed <i>as is</i> to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses <i>reading comprehension questionnaires</i> (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, <i>gap-filling</i> (GF), a form of <i>cloze</i> testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6321/>Simple Fusion : Return of the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a></a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6321><div class="card-body p-3 small">Neural Machine Translation (NMT) typically leverages <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training : We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual probability</a> of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and <a href=https://en.wikipedia.org/wiki/Cold_fusion>cold fusion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6324/>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6324><div class="card-body p-3 small">We work on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation system</a> that addresses these challenges using eight <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language families</a> as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in qualitative evaluation where our translations are akin to human translations in a preliminary study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6325/>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6325><div class="card-body p-3 small">Transfer learning has been proven as an effective technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> under low-resource conditions. Existing methods require a common target language, <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a>, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a parent model for a high-resource language pair and then continue the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> on a low-resource pair only by replacing the training corpus. This <a href=https://en.wikipedia.org/wiki/Child_model>child model</a> performs significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained for low-resource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.</div></div></div><hr><div id=w18-64><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-64.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-64/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6400/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6403/>Findings of the WMT 2018 Biomedical Translation Shared Task : Evaluation on Medline test sets<span class=acl-fixed-case>WMT</span> 2018 Biomedical Translation Shared Task: Evaluation on <span class=acl-fixed-case>M</span>edline test sets</a></strong><br><a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/c/cristian-grozea/>Cristian Grozea</a>
|
<a href=/people/a/amy-siu/>Amy Siu</a>
|
<a href=/people/m/madeleine-kittner/>Madeleine Kittner</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6403><div class="card-body p-3 small">Machine translation enables the <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> and Spanish). We describe the development of the various <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test sets</a>, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year&#8217;s BLEU scores were somewhat higher that last year&#8217;s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6404/>An Empirical Study of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for the Shared Task of WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/y/yiming-wang/>Yiming Wang</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a>
|
<a href=/people/s/shiqi-li/>Shiqi Li</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6404><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT18 shared news translation task. We participated in the English-to-Chinese direction and get the best BLEU (43.8) scores among all the participants. The submitted <a href=https://en.wikipedia.org/wiki/System>system</a> focus on data clearing and techniques to build a competitive <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Unlike other participants, the submitted system are mainly relied on the data filtering to obtain the best BLEU score. We do data filtering not only for provided sentences but also for the back translated sentences. The techniques we apply for data filtering include filtering by rules, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and translation models. We also conduct several experiments to validate the effectiveness of training techniques. According to our experiments, the Annealing Adam optimizing function and ensemble decoding are the most effective techniques for the model training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6406/>The TALP-UPC Machine Translation Systems for WMT18 News Shared Translation Task<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18 News Shared Translation Task</a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6406><div class="card-body p-3 small">In this article we describe the TALP-UPC research group participation in the WMT18 news shared translation task for Finnish-English and Estonian-English within the multi-lingual subtrack. All of our primary submissions implement an attention-based Neural Machine Translation architecture. Given that <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> and Estonian belong to the same language family and are similar, we use as training data the combination of the datasets of both language pairs to paliate the data scarceness of each individual pair. We also report the translation quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>systems</a> trained on individual language pair data to serve as baseline and comparison reference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6411/>The AFRL WMT18 Systems : Ensembling, Continuation and Combination<span class=acl-fixed-case>AFRL</span> <span class=acl-fixed-case>WMT</span>18 Systems: Ensembling, Continuation and Combination</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/k/katherine-young/>Katherine Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6411><div class="card-body p-3 small">This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT18 evaluation campaign. This year, we examined the developments and additions to popular neural machine translation toolkits and measure improvements in performance on the RussianEnglish language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6412/>The University of Edinburgh’s Submissions to the WMT18 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>18 News Translation Task</a></strong><br><a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6412><div class="card-body p-3 small">The University of Edinburgh made submissions to all 14 language pairs in the news translation task, with strong performances in most pairs. We introduce new RNN-variant, mixed RNN / Transformer ensembles, data selection and weighting, and extensions to back-translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6416/>CUNI Submissions in WMT18<span class=acl-fixed-case>CUNI</span> Submissions in <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/r/roman-sudarikov/>Roman Sudarikov</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6416><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/Estonian_language>English-Estonian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>English-Czech</a>. Our main focus was the low-resource language pair of Estonian and English for which we utilized Finnish parallel data in a simple method. We first train a parent model for the high-resource language pair followed by <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> on the related low-resource language pair. This approach brings a substantial performance boost over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> trained only on Estonian-English parallel data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Transformer architecture. For the English to Czech translation, we have evaluated our last year models of hybrid phrase-based approach and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> mainly for comparison purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6418/>JUCBNMT at WMT2018 News Translation Task : Character Based Neural Machine Translation of Finnish to English<span class=acl-fixed-case>JUCBNMT</span> at <span class=acl-fixed-case>WMT</span>2018 News Translation Task: Character Based Neural Machine Translation of <span class=acl-fixed-case>F</span>innish to <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/sainik-mahata/>Sainik Kumar Mahata</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6418><div class="card-body p-3 small">In the current work, we present a description of the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to WMT 2018 News Translation Shared task. The <a href=https://en.wikipedia.org/wiki/System>system</a> was created to translate news text from <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> used a Character Based Neural Machine Translation model to accomplish the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The current paper documents the preprocessing steps, the description of the submitted <a href=https://en.wikipedia.org/wiki/System>system</a> and the results produced using the same. Our <a href=https://en.wikipedia.org/wiki/System>system</a> garnered a BLEU score of 12.9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6420/>PROMT Systems for WMT 2018 Shared Translation Task<span class=acl-fixed-case>PROMT</span> Systems for <span class=acl-fixed-case>WMT</span> 2018 Shared Translation Task</a></strong><br><a href=/people/a/alexander-molchanov/>Alexander Molchanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6420><div class="card-body p-3 small">This paper describes the PROMT submissions for the WMT 2018 Shared News Translation Task. This year we participated only in the English-Russian language pair. We built two primary neural networks-based systems : 1) a pure Marian-based neural system and 2) a hybrid system which incorporates OpenNMT-based neural post-editing component into our RBMT engine. We also submitted pure rule-based translation (RBMT) for <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrast</a>. We show competitive results with both primary submissions which significantly outperform the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>RBMT baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6422/>The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018<span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6422><div class="card-body p-3 small">We present our experiments in the scope of the news translation task in WMT 2018, in directions : EnglishGerman. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with a deeper architecture. By using techniques to limit the memory consumption, we were able to train <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> that are 4 times larger on one <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by 0.5 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6424/>CUNI Transformer Neural MT System for WMT18<span class=acl-fixed-case>CUNI</span> Transformer Neural <span class=acl-fixed-case>MT</span> System for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6424><div class="card-body p-3 small">We describe our NMT system submitted to the WMT2018 shared task in news translation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering of the synthetic data</a>. We pre-process the input sentences using <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p 0.05) better than all other English-Czech and Czech-English systems in WMT2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6426" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6426/>The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/p/parnia-bahar/>Parnia Bahar</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/a/arne-nix/>Arne Nix</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6426><div class="card-body p-3 small">This paper describes the statistical machine translation systems developed at RWTH Aachen University for the GermanEnglish, EnglishTurkish and ChineseEnglish translation tasks of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use ensembles of neural machine translation systems based on the Transformer architecture. Our main focus is on the GermanEnglish task where we to all automatic scored first with respect metrics provided by the organizers. We identify data selection, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, <a href=https://en.wikipedia.org/wiki/Batch_processing>batch size</a> and model dimension as important hyperparameters. In total we improve by 6.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over our last year&#8217;s submission and by 4.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over the winning system of the 2017 GermanEnglish task. In EnglishTurkish task, we show 3.6 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvement over the last year&#8217;s winning system. We further report results on the <a href=https://en.wikipedia.org/wiki/English_language>ChineseEnglish task</a> where we improve 2.2 % BLEU on average over our baseline systems but stay behind the 2018 winning systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6427/>The University of Cambridge’s Machine Translation Systems for WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>C</span>ambridge’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6427><div class="card-body p-3 small">The University of Cambridge submission to the WMT18 news translation task focuses on the combination of diverse models of translation. We compare recurrent, convolutional, and self-attention-based neural models on German-English, English-German, and Chinese-English. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> combines all neural models together with a phrase-based SMT system in an MBR-based scheme. We report small but consistent gains on top of strong Transformer ensembles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6428/>The LMU Munich Unsupervised Machine Translation Systems<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation Systems</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6428><div class="card-body p-3 small">We describe LMU Munich&#8217;s unsupervised machine translation systems for EnglishGerman translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6429/>Tencent Neural Machine Translation Systems for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/li-gong/>Li Gong</a>
|
<a href=/people/w/wenhuan-zhu/>Wenhuan Zhu</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/c/chao-bian/>Chao Bian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6429><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task on EnglishChinese language pair. Our systems are based on attentional sequence-to-sequence models with some form of <a href=https://en.wikipedia.org/wiki/Recursion>recursion</a> and self-attention. Some data augmentation methods are also introduced to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. The best <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> result is obtained with ensemble and reranking techniques. Our ChineseEnglish system achieved the highest cased BLEU score among all 16 submitted systems, and our EnglishChinese system ranked the third out of 18 submitted systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6431/>The University of Maryland’s Chinese-English Neural Machine Translation Systems at WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6431><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submission to the WMT 2018 ChineseEnglish news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>, we improve over the Transformer baseline by +1.4 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for ChineseEnglish and +3.97 BLEU for EnglishChinese on newstest2017. Our best systems reach <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> of 24.4 for <a href=https://en.wikipedia.org/wiki/Chinese_English>ChineseEnglish</a> and 39.0 for <a href=https://en.wikipedia.org/wiki/Chinese_English>EnglishChinese</a> on newstest2018.<i>newstest2017</i>. Our best systems reach BLEU scores of 24.4 for Chinese&#8594;English and 39.0 for English&#8594;Chinese on <i>newstest2018</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6432/>EvalD Reference-Less Discourse Evaluation for WMT18<span class=acl-fixed-case>E</span>val<span class=acl-fixed-case>D</span> Reference-Less Discourse Evaluation for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jiri-mirovsky/>Jiří Mírovský</a>
|
<a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6432><div class="card-body p-3 small">We present the results of automatic evaluation of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> in machine translation (MT) outputs using the EVALD tool. EVALD was originally designed and trained to assess the quality of <a href=https://en.wikipedia.org/wiki/Writing>human writing</a>, for <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.<i>human</i> writing, for native speakers and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6436/>Fine-grained evaluation of German-English Machine Translation based on a Test Suite<span class=acl-fixed-case>G</span>erman-<span class=acl-fixed-case>E</span>nglish Machine Translation based on a Test Suite</a></strong><br><a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/e/eleftherios-avramidis/>Eleftherios Avramidis</a>
|
<a href=/people/a/aljoscha-burchardt/>Aljoscha Burchardt</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6436><div class="card-body p-3 small">We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The <a href=https://en.wikipedia.org/wiki/Test_suite>test suite</a> has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>verb tenses</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_mood>moods</a>. The MT outputs are evaluated in a semi-automatic way through <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare <a href=https://en.wikipedia.org/wiki/System>systems</a> based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> and we identify grammatical phenomena where the overall performance of MT is relatively low.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6437/>The Word Sense Disambiguation Test Suite at WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6437><div class="card-body p-3 small">We present a task to measure an MT system&#8217;s capability to translate ambiguous words with their correct sense according to the given context. The task is based on the GermanEnglish Word Sense Disambiguation (WSD) test set ContraWSD (Rios Gonzales et al., 2017), but it has been filtered to reduce noise, and the evaluation has been adapted to assess MT output directly rather than scoring existing translations. We evaluate all GermanEnglish submissions to the WMT&#8217;18 shared translation task, plus a number of submissions from previous years, and find that performance on the task has markedly improved compared to the 2016 WMT submissions (81%93 % accuracy on the WSD task). We also find that the unsupervised submissions to the task have a low WSD capability, and predominantly translate ambiguous source words with the same sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6440 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6440/>The AFRL-Ohio State WMT18 Multimodal System : Combining Visual with Traditional<span class=acl-fixed-case>AFRL</span>-<span class=acl-fixed-case>O</span>hio <span class=acl-fixed-case>S</span>tate <span class=acl-fixed-case>WMT</span>18 Multimodal System: Combining Visual with Traditional</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/j/joshua-sandvick/>Joshua Sandvick</a>
|
<a href=/people/m/michael-hutt/>Michael Hutt</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/john-duselis/>John Duselis</a>
|
<a href=/people/j/james-davis/>James Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6440><div class="card-body p-3 small">AFRL-Ohio State extends its usage of visual domain-driven machine translation for use as a peer with traditional <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6441/>CUNI System for the WMT18 Multimodal Translation Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6441><div class="card-body p-3 small">We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. We evaluate two methods of incorporating the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> in the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> : first, we include the image representation as another input to the network ; second, we train the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> to predict the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a> and self-attentive textual baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6442/>Sheffield Submissions for WMT18 Multimodal Translation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Shared Task</a></strong><br><a href=/people/c/chiraag-lala/>Chiraag Lala</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6442><div class="card-body p-3 small">This paper describes the University of Sheffield&#8217;s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking approaches. More specifically, n-best translation candidates from this system are re-ranked using novel multimodal cross-lingual word sense disambiguation models. For task 1b, we explore three approaches : (i) re-ranking based on cross-lingual word sense disambiguation (as for task 1), (ii) re-ranking based on consensus of NMT n-best lists from German-Czech, French-Czech and English-Czech systems, and (iii) data augmentation by generating English source data through <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from French to English and from German to English followed by hypothesis selection using a multimodal-reranker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6444/>Translation of Biomedical Documents with Focus on Spanish-English<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/mirela-stefania-duma/>Mirela-Stefania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6444><div class="card-body p-3 small">For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> performed best on <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese-English</a>, where a BLEU score of 41.84 placed <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> third out of seven runs submitted by three institutions. In this paper, we describe our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and results with a special focus on <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish-English</a> where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for selecting domain-specific data for training models which obtain good results using only 10 % of the general domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6445/>Ensemble of Translators with Automatic Selection of the Best Translation the submission of FOKUS to the WMT 18 biomedical translation task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 18 biomedical translation task –</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6445><div class="card-body p-3 small">This paper describes the system of Fraunhofer FOKUS for the WMT 2018 biomedical translation task. Our approach, described here, was to automatically select the most promising <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> from a set of candidates produced with NMT (Transformer) models. We selected the highest fidelity translation of each sentence by using a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, <a href=https://en.wikipedia.org/wiki/Stemming>stemming</a> and a set of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. Our method is simple, can use any machine translators, and requires no further training in addition to that already employed to build the NMT models. The downside is that the score did not increase over the best in <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a>, but was quite close to it (difference about 0.5 BLEU).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6446/>LMU Munich’s Neural Machine Translation Systems at WMT 2018<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich’s Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6446><div class="card-body p-3 small">We present the LMU Munich machine translation systems for the EnglishGerman language pair. We have built neural machine translation systems for both translation directions (EnglishGerman and GermanEnglish) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year&#8217;s strong biomedical translation engine for EnglishGerman (Huck et al., 2017a). Considerable progress has been made in the latter <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which we report on in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6447/>Hunter NMT System for WMT18 Biomedical Translation Task : Transfer Learning in Neural Machine Translation<span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WMT</span>18 Biomedical Translation Task: Transfer Learning in Neural Machine Translation</a></strong><br><a href=/people/a/abdul-khan/>Abdul Khan</a>
|
<a href=/people/s/subhadarshi-panda/>Subhadarshi Panda</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a>
|
<a href=/people/l/lampros-flokas/>Lampros Flokas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6447><div class="card-body p-3 small">This paper describes the submission of Hunter Neural Machine Translation (NMT) to the WMT&#8217;18 Biomedical translation task from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a>. The discrepancy between training and test data distribution brings a challenge to translate text in new domains. Beyond the previous work of combining in-domain with out-of-domain models, we found accuracy and efficiency gain in combining different in-domain models. We conduct extensive experiments on NMT with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Furthermore, we applied <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble learning</a> on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.<i>transfer learning</i>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the baseline. Furthermore, we applied ensemble learning on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the baseline. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6448 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6448/>UFRGS Participation on the WMT Biomedical Translation Shared Task<span class=acl-fixed-case>UFRGS</span> Participation on the <span class=acl-fixed-case>WMT</span> Biomedical Translation Shared Task</a></strong><br><a href=/people/f/felipe-soares/>Felipe Soares</a>
|
<a href=/people/k/karin-becker/>Karin Becker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6448><div class="card-body p-3 small">This paper describes the machine translation systems developed by the Universidade Federal do Rio Grande do Sul (UFRGS) team for the biomedical translation shared task. Our systems are based on <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, using the Moses and OpenNMT toolkits, respectively. We participated in four translation directions for the English / Spanish and English / Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved the best <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> according to the official shared task evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6449/>Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task<span class=acl-fixed-case>R</span>omance Languages for the Biomedical <span class=acl-fixed-case>WMT</span> 2018 task</a></strong><br><a href=/people/b/brian-tubay/>Brian Tubay</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6449><div class="card-body p-3 small">The Transformer architecture has become the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which relies on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention-based mechanisms</a>, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using <a href=https://en.wikipedia.org/wiki/List_of_programming_languages_by_type>multi-source languages</a> from the same family allows improvements of over 6 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6455/>ITER : Improving Translation Edit Rate through Optimizable Edit Costs<span class=acl-fixed-case>ITER</span>: Improving Translation Edit Rate through Optimizable Edit Costs</a></strong><br><a href=/people/j/joybrata-panja/>Joybrata Panja</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6455><div class="card-body p-3 small">The paper presents our participation in the WMT 2018 Metrics Shared Task. We propose an improved version of Translation Edit / Error Rate (TER). In addition to including the basic edit operations in TER, namely-insertion, deletion, substitution and shift, our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> also allows stem matching, optimizable edit costs and better <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> so as to correlate better with human judgement scores. The proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> shows much higher correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> than TER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6456 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6456" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6456/>RUSE : Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation<span class=acl-fixed-case>RUSE</span>: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6456><div class="card-body p-3 small">We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that can not be captured by local features based on character or word N-grams. Although training <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using small-scale translation datasets with manual evaluation is difficult, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use a multi-layer perceptron regressor based on three types of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6459 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6459/>Neural Machine Translation for English-Tamil<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/h/himanshu-choudhary/>Himanshu Choudhary</a>
|
<a href=/people/a/aditya-kumar-pathak/>Aditya Kumar Pathak</a>
|
<a href=/people/r/rajiv-ratan-saha/>Rajiv Ratan Saha</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6459><div class="card-body p-3 small">A huge amount of valuable resources is available on the web in English, which are often translated into local languages to facilitate knowledge sharing among local people who are not much familiar with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. However, translating such content manually is very tedious, costly, and time-consuming process. To this end, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is an efficient approach to translate text without any human involvement. Neural machine translation (NMT) is one of the most recent and effective <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation technique</a> amongst all existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. In this paper, we apply NMT for English-Tamil language pair. We propose a novel neural machine translation technique using word-embedding along with Byte-Pair-Encoding (BPE) to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for languages which do not have much translations available online. We use the BLEU score for evaluating the <a href=https://en.wikipedia.org/wiki/System>system</a> performance. Experimental results confirm that our proposed MIDAS translator (8.33 BLEU score) outperforms <a href=https://en.wikipedia.org/wiki/Google_Translate>Google translator</a> (3.75 BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6460 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6460/>The Benefit of Pseudo-Reference Translations in Quality Estimation of MT Output<span class=acl-fixed-case>MT</span> Output</a></strong><br><a href=/people/m/melania-duma/>Melania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6460><div class="card-body p-3 small">In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before. Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair. They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6461 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6461/>Supervised and Unsupervised Minimalist Quality Estimators : Vicomtech’s Participation in the WMT 2018 Quality Estimation Task<span class=acl-fixed-case>WMT</span> 2018 Quality Estimation Task</a></strong><br><a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/e/eva-martinez-garcia/>Eva Martínez Garcia</a>
|
<a href=/people/a/andoni-azpeitia/>Andoni Azpeitia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6461><div class="card-body p-3 small">We describe Vicomtech&#8217;s participation in the WMT 2018 shared task on quality estimation, for which we submitted minimalist quality estimators. The core of our approach is based on two simple features : <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical translation overlaps</a> and language model cross-entropy scores. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are exploited in two system variants : uMQE is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a>, where the final quality score is obtained by averaging individual feature scores ; sMQE is a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised variant</a>, where the final score is estimated by a Support Vector Regressor trained on the available annotated datasets. The main goal of our minimalist approach to quality estimation is to provide reliable estimators that require minimal deployment effort, few resources, and, in the case of uMQE, do not depend on costly data annotation or <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Our approach was applied to all language pairs in sentence quality estimation, obtaining competitive results across the board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6463 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6463/>Sheffield Submissions for the WMT18 Quality Estimation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for the <span class=acl-fixed-case>WMT</span>18 Quality Estimation Shared Task</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6463><div class="card-body p-3 small">In this paper we present the University of Sheffield submissions for the WMT18 Quality Estimation shared task. We discuss our submissions to all four sub-tasks, where ours is the only team to participate in all language pairs and variations (37 combinations). Our <a href=https://en.wikipedia.org/wiki/System>systems</a> show competitive results and outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> in nearly all cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6464 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6464/>UAlacant machine translation quality estimation at WMT 2018 : a simple approach using phrase tables and feed-forward neural networks<span class=acl-fixed-case>UA</span>lacant machine translation quality estimation at <span class=acl-fixed-case>WMT</span> 2018: a simple approach using phrase tables and feed-forward neural networks</a></strong><br><a href=/people/f/felipe-sanchez-martinez/>Felipe Sánchez-Martínez</a>
|
<a href=/people/m/miquel-espla-gomis/>Miquel Esplà-Gomis</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6464><div class="card-body p-3 small">We describe the Universitat d&#8217;Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.<i>OK</i> or <i>BAD</i>, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6465 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6465/>Alibaba Submission for WMT18 Quality Estimation Task<span class=acl-fixed-case>A</span>libaba Submission for <span class=acl-fixed-case>WMT</span>18 Quality Estimation Task</a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/k/kai-fan/>Kai Fan</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/f/fengming-zhou/>Fengming Zhou</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6465><div class="card-body p-3 small">The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The <a href=https://en.wikipedia.org/wiki/System>system</a> has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6466 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6466/>Quality Estimation with Force-Decoded Attention and Cross-lingual Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6466><div class="card-body p-3 small">This paper describes the submissions of the team from the University of Tartu for the sentence-level Quality Estimation shared task of WMT18. The proposed models use features based on attention weights of a neural machine translation system and cross-lingual phrase embeddings as input features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Two of the proposed models require only a neural machine translation system with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> with no additional resources. Results show that combining <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and baseline features leads to significant improvements over the baseline features alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6467 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6467/>MS-UEdin Submission to the WMT2018 APE Shared Task : Dual-Source Transformer for Automatic Post-Editing<span class=acl-fixed-case>MS</span>-<span class=acl-fixed-case>UE</span>din Submission to the <span class=acl-fixed-case>WMT</span>2018 <span class=acl-fixed-case>APE</span> Shared Task: Dual-Source Transformer for Automatic Post-Editing</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6467><div class="card-body p-3 small">This paper describes the Microsoft and University of Edinburgh submission to the Automatic Post-editing shared task at WMT2018. Based on training data and systems from the WMT2017 shared task, we re-implement our own models from the last shared task and introduce improvements based on extensive parameter sharing. Next we experiment with our implementation of dual-source transformer models and data selection for the IT domain. Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6468 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6468/>A Transformer-Based Multi-Source Automatic Post-Editing System</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6468><div class="card-body p-3 small">This paper presents our EnglishGerman Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture : two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt) for generating the post-edited sentence. We compare this multi-source architecture (i.e, <a href=https://en.wikipedia.org/wiki/Source_code>src</a>, mt pe) to a monolingual transformer (i.e., mt pe) model and an ensemble combining the multi-source src, mt pe and single-source mt pe models. For both the PBSMT and the NMT task, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> yields the best results, followed by the multi-source model and last the single-source approach. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a>, achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6469 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6469/>DFKI-MLT System Description for the WMT18 Automatic Post-editing Task<span class=acl-fixed-case>DFKI</span>-<span class=acl-fixed-case>MLT</span> System Description for the <span class=acl-fixed-case>WMT</span>18 Automatic Post-editing Task</a></strong><br><a href=/people/d/daria-pylypenko/>Daria Pylypenko</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6469><div class="card-body p-3 small">This paper presents the Automatic Post-editing (APE) systems submitted by the DFKI-MLT group to the WMT&#8217;18 APE shared task. Three monolingual neural sequence-to-sequence APE systems were trained using target-language data only : one using an attentional recurrent neural network architecture and two using the attention-only (transformer) architecture. The <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.<i>transformer</i>) architecture. The training data was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6472 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6472/>The Speechmatics Parallel Corpus Filtering System for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-ash/>Tom Ash</a>
|
<a href=/people/r/remi-francis/>Remi Francis</a>
|
<a href=/people/w/will-williams/>Will Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6472><div class="card-body p-3 small">Our entry to the parallel corpus filtering task uses a two-step strategy. The first step uses a series of pragmatic hard &#8216;rules&#8217; to remove the worst example sentences. This first step reduces the effective corpus size down from the initial 1 billion to 160 million tokens. The second step uses four different <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a> weighted to produce a score that is then used for further filtering down to 100 or 10 million tokens. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> produces competitive results without requiring excessive fine tuning to the exact task or language pair. The first step in isolation provides a very fast filter that gives most of the gains of the final <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6477 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6477/>An Unsupervised System for Parallel Corpus Filtering</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6477><div class="card-body p-3 small">In this paper we describe LMU Munich&#8217;s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.<i>WMT 2018 Parallel Corpus Filtering</i> shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6478 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6478/>Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6478><div class="card-body p-3 small">In this work we introduce dual conditional cross-entropy filtering for noisy parallel data. For each sentence pair of the noisy parallel corpus we compute cross-entropy scores according to two inverse translation models trained on clean data. We penalize divergent cross-entropies and weigh the penalty by the cross-entropy average of both models. Sorting or thresholding according to these scores results in better subsets of parallel data. We achieve higher BLEU scores with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on parallel data filtered only from Paracrawl than with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean WMT data. We further evaluate our method in the context of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6480 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6480/>Measuring sentence parallelism using Mahalanobis distances : The NRC unsupervised submissions to the WMT18 Parallel Corpus Filtering shared task<span class=acl-fixed-case>NRC</span> unsupervised submissions to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6480><div class="card-body p-3 small">The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a clean corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the only sample of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel text</a> in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one tasktranslating the European Medicines Agency corpus (Tiedemann, 2009)scored among the best systems even in the 10M-word conditions.<i>only</i> sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task&#8212;translating the European Medicines Agency corpus (Tiedemann, 2009)&#8212;scored among the best systems even in the 10M-word conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6481 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6481/>Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric : The NRC supervised submissions to the Parallel Corpus Filtering task<span class=acl-fixed-case>NRC</span> supervised submissions to the Parallel Corpus Filtering task</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6481><div class="card-body p-3 small">We present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSia novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-million-word evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing systemNRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6482 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6482/>Alibaba Submission to the WMT18 Parallel Corpus Filtering Task<span class=acl-fixed-case>A</span>libaba Submission to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering Task</a></strong><br><a href=/people/j/jun-lu/>Jun Lu</a>
|
<a href=/people/x/xiaoyu-lv/>Xiaoyu Lv</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6482><div class="card-body p-3 small">This paper describes the Alibaba Machine Translation Group submissions to the WMT 2018 Shared Task on Parallel Corpus Filtering. While evaluating the quality of the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, the three characteristics of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> are investigated, i.e. 1) the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual / translation quality</a>, 2) the <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual quality</a> and 3) the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus diversity</a>. Both rule-based and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6483 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6483/>UTFPR at WMT 2018 : Minimalistic Supervised Corpora Filtering for Machine Translation<span class=acl-fixed-case>UTFPR</span> at <span class=acl-fixed-case>WMT</span> 2018: Minimalistic Supervised Corpora Filtering for Machine Translation</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6483><div class="card-body p-3 small">We present the UTFPR systems at the WMT 2018 parallel corpus filtering task. Our supervised approach discerns between good and bad translations by training classic binary classification models over an artificially produced binary classification dataset derived from a high-quality translation set, and a minimalistic set of 6 semantic distance features that rely only on easy-to-gather resources. We rank translations by their probability for the good label. Our results show that <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> pairs best with our approach, yielding more consistent results throughout the different settings evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6484 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6484/>The ILSP / ARC submission to the WMT 2018 Parallel Corpus Filtering Shared Task<span class=acl-fixed-case>ILSP</span>/<span class=acl-fixed-case>ARC</span> submission to the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Shared Task</a></strong><br><a href=/people/v/vassilis-papavassiliou/>Vassilis Papavassiliou</a>
|
<a href=/people/s/sokratis-sofianopoulos/>Sokratis Sofianopoulos</a>
|
<a href=/people/p/prokopis-prokopidis/>Prokopis Prokopidis</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6484><div class="card-body p-3 small">This paper describes the submission of the Institute for Language and Speech Processing / Athena Research and Innovation Center (ILSP / ARC) for the WMT 2018 Parallel Corpus Filtering shared task. We explore several properties of sentences and sentence pairs that our system explored in the context of the task with the purpose of clustering sentence pairs according to their appropriateness in training MT systems. We also discuss alternative methods for ranking the sentence pairs of the most appropriate clusters with the aim of generating the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (of 10 and 100 million words as required in the task) that were evaluated. By summarizing the results of several experiments that were carried out by the organizers during the evaluation phase, our submission achieved an average BLEU score of 26.41, even though it does not make use of any language-specific resources like <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual lexica</a>, monolingual corpora, or MT output, while the average score of the best participant system was 27.91.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6485 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6485/>SYSTRAN Participation to the WMT2018 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>SYSTRAN</span> Participation to the <span class=acl-fixed-case>WMT</span>2018 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6485><div class="card-body p-3 small">This paper describes the participation of <a href=https://en.wikipedia.org/wiki/SYSTRAN>SYSTRAN</a> to the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018). We participate for the first time using a neural sentence similarity classifier which aims at predicting the relatedness of sentence pairs in a multilingual context. The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6487 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6487/>The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Filtering System for the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Task</a></strong><br><a href=/people/n/nick-rossenbach/>Nick Rossenbach</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/a/aman-gokrani/>Aman Gokrani</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6487><div class="card-body p-3 small">This paper describes the submission of RWTH Aachen University for the DeEn parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence pairs</a> are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural language models</a> and <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a> based on the transformer architecture. A <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on 10 M randomly sampled tokens reaches a performance of 9.2 % BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8 % BLEU.<i>EMNLP 2018 Third Conference on Machine Translation</i> (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6488 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6488" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6488/>Prompsit’s submission to WMT 2018 Parallel Corpus Filtering shared task<span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a>
|
<a href=/people/m/marta-banon/>Marta Bañón</a>
|
<a href=/people/s/sergio-ortiz-rojas/>Sergio Ortiz-Rojas</a>
|
<a href=/people/g/gema-ramirez-sanchez/>Gema Ramírez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6488><div class="card-body p-3 small">This paper describes Prompsit Language Engineering&#8217;s submissions to the WMT 2018 parallel corpus filtering shared task. Our four submissions were based on an automatic classifier for identifying pairs of sentences that are mutual translations. A set of hand-crafted hard rules for discarding sentences with evident flaws were applied before the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. We explored different strategies for achieving a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> with diverse vocabulary and fluent sentences : language model scoring, an active-learning-inspired data selection algorithm and n-gram saturation. Our submissions were very competitive in comparison with other participants on the 100 million word training corpus.</div></div></div><hr><div id=w18-65><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-65.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-65/>Proceedings of the 11th International Conference on Natural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6500/>Proceedings of the 11th International Conference on Natural Language Generation</a></strong><br><a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/martijn-goudbeek/>Martijn Goudbeek</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6501/>Deep Graph Convolutional Encoders for <a href=https://en.wikipedia.org/wiki/Structured_data>Structured Data</a> to Text Generation</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6501><div class="card-body p-3 small">Most previous work on neural text generation from <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured data</a> relies on standard sequence-to-sequence methods. These approaches linearise the input graph to be fed to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. In this paper, we propose an alternative <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> based on graph convolutional networks that directly exploits the input structure. We report results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6502/>Describing a Knowledge Base</a></strong><br><a href=/people/q/qingyun-wang/>Qingyun Wang</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/z/zhiying-jiang/>Zhiying Jiang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6502><div class="card-body p-3 small">We aim to automatically generate <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> about an input structured knowledge base (KB). We build our generation framework based on a pointer network which can copy facts from the input KB, and add two attention mechanisms : (i) slot-aware attention to capture the association between a slot type and its corresponding slot value ; and (ii) a new table position self-attention to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, and ROUGE, we propose a <a href=https://en.wikipedia.org/wiki/Binary_logarithm>KB reconstruction based metric</a> by extracting a <a href=https://en.wikipedia.org/wiki/Binary_logarithm>KB</a> from the generation output and comparing it with the input KB. We also create a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> which includes 106,216 pairs of structured KBs and their corresponding <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> for two distinct entity types. Experiments show that our approach significantly outperforms <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a>. The reconstructed KB achieves 68.8 %-72.6 % F-score.<i>slot-aware attention</i> to capture the association between a slot type and its corresponding slot value; and (ii) a new <i>table position self-attention</i> to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we propose a <i>KB reconstruction</i> based metric by extracting a KB from the generation output and comparing it with the input KB. We also create a new data set which includes 106,216 pairs of structured KBs and their corresponding natural language descriptions for two distinct entity types. Experiments show that our approach significantly outperforms state-of-the-art methods. The reconstructed KB achieves 68.8% - 72.6% F-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6506" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6506/>SimpleNLG-ZH : a Linguistic Realisation Engine for <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a><span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span>-<span class=acl-fixed-case>ZH</span>: a Linguistic Realisation Engine for <span class=acl-fixed-case>M</span>andarin</a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6506><div class="card-body p-3 small">We introduce SimpleNLG-ZH, a realisation engine for <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a> that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other languages for which SimpleNLG engines have been built. The <a href=https://en.wikipedia.org/wiki/System>system</a> was evaluated by regenerating expressions from a body of test sentences and a corpus of human-authored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6507/>Adapting SimpleNLG to Galician language<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span> to <span class=acl-fixed-case>G</span>alician language</a></strong><br><a href=/people/a/andrea-cascallar-fuentes/>Andrea Cascallar-Fuentes</a>
|
<a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/a/alberto-bugarin-diz/>Alberto Bugarín Diz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6507><div class="card-body p-3 small">In this paper, we describe SimpleNLG-GL, an adaptation of the linguistic realisation SimpleNLG library for the <a href=https://en.wikipedia.org/wiki/Galician_language>Galician language</a>. This <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is derived from SimpleNLG-ES, the English-Spanish version of this <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>. It has been tested using a battery of examples which covers the most common rules for Galician.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6510/>Stylistically User-Specific Generation</a></strong><br><a href=/people/a/abdurrisyad-fikri/>Abdurrisyad Fikri</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6510><div class="card-body p-3 small">Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker / responder, similar utterances should require different responses. In this study, we attempt to consider individual user&#8217;s information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses. We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task. Furthermore, we prove that even for unseen or unknown users, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can provide more diverse and interesting responses, while maintaining correlation with input utterances. Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6512/>Treat the <a href=https://en.wikipedia.org/wiki/System>system</a> like a human student : Automatic naturalness evaluation of generated text without reference texts</a></strong><br><a href=/people/i/isabel-groves/>Isabel Groves</a>
|
<a href=/people/y/ye-tian/>Ye Tian</a>
|
<a href=/people/i/ioannis-douratsos/>Ioannis Douratsos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6512><div class="card-body p-3 small">The current most popular method for automatic Natural Language Generation (NLG) evaluation is comparing generated text with human-written reference sentences using a metrics system, which has drawbacks around <a href=https://en.wikipedia.org/wiki/Reliability_(computer_networking)>reliability</a> and <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a>. We draw inspiration from second language (L2) assessment and extract a set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to predict human judgments of sentence naturalness. Our experiment using a small dataset showed that the feature-based approach yields promising results, with the added potential of providing <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> into the source of the problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6513/>Content Aware Source Code Change Description Generation</a></strong><br><a href=/people/p/pablo-loyola/>Pablo Loyola</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a>
|
<a href=/people/f/fumiko-satoh/>Fumiko Satoh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6513><div class="card-body p-3 small">We propose to study the generation of descriptions from source code changes by integrating the messages included on code commits and the intra-code documentation inside the source in the form of docstrings. Our hypothesis is that although both types of descriptions are not directly aligned in semantic terms one explaining a change and the other the actual functionality of the code being modified there could be certain common ground that is useful for the generation. To this end, we propose an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that uses the source code-docstring relationship to guide the description generation. We discuss the results of the approach comparing against a baseline based on a sequence-to-sequence model, using standard automatic natural language generation metrics as well as with a human study, thus offering a comprehensive view of the feasibility of the approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6514" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6514/>Improving Context Modelling in Multimodal Dialogue Generation</a></strong><br><a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6514><div class="card-body p-3 small">In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> on our <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6515/>Generating Market Comments Referring to External Resources</a></strong><br><a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/a/akira-miyazawa/>Akira Miyazawa</a>
|
<a href=/people/t/tatsuya-ishigaki/>Tatsuya Ishigaki</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/k/kasumi-aoki/>Kasumi Aoki</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6515><div class="card-body p-3 small">Comments on a <a href=https://en.wikipedia.org/wiki/Stock_market>stock market</a> often include the reason or cause of changes in stock prices, such as <a href=https://en.wikipedia.org/wiki/Nikkei_225>Nikkei</a> turns lower as yen&#8217;s rise hits exporters. Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for automatically generating such informative market comments that refer to <a href=https://en.wikipedia.org/wiki/Resource_(economics)>external resources</a>. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> through an automatic metric in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluation done by an expert in finance. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> both in BLEU scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6518" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6518/>Automatic Opinion Question Generation</a></strong><br><a href=/people/y/yllias-chali/>Yllias Chali</a>
|
<a href=/people/t/tina-baghaee/>Tina Baghaee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6518><div class="card-body p-3 small">We study the problem of opinion question generation from sentences with the help of community-based question answering systems. For this purpose, we use a sequence to sequence attentional model, and we adopt coverage mechanism to prevent sentences from repeating themselves. Experimental results on the Amazon question / answer dataset show an improvement in automatic evaluation metrics as well as human evaluations from the state-of-the-art question generation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6519/>Modelling Pro-drop with the Rational Speech Acts Model</a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6519><div class="card-body p-3 small">We extend the classic Referring Expressions Generation task by considering zero pronouns in pro-drop languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, modelling their use by means of the Bayesian Rational Speech Acts model (Frank and Goodman, 2012). By assuming that highly salient referents are most likely to be referred to by <a href=https://en.wikipedia.org/wiki/Zero_pronoun>zero pronouns</a> (i.e., <a href=https://en.wikipedia.org/wiki/Pro-drop_language>pro-drop</a> is more likely for salient referents than the less salient ones), the model offers an attractive explanation of a phenomenon not previously addressed probabilistically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6521 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6521/>Enriching the WebNLG corpus<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>NLG</span> corpus</a></strong><br><a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/d/diego-moussallem/>Diego Moussallem</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6521><div class="card-body p-3 small">This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a, b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, <a href=https://en.wikipedia.org/wiki/Lexicalization>Lexicalization</a> and Referring Expression Generation. We also produce a silver-standard German translation of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to enable the exploitation of NLG approaches to other languages than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The enriched corpus is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6523/>Template-based multilingual football reports generation using <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> as a knowledge base<span class=acl-fixed-case>W</span>ikidata as a knowledge base</a></strong><br><a href=/people/l/lorenzo-gatti/>Lorenzo Gatti</a>
|
<a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6523><div class="card-body p-3 small">This paper presents a new version of a football reports generation system called PASS. The original <a href=https://en.wikipedia.org/wiki/Software_versioning>version</a> generated <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch text</a> and relied on a limited hand-crafted knowledge base. We describe how, in a short amount of time, we extended PASS to produce English texts, exploiting <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> as a large-scale source of multilingual knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6526/>Japanese Advertising Slogan Generator using <a href=https://en.wikipedia.org/wiki/Case_frame>Case Frame</a> and Word Vector<span class=acl-fixed-case>J</span>apanese Advertising Slogan Generator using Case Frame and Word Vector</a></strong><br><a href=/people/k/kango-iwama/>Kango Iwama</a>
|
<a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6526><div class="card-body p-3 small">There has been many works published for automatic sentence generation of a variety of domains. However, there would be still no single <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> available at present that can generate sentences for all of domains. Each domain will require a suitable generation method. We focus on automatic generation of <a href=https://en.wikipedia.org/wiki/Advertising_slogan>Japanese advertisement slogans</a> in this paper. We use our advertisement slogan database, case frame information, and word vector information. We employed our <a href=https://en.wikipedia.org/wiki/System>system</a> to apply for a copy competition for human copywriters, where our <a href=https://en.wikipedia.org/wiki/Advertising_slogan>advertisement slogan</a> was left as a finalist. Our <a href=https://en.wikipedia.org/wiki/System>system</a> could be regarded as the world first system that generates <a href=https://en.wikipedia.org/wiki/Slogan>slogans</a> in a practical level, as an advertising agency already employs our <a href=https://en.wikipedia.org/wiki/System>system</a> in their business.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6527/>Underspecified Universal Dependency Structures as Inputs for Multilingual Surface Realisation<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Structures as Inputs for Multilingual Surface Realisation</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6527><div class="card-body p-3 small">In this paper, we present the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used in the Shallow and Deep Tracks of the First Multilingual Surface Realisation Shared Task (SR&#8217;18). For the Shallow Track, data in ten languages has been released : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. For the Deep Track, data in three languages is made available : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We describe in detail how the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> were derived from the Universal Dependencies V2.0, and report on an evaluation of the Deep Track input quality. In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6528 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6528/>LSTM Hypertagging<span class=acl-fixed-case>LSTM</span> Hypertagging</a></strong><br><a href=/people/r/reid-fu/>Reid Fu</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6528><div class="card-body p-3 small">Hypertagging, or supertagging for surface realization, is the process of assigning <a href=https://en.wikipedia.org/wiki/Lexical_category>lexical categories</a> to nodes in an input semantic graph. Previous work has shown that hypertagging significantly increases realization speed and quality by reducing the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the <a href=https://en.wikipedia.org/wiki/Realizer>realizer</a>. Building on recent work using LSTMs to improve accuracy on supertagging for parsing, we develop an LSTM hypertagging method for OpenCCG, an open source NLP toolkit for CCG. Our results show significant improvements in both hypertagging accuracy and downstream realization performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6530/>Generating E-Commerce Product Titles and Predicting their Quality<span class=acl-fixed-case>E</span>-Commerce Product Titles and Predicting their Quality</a></strong><br><a href=/people/j/jose-g-c-de-souza/>José G. Camargo de Souza</a>
|
<a href=/people/m/michael-kozielski/>Michael Kozielski</a>
|
<a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6530><div class="card-body p-3 small">E-commerce platforms present products using titles that summarize product information. These titles can not be created by hand, therefore an algorithmic solution is required. The task of automatically generating these <a href=https://en.wikipedia.org/wiki/Title_(publishing)>titles</a> given noisy user provided titles is one way to achieve the goal. The setting requires the generation process to be fast and the generated title to be both human-readable and concise. Furthermore, we need to understand if such generated titles are usable. As such, we propose approaches that (i) automatically generate product titles, (ii) predict their quality. Our approach scales to millions of products and both automatic and human evaluations performed on real-world data indicate our approaches are effective and applicable to existing e-commerce scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6531/>Designing and testing the messages produced by a virtual dietitian</a></strong><br><a href=/people/l/luca-anselma/>Luca Anselma</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6531><div class="card-body p-3 small">This paper presents a project about the automatic generation of persuasive messages in the context of the <a href=https://en.wikipedia.org/wiki/Diet_(nutrition)>diet management</a>. In the first part of the paper we introduce the basic mechanisms related to data interpretation and content selection for a numerical data-to-text generation architecture. In the second part of the paper we discuss a number of factors influencing the design of the <a href=https://en.wikipedia.org/wiki/Message>messages</a>. In particular, we consider the design of the aggregation procedure. Finally, we present the results of a human-based evaluation concerning this design factor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6533/>Automatically Generating Questions about Novel Metaphors in Literature</a></strong><br><a href=/people/n/natalie-parde/>Natalie Parde</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6533><div class="card-body p-3 small">The automatic generation of stimulating questions is crucial to the development of intelligent cognitive exercise applications. We developed an approach that generates appropriate Questioning the Author queries based on novel <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> in diverse syntactic relations in literature. We show that the generated questions are comparable to human-generated questions in terms of naturalness, <a href=https://en.wikipedia.org/wiki/Sensibility>sensibility</a>, and depth, and score slightly higher than human-generated questions in terms of clarity. We also show that questions generated about novel <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> are rated as cognitively deeper than questions generated about non- or conventional metaphors, providing evidence that metaphor novelty can be leveraged to promote cognitive exercise.<i>Questioning the Author</i> queries based on novel metaphors in diverse syntactic relations in literature. We show that the generated questions are comparable to human-generated questions in terms of naturalness, sensibility, and depth, and score slightly higher than human-generated questions in terms of clarity. We also show that questions generated about novel metaphors are rated as cognitively deeper than questions generated about non- or conventional metaphors, providing evidence that metaphor novelty can be leveraged to promote cognitive exercise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6534/>A Master-Apprentice Approach to Automatic Creation of Culturally Satirical Movie Titles</a></strong><br><a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a>
|
<a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6534><div class="card-body p-3 small">Satire has played a role in indirectly expressing critique towards an authority or a person from time immemorial. We present an autonomously creative master-apprentice approach consisting of a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> and an NMT model to produce humorous and culturally apt satire out of movie titles automatically. Furthermore, we evaluate the <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> in terms of its creativity and its output. We provide a solid definition for <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a> to maximize the objectiveness of the evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6536 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6536/>Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features</a></strong><br><a href=/people/v/vrindavan-harrison/>Vrindavan Harrison</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6536><div class="card-body p-3 small">Question Generation is the task of automatically creating questions from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual input</a>. In this work we present a new Attentional EncoderDecoder Recurrent Neural Network model for automatic question generation. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> incorporates linguistic features and an additional <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a> to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> improve the quality of the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6537/>Evaluation methodologies in Automatic Question Generation 2013-2018</a></strong><br><a href=/people/j/jacopo-amidei/>Jacopo Amidei</a>
|
<a href=/people/p/paul-piwek/>Paul Piwek</a>
|
<a href=/people/a/alistair-willis/>Alistair Willis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6537><div class="card-body p-3 small">In the last few years Automatic Question Generation (AQG) has attracted increasing interest. In this paper we survey the evaluation methodologies used in <a href=https://en.wikipedia.org/wiki/AQG>AQG</a>. Based on a sample of 37 papers, our research shows that the <a href=https://en.wikipedia.org/wiki/System>systems</a>&#8217; development has not been accompanied by similar developments in the <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> used for the <a href=https://en.wikipedia.org/wiki/System>systems&#8217; evaluation</a>. Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies. Such diverse evaluation practices make it difficult to reliably compare the quality of different <a href=https://en.wikipedia.org/wiki/Electricity_generation>generation systems</a>. Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6538 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6538/>Task Proposal : The TL;DR Challenge<span class=acl-fixed-case>TL</span>;<span class=acl-fixed-case>DR</span> Challenge</a></strong><br><a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/m/michael-volske/>Michael Völske</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/n/nedim-lipka/>Nedim Lipka</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6538><div class="card-body p-3 small">The TL;DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, which has been overlooked by summarization research so far. The challenge owes its name to the frequent practice of social media users to supplement long posts with a TL;DRfor too long ; did n&#8217;t readfollowed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length. Posts featuring TL;DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, opening the door to all kinds of generative models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6541" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6541/>BENGAL : An <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>Automatic Benchmark Generator</a> for Entity Recognition and Linking<span class=acl-fixed-case>BENGAL</span>: An Automatic Benchmark Generator for Entity Recognition and Linking</a></strong><br><a href=/people/a/axel-cyrille-ngonga-ngomo/>Axel-Cyrille Ngonga Ngomo</a>
|
<a href=/people/m/michael-roder/>Michael Röder</a>
|
<a href=/people/d/diego-moussallem/>Diego Moussallem</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a>
|
<a href=/people/r/rene-speck/>René Speck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6541><div class="card-body p-3 small">The manual creation of gold standards for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> is time- and resource-intensive. Moreover, recent works show that such <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standards</a> contain a large proportion of mistakes in addition to being difficult to maintain. We hence present <a href=https://en.wikipedia.org/wiki/Bengal>Bengal</a>, a novel automatic generation of such <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standards</a> as a complement to manually created benchmarks. The main advantage of our <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> is that they can be readily generated at any time. They are also cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> generated by <a href=https://en.wikipedia.org/wiki/Bengal>Bengal</a> and on 16 <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> created manually. We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is open-source. Our experimental results are available at and the code at.<url>http://faturl.com/bengalexpinlg</url> and the code at <url>https://github.com/dice-group/BENGAL</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6543 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6543/>Handling Rare Items in Data-to-Text Generation</a></strong><br><a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6543><div class="card-body p-3 small">Neural approaches to data-to-text generation generally handle rare input items using either delexicalisation or a copy mechanism. We investigate the relative impact of these two methods on two datasets (E2E and WebNLG) and using two evaluation settings. We show (i) that rare items strongly impact performance ; (ii) that combining delexicalisation and <a href=https://en.wikipedia.org/wiki/Copying>copying</a> yields the strongest improvement ; (iii) that <a href=https://en.wikipedia.org/wiki/Copying>copying</a> underperforms for rare and unseen items and (iv) that the impact of these two mechanisms greatly varies depending on how the dataset is constructed and on how it is split into train, dev and test.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6545/>Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization : A Pilot Study</a></strong><br><a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6545><div class="card-body p-3 small">Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a>. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6546/>Toward Bayesian Synchronous Tree Substitution Grammars for Sentence Planning<span class=acl-fixed-case>B</span>ayesian Synchronous Tree Substitution Grammars for Sentence Planning</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6546><div class="card-body p-3 small">Developing conventional natural language generation systems requires extensive attention from human experts in order to craft complex sets of sentence planning rules. We propose a Bayesian nonparametric approach to learn sentence planning rules by inducing synchronous tree substitution grammars for pairs of text plans and morphosyntactically-specified dependency trees. Our system is able to learn rules which can be used to generate novel texts after training on small datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6547/>The Task Matters : Comparing Image Captioning and Task-Based Dialogical Image Description</a></strong><br><a href=/people/n/nikolai-ilinykh/>Nikolai Ilinykh</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6547><div class="card-body p-3 small">Image captioning models are typically trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> that is collected from people who are asked to describe an image, without being given any further task context. As we argue here, this context independence is likely to cause problems for transferring to task settings in which image description is bound by task demands. We demonstrate that careful design of <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> is required to obtain image descriptions which are contextually bounded to a particular meta-level task. As a task, we use MeetUp !, a text-based communication game where two players have the goal of finding each other in a visual environment. To reach this goal, the players need to describe images representing their current location. We analyse a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from this <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>domain</a> and show that the nature of image descriptions found in MeetUp ! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6548 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6548/>Generating Summaries of Sets of Consumer Products : Learning from Experiments</a></strong><br><a href=/people/k/kittipitch-kuptavanich/>Kittipitch Kuptavanich</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/k/kees-van-deemter/>Kees Van Deemter</a>
|
<a href=/people/a/advaith-siddharthan/>Advaith Siddharthan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6548><div class="card-body p-3 small">We explored the task of creating a textual summary describing a large set of objects characterised by a small number of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> using an e-commerce dataset. When a set of consumer products is large and varied, it can be difficult for a consumer to understand how the products in the set differ ; consequently, it can be challenging to choose the most suitable product from the set. To assist consumers, we generated high-level summaries of product sets. Two generation algorithms are presented, discussed, and evaluated with human users. Our evaluation results suggest a positive contribution to consumers&#8217; understanding of the domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6549 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6549/>Neural sentence generation from <a href=https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)>formal semantics</a></a></strong><br><a href=/people/k/kana-manome/>Kana Manome</a>
|
<a href=/people/m/masashi-yoshikawa/>Masashi Yoshikawa</a>
|
<a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/p/pascual-martinez-gomez/>Pascual Martínez-Gómez</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6549><div class="card-body p-3 small">Sequence-to-sequence models have shown strong performance in a wide range of NLP tasks, yet their applications to sentence generation from logical representations are underdeveloped. In this paper, we present a sequence-to-sequence model for generating sentences from logical meaning representations based on event semantics. We use a semantic parsing system based on Combinatory Categorial Grammar (CCG) to obtain data annotated with logical formulas. We augment our sequence-to-sequence model with masking for <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a> to constrain output sentences. We also propose a novel evaluation method for generation using Recognizing Textual Entailment (RTE). Combining <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and generation, we test whether or not the output sentence entails the original text and vice versa. Experiments showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperformed a baseline with respect to both BLEU scores and accuracies in RTE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6550 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6550" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6550/>Talking about other people : an endless range of possibilities</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6550><div class="card-body p-3 small">Image description datasets, such as Flickr30 K and MS COCO, show a high degree of variation in the ways that crowd-workers talk about the world. Although this gives us a rich and diverse collection of data to work with, it also introduces uncertainty about how the world should be described. This paper shows the extent of this <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> in the PEOPLE-domain. We present a <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> of different ways to talk about other people. This <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a> serves as a reference point to think about how other people should be described, and can be used to classify and compute statistics about labels applied to people.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6551 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6551/>Meteorologists and Students : A resource for language grounding of geographical descriptors</a></strong><br><a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/j/jose-m-alonso/>Jose Alonso</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6551><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_(computing)>data resource</a> which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as <a href=https://en.wikipedia.org/wiki/Polygon_(computer_graphics)>polygons</a> on a map by two groups of human subjects : teenage students and expert meteorologists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6553 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6553/>Neural Transition-based Syntactic Linearization</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6553><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a> is to find a grammatical order given a set of words. Traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> use <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. Syntactic linearization systems, which generate a sentence along with its <a href=https://en.wikipedia.org/wiki/Syntactic_tree>syntactic tree</a>, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6557 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6557/>E2E NLG Challenge : Neural Models vs. Templates<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge: Neural Models vs. Templates</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6557><div class="card-body p-3 small">E2E NLG Challenge is a shared task on generating restaurant descriptions from sets of key-value pairs. This paper describes the results of our participation in the challenge. We develop a simple, yet effective neural encoder-decoder model which produces fluent restaurant descriptions and outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. We further analyze the data provided by the organizers and conclude that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can also be approached with a template-based model developed in just a few hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6558 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6558/>The E2E NLG Challenge : A Tale of Two Systems<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge: A Tale of Two Systems</a></strong><br><a href=/people/c/charese-smiley/>Charese Smiley</a>
|
<a href=/people/e/elnaz-davoodi/>Elnaz Davoodi</a>
|
<a href=/people/d/dezhao-song/>Dezhao Song</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6558><div class="card-body p-3 small">This paper presents the two systems we entered into the 2017 E2E NLG Challenge : TemplGen, a templated-based system and SeqGen, a neural network-based system. Through the automatic evaluation, SeqGen achieved competitive results compared to the template-based approach and to other participating <a href=https://en.wikipedia.org/wiki/System>systems</a> as well. In addition to the automatic evaluation, in this paper we present and discuss the human evaluation results of our two <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6560 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6560/>Multi-Language Surface Realisation as REST API based NLG Microservice<span class=acl-fixed-case>REST</span> <span class=acl-fixed-case>API</span> based <span class=acl-fixed-case>NLG</span> Microservice</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/j/johanna-heininger/>Johanna Heininger</a>
|
<a href=/people/n/nyamsuren-davaasambuu/>Nyamsuren Davaasambuu</a>
|
<a href=/people/v/vitaliia-voronik/>Vitaliia Voronik</a>
|
<a href=/people/m/michael-kaufl/>Michael Käufl</a>
|
<a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6560><div class="card-body p-3 small">We present a readily available <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> that solves the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology component</a> for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>surface realizers</a> in 10 languages (e.g., English, German and Finnish) for any topic and is available as <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST API</a>. This can be used to add <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> to any kind of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLG application</a> (e.g., a multi-language chatbot), without requiring <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistic knowledge</a> by the integrator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6561 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6561/>Statistical NLG for Generating the Content and Form of Referring Expressions<span class=acl-fixed-case>NLG</span> for Generating the Content and Form of Referring Expressions</a></strong><br><a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6561><div class="card-body p-3 small">This paper argues that a new generic approach to statistical NLG can be made to perform Referring Expression Generation (REG) successfully. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> does not only select attributes and values for referring to a target referent, but also performs <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Realisation</a>, generating an actual <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrase</a>. Our evaluations suggest that the attribute selection aspect of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> exceeds classic REG algorithms, while the <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrases</a> generated are as similar to those in a previously developed corpus as were <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrases</a> produced by a new set of human speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6562 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6562/>Specificity measures and reference</a></strong><br><a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/n/nicolas-marin/>Nicolás Marín</a>
|
<a href=/people/g/gustavo-rivas-gervilla/>Gustavo Rivas-Gervilla</a>
|
<a href=/people/d/daniel-sanchez-cisneros/>Daniel Sánchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6562><div class="card-body p-3 small">In this paper we study empirically the validity of measures of referential success for referring expressions involving <a href=https://en.wikipedia.org/wiki/Gradualism>gradual properties</a>. More specifically, we study the ability of several measures of referential success to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>human accuracy</a> in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expression</a> produced by a generation algorithm, especially in case the properties in a domain can not be assumed to have crisp denotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6563 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6563/>Decoding Strategies for Neural Referring Expression Generation</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6563><div class="card-body p-3 small">RNN-based sequence generation is now widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and NLG (natural language generation). Most work focusses on how to train RNNs, even though also decoding is not necessarily straightforward : previous work on neural MT found seq2seq models to radically prefer short candidates, and has proposed a number of beam search heuristics to deal with this. In this work, we assess decoding strategies for referring expression generation with neural models. Here, expression length is crucial : output should neither contain too much or too little information, in order to be pragmatically adequate. We find that most beam search heuristics developed for MT do not generalize well to referring expression generation (REG), and do not generally outperform greedy decoding. We observe that beam search heuristics for termination seem to override the model&#8217;s knowledge of what a good stopping point is. Therefore, we also explore a recent approach called trainable decoding, which uses a small network to modify the RNN&#8217;s hidden state for better <a href=https://en.wikipedia.org/wiki/Decoding_methods>decoding</a> results. We find this approach to consistently outperform greedy decoding for REG.</div></div></div><hr><div id=w18-66><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-66.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-66/>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6600/>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (<span class=acl-fixed-case>CC</span>-<span class=acl-fixed-case>NLG</span> 2018)</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a>
|
<a href=/people/b/ben-burtenshaw/>Ben Burtenshaw</a>
|
<a href=/people/r/raquel-hervas/>Raquel Hervás</a></span></p></div><hr><div id=w18-67><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-67.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-67/>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&NLG)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6700/>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2<span class=acl-fixed-case>IS</span>&<span class=acl-fixed-case>NLG</span>)</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alejandro-catala/>Alejandro Catala</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p></div><hr><div id=w18-69><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-69.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-69/>Proceedings of the Workshop on NLG for Human–Robot Interaction</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6900/>Proceedings of the Workshop on <span class=acl-fixed-case>NLG</span> for Human–Robot Interaction</a></strong><br><a href=/people/m/mary-ellen-foster/>Mary Ellen Foster</a>
|
<a href=/people/h/hendrik-buschmeier/>Hendrik Buschmeier</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6906/>Being data-driven is not enough : Revisiting interactive instruction giving as a challenge for NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6906><div class="card-body p-3 small">Modeling traditional NLG tasks with data-driven techniques has been a major focus of research in NLG in the past decade. We argue that existing modeling techniques are mostly tailored to textual data and are not sufficient to make NLG technology meet the requirements of <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> which target fluid interaction and collaboration in the real world. We revisit interactive instruction giving as a challenge for datadriven NLG and, based on insights from previous GIVE challenges, propose that instruction giving should be addressed in a setting that involves visual grounding and <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>. These basic design decisions will require NLG frameworks that are capable of monitoring their environment as well as timing and revising their verbal output. We believe that these are core capabilities for making NLG technology transferrable to interactive systems.</div></div></div><hr><div id=w18-70><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-70.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-70/>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-7000/>Proceedings of the 1st Workshop on Automatic Text Adaptation (<span class=acl-fixed-case>ATA</span>)</a></strong><br><a href=/people/a/arne-jonsson/>Arne Jönsson</a>
|
<a href=/people/e/evelina-rennes/>Evelina Rennes</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a>
|
<a href=/people/s/sanja-stajner/>Sanja Stajner</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a></span></p></div><hr><div id=w18-71><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-71.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-71/>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-7100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-7100/>Proceedings of the 7th workshop on <span class=acl-fixed-case>NLP</span> for Computer Assisted Language Learning</a></strong><br><a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a>
|
<a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a></span></p></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>