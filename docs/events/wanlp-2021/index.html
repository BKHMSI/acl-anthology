<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Arabic Natural Language Processing (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Arabic Natural Language Processing (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021wanlp-1>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">23&nbsp;papers</span></li></ul></div></div><div id=2021wanlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.wanlp-1/>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.0/>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/s/samia-touileb/>Samia Touileb</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.3/>Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection<span class=acl-fixed-case>A</span>rabic Sentiment and Sarcasm Detection</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--3><div class="card-body p-3 small">The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> were initially developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we evaluate the performance of 24 of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Arabic sentiment</a> and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.5/>Kawarith : an Arabic Twitter Corpus for Crisis Events<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>T</span>witter Corpus for Crisis Events</a></strong><br><a href=/people/a/alaa-alharbi/>Alaa Alharbi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--5><div class="card-body p-3 small">Social media (SM) platforms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> provide large quantities of <a href=https://en.wikipedia.org/wiki/Real-time_data>real-time data</a> that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.9/>ArCOV-19 : The First Arabic COVID-19 Twitter Dataset with Propagation Networks<span class=acl-fixed-case>A</span>r<span class=acl-fixed-case>COV</span>-19: The First <span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>T</span>witter Dataset with Propagation Networks</a></strong><br><a href=/people/f/fatima-haouari/>Fatima Haouari</a>
|
<a href=/people/m/maram-hasanain/>Maram Hasanain</a>
|
<a href=/people/r/reem-suwaileh/>Reem Suwaileh</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--9><div class="card-body p-3 small">In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7 M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand <a href=https://en.wikipedia.org/wiki/Conversation>conversational threads</a> (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, and <a href=https://en.wikipedia.org/wiki/Social_computing>social computing</a>. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the <a href=https://en.wikipedia.org/wiki/Arab_world>Arab world</a>. In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.18/>ALUE : Arabic Language Understanding Evaluation<span class=acl-fixed-case>ALUE</span>: <span class=acl-fixed-case>A</span>rabic Language Understanding Evaluation</a></strong><br><a href=/people/h/haitham-seelawi/>Haitham Seelawi</a>
|
<a href=/people/i/ibraheem-tuffaha/>Ibraheem Tuffaha</a>
|
<a href=/people/m/mahmoud-gzawi/>Mahmoud Gzawi</a>
|
<a href=/people/w/wael-farhan/>Wael Farhan</a>
|
<a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/r/riham-badawi/>Riham Badawi</a>
|
<a href=/people/z/zyad-sober/>Zyad Sober</a>
|
<a href=/people/o/oday-al-dweik/>Oday Al-Dweik</a>
|
<a href=/people/a/abed-alhakim-freihat/>Abed Alhakim Freihat</a>
|
<a href=/people/h/hussein-al-natsheh/>Hussein Al-Natsheh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--18><div class="card-body p-3 small">The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels. Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community, we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online, and publicly accessible leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.19/>Quranic Verses Semantic Relatedness Using AraBERT<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/abdullah-alsaleh/>Abdullah Alsaleh</a>
|
<a href=/people/e/eric-atwell/>Eric Atwell</a>
|
<a href=/people/a/abdulrahman-altahhan/>Abdulrahman Altahhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--19><div class="card-body p-3 small">Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92 % accuracy score using a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprised of label &#8216;2&#8217; and label&#8217; -1&#8217;, the latter was generated outside of QurSim dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.20/>AraELECTRA : Pre-Training Text Discriminators for Arabic Language Understanding<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>ELECTRA</span>: Pre-Training Text Discriminators for <span class=acl-fixed-case>A</span>rabic Language Understanding</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--20><div class="card-body p-3 small">Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a> and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.21/>AraGPT2 : Pre-Trained Transformer for Arabic Language Generation<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>GPT</span>2: Pre-Trained Transformer for <span class=acl-fixed-case>A</span>rabic Language Generation</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--21><div class="card-body p-3 small">Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> of 29.8 on <a href=https://en.wikipedia.org/wiki/Wikipedia>held-out Wikipedia articles</a>. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98 % percent <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in detecting model-generated text. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.24/>SERAG : Semantic Entity Retrieval from Arabic Knowledge Graphs<span class=acl-fixed-case>SERAG</span>: Semantic Entity Retrieval from <span class=acl-fixed-case>A</span>rabic Knowledge Graphs</a></strong><br><a href=/people/s/saher-esmeir/>Saher Esmeir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--24><div class="card-body p-3 small">Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity retrieval</a> from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for entity retrieval have shown a significant improvement over traditional methods. These <a href=https://en.wikipedia.org/wiki/Physical_model>models</a>, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses <a href=https://en.wikipedia.org/wiki/Random_walk>random walks</a> to generate <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity embeddings</a>. DBpedia-Entity v2 is considered the standard test collection for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity retrieval</a>. We discuss the challenges of using <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> for non-English languages in general and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in particular. We provide an Arabic version of this standard <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a>, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.25/>Introducing A large Tunisian Arabizi Dialectal Dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>T</span>unisian <span class=acl-fixed-case>A</span>rabizi Dialectal Dataset for Sentiment Analysis</a></strong><br><a href=/people/c/chayma-fourati/>Chayma Fourati</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/a/abir-messaoudi/>Abir Messaoudi</a>
|
<a href=/people/m/moez-benhajhmida/>Moez BenHajhmida</a>
|
<a href=/people/a/aymen-ben-elhaj-mabrouk/>Aymen Ben Elhaj Mabrouk</a>
|
<a href=/people/m/malek-naski/>Malek Naski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--25><div class="card-body p-3 small">On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments : their local dialects. In <a href=https://en.wikipedia.org/wiki/Africa>Africa</a>, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a> and numbers rather than <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic ones</a>. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of a total of 100k comments (about <a href=https://en.wikipedia.org/wiki/Film>movies</a>, <a href=https://en.wikipedia.org/wiki/Politics>politic</a>, <a href=https://en.wikipedia.org/wiki/Sport>sport</a>, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.27/>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--27><div class="card-body p-3 small">We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a> to demonstrate the effectiveness of the proposed method for CEAE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.31/>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification<span class=acl-fixed-case>BERT</span>-based Multi-Task Model for Country and Province Level <span class=acl-fixed-case>MSA</span> and Dialectal <span class=acl-fixed-case>A</span>rabic Identification</a></strong><br><a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/a/abdelkader-el-mahdaouy/>Abdelkader El Mahdaouy</a>
|
<a href=/people/k/kabil-essefar/>Kabil Essefar</a>
|
<a href=/people/n/nabil-el-mamoun/>Nabil El Mamoun</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a>
|
<a href=/people/a/ahmed-khoumsi/>Ahmed Khoumsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--31><div class="card-body p-3 small">Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA / DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA / DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.32/>Country-level Arabic Dialect Identification using RNNs with and without <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a><span class=acl-fixed-case>A</span>rabic Dialect Identification using <span class=acl-fixed-case>RNN</span>s with and without Linguistic Features</a></strong><br><a href=/people/e/elsayed-issa/>Elsayed Issa</a>
|
<a href=/people/m/mohammed-alshakhori1/>Mohammed AlShakhori1</a>
|
<a href=/people/r/reda-al-bahrani/>Reda Al-Bahrani</a>
|
<a href=/people/g/gus-hahn-powell/>Gus Hahn-Powell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--32><div class="card-body p-3 small">This work investigates the value of augmenting <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> with <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2 : Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.33/>Arabic Dialect Identification based on a Weighted Concatenation of TF-IDF Features<span class=acl-fixed-case>A</span>rabic Dialect Identification based on a Weighted Concatenation of <span class=acl-fixed-case>TF</span>-<span class=acl-fixed-case>IDF</span> Features</a></strong><br><a href=/people/m/mohamed-lichouri/>Mohamed Lichouri</a>
|
<a href=/people/m/mourad-abbas/>Mourad Abbas</a>
|
<a href=/people/k/khaled-lounnas/>Khaled Lounnas</a>
|
<a href=/people/b/besma-benaziz/>Besma Benaziz</a>
|
<a href=/people/a/aicha-zitouni/>Aicha Zitouni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--33><div class="card-body p-3 small">In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two <a href=https://en.wikipedia.org/wiki/Design_of_experiments>subtasks</a> : subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), <a href=https://en.wikipedia.org/wiki/Perceptron>Perceptron</a>, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our <a href=https://en.wikipedia.org/wiki/System>system</a> gives F1 scores of 14.87 % and 21.49 %, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> and recorded for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a> (18.70 % and 24.23 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.34/>Machine Learning-Based Approach for Arabic Dialect Identification<span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/h/hamada-nayel/>Hamada Nayel</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan</a>
|
<a href=/people/m/mahmoud-sobhi/>Mahmoud Sobhi</a>
|
<a href=/people/a/ahmed-el-sawy/>Ahmed El-Sawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--34><div class="card-body p-3 small">This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four <a href=https://en.wikipedia.org/wiki/Russian_language>subtasks</a>, two subtasks for country-level identification and the other two <a href=https://en.wikipedia.org/wiki/Russian_language>subtasks</a> for province-level identification. The data in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> covers a total of 100 provinces from all 21 Arab countries and come from the <a href=https://en.wikipedia.org/wiki/Twitter>Twitter domain</a>. The proposed systems depend on five machine-learning approaches namely Complement Nave Bayes, <a href=https://en.wikipedia.org/wiki/Support-vector_machine>Support Vector Machine</a>, <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Tree</a>, <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a> and Random Forest Classifiers. F1 macro-averaged score of Nave Bayes classifier outperformed all other <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> for development and test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.36/>Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic<span class=acl-fixed-case>WANLP</span> 2021 Shared Task on Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--36><div class="card-body p-3 small">This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. The shared task has two subtasks : sarcasm detection (subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> and <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a>. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> and 0.748 <a href=https://en.wikipedia.org/wiki/F-number>F1-PN</a> respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.38/>Sarcasm and Sentiment Detection In Arabic Tweets Using BERT-based Models and Data Augmentation<span class=acl-fixed-case>A</span>rabic Tweets Using <span class=acl-fixed-case>BERT</span>-based Models and Data Augmentation</a></strong><br><a href=/people/a/abeer-abuzayed/>Abeer Abuzayed</a>
|
<a href=/people/h/hend-al-khalifa/>Hend Al-Khalifa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--38><div class="card-body p-3 small">In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Abu Farha et al., 2021). The shared <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> consists of two sub-tasks : Sarcasm Detection (Subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> outperformed other models with an increase of the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> by 15 % for both tasks which shows the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.39/>Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for Arabic Sarcasm Detection and Sentiment Analysis<span class=acl-fixed-case>A</span>rabic Sarcasm Detection and Sentiment Analysis</a></strong><br><a href=/people/a/abdullah-i-alharbi/>Abdullah I. Alharbi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--39><div class="card-body p-3 small">Sarcasm detection and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> are important tasks in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a>. Sarcasm is a type of <a href=https://en.wikipedia.org/wiki/Emotional_expression>expression</a> where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wanlp-1.41.Dataset.pdf data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wanlp-1.41.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.41/>Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> : investigating the interest of character-level features<span class=acl-fixed-case>A</span>rabic: investigating the interest of character-level features</a></strong><br><a href=/people/d/dhaou-ghoul/>Dhaou Ghoul</a>
|
<a href=/people/g/gael-lejeune/>GaÃ«l Lejeune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--41><div class="card-body p-3 small">We present three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> developed for the Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We present a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that uses character n-gram features. We also propose two more sophisticated methods : a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with a word level representation and an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifier</a> relying on word and character-level features. We chose to present results from an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifier</a> but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> could have been improved and beat those results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.44/>SarcasmDet at Sarcasm Detection Task 2021 in Arabic using AraBERT Pretrained Model<span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>D</span>et at Sarcasm Detection Task 2021 in <span class=acl-fixed-case>A</span>rabic using <span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span> Pretrained Model</a></strong><br><a href=/people/d/dalya-faraj/>Dalya Faraj</a>
|
<a href=/people/d/dalya-faraj/>Dalya Faraj</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--44><div class="card-body p-3 small">This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Subtask-1 Sarcasm Detection). The goal of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to identify whether a tweet is sarcastic or not. Our solution has been developed using <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble technique</a> with AraBERT pre-trained model. We describe the architecture of the submitted <a href=https://en.wikipedia.org/wiki/Solution>solution</a> in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that we trained or tested to achieve a better score in a table design. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.7830</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.45/>Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> A Hybrid Approach Combining Embeddings and Rule-based Features<span class=acl-fixed-case>A</span>rabic language A Hybrid Approach Combining Embeddings and Rule-based Features</a></strong><br><a href=/people/k/kamel-gaanoun/>Kamel Gaanoun</a>
|
<a href=/people/i/imade-benelallam/>Imade Benelallam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--45><div class="card-body p-3 small">This paper presents the ArabicProcessors team&#8217;s system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using <a href=https://en.wikipedia.org/wiki/Linear_transform>transformers</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The system&#8217;s architecture is an ensemble of <a href=https://en.wikipedia.org/wiki/Naive_bayes>Naive bayes</a>, MarBERT and Mazajak embedding. This process scored an F1-score of 51 % on <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and 71 % for sentiment detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.50/>iCompass at Shared Task on <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> and Sentiment Detection in Arabic<span class=acl-fixed-case>C</span>ompass at Shared Task on Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/malek-naski/>Malek Naski</a>
|
<a href=/people/a/abir-messaoudi/>Abir Messaoudi</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/m/moez-benhajhmida/>Moez BenHajhmida</a>
|
<a href=/people/c/chayma-fourati/>Chayma Fourati</a>
|
<a href=/people/a/aymen-ben-elhaj-mabrouk/>Aymen Ben Elhaj Mabrouk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--50><div class="card-body p-3 small">We describe our submitted <a href=https://en.wikipedia.org/wiki/System>system</a> to the 2021 Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google&#8217;s multilingual BERT and then other Arabic variants : AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.53/>AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in Arabic Tweets<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span> and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic Tweets</a></strong><br><a href=/people/a/anshul-wadhawan/>Anshul Wadhawan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--53><div class="card-body p-3 small">This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2 : Sarcasm and Sentiment Detection. One of the subtasks aims at developing a <a href=https://en.wikipedia.org/wiki/System>system</a> that identifies whether a given <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweet</a> is sarcastic in nature or not, while the other aims to identify the sentiment of the <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweet</a>. We approach the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in two steps. The first step involves pre processing the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>