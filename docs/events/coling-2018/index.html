<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Conference on Computational Linguistics (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Conference on Computational Linguistics (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#c18-1>Proceedings of the 27th International Conference on Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">201&nbsp;papers</span></li><li><a class=align-middle href=#c18-2>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#c18-3>Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-38>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w18-39>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w18-40>Proceedings of the Third Workshop on Semantic Deep Learning</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-41>Proceedings of the First International Workshop on Language Cognition and Computational Models</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-42>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-43>Proceedings of the Workshop Events and Stories in the News 2018</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-44>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w18-45>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-47>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-48>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-49>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li></ul></div></div><div id=c18-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/C18-1/>Proceedings of the 27th International Conference on Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1000/>Proceedings of the 27th International Conference on Computational Linguistics</a></strong><br><a href=/people/e/emily-m-bender/>Emily M. Bender</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/p/pierre-isabelle/>Pierre Isabelle</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1002/>Zero Pronoun Resolution with Attention-based Neural Network</a></strong><br><a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1002><div class="card-body p-3 small">Recent neural network methods for zero pronoun resolution explore multiple models for generating <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vectors</a> for zero pronouns and their candidate antecedents. Typically, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> is utilized to encode the zero pronouns since they are simply gaps that contain no actual content. To better utilize contexts of the zero pronouns, we here introduce the self-attention mechanism for encoding zero pronouns. With the help of the multiple hops of attention, our model is able to focus on some informative parts of the associated texts and therefore produces an efficient way of encoding the zero pronouns. In addition, an attention-based recurrent neural network is proposed for encoding candidate antecedents by their contents. Experiment results are encouraging : our proposed attention-based model gains the best performance on the Chinese portion of the OntoNotes corpus, substantially surpasses existing Chinese zero pronoun resolution baseline systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1003/>They Exist ! Introducing Plural Mentions to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> and Entity Linking</a></strong><br><a href=/people/e/ethan-zhou/>Ethan Zhou</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1003><div class="card-body p-3 small">This paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, that is the resolution of plural mentions. Unlike <a href=https://en.wikipedia.org/wiki/Grammatical_number>singular mentions</a> each of which represents one entity, <a href=https://en.wikipedia.org/wiki/Grammatical_number>plural mentions</a> stand for multiple entities. To tackle this aspect, we take the character identification corpus from the SemEval 2018 shared task that consists of entity annotation for singular mentions, and expand it by adding <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for plural mentions. We then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions, and also a deep learning-based entity linking model that jointly handles both types of mentions through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Adjusted evaluation metrics are proposed for these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> as well to handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> designed only for singular mentions. To the best of our knowledge, this is the first time that <a href=https://en.wikipedia.org/wiki/Plural>plural mentions</a> are thoroughly analyzed for these two resolution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1006/>Challenges of <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> for the indigenous languages of the Americas<span class=acl-fixed-case>A</span>mericas</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/x/ximena-gutierrez-vasques/>Ximena Gutierrez-Vasques</a>
|
<a href=/people/g/gerardo-sierra/>Gerardo Sierra</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Meza-Ruiz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1006><div class="card-body p-3 small">Indigenous languages of the American continent are highly diverse. However, they have received little attention from the technological perspective. In this paper, we review the research, the <a href=https://en.wikipedia.org/wiki/Digital_data>digital resources</a> and the available <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> that focus on these <a href=https://en.wikipedia.org/wiki/Natural_language_processing>languages</a>. We present the main challenges and research questions that arise when distant languages and low-resource scenarios are faced. We would like to encourage <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a> in linguistically rich and diverse areas like the Americas.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1008/>Neural Transition-based String Transduction for Limited-Resource Setting in Morphology</a></strong><br><a href=/people/p/peter-makarov/>Peter Makarov</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1008><div class="card-body p-3 small">We present a neural transition-based model that uses a simple set of edit actions (copy, delete, insert) for morphological transduction tasks such as <a href=https://en.wikipedia.org/wiki/Inflection>inflection generation</a>, <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>, and reinflection. In a large-scale evaluation on four datasets and dozens of languages, our approach consistently outperforms state-of-the-art systems on low and medium training-set sizes and is competitive in the high-resource setting. Learning to apply a generic copy action enables our approach to generalize quickly from a few data points. We successfully leverage minimum risk training to compensate for the weaknesses of MLE parameter learning and neutralize the negative effects of training a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> with a separate character aligner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1009/>Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/y/yuichiroh-matsubayashi/>Yuichiroh Matsubayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1009><div class="card-body p-3 small">Capturing interactions among multiple predicate-argument structures (PASs) is a crucial issue in the task of analyzing PAS in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. In this paper, we propose new Japanese PAS analysis models that integrate the label prediction information of arguments in multiple PASs by extending the input and last layers of a standard deep bidirectional recurrent neural network (bi-RNN) model. In these models, using the mechanisms of pooling and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, we aim to directly capture the potential interactions among multiple PASs, without being disturbed by the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> and <a href=https://en.wikipedia.org/wiki/Distance>distance</a>. Our experiments show that the proposed models improve the prediction accuracy specifically for cases where the predicate and argument are in an indirect dependency relation and achieve a new state of the art in the overall <a href=https://en.wikipedia.org/wiki/F-number>F_1</a> on a standard benchmark corpus.<tex-math>F_1</tex-math> on a standard benchmark corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1010/>Sprucing up the trees Error detection in treebanks</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1010><div class="card-body p-3 small">We present a method for detecting annotation errors in manually and automatically annotated dependency parse trees, based on ensemble parsing in combination with <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a>, guided by <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We evaluate our method in different scenarios : (i) for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in dependency treebanks and (ii) for improving parsing accuracy on in- and out-of-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1011/>Two Local Models for Neural Constituent Parsing</a></strong><br><a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1011><div class="card-body p-3 small">Non-local features have been exploited by syntactic parsers for capturing dependencies between sub output structures. Such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> have been a key to the success of state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>statistical parsers</a>. With the rise of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, however, it has been shown that local output decisions can give highly competitive accuracies, thanks to the power of dense neural input representations that embody global syntactic information. We investigate two conceptually simple local neural models for constituent parsing, which make local decisions to constituent spans and CFG rules, respectively. Consistent with previous findings along the line, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gives highly competitive results, achieving the labeled bracketing F1 scores of 92.4 % on <a href=https://en.wikipedia.org/wiki/Test_score>PTB</a> and 87.3 % on <a href=https://en.wikipedia.org/wiki/Test_score>CTB</a> 5.1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1012/>RNN Simulations of Grammaticality Judgments on Long-distance Dependencies<span class=acl-fixed-case>RNN</span> Simulations of Grammaticality Judgments on Long-distance Dependencies</a></strong><br><a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a>
|
<a href=/people/r/roberto-zamparelli/>Roberto Zamparelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1012><div class="card-body p-3 small">The paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism. The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, at least in the domain we tested.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1013/>How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level</a></strong><br><a href=/people/v/vladimir-eidelman/>Vladimir Eidelman</a>
|
<a href=/people/a/anastassia-kornilova/>Anastassia Kornilova</a>
|
<a href=/people/d/daniel-argyle/>Daniel Argyle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1013><div class="card-body p-3 small">Modeling <a href=https://en.wikipedia.org/wiki/United_States_Congress>U.S. Congressional legislation</a> and <a href=https://en.wikipedia.org/wiki/Voting_methods_in_deliberative_assemblies>roll-call votes</a> has received significant attention in previous literature, and while legislators across 50 state governments and D.C. propose over 100,000 bills each year, enacting over 30 % of them on average, state level analysis has received relatively less attention due in part to the difficulty in obtaining the necessary data. Since each state legislature is guided by their own procedures, politics and issues, however, it is difficult to qualitatively asses the factors that affect the likelihood of a legislative initiative succeeding. We present several methods for modeling the likelihood of a bill receiving floor action across all 50 states and D.C. We utilize the lexical content of over 1 million bills, along with contextual legislature and legislator derived features to build our predictive models, allowing a comparison of what factors are important to the lawmaking process. Furthermore, we show that these <a href=https://en.wikipedia.org/wiki/Signal>signals</a> hold complementary predictive power, together achieving an average improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 18 % over state specific baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1015/>Incorporating <a href=https://en.wikipedia.org/wiki/Image_matching>Image Matching</a> Into <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>Knowledge Acquisition</a> for Event-Oriented Relation Recognition</a></strong><br><a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/h/huibin-ruan/>Huibin Ruan</a>
|
<a href=/people/b/bowei-zou/>Bowei Zou</a>
|
<a href=/people/j/jianmin-yao/>Jianmin Yao</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1015><div class="card-body p-3 small">Event relation recognition is a challenging <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing task</a>. It is required to determine the relation class of a pair of query events, such as <a href=https://en.wikipedia.org/wiki/Causality>causality</a>, under the condition that there is n&#8217;t any reliable clue for use. We follow the traditional statistical approach in this paper, speculating the relation class of the target events based on the relation-class distributions on the similar events. There is minimal <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> used during the <a href=https://en.wikipedia.org/wiki/Speculation>speculation process</a>. In particular, we incorporate <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a> into the acquisition of similar event instances, including the utilization of images for visually representing event scenes, and the use of the neural network based image matching for approximate calculation between events. We test our method on the ACE-R2 corpus and compared our model with the fully-supervised neural network models. Experimental results show that we achieve a comparable performance to CNN while slightly better than LSTM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1018/>Neural Math Word Problem Solver with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/d/danqing-huang/>Danqing Huang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1018><div class="card-body p-3 small">Sequence-to-sequence model has been applied to solve <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>math word problems</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes math problem descriptions as input and generates equations as output. The advantage of sequence-to-sequence model requires no <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and can generate equations that do not exist in training data. However, our experimental analysis reveals that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> suffers from two shortcomings : (1) generate spurious numbers ; (2) generate numbers at wrong positions. In this paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings. To train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to directly optimize the solution accuracy. It overcomes the train-test discrepancy issue of <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a>, which uses the surrogate objective of maximizing equation likelihood during training while the evaluation metric is solution accuracy (non-differentiable) at test time. Furthermore, to explore the effectiveness of our neural model, we use our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> output as a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> and incorporate it into the feature-based model. Experimental results show that (1) The copy and alignment mechanism is effective to address the two issues ; (2) <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement learning</a> leads to better performance than <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a> on this task ; (3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1021/>Lexi : A tool for adaptive, personalized text simplification<span class=acl-fixed-case>L</span>exi: A tool for adaptive, personalized text simplification</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/a/anders-sogaard/>Anders SÃ¸gaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1021><div class="card-body p-3 small">Most previous research in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> has aimed to develop generic solutions, assuming very homogeneous target audiences with consistent intra-group simplification needs. We argue that this assumption does not hold, and that instead we need to develop simplification systems that adapt to the individual needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a <a href=https://en.wikipedia.org/wiki/Browser_extension>browser extension</a>, enabling easy access to the service for its users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1022/>Identifying Emergent Research Trends by Key Authors and Phrases</a></strong><br><a href=/people/s/shenhao-jiang/>Shenhao Jiang</a>
|
<a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/k/kazunari-sugiyama/>Kazunari Sugiyama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1022><div class="card-body p-3 small">Identifying emergent research trends is a key issue for both primary researchers as well as secondary research managers. Such processes can uncover the historical development of an area, and yield insight on developing topics. We propose an embedded trend detection framework for this task which incorporates our bijunctive hypothesis that important phrases are written by important authors within a field and vice versa. By ranking both author and phrase information in a <a href=https://en.wikipedia.org/wiki/Multigraph>multigraph</a>, our method jointly determines key phrases and authoritative authors. We represent this intermediate output as phrasal embeddings, and feed this to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> to compute trend scores that identify research trends. Over two large datasets of scientific articles, we demonstrate that our approach successfully detects past trends from the field, outperforming baselines based solely on text centrality or <a href=https://en.wikipedia.org/wiki/Citation>citation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1023/>Embedding WordNet Knowledge for Textual Entailment<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Knowledge for Textual Entailment</a></strong><br><a href=/people/y/yunshi-lan/>Yunshi Lan</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1023><div class="card-body p-3 small">In this paper, we study how we can improve a deep learning approach to <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a> by incorporating lexical entailment relations from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. Our idea is to embed the lexical entailment knowledge contained in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> in specially-learned word vectors, which we call entailment vectors. We present a standard neural network model and a novel set-theoretic model to learn these entailment vectors from word pairs with known lexical entailment relations derived from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. We further incorporate these entailment vectors into a decomposable attention model for textual entailment and evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the SICK and the SNLI dataset. We find that using these special entailment word vectors, we can significantly improve the performance of textual entailment compared with a baseline that uses only standard word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1025/>Joint Learning from Labeled and Unlabeled Data for <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a></a></strong><br><a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/p/ping-cheng/>Ping Cheng</a>
|
<a href=/people/l/le-jia/>Le Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1025><div class="card-body p-3 small">Recently, a significant number of studies have focused on neural information retrieval (IR) models. One category of works use unlabeled data to train general word embeddings based on term proximity, which can be integrated into traditional IR models. The other category employs <a href=https://en.wikipedia.org/wiki/Data_(computing)>labeled data</a> (e.g. click-through data) to train end-to-end neural IR models consisting of layers for target-specific representation learning. The latter idea accounts better for the IR task and is favored by recent research works, which is the one we will follow in this paper. We hypothesize that <a href=https://en.wikipedia.org/wiki/General_semantics>general semantics</a> learned from unlabeled data can complement task-specific representation learned from labeled data of limited quality, and that a combination of the two is favorable. To this end, we propose a learning framework which can benefit from both labeled and more abundant unlabeled data for <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> in the context of <a href=https://en.wikipedia.org/wiki/Infrared>IR</a>. Through a joint learning fashion in a single neural framework, the learned representation is optimized to minimize both the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised loss</a> on query-document matching and the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised loss</a> on text reconstruction. Standard <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> experiments on TREC collections indicate that the joint learning methodology leads to significant better performance of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> over several strong baselines for IR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1031/>Enriching Word Embeddings with <a href=https://en.wikipedia.org/wiki/Domain_knowledge>Domain Knowledge</a> for Readability Assessment</a></strong><br><a href=/people/z/zhiwei-jiang/>Zhiwei Jiang</a>
|
<a href=/people/q/qing-gu/>Qing Gu</a>
|
<a href=/people/y/yafeng-yin/>Yafeng Yin</a>
|
<a href=/people/d/daoxu-chen/>Daoxu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1031><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> which learns the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> for readability assessment. For the existing word embedding models, they typically focus on the syntactic or semantic relations of words, while ignoring the reading difficulty, thus they may not be suitable for readability assessment. Hence, we provide the knowledge-enriched word embedding (KEWE), which encodes the knowledge on reading difficulty into the representation of words. Specifically, we extract the knowledge on word-level difficulty from three perspectives to construct a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>, and develop two word embedding models to incorporate the difficulty context derived from the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> to define the <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a>. Experiments are designed to apply KEWE for readability assessment on both English and Chinese datasets, and the results demonstrate both effectiveness and potential of KEWE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1032/>WikiRef : Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>R</span>ef: Wikilinks as a route to recommending appropriate references for scientific <span class=acl-fixed-case>W</span>ikipedia pages</a></strong><br><a href=/people/a/abhik-jana/>Abhik Jana</a>
|
<a href=/people/p/pranjal-kanojiya/>Pranjal Kanojiya</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1032><div class="card-body p-3 small">The exponential increase in the usage of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> as a key source of scientific knowledge among the researchers is making it absolutely necessary to metamorphose this knowledge repository into an integral and self-contained source of information for direct utilization. Unfortunately, the references which support the content of each Wikipedia entity page, are far from complete. Why are the reference section ill-formed for most Wikipedia pages? Is this section edited as frequently as the other sections of a page? Can there be appropriate surrogates that can automatically enhance the reference section? In this paper, we propose a novel two step approach WikiRef that (i) leverages the wikilinks present in a scientific Wikipedia target page and, thereby, (ii) recommends highly relevant references to be included in that target page appropriately and automatically borrowed from the reference section of the wikilinks. In the first step, we build a classifier to ascertain whether a <a href=https://en.wikipedia.org/wiki/Wiki>wikilink</a> is a potential source of reference or not. In the following step, we recommend references to the target page from the reference section of the wikilinks that are classified as potential sources of references in the first step. We perform an extensive evaluation of our approach on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from two different domains Computer Science and <a href=https://en.wikipedia.org/wiki/Physics>Physics</a>. For <a href=https://en.wikipedia.org/wiki/Computer_science>Computer Science</a> we achieve a notably good performance with a precision@1 of 0.44 for reference recommendation as opposed to 0.38 obtained from the most competitive baseline. For the Physics dataset, we obtain a similar performance boost of 10 % with respect to the most competitive baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1033/>Authorship Identification for Literary Book Recommendations</a></strong><br><a href=/people/h/haifa-alharthi/>Haifa Alharthi</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1033><div class="card-body p-3 small">Book recommender systems can help promote the practice of reading for pleasure, which has been declining in recent years. One factor that influences reading preferences is <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. We propose a <a href=https://en.wikipedia.org/wiki/System>system</a> that recommends books after learning their authors&#8217; style. To our knowledge, this is the first work that applies the information learned by an author-identification model to <a href=https://en.wikipedia.org/wiki/Bookselling>book recommendations</a>. We evaluated the <a href=https://en.wikipedia.org/wiki/System>system</a> according to a top-k recommendation scenario. Our <a href=https://en.wikipedia.org/wiki/System>system</a> gives better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> when compared with many state-of-the-art methods. We also conducted a qualitative analysis by checking if similar books / authors were annotated similarly by experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1034/>A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese<span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/s/sidney-evaldo-leal/>Sidney Evaldo Leal</a>
|
<a href=/people/m/magali-sanches-duran/>Magali Sanches Duran</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Maria AluÃ­sio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1034><div class="card-body p-3 small">Effective textual communication depends on readers being proficient enough to comprehend texts, and texts being clear enough to be understood by the intended audience, in a reading task. When the meaning of textual information and instructions is not well conveyed, many losses and damages may occur. Among the solutions to alleviate this problem is the automatic evaluation of sentence readability, <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> which has been receiving a lot of attention due to its large applicability. However, a shortage of resources, such as corpora for training and evaluation, hinders the full development of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we generate a nontrivial sentence corpus in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>. We evaluate three scenarios for building it, taking advantage of a parallel corpus of simplification, in which each sentence triplet is aligned and has simplification operations annotated, being ideal for justifying possible mistakes of future methods. The best scenario of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> PorSimplesSent is composed of 4,888 pairs, which is bigger than a similar corpus for <a href=https://en.wikipedia.org/wiki/English_language>English</a> ; all the three versions of it are publicly available. We created four baselines for PorSimplesSent and made available a pairwise ranking method, using 17 linguistic and psycholinguistic features, which correctly identifies the ranking of sentence pairs with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.2 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1035/>Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference</a></strong><br><a href=/people/q/qianlong-du/>Qianlong Du</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/k/keh-yih-su/>Keh-Yih Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1035><div class="card-body p-3 small">This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets. Most previous DNN-based approaches either ignore syntactic dependency among words, or directly use tree-LSTM to generate sentence representation with irrelevant information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets to improve <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference judgment</a>. To be specific, instead of comparing each <a href=https://en.wikipedia.org/wiki/Tuplet>triplet</a> from one passage with the merged information of another passage, we first propose to perform comparison directly between the <a href=https://en.wikipedia.org/wiki/Tuplet>triplets</a> of the given passage-pair to make the judgement more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1037/>Adversarial Feature Adaptation for Cross-lingual Relation Classification</a></strong><br><a href=/people/b/bowei-zou/>Bowei Zou</a>
|
<a href=/people/z/zengzhuang-xu/>Zengzhuang Xu</a>
|
<a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1037><div class="card-body p-3 small">Relation Classification aims to classify the <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relationship</a> between two marked entities in a given sentence. It plays a vital role in a variety of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. Most existing methods focus on exploiting <a href=https://en.wikipedia.org/wiki/Monolingualism>mono-lingual data</a>, e.g., in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, due to the lack of <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> in other languages. In this paper, we come up with a feature adaptation approach for cross-lingual relation classification, which employs a generative adversarial network (GAN) to transfer feature representations from one language with rich annotated data to another language with scarce annotated data. Such a feature adaptation approach enables feature imitation via the competition between a relation classification network and a rival discriminator. Experimental results on the ACE 2005 multilingual training corpus, treating <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the source language and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> the target, demonstrate the effectiveness of our proposed approach, yielding an improvement of 5.7 % over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1038/>One-shot Learning for <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a> in Gaokao History Challenge<span class=acl-fixed-case>G</span>aokao History Challenge</a></strong><br><a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1038><div class="card-body p-3 small">Answering questions from university admission exams (Gaokao in Chinese) is a challenging AI task since it requires effective <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to capture complicated semantic relations between questions and answers. In this work, we propose a hybrid neural model for deep question-answering task from <a href=https://en.wikipedia.org/wiki/Test_(assessment)>history examinations</a>. Our model employs a cooperative gated neural network to retrieve answers with the assistance of extra labels given by a neural turing machine labeler. Empirical study shows that the <a href=https://en.wikipedia.org/wiki/Labeler>labeler</a> works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> demonstrate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1041/>Few-Shot Charge Prediction with Discriminative Legal Attributes</a></strong><br><a href=/people/z/zikun-hu/>Zikun Hu</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1041><div class="card-body p-3 small">Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and <a href=https://en.wikipedia.org/wiki/Charge_(physics)>charges</a>. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other baselines by more than 50 % in the few-shot scenario. Our codes and datasets can be obtained from https://github.com/thunlp/attribute_charge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1042/>Can <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>Taxonomy</a> Help? Improving Semantic Question Matching using Question Taxonomy</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/r/rajkumar-pujari/>Rajkumar Pujari</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/anutosh-maitra/>Anutosh Maitra</a>
|
<a href=/people/t/tom-jain/>Tom Jain</a>
|
<a href=/people/s/shubhashis-sengupta/>Shubhashis Sengupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1042><div class="card-body p-3 small">In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> is more effective than either <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> or <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy-based knowledge</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1043/>Natural Language Interface for Databases Using a Dual-Encoder Model</a></strong><br><a href=/people/i/ionel-alexandru-hosu/>Ionel Alexandru Hosu</a>
|
<a href=/people/r/radu-cristian-alexandru-iacob/>Radu Cristian Alexandru Iacob</a>
|
<a href=/people/f/florin-brad/>Florin Brad</a>
|
<a href=/people/s/stefan-ruseti/>Stefan Ruseti</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1043><div class="card-body p-3 small">We propose a sketch-based two-step neural model for generating structured queries (SQL) based on a user&#8217;s request in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. The sketch is obtained by using placeholders for specific entities in the SQL query, such as <a href=https://en.wikipedia.org/wiki/Column_(database)>column names</a>, <a href=https://en.wikipedia.org/wiki/Table_(database)>table names</a>, aliases and <a href=https://en.wikipedia.org/wiki/Variable_(computer_science)>variables</a>, in a process similar to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. The first step is to apply a sequence-to-sequence (SEQ2SEQ) model to determine the most probable SQL sketch based on the request in natural language. Then, a second network designed as a dual-encoder SEQ2SEQ model using both the text query and the previously obtained sketch is employed to generate the final SQL query. Our approach shows improvements over previous approaches on two recent large datasets (WikiSQL and SENLIDB) suitable for data-driven solutions for <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interfaces</a> for <a href=https://en.wikipedia.org/wiki/Database>databases</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1045/>Joint Modeling of Structure Identification and Nuclearity Recognition in Macro Chinese Discourse Treebank<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>D</span>iscourse <span class=acl-fixed-case>T</span>reebank</a></strong><br><a href=/people/x/xiaomin-chu/>Xiaomin Chu</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1045><div class="card-body p-3 small">Discourse parsing is a challenging task and plays a critical role in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. This paper focus on the macro level discourse structure analysis, which has been less studied in the previous researches. We explore a macro discourse structure presentation schema to present the macro level discourse structure, and propose a corresponding corpus, named Macro Chinese Discourse Treebank. On these bases, we concentrate on two tasks of macro discourse structure analysis, including structure identification and nuclearity recognition. In order to reduce the error transmission between the associated tasks, we adopt a joint model of the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, and an Integer Linear Programming approach is proposed to achieve <a href=https://en.wikipedia.org/wiki/Global_optimization>global optimization</a> with various kinds of constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1046/>Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning</a></strong><br><a href=/people/f/fengyu-guo/>Fengyu Guo</a>
|
<a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/j/jianwu-dang/>Jianwu Dang</a>
|
<a href=/people/l/longbiao-wang/>Longbiao Wang</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1046><div class="card-body p-3 small">Implicit discourse relation recognition aims to understand and annotate the latent relations between two discourse arguments, such as temporal, comparison, etc. Most previous methods encode two discourse arguments separately, the ones considering pair specific clues ignore the bidirectional interactions between two arguments and the sparsity of pair patterns. In this paper, we propose a novel neural Tensor network framework with Interactive Attention and Sparse Learning (TIASL) for implicit discourse relation recognition. (1) We mine the most correlated word pairs from two discourse arguments to model pair specific clues, and integrate them as interactive attention into argument representations produced by the bidirectional long short-term memory network. Meanwhile, (2) the neural tensor network with sparse constraint is proposed to explore the deeper and the more important pair patterns so as to fully recognize discourse relations. The experimental results on PDTB show that our proposed TIASL framework is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1048/>Deep Enhanced Representation for Implicit Discourse Relation Recognition</a></strong><br><a href=/people/h/hongxiao-bai/>Hongxiao Bai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1048><div class="card-body p-3 small">Implicit discourse relation recognition is a challenging task as the relation prediction without explicit connectives in discourse parsing needs understanding of text spans and can not be easily derived from surface features from the input sentence pairs. Thus, properly representing the text is very crucial to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> augmented with different grained text representations, including character, subword, word, sentence, and sentence pair levels. The proposed deeper model is evaluated on the benchmark treebank and achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with greater than 48 % in 11-way and F1 score greater than 50 % in 4-way classifications for the first time according to our best knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1050/>Modeling Coherence for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Dynamic and Topic Caches</a></strong><br><a href=/people/s/shaohui-kuang/>Shaohui Kuang</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1050><div class="card-body p-3 small">Sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text. Current neural machine translation (NMT) systems translate a text in a conventional sentence-by-sentence fashion, ignoring such cross-sentence links and dependencies. This may lead to generate an incoherent target text for a coherent source text. In order to handle this issue, we propose a cache-based approach to modeling <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> by capturing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> either from recently translated sentences or the entire document. Particularly, we explore two types of caches : a dynamic cache, which stores words from the best translation hypotheses of preceding sentences, and a topic cache, which maintains a set of target-side topical words that are semantically related to the document to be translated. On this basis, we build a new layer to score target words in these two <a href=https://en.wikipedia.org/wiki/Cache_(computing)>caches</a> with a cache-based neural model. Here the estimated probabilities from the cache-based neural model are combined with NMT probabilities into the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1051/>Fusing Recency into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with an Inter-Sentence Gate Model</a></strong><br><a href=/people/s/shaohui-kuang/>Shaohui Kuang</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1051><div class="card-body p-3 small">Neural machine translation (NMT) systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring inter-sentence information. This may make the translation of a sentence ambiguous or even inconsistent with the translations of neighboring sentences. In order to handle this issue, we propose an inter-sentence gate model that uses the same <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to encode two adjacent sentences and controls the amount of information flowing from the preceding sentence to the translation of the current sentence with an inter-sentence gate. In this way, our proposed model can capture the connection between sentences and fuse recency from neighboring sentences into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. On several NIST Chinese-English translation tasks, our experiments demonstrate that the proposed inter-sentence gate model achieves substantial improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1052/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> by Incorporating Hierarchical Subword Features</a></strong><br><a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1052><div class="card-body p-3 small">This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ : (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the <a href=https://en.wikipedia.org/wiki/Subword>subword</a> based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> consistently improves BLEU scores on the IWSLT evaluation datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1053/>Design Challenges in Named Entity Transliteration</a></strong><br><a href=/people/y/yuval-merhav/>Yuval Merhav</a>
|
<a href=/people/s/stephen-ash/>Stephen Ash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1053><div class="card-body p-3 small">We analyze some of the fundamental design challenges that impact the development of a multilingual state-of-the-art named entity transliteration system, including curating bilingual named entity datasets and evaluation of multiple transliteration methods. We empirically evaluate the transliteration task using the traditional weighted finite state transducer (WFST) approach against two neural approaches : the encoder-decoder recurrent neural network method and the recent, non-sequential Transformer method. In order to improve availability of bilingual named entity transliteration datasets, we release personal name bilingual dictionaries mined from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> for English to Russian, <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Katakana>Japanese Katakana</a>. Our code and dictionaries are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1056/>Systematic Study of Long Tail Phenomena in Entity Linking</a></strong><br><a href=/people/f/filip-ilievski/>Filip Ilievski</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/s/stefan-schlobach/>Stefan Schlobach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1056><div class="card-body p-3 small">State-of-the-art entity linkers achieve high accuracy scores with probabilistic methods. However, these <a href=https://en.wikipedia.org/wiki/Score_(statistics)>scores</a> should be considered in relation to the properties of the datasets they are evaluated on. Until now, there has not been a systematic investigation of the properties of entity linking datasets and their impact on system performance. In this paper we report on a series of hypotheses regarding the long tail phenomena in entity linking datasets, their interaction, and their impact on <a href=https://en.wikipedia.org/wiki/System>system</a> performance. Our systematic study of these hypotheses shows that evaluation datasets mainly capture head entities and only incidentally cover data from the tail, thus encouraging systems to overfit to popular / frequent and non-ambiguous cases. We find the most difficult cases of <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> among the infrequent candidates of ambiguous forms. With our findings, we hope to inspire future designs of both <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking systems</a> and <a href=https://en.wikipedia.org/wiki/Data_set>evaluation datasets</a>. To support this goal, we provide a list of recommended actions for better inclusion of tail cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1057" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1057/>Neural Collective Entity Linking</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1057><div class="card-body p-3 small">Entity Linking aims to link entity mentions in texts to <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL apply Graph Convolutional Network to integrate both local contextual features and global coherence information for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>. To improve the computation efficiency, we approximately perform <a href=https://en.wikipedia.org/wiki/Convolution>graph convolution</a> on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>data noise</a> and train the model on Wikipedia hyperlinks to avoid <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the <a href=https://en.wikipedia.org/wiki/Linker_(computing)>linking</a> performance as well as generalization ability. We also conduct an extensive analysis of <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a>, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1058/>Exploiting Structure in Representation of Named Entities using <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a></a></strong><br><a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/h/h-v-jagadish/>H. V. Jagadish</a>
|
<a href=/people/m/mauricio-hernandez/>Mauricio Hernandez</a>
|
<a href=/people/m/mitesh-vasa/>Mitesh Vasa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1058><div class="card-body p-3 small">Fundamental to several knowledge-centric applications is the need to identify <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> from their textual mentions. However, entities lack a unique representation and their mentions can differ greatly. These variations arise in complex ways that can not be captured using textual similarity metrics. However, entities have underlying structures, typically shared by entities of the same entity type, that can help reason over their name variations. Discovering, learning and manipulating these structures typically requires high manual effort in the form of large amounts of labeled training data and handwritten transformation programs. In this work, we propose an active-learning based framework that drastically reduces the labeled data required to learn the structures of entities. We show that <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> for mapping entity mentions to their structures can be automatically generated using human-comprehensible labels. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> consistently outperforms both <a href=https://en.wikipedia.org/wiki/Handwriting>handwritten programs</a> and <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning models</a>. We also demonstrate the utility of our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in relation extraction and entity resolution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1060/>An Empirical Study on Fine-Grained Named Entity Recognition</a></strong><br><a href=/people/k/khai-mai/>Khai Mai</a>
|
<a href=/people/t/thai-hoang-pham/>Thai-Hoang Pham</a>
|
<a href=/people/m/minh-trung-nguyen/>Minh Trung Nguyen</a>
|
<a href=/people/t/tuan-duc-nguyen/>Tuan Duc Nguyen</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1060><div class="card-body p-3 small">Named entity recognition (NER) has attracted a substantial amount of research. Recently, several neural network-based models have been proposed and achieved high performance. However, there is little research on fine-grained NER (FG-NER), in which hundreds of named entity categories must be recognized, especially for non-English languages. It is still an open question whether there is a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> that is robust across various settings or the proper <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> varies depending on the language, the number of named entity categories, and the size of training datasets. This paper first presents an empirical comparison of FG-NER models for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and demonstrates that LSTM+CNN+CRF (Ma and Hovy, 2016), one of the state-of-the-art methods for <a href=https://en.wikipedia.org/wiki/English_language>English NER</a>, also works well for <a href=https://en.wikipedia.org/wiki/English_language>English FG-NER</a> but does not work well for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, a language that has a large number of character types. To tackle this problem, we propose a method to improve the neural network-based Japanese FG-NER performance by removing the CNN layer and utilizing dictionary and category embeddings. Experiment results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves Japanese FG-NER F-score from 66.76 % to 75.18 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1062/>Ant Colony System for Multi-Document Summarization</a></strong><br><a href=/people/a/asma-al-saleh/>Asma Al-Saleh</a>
|
<a href=/people/m/mohamed-el-bachir-menai/>Mohamed El Bachir Menai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1062><div class="card-body p-3 small">This paper proposes an extractive multi-document summarization approach based on an <a href=https://en.wikipedia.org/wiki/Ant_colony>ant colony system</a> to optimize the information coverage of summary sentences. The implemented system was evaluated on both English and Arabic versions of the corpus of the Text Analysis Conference 2011 MultiLing Pilot by using ROUGE metrics. The evaluation results are promising in comparison to those of the participating systems. Indeed, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the best scores based on several ROUGE metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1063/>Multi-task dialog act and sentiment recognition on Mastodon</a></strong><br><a href=/people/c/christophe-cerisara/>Christophe Cerisara</a>
|
<a href=/people/s/somayeh-jafaritazehjani/>Somayeh Jafaritazehjani</a>
|
<a href=/people/a/adedayo-oluokun/>Adedayo Oluokun</a>
|
<a href=/people/h/hoa-t-le/>Hoa T. Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1063><div class="card-body p-3 small">Because of <a href=https://en.wikipedia.org/wiki/Creative_Commons_license>license restrictions</a>, it often becomes impossible to strictly reproduce most research results on Twitter data already a few months after the creation of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. This situation worsened gradually as time passes and <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> become inaccessible. This is a critical issue for reproducible and accountable research on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We partly solve this challenge by annotating a new Twitter-like corpus from an alternative large social medium with licenses that are compatible with reproducible experiments : <a href=https://en.wikipedia.org/wiki/Mastodon_(software)>Mastodon</a>. We manually annotate both dialogues and sentiments on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and train a multi-task hierarchical recurrent network on joint sentiment and dialog act recognition. We experimentally demonstrate that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> may be efficiently achieved between both tasks, and further analyze some specific correlations between sentiments and dialogues on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Both the annotated corpus and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a> are released with an open-source license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1065/>Self-Normalization Properties of <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a></a></strong><br><a href=/people/j/jacob-goldberger/>Jacob Goldberger</a>
|
<a href=/people/o/oren-melamud/>Oren Melamud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1065><div class="card-body p-3 small">Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. In the context of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, this property is particularly appealing as it may significantly reduce <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-times</a> due to large word vocabularies. In this study, we provide a comprehensive investigation of language modeling self-normalization. First, we theoretically analyze the inherent self-normalization properties of Noise Contrastive Estimation (NCE) language models. Then, we compare them empirically to softmax-based approaches, which are self-normalized using <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>explicit regularization</a>, and suggest a hybrid model with compelling properties. Finally, we uncover a surprising negative correlation between self-normalization and perplexity across the board, as well as some regularity in the observed errors, which may potentially be used for improving self-normalization algorithms in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1067/>Dynamic Feature Selection with Attention in Incremental Parsing</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1067><div class="card-body p-3 small">One main challenge for incremental transition-based parsers, when future inputs are invisible, is to extract good features from a limited local context. In this work, we present a simple technique to maximally utilize the local features with an attention mechanism, which works as context- dependent dynamic feature selection. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns, for example, which tokens should a parser focus on, to decide the next action. Our multilingual experiment shows its effectiveness across many languages. We also present an experiment with augmented test dataset and demon- strate it helps to understand the model&#8217;s behavior on locally ambiguous points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1069/>Reading Comprehension with Graph-based Temporal-Casual Reasoning</a></strong><br><a href=/people/y/yawei-sun/>Yawei Sun</a>
|
<a href=/people/g/gong-cheng/>Gong Cheng</a>
|
<a href=/people/y/yuzhong-qu/>Yuzhong Qu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1069><div class="card-body p-3 small">Complex questions in reading comprehension tasks require integrating information from multiple sentences. In this work, to answer such questions involving temporal and causal relations, we generate event graphs from text based on dependencies, and rank answers by aligning event graphs. In particular, the <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> are constrained by graph-based reasoning to ensure temporal and causal agreement. Our focused approach self-adaptively complements existing solutions ; <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is automatically triggered only when applicable. Experiments on RACE and MCTest show that state-of-the-art methods are notably improved by using our approach as an add-on.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1070" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1070/>Projecting Embeddings for Domain Adaption : Joint Modeling of Sentiment Analysis in Diverse Domains</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1070><div class="card-body p-3 small">Domain adaptation for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> is challenging due to the fact that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> are very sensitive to changes in domain. The two most prominent approaches to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> are structural correspondence learning and <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoders</a>. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain_blse</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1071/>Cross-lingual Argumentation Mining : <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> (and a bit of Projection) is All You Need !</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1071><div class="card-body p-3 small">Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling_argument_mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1075" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1075/>Open-Domain Event Detection using Distant Supervision</a></strong><br><a href=/people/j/jun-araki/>Jun Araki</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1075><div class="card-body p-3 small">This paper introduces open-domain event detection, a new event detection paradigm to address issues of prior work on restricted domains and event annotation. The goal is to detect all kinds of events regardless of domains. Given the absence of training data, we propose a distant supervision method that is able to generate high-quality training data. Using a manually annotated event corpus as gold standard, our experiments show that despite no direct supervision, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> outperforms <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a>. This result indicates that the distant supervision enables robust event detection in various domains, while obviating the need for human annotation of events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1076/>Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing</a></strong><br><a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/b/bo-an/>Bo An</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1076><div class="card-body p-3 small">Semantic parsers critically rely on accurate and high-coverage lexicons. However, traditional semantic parsers usually utilize annotated logical forms to learn the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>, which often suffer from the lexicon coverage problem. In this paper, we propose a graph-based semi-supervised learning framework that makes use of large text corpora and lexical resources. This framework first constructs a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with a phrase similarity model learned by utilizing many <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> and lexical resources. Next, graph propagation algorithm identifies the label distribution of unlabeled phrases from labeled ones. We evaluate our approach on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> : <a href=https://en.wikipedia.org/wiki/WebQuest>Webquestions</a> and Free917. The results show that, in both datasets, our method achieves substantial improvement when comparing to the base system that does not utilize the learned lexicon, and gains competitive results when comparing to state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1079/>Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings</a></strong><br><a href=/people/j/junjie-li/>Junjie Li</a>
|
<a href=/people/h/haitong-yang/>Haitong Yang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1079><div class="card-body p-3 small">Document-level multi-aspect sentiment classification aims to predict user&#8217;s sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text information</a>. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically, HUARN adopts a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical architecture</a> to encode word, sentence, and document level information. Then, user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1080/>Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora</a></strong><br><a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1080><div class="card-body p-3 small">Recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models. This can be partially explained by the lack of large specialized comparable corpora to build efficient <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>. Within this context, we try to answer the following questions : First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction? Second (ii) is it worth it to combine multiple embeddings trained on different data sets? For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora. We emphasize how the character-based embedding model outperforms other <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> on the quality of the extracted bilingual lexicons. Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. Our approach leads to higher performance than the best individual embedding model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1082/>Evaluating the text quality, human likeness and tailoring component of PASS : A Dutch data-to-text system for soccer<span class=acl-fixed-case>PASS</span>: A <span class=acl-fixed-case>D</span>utch data-to-text system for soccer</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/b/bart-verduijn/>Bart Verduijn</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1082><div class="card-body p-3 small">We present an evaluation of PASS, a data-to-text system that generates Dutch soccer reports from match statistics which are automatically tailored towards fans of one club or the other. The evaluation in this paper consists of two studies. An intrinsic human-based evaluation of the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s output is described in the first study. In this study it was found that compared to human-written texts, computer-generated texts were rated slightly lower on style-related text components (fluency and clarity) and slightly higher in terms of the correctness of given information. Furthermore, results from the first study showed that tailoring was accurately recognized in most cases, and that participants struggled with correctly identifying whether a text was written by a human or computer. The second study investigated if <a href=https://en.wikipedia.org/wiki/Tailor>tailoring</a> affects perceived text quality, for which no results were garnered. This lack of results might be due to negative preconceptions about computer-generated texts which were found in the first study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1083/>Answerable or Not : Devising a Dataset for Extending Machine Reading Comprehension</a></strong><br><a href=/people/m/mao-nakanishi/>Mao Nakanishi</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1083><div class="card-body p-3 small">Machine-reading comprehension (MRC) has recently attracted attention in the fields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. One of the problematic presumptions with current MRC technologies is that each question is assumed to be answerable by looking at a given text passage. However, to realize human-like language comprehension ability, a machine should also be able to distinguish not-answerable questions (NAQs) from answerable questions. To develop this <a href=https://en.wikipedia.org/wiki/Function_(engineering)>functionality</a>, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> incorporating hard-to-detect NAQs is vital ; however, its manual construction would be expensive. This paper proposes a <a href=https://en.wikipedia.org/wiki/Data_set>dataset creation method</a> that alters an existing MRC dataset, the Stanford Question Answering Dataset, and describes the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The value of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is likely to increase if each <a href=https://en.wikipedia.org/wiki/Question_mark>NAQ</a> in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is properly classified with the difficulty of identifying it as an <a href=https://en.wikipedia.org/wiki/Question_mark>NAQ</a>. This difficulty level would allow researchers to evaluate a machine&#8217;s NAQ detection performance more precisely. Therefore, we propose a method for automatically assigning difficulty level labels, which measures the similarity between a question and the target text passage. Our NAQ detection experiments demonstrate that the resulting dataset, having difficulty level annotations, is valid and potentially useful in the development of advanced MRC models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1084" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1084/>Style Obfuscation by Invariance</a></strong><br><a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/e/enrique-manjavacas/>Enrique Manjavacas Arevalo</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz ChrupaÅa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1084><div class="card-body p-3 small">The task of obfuscating writing style using sequence models has previously been investigated under the framework of obfuscation-by-transfer, where the input text is explicitly rewritten in another style. A side effect of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> are the frequent major alterations to the <a href=https://en.wikipedia.org/wiki/Semantic_Web>semantic content</a> of the input. In this work, we propose obfuscation-by-invariance, and investigate to what extent <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained to be explicitly style-invariant preserve semantics. We evaluate our architectures in parallel and non-parallel settings, and compare automatic and human evaluations on the obfuscated sentences. Our experiments show that the performance of a style classifier can be reduced to chance level, while the output is evaluated to be of equal quality to <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> applying style-transfer. Additionally, human evaluation indicates a trade-off between the level of <a href=https://en.wikipedia.org/wiki/Obfuscation>obfuscation</a> and the observed quality of the output in terms of meaning preservation and <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1087/>Towards a Language for Natural Language Treebank Transductions</a></strong><br><a href=/people/c/carlos-a-prolo/>Carlos A. Prolo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1087><div class="card-body p-3 small">This paper describes a transduction language suitable for natural language treebank transformations and motivates its application to tasks that have been used and described in the literature. The language, which is the basis for a tree transduction tool allows for clean, precise and concise description of what has been very confusingly, ambiguously, and incompletely textually described in the literature also allowing easy non-hard-coded implementation. We also aim at getting feedback from the NLP community to eventually converge to a de facto standard for such transduction language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1089/>Point Precisely : Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism</a></strong><br><a href=/people/l/liunian-li/>Liunian Li</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1089><div class="card-body p-3 small">The task of data-to-text generation aims to generate descriptive texts conditioned on a number of database records, and recent neural models have shown significant progress on this task. The attention based encoder-decoder models with copy mechanism have achieved state-of-the-art results on a few data-to-text datasets. However, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> still face the problem of putting incorrect data records in the generated texts, especially on some more challenging datasets like <a href=https://en.wikipedia.org/wiki/RotoWire>RotoWire</a>. In this paper, we propose a two-stage approach with a delayed copy mechanism to improve the <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of data records in the generated texts. Our approach first adopts an encoder-decoder model to generate a template text with data slots to be filled and then leverages a proposed delayed copy mechanism to fill in the slots with proper data records. Our delayed copy mechanism can take into account all the information of the input data records and the full generated template text by using double attention, position-aware attention and a pairwise ranking loss. The two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in the two stages are trained separately. Evaluation results on the RotoWire dataset verify the efficacy of our proposed approach to generate better templates and copy data records more precisely.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1092/>Enhanced Aspect Level Sentiment Classification with <a href=https://en.wikipedia.org/wiki/Auxiliary_memory>Auxiliary Memory</a></a></strong><br><a href=/people/p/peisong-zhu/>Peisong Zhu</a>
|
<a href=/people/t/tieyun-qian/>Tieyun Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1092><div class="card-body p-3 small">In aspect level sentiment classification, there are two common tasks : to identify the sentiment of an aspect (category) or a term. As specific instances of <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a>, terms explicitly occur in sentences. It is beneficial for <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to focus on <a href=https://en.wikipedia.org/wiki/Context_(language_use)>nearby context words</a>. In contrast, as high level semantic concepts of terms, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> usually have more generalizable representations. However, conventional methods can not utilize the information of aspects and terms at the same time, because few datasets are annotated with both <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and terms. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Deep_learning>deep memory network</a> with <a href=https://en.wikipedia.org/wiki/Auxiliary_memory>auxiliary memory</a> to address this problem. In our model, a <a href=https://en.wikipedia.org/wiki/Main_memory>main memory</a> is used to capture the important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a> for sentiment classification. In addition, we build an <a href=https://en.wikipedia.org/wiki/Auxiliary_memory>auxiliary memory</a> to implicitly convert aspects and terms to each other, and feed both of them to the <a href=https://en.wikipedia.org/wiki/Computer_data_storage>main memory</a>. With the interaction between two memories, the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> of aspects and terms can be learnt simultaneously. We compare our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with the state-of-the-art methods on four datasets from different domains. The experimental results demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/C18-1097.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1097" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1097/>Bringing replication and reproduction together with generalisability in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : Three reproduction studies for Target Dependent Sentiment Analysis<span class=acl-fixed-case>NLP</span>: Three reproduction studies for Target Dependent Sentiment Analysis</a></strong><br><a href=/people/a/andrew-moore/>Andrew Moore</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1097><div class="card-body p-3 small">Lack of <a href=https://en.wikipedia.org/wiki/Repeatability>repeatability</a> and generalisability are two significant threats to continuing scientific development in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Language models and learning methods are so complex that <a href=https://en.wikipedia.org/wiki/Academic_publishing>scientific conference papers</a> no longer contain enough space for the technical depth required for replication or reproduction. Taking Target Dependent Sentiment Analysis as a case study, we show how recent work in the field has not consistently released code, or described settings for learning methods in enough detail, and lacks comparability and generalisability in train, test or validation data. To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and perform the first large-scale mass evaluation on six different English datasets. Reflecting on our experiences, we recommend that future replication or reproduction experiments should always consider a variety of datasets alongside documenting and releasing their methods and published code in order to minimise the barriers to both <a href=https://en.wikipedia.org/wiki/Repeatability>repeatability</a> and generalisability. We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1098/>Multilevel Heuristics for Rationale-Based Entity Relation Classification in Sentences</a></strong><br><a href=/people/s/shiou-tian-hsu/>Shiou Tian Hsu</a>
|
<a href=/people/m/mandar-chaudhary/>Mandar Chaudhary</a>
|
<a href=/people/n/nagiza-samatova/>Nagiza Samatova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1098><div class="card-body p-3 small">Rationale-based models provide a unique way to provide justifiable results for relation classification models by identifying rationales (key words and phrases that a person can use to justify the relation in the sentence) during the process. However, existing generative networks used to extract rationales come with a trade-off between extracting diversified rationales and achieving good <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results. In this paper, we propose a multilevel heuristic approach to regulate rationale extraction to avoid extracting monotonous rationales without compromising <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. In our model, rationale selection is regularized by a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised process</a> and features from different levels : <a href=https://en.wikipedia.org/wiki/Word>word</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence</a>, and corpus. We evaluate our approach on the SemEval 2010 dataset that includes 19 relation classes and the quality of extracted rationales with our manually-labeled rationales. Experiments show a significant improvement in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification performance</a> and a 20 % gain in rationale interpretability compared to state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1099/>Adversarial Multi-lingual Neural Relation Extraction</a></strong><br><a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1099><div class="card-body p-3 small">Multi-lingual relation extraction aims to find unknown relational facts from text in various languages. Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can not well capture the consistency and diversity of relation patterns in different languages. To address these issues, we propose an adversarial multi-lingual neural relation extraction (AMNRE) model, which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages. Further, we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language-consistent relation patterns. The experimental results on real-world datasets demonstrate that our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1101/>Abstract Meaning Representation for Multi-Document Summarization<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Multi-Document Summarization</a></strong><br><a href=/people/k/kexin-liao/>Kexin Liao</a>
|
<a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1101><div class="card-body p-3 small">Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theory</a>, as a form of content representation. Our approach condenses source documents to a set of <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>summary graphs</a> following the <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AMR formalism</a>. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1102/>Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion</a></strong><br><a href=/people/m/mir-tafseer-nayeem/>Mir Tafseer Nayeem</a>
|
<a href=/people/t/tanvir-ahmed-fuad/>Tanvir Ahmed Fuad</a>
|
<a href=/people/y/yllias-chali/>Yllias Chali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1102><div class="card-body p-3 small">In this work, we aim at developing an unsupervised abstractive summarization system in the multi-document setting. We design a paraphrastic sentence fusion model which jointly performs sentence fusion and <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> using skip-gram word embedding model at the sentence level. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves the <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>information coverage</a> and at the same time abstractiveness of the generated sentences. We conduct our experiments on the human-generated multi-sentence compression datasets and evaluate our system on several newly proposed Machine Translation (MT) evaluation metrics. Furthermore, we apply our sentence level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. We also propose an optimal solution for the classical summary length limit problem which was not addressed in the past research. For the document level summary, we conduct experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of two different domains (e.g., <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a> and user reviews) which are well suited for multi-document abstractive summarization. Our experiments demonstrate that the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> bring significant improvements over the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1103/>Adversarial Domain Adaptation for Variational Neural Language Generation in Dialogue Systems</a></strong><br><a href=/people/v/van-khanh-tran/>Van-Khanh Tran</a>
|
<a href=/people/m/minh-le-nguyen/>Le-Minh Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1103><div class="card-body p-3 small">Domain Adaptation arises when we aim at learning from source domain a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that can perform acceptably well on a different target domain. It is especially crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when there are sufficient annotated data in the source domain, but there is a limited labeled data in the target domain. How to effectively utilize as much of existing abilities from source domains is a crucial issue in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. In this paper, we propose an adversarial training procedure to train a Variational encoder-decoder based language generator via multiple adaptation steps. In this procedure, a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is first trained on a source domain data and then fine-tuned on a small set of target domain utterances under the guidance of two proposed critics. Experimental results show that the proposed method can effectively leverage the existing knowledge in the source domain to adapt to another related domain by using only a small amount of in-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1104/>Ask No More : Deciding when to guess in referential visual dialogue</a></strong><br><a href=/people/r/ravi-shekhar/>Ravi Shekhar</a>
|
<a href=/people/t/tim-baumgartner/>Tim BaumgÃ¤rtner</a>
|
<a href=/people/a/aashish-venkatesh/>Aashish Venkatesh</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1104><div class="card-body p-3 small">Our goal is to explore how the abilities brought in by a <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a> can be included in end-to-end visually grounded conversational agents. We make initial steps towards this general goal by augmenting a task-oriented visual dialogue model with a decision-making component that decides whether to ask a follow-up question to identify a target referent in an image, or to stop the conversation to make a guess. Our analyses show that adding a decision making component produces dialogues that are less repetitive and that include fewer unnecessary questions, thus potentially leading to more efficient and less unnatural interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1105/>Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding</a></strong><br><a href=/people/y/yutai-hou/>Yutai Hou</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1105><div class="card-body p-3 small">In this paper, we study the problem of <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> in task-oriented dialogue system. In contrast to previous work which augments an utterance without considering its relation with other utterances, we propose a sequence-to-sequence generation based data augmentation framework that leverages one utterance&#8217;s same semantic alternatives in the training data. A novel diversity rank is incorporated into the utterance representation to make the model produce diverse utterances and these diversely augmented utterances help to improve the language understanding module. Experimental results on the Airline Travel Information System dataset and a newly created semantic frame annotation on Stanford Multi-turn, Multi-domain Dialogue Dataset show that our framework achieves significant improvements of 6.38 and 10.04 F-scores respectively when only a training set of hundreds utterances is represented. Case studies also confirm that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> generates diverse utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1106/>Dialogue-act-driven Conversation Model : An Experimental Study</a></strong><br><a href=/people/h/harshit-kumar/>Harshit Kumar</a>
|
<a href=/people/a/arvind-agarwal/>Arvind Agarwal</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1106><div class="card-body p-3 small">The utility of additional semantic information for the task of next utterance selection in an automated dialogue system is the focus of study in this paper. In particular, we show that additional information available in the form of dialogue acts when used along with context given in the form of dialogue history improves the performance irrespective of the underlying model being generative or discriminative. In order to show the model agnostic behavior of dialogue acts, we experiment with several well-known models such as sequence-to-sequence encoder-decoder model, hierarchical encoder-decoder model, and Siamese-based models with and without hierarchy ; and show that in all models, incorporating dialogue acts improves the performance by a significant margin. We, furthermore, propose a novel way of encoding dialogue act information, and use it along with hierarchical encoder to build a model that can use the sequential dialogue act information in a natural way. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an MRR of about 84.8 % for the task of next utterance selection on a newly introduced Daily Dialogue dataset, and outperform the baseline models. We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialog act information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1107/>Structured Dialogue Policy with Graph Neural Networks</a></strong><br><a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/b/bowen-tan/>Bowen Tan</a>
|
<a href=/people/s/sishan-long/>Sishan Long</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1107><div class="card-body p-3 small">Recently, deep reinforcement learning (DRL) has been used for dialogue policy optimization. However, many DRL-based policies are not sample-efficient. Most recent advances focus on improving DRL optimization algorithms to address this issue. Here, we take an alternative route of designing neural network structure that is better suited for DRL-based dialogue management. The proposed structured deep reinforcement learning is based on graph neural networks (GNN), which consists of some sub-networks, each one for a <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a> on a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>. The <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is defined according to the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> and each node can be considered as a sub-agent. During <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>, these sub-agents have internal message exchange between neighbors on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We also propose an approach to jointly optimize the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> as well as the parameters of GNN. Experiments show that structured DRL significantly outperforms previous state-of-the-art approaches in almost all of the 18 tasks of the PyDial benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1108/>JTAV : Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features<span class=acl-fixed-case>JTAV</span>: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features</a></strong><br><a href=/people/h/hongru-liang/>Hongru Liang</a>
|
<a href=/people/h/haozheng-wang/>Haozheng Wang</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a>
|
<a href=/people/s/shaodi-you/>Shaodi You</a>
|
<a href=/people/z/zhe-sun/>Zhe Sun</a>
|
<a href=/people/j/jin-mao-wei/>Jin-Mao Wei</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1108><div class="card-body p-3 small">Learning social media content is the basis of many real-world applications, including information retrieval and recommendation systems, among others. In contrast with previous works that focus mainly on single modal or bi-modal learning, we propose to learn social media content by fusing jointly textual, acoustic, and visual information (JTAV). Effective strategies are proposed to extract fine-grained features of each modality, that is, attBiGRU and DCRNN. We also introduce cross-modal fusion and attentive pooling techniques to integrate multi-modal information comprehensively. Extensive experimental evaluation conducted on real-world datasets demonstrate our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art approaches by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1109/>MEMD : A Diversity-Promoting Learning Framework for Short-Text Conversation<span class=acl-fixed-case>MEMD</span>: A Diversity-Promoting Learning Framework for Short-Text Conversation</a></strong><br><a href=/people/m/meng-zou/>Meng Zou</a>
|
<a href=/people/x/xihan-li/>Xihan Li</a>
|
<a href=/people/h/haokun-liu/>Haokun Liu</a>
|
<a href=/people/z/zhi-hong-deng/>Zhihong Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1109><div class="card-body p-3 small">Neural encoder-decoder models have been widely applied to conversational response generation, which is a research hot spot in recent years. However, conventional neural encoder-decoder models tend to generate commonplace responses like I do n&#8217;t know regardless of what the input is. In this paper, we analyze this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> from a new perspective : <a href=https://en.wikipedia.org/wiki/Latent_vector>latent vectors</a>. Based on it, we propose an easy-to-extend learning framework named MEMD (Multi-Encoder to Multi-Decoder), in which an auxiliary encoder and an auxiliary decoder are introduced to provide necessary training guidance without resorting to extra data or complicating network&#8217;s inner structure. Experimental results demonstrate that our method effectively improve the quality of generated responses according to automatic metrics and human evaluations, yielding more diverse and smooth replies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1112/>An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/f/fabienne-cap/>Fabienne Cap</a>
|
<a href=/people/e/eva-pettersson/>Eva Pettersson</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1112><div class="card-body p-3 small">In this paper, we apply different NMT models to the problem of historical spelling normalization for five languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. The NMT models are at different levels, have different <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, and different neural network architectures. Our results show that NMT models are much better than SMT models in terms of character error rate. The vanilla RNNs are competitive to GRUs / LSTMs in historical spelling normalization. Transformer models perform better only when provided with more training data. We also find that subword-level models with a small subword vocabulary are better than character-level models. In addition, we propose a hybrid method which further improves the performance of historical spelling normalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1115/>Local String Transduction as Sequence Labeling</a></strong><br><a href=/people/j/joana-ribeiro/>Joana Ribeiro</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/x/xavier-carreras/>Xavier Carreras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1115><div class="card-body p-3 small">We show that the general problem of <a href=https://en.wikipedia.org/wiki/Transduction_(genetics)>string transduction</a> can be reduced to the problem of <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. While character deletion and insertions are allowed in string transduction, they do not exist in <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. We show how to overcome this difference. Our approach can be used with any sequence labeling algorithm and it works best for problems in which string transduction imposes a strong notion of <a href=https://en.wikipedia.org/wiki/Locality_of_reference>locality</a> (no long range dependencies). We experiment with spelling correction for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR correction</a>, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1117/>Diachronic word embeddings and <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shifts</a> : a survey</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Ãvrelid</a>
|
<a href=/people/t/terrence-szymanski/>Terrence Szymanski</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1117><div class="card-body p-3 small">Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, as well as prospects and possible applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1118/>Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention</a></strong><br><a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/x/xuefei-zhang/>Xuefei Zhang</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/l/longbiao-wang/>Longbiao Wang</a>
|
<a href=/people/j/jianwu-dang/>Jianwu Dang</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1118><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> are insufficient for topic extraction in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The existing methods only consider text information or simultaneously model the posts and the static characteristics of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. They ignore that one discusses diverse topics when dynamically interacting with different people. Moreover, people who talk about the same topic have different effects on the topic. In this paper, we propose an Interaction-Aware Topic Model (IATM) for <a href=https://en.wikipedia.org/wiki/Microblogging>microblog conversations</a> by integrating network embedding and user attention. A conversation network linking users based on reposting and replying relationship is constructed to mine the dynamic user behaviours. We model dynamic interactions and user attention so as to learn interaction-aware edge embeddings with <a href=https://en.wikipedia.org/wiki/Social_environment>social context</a>. Then they are incorporated into neural variational inference for generating the more consistent topics. The experiments on three real-world datasets show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1122/>Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation</a></strong><br><a href=/people/f/francis-gregoire/>Francis GrÃ©goire</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1122><div class="card-body p-3 small">Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. Our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. To justify the utility of our approach, we extract <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence pairs</a> from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia articles</a> to train <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> and show significant improvements in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1125/>Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical Handwriting Recognition with Scarce Resources</a></strong><br><a href=/people/a/adeline-granet/>Adeline Granet</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/h/harold-mouchere/>Harold MouchÃ¨re</a>
|
<a href=/people/s/solen-quiniou/>Solen Quiniou</a>
|
<a href=/people/c/christian-viard-gaudin/>Christian Viard-Gaudin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1125><div class="card-body p-3 small">Lack of data can be an issue when beginning a new study on historical handwritten documents. In order to deal with this, we present the character-based decoder part of a multilingual approach based on transductive transfer learning for a historical handwriting recognition task on Italian Comedy Registers. The <a href=https://en.wikipedia.org/wiki/Code>decoder</a> must build a sequence of characters that corresponds to a word from a vector of letter-ngrams. As learning data, we created a new dataset from untapped resources that covers the same domain and period of our Italian Comedy data, as well as resources from common domains, periods, or languages. We obtain a 97.42 % <a href=https://en.wikipedia.org/wiki/Character_recognition>Character Recognition Rate</a> and a 86.57 % <a href=https://en.wikipedia.org/wiki/Word_recognition>Word Recognition Rate</a> on our Italian Comedy data, despite a lexical coverage of 67 % between the Italian Comedy data and the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. These results show that an efficient <a href=https://en.wikipedia.org/wiki/System>system</a> can be obtained by a carefully selecting the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used for the <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1126/>SMHD : a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions<span class=acl-fixed-case>SMHD</span>: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions</a></strong><br><a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/b/bart-desmet/>Bart Desmet</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1126><div class="card-body p-3 small">Mental health is a significant and growing public health concern. As language usage can be leveraged to obtain crucial insights into mental health conditions, there is a need for large-scale, labeled, mental health-related datasets of users who have been diagnosed with one or more of such conditions. In this paper, we investigate the creation of high-precision patterns to identify self-reported diagnoses of nine different <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a>, and obtain high-quality labeled data without the need for manual labelling. We introduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it available. SMHD is a novel large dataset of social media posts from users with one or multiple <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a> along with matched control users. We examine distinctions in users&#8217; language, as measured by linguistic and psychological variables. We further explore text classification methods to identify individuals with <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental conditions</a> through their language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1128/>Cross-lingual Knowledge Projection Using <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and Target-side Knowledge Base Completion</a></strong><br><a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/h/hirokazu-kiyomaru/>Hirokazu Kiyomaru</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1128><div class="card-body p-3 small">Considerable effort has been devoted to building <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge_base>commonsense knowledge bases</a>. However, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not available in many languages because the <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>construction of KBs</a> is expensive. To bridge the gap between languages, this paper addresses the problem of projecting the knowledge in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, a resource-rich language, into other languages, where the main challenge lies in projection ambiguity. This <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> is partially solved by <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and target-side knowledge base completion, but neither of them is adequately reliable by itself. We show their combination can project English commonsense knowledge into <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> with high precision. Our method also achieves a top-10 accuracy of 90 % on the crowdsourced EnglishJapanese benchmark. Furthermore, we use our method to obtain 18,747 facts of accurate Japanese commonsense within a very short period.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1129 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1129/>Assessing Quality Estimation Models for Sentence-Level Prediction</a></strong><br><a href=/people/h/hoang-cuong/>Hoang Cuong</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1129><div class="card-body p-3 small">This paper provides an evaluation of a wide range of advanced sentence-level Quality Estimation models, including Support Vector Regression, Ride Regression, <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>, Gaussian Processes, Bayesian Neural Networks, Deep Kernel Learning and Deep Gaussian Processes. Beside the accurateness, our main concerns are also the robustness of Quality Estimation models. Our work raises the difficulty in building strong <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Specifically, we show that Quality Estimation models often behave differently in Quality Estimation feature space, depending on whether the scale of feature space is small, medium or large. We also show that Quality Estimation models often behave differently in evaluation settings, depending on whether test data come from the same domain as the training data or not. Our work suggests several strong candidates to use in different circumstances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1136/>Ab Initio : Automatic Latin Proto-word Reconstruction<span class=acl-fixed-case>L</span>atin Proto-word Reconstruction</a></strong><br><a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1136><div class="card-body p-3 small">Proto-word reconstruction is central to the study of <a href=https://en.wikipedia.org/wiki/Evolutionary_linguistics>language evolution</a>. It consists of recreating the words in an ancient language from its modern daughter languages. In this paper we investigate automatic word form reconstruction for Latin proto-words. Having modern word forms in multiple Romance languages (French, Italian, Spanish, Portuguese and Romanian), we infer the form of their common Latin ancestors. Our approach relies on the regularities that occurred when the <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(full)>Latin words</a> entered the <a href=https://en.wikipedia.org/wiki/Modern_language>modern languages</a>. We leverage information from all modern languages, building an ensemble system for proto-word reconstruction. We use <a href=https://en.wikipedia.org/wiki/Conditional_random_field>conditional random fields</a> for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, but we conduct preliminary experiments with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> as well. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on multiple datasets, showing that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves on previous results, having also the advantage of requiring less input data, which is essential in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>, where resources are generally scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1137/>A Computational Model for the Linguistic Notion of Morphological Paradigm</a></strong><br><a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/l/ling-liu/>Ling Liu</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1137><div class="card-body p-3 small">In supervised learning of morphological patterns, the strategy of generalizing inflectional tables into more abstract paradigms through alignment of the longest common subsequence found in an inflection table has been proposed as an efficient method to deduce the inflectional behavior of unseen word forms. In this paper, we extend this notion of morphological &#8216;paradigm&#8217; from earlier work and provide a formalization that more accurately matches linguist intuitions about what an <a href=https://en.wikipedia.org/wiki/Inflectional_paradigm>inflectional paradigm</a> is. Additionally, we propose and evaluate a mechanism for learning full human-readable paradigm specifications from incomplete dataa scenario when we only have access to a few inflected forms for each <a href=https://en.wikipedia.org/wiki/Lexeme>lexeme</a>, and want to reconstruct the missing inflections as well as generalize and group the witnessed patterns into a model of more abstract paradigmatic behavior of lexemes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1138/>Relation Induction in Word Embeddings Revisited</a></strong><br><a href=/people/z/zied-bouraoui/>Zied Bouraoui</a>
|
<a href=/people/s/shoaib-jameel/>Shoaib Jameel</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1138><div class="card-body p-3 small">Given a set of instances of some relation, the relation induction task is to predict which other word pairs are likely to be related in the same way. While it is natural to use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for this task, standard approaches based on vector translations turn out to perform poorly. To address this issue, we propose two probabilistic relation induction models. The first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on translations, but uses <a href=https://en.wikipedia.org/wiki/List_of_things_named_after_Carl_Friedrich_Gauss>Gaussians</a> to explicitly model the variability of these translations and to encode soft constraints on the source and target words that may be chosen. In the second model, we use <a href=https://en.wikipedia.org/wiki/Bayesian_linear_regression>Bayesian linear regression</a> to encode the assumption that there is a <a href=https://en.wikipedia.org/wiki/Linear_relationship>linear relationship</a> between the vector representations of related words, which is considerably weaker than the assumption underlying translation based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1139/>Contextual String Embeddings for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>Sequence Labeling</a></a></strong><br><a href=/people/a/alan-akbik/>Alan Akbik</a>
|
<a href=/people/d/duncan-blythe/>Duncan Blythe</a>
|
<a href=/people/r/roland-vollgraf/>Roland Vollgraf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1139><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Language>language modeling</a> using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> have made it viable to model <a href=https://en.wikipedia.org/wiki/Language>language</a> as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> which we refer to as contextual string embeddings. Our proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> depending on its contextual use. We conduct a comparative evaluation against previous <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> and find that our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are highly useful for downstream tasks : across four classic sequence labeling tasks we consistently outperform the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks : https://github.com/zalandoresearch/flair</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1142/>Variational Attention for Sequence-to-Sequence Models</a></strong><br><a href=/people/h/hareesh-bahuleyan/>Hareesh Bahuleyan</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/o/olga-vechtomova/>Olga Vechtomova</a>
|
<a href=/people/p/pascal-poupart/>Pascal Poupart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1142><div class="card-body p-3 small">The variational encoder-decoder (VED) encodes source information as a set of <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a> using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, which in turn is decoded into target data using another <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, sequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder networks. When combined with a traditional (deterministic) attention mechanism, the variational latent space may be bypassed by the attention model, and thus becomes ineffective. In this paper, we propose a variational attention mechanism for VED, where the attention vector is also modeled as <a href=https://en.wikipedia.org/wiki/Normal_distribution>Gaussian distributed random variables</a>. Results on two experiments show that, without loss of quality, our proposed method alleviates the bypassing phenomenon as it increases the diversity of generated sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1144 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1144/>Learning from Measurements in Crowdsourcing Models : Inferring Ground Truth from Diverse Annotation Types</a></strong><br><a href=/people/p/paul-felt/>Paul Felt</a>
|
<a href=/people/e/eric-ringger/>Eric Ringger</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1144><div class="card-body p-3 small">Annotated corpora enable <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> and <a href=https://en.wikipedia.org/wiki/Data_analysis>data analysis</a>. To reduce the cost of manual annotation, tasks are often assigned to internet workers whose judgments are reconciled by crowdsourcing models. We approach the problem of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> using a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for learning from rich prior knowledge, and we identify a family of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing models</a> with the novel ability to combine <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> with differing structures : e.g., document labels and word labels. Annotator judgments are given in the form of the predicted expected value of <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>measurement functions</a> computed over <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and the data, unifying annotation models. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, a specific instance of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, compares favorably with previous work. Furthermore, it enables active sample selection, jointly selecting annotator, data item, and annotation structure to reduce annotation effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1146/>Structure-Infused Copy Mechanisms for Abstractive Summarization</a></strong><br><a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/l/lin-zhao/>Lin Zhao</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1146><div class="card-body p-3 small">Seq2seq learning has produced promising results on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the <a href=https://en.wikipedia.org/wiki/System>system</a>, and our proposed approach compares favorably to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1147/>Measuring the Diversity of Automatic Image Descriptions</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1147><div class="card-body p-3 small">Automatic image description systems typically produce generic sentences that only make use of a small subset of the vocabulary available to them. In this paper, we consider the production of generic descriptions as a lack of diversity in the output, which we quantify using established <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> and two new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that frame image description as a word recall task. This framing allows us to evaluate <a href=https://en.wikipedia.org/wiki/System>system</a> performance on the head of the vocabulary, as well as on the long tail, where <a href=https://en.wikipedia.org/wiki/System>system</a> performance degrades. We use these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to examine the <a href=https://en.wikipedia.org/wiki/Diversity_index>diversity</a> of the sentences generated by nine state-of-the-art systems on the MS COCO data set. We find that the systems trained with maximum likelihood objectives produce less diverse output than those trained with additional adversarial objectives. However, the adversarially-trained models only produce more types from the head of the vocabulary and not the tail. Besides vocabulary-based methods, we also look at the compositional capacity of the systems, specifically their ability to create <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound nouns</a> and <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositional phrases</a> of different lengths. We conclude that there is still much room for improvement, and offer a toolkit to measure progress towards the goal of generating more diverse image descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1149/>A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task</a></strong><br><a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/z/ziwei-li/>Ziwei Li</a>
|
<a href=/people/j/jin-mao-wei/>Jin-Mao Wei</a>
|
<a href=/people/y/yanhui-gu/>Yanhui Gu</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1149><div class="card-body p-3 small">Enabling a mechanism to understand a temporal story and predict its ending is an interesting issue that has attracted considerable attention, as in case of the ROC Story Cloze Task (SCT). In this paper, we develop a multi-attention-based neural network (MANN) with well-designed optimizations, like Highway Network, and concatenated features with embedding representations into the hierarchical neural network model. Considering the particulars of the specific <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we thoughtfully extend MANN with external knowledge resources, exceeding state-of-the-art results obviously. Furthermore, we develop a thorough understanding of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> through a careful hand analysis on a subset of the stories. We identify what traits of MANN contribute to its outperformance and how external knowledge is obtained in such an ending prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1150/>A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators</a></strong><br><a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1150><div class="card-body p-3 small">Visual Question Generation (VQG) aims to ask natural questions about an <a href=https://en.wikipedia.org/wiki/Image>image</a> automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> in <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1154" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1154/>Enhancing Sentence Embedding with Generalized Pooling</a></strong><br><a href=/people/q/qian-chen/>Qian Chen</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1154><div class="card-body p-3 small">Pooling is an essential component of a wide variety of sentence representation and embedding models. This paper explores generalized pooling methods to enhance <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>. We propose vector-based multi-head attention that includes the widely used <a href=https://en.wikipedia.org/wiki/Max_pooling>max pooling</a>, mean pooling, and scalar self-attention as special cases. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> benefits from properly designed penalization terms to reduce redundancy in multi-head attention. We evaluate the proposed model on three different tasks : natural language inference (NLI), <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a>, and sentiment classification. The experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> than we discuss in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1156/>CASCADE : Contextual Sarcasm Detection in Online Discussion Forums<span class=acl-fixed-case>CASCADE</span>: Contextual Sarcasm Detection in Online Discussion Forums</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/s/sruthi-gorantla/>Sruthi Gorantla</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1156><div class="card-body p-3 small">The literature in automated sarcasm detection has mainly focused on lexical-, syntactic- and semantic-level analysis of text. However, a <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic sentence</a> can be expressed with contextual presumptions, background and commonsense knowledge. In this paper, we propose a ContextuAl SarCasm DEtector (CASCADE), which adopts a hybrid approach of both content- and context-driven modeling for sarcasm detection in online social media discussions. For the latter, CASCADE aims at extracting <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> from the discourse of a discussion thread. Also, since the sarcastic nature and form of expression can vary from person to person, CASCADE utilizes user embeddings that encode stylometric and personality features of users. When used along with content-based feature extractors such as <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, we see a significant boost in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance on a large Reddit corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1157/>Recognizing Humour using <a href=https://en.wikipedia.org/wiki/Word_association>Word Associations</a> and Humour Anchor Extraction</a></strong><br><a href=/people/a/andrew-cattle/>Andrew Cattle</a>
|
<a href=/people/x/xiaojuan-ma/>Xiaojuan Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1157><div class="card-body p-3 small">This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for <a href=https://en.wikipedia.org/wiki/Humour>humour</a>. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1160/>An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection</a></strong><br><a href=/people/z/zhenni-you/>Zhenni You</a>
|
<a href=/people/t/tieyun-qian/>Tieyun Qian</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1160><div class="card-body p-3 small">Spam detection has long been a research topic in both academic and industry due to its wide applications. Previous studies are mainly focused on extracting linguistic or behavior features to distinguish the <a href=https://en.wikipedia.org/wiki/Spamming>spam and legitimate reviews</a>. Such <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are either ineffective or take long time to collect and thus are hard to be applied to cold-start spam review detection tasks. Recent advance leveraged the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to encode the textual and behavior features for the cold-start problem. However, the abundant attribute information are largely neglected by the existing <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. In this paper, we propose a novel deep learning architecture for incorporating entities and their inherent attributes from various domains into a unified framework. Specifically, our model not only encodes the entities of reviewer, item, and review, but also their attributes such as location, date, price ranges. Furthermore, we present a domain classifier to adapt the knowledge from one domain to the other. With the abundant attributes in existing entities and knowledge in other domains, we successfully solve the problem of data scarcity in the cold-start settings. Experimental results on two Yelp datasets prove that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> significantly outperforms the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1165/>A Neural Question Answering Model Based on Semi-Structured Tables</a></strong><br><a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/x/xiaodong-zhang/>Xiaodong Zhang</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/m/mengxiang-wang/>Mengxiang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1165><div class="card-body p-3 small">Most question answering (QA) systems are based on raw text and structured knowledge graph. However, raw text corpora are hard for QA system to understand, and structured knowledge graph needs intensive manual work, while it is relatively easy to obtain semi-structured tables from many sources directly, or build them automatically. In this paper, we build an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end system</a> to answer multiple choice questions with <a href=https://en.wikipedia.org/wiki/Semi-structured_model>semi-structured tables</a> as its knowledge. Our <a href=https://en.wikipedia.org/wiki/System>system</a> answers queries by two steps. First, it finds the most similar tables. Then the <a href=https://en.wikipedia.org/wiki/System>system</a> measures the relevance between each question and candidate table cells, and choose the most related cell as the source of answer. The <a href=https://en.wikipedia.org/wiki/System>system</a> is evaluated with TabMCQ dataset, and gets a huge improvement compared to the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1166/>LCQMC : A Large-scale Chinese Question Matching Corpus<span class=acl-fixed-case>LCQMC</span>:A Large-scale <span class=acl-fixed-case>C</span>hinese Question Matching Corpus</a></strong><br><a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/c/chong-deng/>Chong Deng</a>
|
<a href=/people/h/huajun-zeng/>Huajun Zeng</a>
|
<a href=/people/j/jing-chen/>Jing Chen</a>
|
<a href=/people/d/dongfang-li/>Dongfang Li</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1166><div class="card-body p-3 small">The lack of large-scale question matching corpora greatly limits the development of matching methods in question answering (QA) system, especially for non-English languages. To ameliorate this situation, in this paper, we introduce a large-scale Chinese question matching corpus (named LCQMC), which is released to the public1. LCQMC is more general than paraphrase corpus as it focuses on intent matching rather than <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a>. How to collect a large number of question pairs in variant linguistic forms, which may present the same intent, is the key point for such corpus construction. In this paper, we first use a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to collect large-scale question pairs related to high-frequency words from various domains, then filter irrelevant pairs by the <a href=https://en.wikipedia.org/wiki/Wasserstein_distance>Wasserstein distance</a>, and finally recruit three annotators to manually check the left pairs. After this process, a question matching corpus that contains 260,068 question pairs is constructed. In order to verify the LCQMC corpus, we split it into three parts, i.e., a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training set</a> containing 238,766 question pairs, a development set with 8,802 question pairs, and a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>test set</a> with 12,500 question pairs, and test several well-known sentence matching methods on it. The experimental results not only demonstrate the good quality of LCQMC but also provide solid baseline performance for further researches on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1168/>Transfer Learning for Entity Recognition of Novel Classes</a></strong><br><a href=/people/j/juan-diego-rodriguez/>Juan Diego Rodriguez</a>
|
<a href=/people/a/adam-caldwell/>Adam Caldwell</a>
|
<a href=/people/a/alex-liu/>Alexander Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1168><div class="card-body p-3 small">In this reproduction paper, we replicate and extend several past studies on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity recognition</a>. In particular, we are interested in entity recognition problems where the class labels in the source and target domains are different. Our work is the first direct comparison of these previously published approaches in this problem setting. In addition, we perform experiments on seven new source / target corpus pairs, nearly doubling the total number of corpus pairs that have been studied in all past work combined. Our results empirically demonstrate when each of the published <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> tends to do well. In particular, simpler approaches often work best when there is very little labeled target data, while neural transfer approaches tend to do better when there is more labeled target data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1169/>Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models</a></strong><br><a href=/people/h/hussein-al-olimat/>Hussein Al-Olimat</a>
|
<a href=/people/k/krishnaprasad-thirunarayan/>Krishnaprasad Thirunarayan</a>
|
<a href=/people/v/valerie-shalin/>Valerie Shalin</a>
|
<a href=/people/a/amit-sheth/>Amit Sheth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1169><div class="card-body p-3 small">Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names (Carroll, 1983), challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as <a href=https://en.wikipedia.org/wiki/OpenStreetMap>OpenStreetMap (OSM)</a>, <a href=https://en.wikipedia.org/wiki/Geonames>Geonames</a>, and <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>. This permits evaluation of an observed <a href=https://en.wikipedia.org/wiki/N-gram>n-gram</a> in Twitter targeted text as a legitimate location name variant from the same <a href=https://en.wikipedia.org/wiki/Context_(language_use)>location-context</a>. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool (LNEx) handles abbreviations and automatically filters and augments the location names in gazetteers (handling name contractions and auxiliary contents) to help detect the boundaries of multi-word location names and thereby delimit them in texts. We evaluated our approach on 4,500 event-specific tweets from three targeted streams to compare the performance of LNEx against that of ten state-of-the-art taggers that rely on standard semantic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179 %, outperforming all <a href=https://en.wikipedia.org/wiki/Tag_team>taggers</a>. Further, LNEx is capable of <a href=https://en.wikipedia.org/wiki/Stream_processing>stream processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1170/>The APVA-TURBO Approach To Question Answering in Knowledge Base<span class=acl-fixed-case>APVA</span>-<span class=acl-fixed-case>TURBO</span> Approach To Question Answering in Knowledge Base</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/r/richong-zhang/>Richong Zhang</a>
|
<a href=/people/c/cheng-xu/>Cheng Xu</a>
|
<a href=/people/y/yongyi-mao/>Yongyi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1170><div class="card-body p-3 small">In this paper, we study the problem of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We identify that the primary bottleneck in this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is the difficulty in accurately predicting the relations connecting the subject entity to the object entities. We advocate a new model architecture, APVA, which includes a verification mechanism responsible for checking the correctness of predicted relations. The APVA framework naturally supports a well-principled iterative training procedure, which we call turbo training. We demonstrate via experiments that the APVA-TUBRO approach drastically improves the <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1171/>An Interpretable Reasoning Network for Multi-Relation Question Answering</a></strong><br><a href=/people/m/mantong-zhou/>Mantong Zhou</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1171><div class="card-body p-3 small">Multi-relation Question Answering is a challenging task, due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. In this paper, we present a novel model called Interpretable Reasoning Network that employs an interpretable, hop-by-hop reasoning process for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The model dynamically decides which part of an input question should be analyzed at each hop ; predicts a relation that corresponds to the current parsed results ; utilizes the predicted relation to update the question representation and the state of the reasoning process ; and then drives the next-hop reasoning. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields state-of-the-art results on two datasets. More interestingly, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis, thereby allowing manual manipulation in predicting the final answer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1173/>Adaptive Learning of Local Semantic and Global Structure Representations for Text Classification</a></strong><br><a href=/people/j/jianyu-zhao/>Jianyu Zhao</a>
|
<a href=/people/z/zhiqiang-zhan/>Zhiqiang Zhan</a>
|
<a href=/people/q/qichuan-yang/>Qichuan Yang</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a>
|
<a href=/people/z/zhensheng-li/>Zhensheng Li</a>
|
<a href=/people/l/liuxin-zhang/>Liuxin Zhang</a>
|
<a href=/people/z/zhiqiang-he/>Zhiqiang He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1173><div class="card-body p-3 small">Representation learning is a key issue for most Natural Language Processing (NLP) tasks. Most existing representation models either learn little structure information or just rely on pre-defined structures, leading to degradation of performance and generalization capability. This paper focuses on learning both local semantic and global structure representations for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. In detail, we propose a novel Sandwich Neural Network (SNN) to learn semantic and structure representations automatically without relying on <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. More importantly, semantic and structure information contribute unequally to the text representation at corpus and instance level. To solve the fusion problem, we propose two strategies : Adaptive Learning Sandwich Neural Network (AL-SNN) and Self-Attention Sandwich Neural Network (SA-SNN). The former learns the weights at corpus level, and the latter further combines <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to assign the weights at <a href=https://en.wikipedia.org/wiki/Instance_(computer_science)>instance level</a>. Experimental results demonstrate that our approach achieves competitive performance on several text classification tasks, including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, question type classification and <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity classification</a>. Specifically, the accuracies are <a href=https://en.wikipedia.org/wiki/Radiocontrast_agent>MR</a> (82.1 %), SST-5 (50.4 %), <a href=https://en.wikipedia.org/wiki/Thermoelectric_effect>TREC</a> (96 %) and <a href=https://en.wikipedia.org/wiki/Radiocontrast_agent>SUBJ</a> (93.9 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1174/>Lyrics Segmentation : Textual Macrostructure Detection using Convolutions</a></strong><br><a href=/people/m/michael-fell/>Michael Fell</a>
|
<a href=/people/y/yaroslav-nechaev/>Yaroslav Nechaev</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/f/fabien-gandon/>Fabien Gandon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1174><div class="card-body p-3 small">Lyrics contain <a href=https://en.wikipedia.org/wiki/Repetition_(music)>repeated patterns</a> that are correlated with the <a href=https://en.wikipedia.org/wiki/Repetition_(music)>repetitions</a> found in the music they accompany. Repetitions in song texts have been shown to enable lyrics segmentation a fundamental prerequisite of automatically detecting the building blocks (e.g. chorus, <a href=https://en.wikipedia.org/wiki/Verse_(poetry)>verse</a>) of a <a href=https://en.wikipedia.org/wiki/Song_structure>song text</a>. In this article we improve on the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics segmentation</a> by applying a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to the task, and experiment with novel features as a step towards deeper macrostructure detection of <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1178/>Farewell Freebase : Migrating the SimpleQuestions Dataset to <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a><span class=acl-fixed-case>F</span>reebase: Migrating the <span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>Q</span>uestions Dataset to <span class=acl-fixed-case>DB</span>pedia</a></strong><br><a href=/people/m/michael-azmy/>Michael Azmy</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/i/ihab-ilyas/>Ihab Ilyas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1178><div class="card-body p-3 small">Question answering over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> is an important problem of interest both commercially and academically. There is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular SimpleQuestions dataset. The problem with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, however, is that answer triples are provided from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a>, which has been defunct for several years. As a result, it is difficult to build real-world question answering systems that are operationally deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. To address this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> to <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>. Although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. To lay the foundation for future research using this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we leverage recent work to provide simple yet strong baselines with and without <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1180/>Investigating the Working of Text Classifiers</a></strong><br><a href=/people/d/devendra-sachan/>Devendra Sachan</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1180><div class="card-body p-3 small">Text classification is one of the most widely studied tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Motivated by the principle of <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>, large multilayer neural network models have been employed for this task in an attempt to effectively utilize the constituent expressions. Almost all of the reported work train large networks using discriminative approaches, which come with a caveat of no proper capacity control, as they tend to latch on to any signal that may not generalize. Using various recent state-of-the-art approaches for text classification, we explore whether these models actually learn to compose the meaning of the sentences or still just focus on some <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> or lexicons for classifying the document. To test our hypothesis, we carefully construct <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> where the training and test splits have no direct overlap of such lexicons, but overall language structure would be similar. We study various <a href=https://en.wikipedia.org/wiki/Text_classification>text classifiers</a> and observe that there is a big performance drop on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Finally, we show that even simple models with our proposed regularization techniques, which disincentivize focusing on key lexicons, can substantially improve classification accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1181/>A Review on Deep Learning Techniques Applied to Answer Selection</a></strong><br><a href=/people/t/tuan-lai/>Tuan Manh Lai</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/s/sheng-li/>Sheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1181><div class="card-body p-3 small">Given a question and a set of candidate answers, answer selection is the task of identifying which of the candidates answers the question correctly. It is an important problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, with applications in many areas. Recently, many deep learning based methods have been proposed for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. They produce impressive performance without relying on any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> or expensive external resources. In this paper, we aim to provide a comprehensive review on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> applied to answer selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1182/>A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</a></strong><br><a href=/people/v/vikas-yadav/>Vikas Yadav</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1182><div class="card-body p-3 small">Named Entity Recognition (NER) is a key component in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, etc. NER systems have been studied and developed widely for decades, but accurate <a href=https://en.wikipedia.org/wiki/System>systems</a> using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1183" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1183/>Distantly Supervised NER with Partial Annotation Learning and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a><span class=acl-fixed-case>NER</span> with Partial Annotation Learning and Reinforcement Learning</a></strong><br><a href=/people/y/yaosheng-yang/>Yaosheng Yang</a>
|
<a href=/people/w/wenliang-chen/>Wenliang Chen</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/z/zhengqiu-he/>Zhengqiu He</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1183><div class="card-body p-3 small">A bottleneck problem with Chinese named entity recognition (NER) in new domains is the lack of annotated data. One solution is to utilize the method of distant supervision, which has been widely used in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, to automatically populate annotated training data without humancost. The distant supervision assumption here is that if a string in text is included in a predefined dictionary of entities, the string might be an entity. However, this kind of auto-generated data suffers from two main problems : incomplete and noisy annotations, which affect the performance of NER models. In this paper, we propose a novel approach which can partially solve the above problems of distant supervision for <a href=https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics>NER</a>. In our approach, to handle the incomplete problem, we apply partial annotation learning to reduce the effect of unknown labels of characters. As for noisy annotation, we design an instance selector based on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to distinguish positive sentences from auto-generated annotations. In experiments, we create two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for Chinese named entity recognition in two domains with the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1184/>Joint Neural Entity Disambiguation with Output Space Search</a></strong><br><a href=/people/h/hamed-shahbazi/>Hamed Shahbazi</a>
|
<a href=/people/x/xiaoli-fern/>Xiaoli Fern</a>
|
<a href=/people/r/reza-ghaeini/>Reza Ghaeini</a>
|
<a href=/people/c/chao-ma/>Chao Ma</a>
|
<a href=/people/r/rasha-mohammad-obeidat/>Rasha Mohammad Obeidat</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1184><div class="card-body p-3 small">In this paper, we present a novel model for entity disambiguation that combines both local contextual information and global evidences through Limited Discrepancy Search (LDS). Given an input document, we start from a complete solution constructed by a local model and conduct a <a href=https://en.wikipedia.org/wiki/Search_algorithm>search</a> in the space of possible corrections to improve the local solution from a global view point. Our search utilizes a heuristic function to focus more on the least confident local decisions and a pruning function to score the global solutions based on their local fitness and the global coherences among the predicted entities. Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify the effectiveness of our model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1185/>Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models</a></strong><br><a href=/people/l/lingzhen-chen/>Lingzhen Chen</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1185><div class="card-body p-3 small">In this paper, we propose to use a sequence to sequence model for Named Entity Recognition (NER) and we explore the effectiveness of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a progressive NER setting a Transfer Learning (TL) setting. We train an initial <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on source data and transfer it to a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> that can recognize new NE categories in the target data during a subsequent step, when the source data is no longer available. Our solution consists in : (i) to reshape and re-parametrize the output layer of the first learned model to enable the recognition of new NEs ; (ii) to leave the rest of the architecture unchanged, such that it is initialized with parameters transferred from the initial model ; and (iii) to fine tune the network on the target data. Most importantly, we design a new NER approach based on sequence to sequence (Seq2Seq) models, which can intuitively work better in our progressive setting. We compare our approach with a Bidirectional LSTM, which is a strong neural NER model. Our experiments show that the Seq2Seq model performs very well on the standard NER setting and it is more robust in the progressive setting. Our approach can recognize previously unseen NE categories while preserving the knowledge of the seen data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1188/>Aspect-based summarization of pros and cons in unstructured product reviews</a></strong><br><a href=/people/f/florian-kunneman/>Florian Kunneman</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1188><div class="card-body p-3 small">We developed three <a href=https://en.wikipedia.org/wiki/System>systems</a> for generating pros and cons summaries of product reviews. Automating this task eases the writing of product reviews, and offers readers quick access to the most important information. We compared SynPat, a system based on syntactic phrases selected on the basis of valence scores, against a neural-network-based system trained to map bag-of-words representations of reviews directly to pros and cons, and the same neural system trained on clusters of word-embedding encodings of similar pros and cons. We evaluated the <a href=https://en.wikipedia.org/wiki/System>systems</a> in two ways : first on held-out reviews with gold-standard pros and cons, and second by asking human annotators to rate the <a href=https://en.wikipedia.org/wiki/System>systems</a>&#8217; output on <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> and <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>completeness</a>. In the second evaluation, the gold-standard pros and cons were assessed along with the system output. We find that the human-generated summaries are not deemed as significantly more relevant or complete than the SynPat systems ; the latter are scored higher than the human-generated summaries on a precision metric. The neural approaches yield a lower performance in the <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human assessment</a>, and are outperformed by the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1190/>Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages : A Case Study from Modern Hebrew<span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>H</span>ebrew</a></strong><br><a href=/people/a/adam-amram/>Adam Amram</a>
|
<a href=/people/a/anat-ben-david/>Anat Ben David</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1190><div class="card-body p-3 small">This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> currently exists. We study two dimensions of representational choices : (i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based). We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs fully-connected, convolutional or recurrent. Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>task accuracy</a>, due to their better <a href=https://en.wikipedia.org/wiki/Object-relational_mapping>OOV coverage</a>. To empirically study these effects, we develop a new sentiment analysis benchmark for <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, based on 12 K social media comments, and provide two instances of these data : in token-based and morpheme-based settings. Our experiments show that representation choices empirical effects vary with architecture type. While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morpheme-based representation, in accord with the hypothesis that explicit morphological information may help generalize. Our endeavour also delivers the first state-of-the-art broad-coverage sentiment analyzer for <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, with over 89 % accuracy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks&#8217; task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1191/>Scoring and Classifying Implicit Positive Interpretations : A Challenge of Class Imbalance</a></strong><br><a href=/people/c/chantal-van-son/>Chantal van Son</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1191><div class="card-body p-3 small">This paper reports on a reimplementation of a <a href=https://en.wikipedia.org/wiki/System>system</a> on detecting implicit positive meaning from negated statements. In the original <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> experiment, different positive interpretations per negation are scored according to their likelihood. We convert the scores to classes and report our results on both the regression and classification tasks. We show that a baseline taking the mean score or most frequent class is hard to beat because of class imbalance in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Our error analysis indicates that an approach that takes the <a href=https://en.wikipedia.org/wiki/Information_structure>information structure</a> into account (i.e. which information is new or contrastive) may be promising, which requires looking beyond the syntactic and semantic characteristics of negated statements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1192/>Exploratory Neural Relation Classification for Domain Knowledge Acquisition</a></strong><br><a href=/people/y/yan-fan/>Yan Fan</a>
|
<a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1192><div class="card-body p-3 small">The state-of-the-art methods for relation classification are primarily based on deep neural net- works. This kind of supervised learning method suffers from not only limited training data, but also the large number of low-frequency relations in specific domains. In this paper, we propose the task of exploratory relation classification for domain knowledge harvesting. The goal is to learn a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> on pre-defined relations and discover new relations expressed in texts. A dynamically structured neural network is introduced to classify entity pairs to a continuously expanded relation set. We further propose the similarity sensitive Chinese restaurant process to discover new relations. Experiments conducted on a large corpus show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, while new relations are discovered with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1193/>Who is Killed by Police : Introducing Supervised Attention for Hierarchical LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1193><div class="card-body p-3 small">Finding names of people killed by police has become increasingly important as police shootings get more and more public attention (police killing detection). Unfortunately, there has been not much work in the literature addressing this problem. The early work in this field (Keith etal., 2017) proposed a distant supervision framework based on Expectation Maximization (EM) to deal with the multiple appearances of the names in documents. However, such EM-based framework can not take full advantages of deep learning models, necessitating the use of handdesigned features to improve the detection performance. In this work, we present a novel deep learning method to solve the problem of police killing recognition. The proposed method relies on hierarchical LSTMs to model the multiple sentences that contain the person names of interests, and introduce supervised attention mechanisms based on semantical word lists and dependency trees to upweight the important contextual words. Our experiments demonstrate the benefits of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and yield the state-of-the-art performance for police killing detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1194/>Open Information Extraction from Conjunctive Sentences</a></strong><br><a href=/people/s/swarnadeep-saha/>Swarnadeep Saha</a>
|
<a href=/people/m/mausam/>Mausam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1194><div class="card-body p-3 small">We develop CALM, a coordination analyzer that improves upon the conjuncts identified from dependency parses. It uses a language model based scoring and several linguistic constraints to search over hierarchical conjunct boundaries (for nested coordination). By splitting a conjunctive sentence around these conjuncts, CALM outputs several simple sentences. We demonstrate the value of our coordination analyzer in the end task of Open Information Extraction (Open IE). State-of-the-art Open IE systems lose substantial yield due to ineffective processing of conjunctive sentences. Our Open IE system, CALMIE, performs extraction over the simple sentences identified by CALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1196 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1196/>An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification</a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1196><div class="card-body p-3 small">Several semi-supervised representation learning methods have been proposed recently that mitigate the drawbacks of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> : they reduce the amount of <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> introduced by iterative approaches through <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> ; others address the sparsity of data through the learning of custom, dense representation for the information modeled. In this work, we are the first to adapt three of these methods, most of which have been originally proposed for <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>, to an information extraction task, specifically, named entity classification. Further, we perform a rigorous comparative analysis on two distinct datasets. Our analysis yields several important observations. First, all <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning methods</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised methods</a> that do not rely on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. To the best of our knowledge, we report the latest state-of-the-art results on the semi-supervised named entity classification task. Second, one-shot learning methods clearly outperform iterative representation learning approaches. Lastly, one of the best performers relies on the mean teacher framework (Tarvainen and Valpola, 2017), a simple teacher / student approach that is independent of the underlying task-specific model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1197/>Multimodal Grounding for <a href=https://en.wikipedia.org/wiki/Language_processing>Language Processing</a></a></strong><br><a href=/people/l/lisa-beinborn/>Lisa Beinborn</a>
|
<a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1197><div class="card-body p-3 small">This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1199/>Grounded Textual Entailment</a></strong><br><a href=/people/h/hoa-trong-vu/>Hoa Trong Vu</a>
|
<a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/a/aliia-erofeeva/>Aliia Erofeeva</a>
|
<a href=/people/s/somayeh-jafaritazehjani/>Somayeh Jafaritazehjan</a>
|
<a href=/people/g/guido-linders/>Guido Linders</a>
|
<a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/a/alberto-testoni/>Alberto Testoni</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1199><div class="card-body p-3 small">Capturing semantic relations between sentences, such as <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, is a long-standing challenge for <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>. Logic-based models analyse <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can perform better if, in addition to <a href=https://en.wikipedia.org/wiki/P_(complexity)>P</a> and H, there is also an <a href=https://en.wikipedia.org/wiki/Image>image</a> (corresponding to the relevant world or situation). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare blind and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing grounding in an optimal fashion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1201/>Hybrid Attention based Multimodal Network for Spoken Language Classification</a></strong><br><a href=/people/y/yue-gu/>Yue Gu</a>
|
<a href=/people/k/kangning-yang/>Kangning Yang</a>
|
<a href=/people/s/shiyu-fu/>Shiyu Fu</a>
|
<a href=/people/s/shuhong-chen/>Shuhong Chen</a>
|
<a href=/people/x/xinyu-li/>Xinyu Li</a>
|
<a href=/people/i/ivan-marsic/>Ivan Marsic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1201><div class="card-body p-3 small">We examine the utility of linguistic content and vocal characteristics for multimodal deep learning in human spoken language understanding. We present a deep multimodal network with both feature attention and modality attention to classify utterance-level speech data. The proposed hybrid attention architecture helps the system focus on learning informative representations for both modality-specific feature extraction and model fusion. The experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves state-of-the-art or competitive results on three published multimodal datasets. We also demonstrated the effectiveness and generalization of our system on a medical speech dataset from an actual trauma scenario. Furthermore, we provided a detailed comparison and analysis of traditional approaches and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> on both <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and fusion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1202/>Exploring the Influence of Spelling Errors on Lexical Variation Measures</a></strong><br><a href=/people/r/ryo-nagata/>Ryo Nagata</a>
|
<a href=/people/t/taisei-sato/>Taisei Sato</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1202><div class="card-body p-3 small">This paper explores the influence of spelling errors on lexical variation measures. Lexical richness measures such as Type-Token Ration (TTR) and Yule&#8217;s K are often used for learner English analysis and assessment. When applied to <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>learner English</a>, however, they can be unreliable because of the spelling errors appearing in <a href=https://en.wikipedia.org/wiki/Information_technology>it</a>. Namely, they are, directly or indirectly, based on the counts of distinct word types, and spelling errors undesirably increase the number of distinct words. This paper introduces and examines the hypothesis that lexical richness measures become unstable in <a href=https://en.wikipedia.org/wiki/English_language>learner English</a> because of spelling errors. Specifically, it tests the hypothesis on English learner corpora of three groups (middle school, high school, and college students). To be precise, it estimates the difference in TTR and Yule&#8217;s K caused by spelling errors, by calculating their values before and after spelling errors are manually corrected. Furthermore, it examines the results theoretically and empirically to deepen the understanding of the influence of spelling errors on them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1203/>Stance Detection with Hierarchical Attention Network</a></strong><br><a href=/people/q/qingying-sun/>Qingying Sun</a>
|
<a href=/people/z/zhongqing-wang/>Zhongqing Wang</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1203><div class="card-body p-3 small">Stance detection aims to assign a stance label (for or against) to a post toward a specific target. Recently, there is a growing interest in using neural models to detect <a href=https://en.wikipedia.org/wiki/Stance_(linguistics)>stance of documents</a>. Most of these works model the sequence of words to learn document representation. However, much linguistic information, such as polarity and arguments of the document, is correlated with the stance of the document, and can inspire us to explore the <a href=https://en.wikipedia.org/wiki/Stance_(linguistics)>stance</a>. Hence, we present a neural model to fully employ various <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> to construct the document representation. In addition, since the influences of different <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> are different, we propose a hierarchical attention network to weigh the importance of various <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>, and learn the mutual attention between the document and the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>. The experimental results on two datasets demonstrate the effectiveness of the proposed hierarchical attention neural model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1205/>Retrofitting <a href=https://en.wikipedia.org/wiki/Distribution_(computing)>Distributional Embeddings</a> to <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graphs</a> with Functional Relations</a></strong><br><a href=/people/b/ben-lengerich/>Ben Lengerich</a>
|
<a href=/people/a/andrew-maas/>Andrew Maas</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1205><div class="card-body p-3 small">Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>. Methods for retrofitting pre-trained entity representations to the structure of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> typically assume that entities are embedded in a <a href=https://en.wikipedia.org/wiki/Connectivity_(graph_theory)>connected space</a> and that relations imply similarity. However, useful <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drugdisease treatment pairs in a large, complex health knowledge graph.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1206/>Context-Sensitive Generation of Open-Domain Conversational Responses</a></strong><br><a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/y/yifa-wang/>Yifa Wang</a>
|
<a href=/people/q/qingfu-zhu/>Qingfu Zhu</a>
|
<a href=/people/l/lingzhi-li/>Lingzhi Li</a>
|
<a href=/people/l/lianqiang-zhou/>Lianqiang Zhou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1206><div class="card-body p-3 small">Despite the success of existing works on single-turn conversation generation, taking the coherence in consideration, human conversing is actually a context-sensitive process. Inspired by the existing studies, this paper proposed the static and dynamic attention based approaches for context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1207/>A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction<span class=acl-fixed-case>LSTM</span> Approach with Sub-Word Embeddings for <span class=acl-fixed-case>M</span>ongolian Phrase Break Prediction</a></strong><br><a href=/people/r/rui-liu/>Rui Liu</a>
|
<a href=/people/f/feilong-bao/>Feilong Bao</a>
|
<a href=/people/g/guanglai-gao/>Guanglai Gao</a>
|
<a href=/people/h/hui-zhang/>Hui Zhang</a>
|
<a href=/people/y/yonghe-wang/>Yonghe Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1207><div class="card-body p-3 small">In this paper, we first utilize the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> that focuses on sub-word units to the Mongolian Phrase Break (PB) prediction task by using Long-Short-Term-Memory (LSTM) model. Mongolian is an <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative language</a>. Each root can be followed by several suffixes to form probably millions of words, but the existing Mongolian corpus is not enough to build a robust entire <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, thus it suffers a serious data sparse problem and brings a great difficulty for Mongolian PB prediction. To solve this problem, we look at sub-word units in <a href=https://en.wikipedia.org/wiki/Mongolian_language>Mongolian word</a>, and encode their information to a meaningful representation, then fed it to LSTM to decode the best corresponding PB label. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms traditional CRF model using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>manually features</a> and obtains 7.49 % F-Measure gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1208/>Synonymy in Bilingual Context : The CzEngClass Lexicon<span class=acl-fixed-case>C</span>z<span class=acl-fixed-case>E</span>ng<span class=acl-fixed-case>C</span>lass Lexicon</a></strong><br><a href=/people/z/zdenka-uresova/>ZdeÅka UreÅ¡ovÃ¡</a>
|
<a href=/people/e/eva-fucikova/>Eva FuÄÃ­kovÃ¡</a>
|
<a href=/people/e/eva-hajicova/>Eva HajiÄovÃ¡</a>
|
<a href=/people/j/jan-hajic/>Jan HajiÄ</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1208><div class="card-body p-3 small">This paper describes CzEngClass, a bilingual lexical resource being built to investigate verbal synonymy in bilingual context and to relate semantic roles common to one synonym class to verb arguments (verb valency). In addition, the resource is linked to existing resources with the same of a similar aim : <a href=https://en.wikipedia.org/wiki/English_language>English and Czech WordNet</a>, FrameNet, PropBank, VerbNet (SemLink), and valency lexicons for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> (PDT-Vallex, Vallex, and EngVallex). There are several goals of this work and resource : (a) to provide gold standard data for automatic experiments in the future (such as automatic discovery of synonym classes, word sense disambiguation, assignment of classes to occurrences of verbs in text, coreferential linking of verb and event arguments in text, etc.), (b) to build a core (bilingual) lexicon linked to existing resources, for comparative studies and possibly for training automatic tools, and (c) to enrich the annotation of a parallel treebank, the Prague Czech English Dependency Treebank, which so far contained valency annotation but has not linked synonymous senses of verbs together. The method used for extracting the synonym classes is a semi-automatic process with a substantial amount of manual work during filtering, role assignment to classes and individual Class members&#8217; arguments, and linking to the external lexical resources. We present the first version with 200 classes (about 1800 verbs) and evaluate interannotator agreement using several <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1210/>Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech Tagging<span class=acl-fixed-case>K</span>orean Morphological Analysis and Part-of-Speech Tagging</a></strong><br><a href=/people/a/andrew-matteson/>Andrew Matteson</a>
|
<a href=/people/c/chanhee-lee/>Chanhee Lee</a>
|
<a href=/people/y/youngbum-kim/>Youngbum Kim</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1210><div class="card-body p-3 small">Due to the fact that <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> is a highly agglutinative, character-rich language, previous work on <a href=https://en.wikipedia.org/wiki/Korean_language>Korean morphological analysis</a> typically employs the use of sub-character features known as <a href=https://en.wikipedia.org/wiki/Grapheme>graphemes</a> or otherwise utilizes comprehensive prior linguistic knowledge (i.e., a dictionary of known morphological transformation forms, or actions). These models have been created with the assumption that character-level, dictionary-less morphological analysis was intractable due to the number of actions required. We present, in this study, a multi-stage action-based model that can perform morphological transformation and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> using arbitrary units of input and apply it to the case of character-level Korean morphological analysis. Among models that do not employ prior linguistic knowledge, we achieve state-of-the-art word and sentence-level tagging accuracy with the Sejong Korean corpus using our proposed data-driven Bi-LSTM model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1212/>Real-time Change Point Detection using On-line Topic Models</a></strong><br><a href=/people/y/yunli-wang/>Yunli Wang</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1212><div class="card-body p-3 small">Detecting changes within an unfolding event in real time from news articles or <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> enables to react promptly to serious issues in public safety, public health or natural disasters. In this study, we use on-line Latent Dirichlet Allocation (LDA) to model shifts in topics, and apply on-line change point detection (CPD) algorithms to detect when significant changes happen. We describe an on-line Bayesian change point detection algorithm that we use to detect topic changes from on-line LDA output. Extensive experiments on social media data and news articles show the benefits of on-line LDA versus standard LDA, and of on-line change point detection compared to off-line algorithms. This yields F-scores up to 52 % on the detection of significant real-life changes from these document streams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1213/>Automatically Creating a Lexicon of Verbal Polarity Shifters : Mono- and Cross-lingual Methods for <a href=https://en.wikipedia.org/wiki/German_language>German</a><span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/m/marc-schulder/>Marc Schulder</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1213><div class="card-body p-3 small">In this paper we use methods for creating a large lexicon of verbal polarity shifters and apply them to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Polarity shifters are content words that can move the polarity of a phrase towards its opposite, such as the verb abandon in abandon all hope. This is similar to how <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a> like not can influence <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>polarity</a>. Both shifters and <a href=https://en.wikipedia.org/wiki/Negation>negation</a> are required for high precision sentiment analysis. Lists of negation words are available for many languages, but the only language for which a sizable lexicon of verbal polarity shifters exists is <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> was created by bootstrapping a sample of annotated verbs with a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> that uses a set of data- and resource-driven features. We reproduce and adapt this approach to create a German lexicon of verbal polarity shifters. Thereby, we confirm that the approach works for multiple languages. We further improve <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> by leveraging cross-lingual information from the English shifter lexicon. Using this improved approach, we bootstrap a large number of German verbal polarity shifters, reducing the annotation effort drastically. The resulting German lexicon of verbal polarity shifters is made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1215/>One vs. Many QA Matching with both Word-level and Sentence-level Attention Network<span class=acl-fixed-case>QA</span> Matching with both Word-level and Sentence-level Attention Network</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/l/luo-si/>Luo Si</a>
|
<a href=/people/x/xiaozhong-liu/>Xiaozhong Liu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1215><div class="card-body p-3 small">Question-Answer (QA) matching is a fundamental task in the Natural Language Processing community. In this paper, we first build a novel QA matching corpus with informal text which is collected from a product reviewing website. Then, we propose a novel QA matching approach, namely One vs. Many Matching, which aims to address the novel scenario where one question sentence often has an answer with multiple sentences. Furthermore, we improve our matching approach by employing both word-level and sentence-level attentions for solving the noisy problem in the informal text. Empirical studies demonstrate the effectiveness of the proposed approach to question-answer matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1218/>ReSyf : a French lexicon with ranked synonyms<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>S</span>yf: a <span class=acl-fixed-case>F</span>rench lexicon with ranked synonyms</a></strong><br><a href=/people/m/mokhtar-b-billami/>Mokhtar B. Billami</a>
|
<a href=/people/t/thomas-francois/>Thomas FranÃ§ois</a>
|
<a href=/people/n/nuria-gala/>NÃºria Gala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1218><div class="card-body p-3 small">In this article, we present ReSyf, a lexical resource of monolingual synonyms ranked according to their difficulty to be read and understood by native learners of <a href=https://en.wikipedia.org/wiki/French_language>French</a>. The <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> come from an existing lexical network and they have been semantically disambiguated and refined. A ranking algorithm, based on a wide range of linguistic features and validated through an evaluation campaign with human annotators, automatically sorts the <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> corresponding to a given <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a> by <a href=https://en.wikipedia.org/wiki/Reading_difficulty>reading difficulty</a>. ReSyf is freely available and will be integrated into a <a href=https://en.wikipedia.org/wiki/Web_application>web platform</a> for reading assistance. It can also be applied to perform <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a> of <a href=https://en.wikipedia.org/wiki/French_language>French texts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1219/>If youâve seen some, youâve seen them all : Identifying variants of multiword expressions</a></strong><br><a href=/people/c/caroline-pasquer/>Caroline Pasquer</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/j/jean-yves-antoine/>Jean-Yves Antoine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1219><div class="card-body p-3 small">Multiword expressions, especially verbal ones (VMWEs), show idiosyncratic variability, which is challenging for NLP applications, hence the need for VMWE identification. We focus on the task of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variant identification</a>, i.e. identifying variants of previously seen VMWEs, whatever their surface form. We model the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a>. Syntactic subtrees with previously seen combinations of lemmas are first extracted, and then classified on the basis of features relevant to morpho-syntactic variation of VMWEs. Feature values are both absolute, i.e. hold for a particular VMWE candidate, and relative, i.e. based on comparing a candidate with previously seen VMWEs. This approach outperforms a <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baseline</a> by 4 percent points of <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> on a <a href=https://en.wikipedia.org/wiki/French_language>French corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1221/>Using Word Embeddings for Unsupervised Acronym Disambiguation</a></strong><br><a href=/people/j/jean-charbonnier/>Jean Charbonnier</a>
|
<a href=/people/c/christian-wartena/>Christian Wartena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1221><div class="card-body p-3 small">Scientific papers from all disciplines contain many <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> and acronyms. In many cases these <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> are ambiguous. We present a method to choose the contextual correct definition of an acronym that does not require training for each <a href=https://en.wikipedia.org/wiki/Acronym>acronym</a> and thus can be applied to a large number of different <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> with only few instances. We constructed a set of 19,954 examples of 4,365 ambiguous acronyms from image captions in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> along with their contextually correct definition from different domains. We learn word embeddings for all words in the corpus and compare the averaged context vector of the words in the expansion of an acronym with the weighted average vector of the words in the context of the acronym. We show that this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> clearly outperforms (classical) cosine similarity. Furthermore, we show that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned from a 1 billion word corpus of scientific texts outperform <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned on much large general corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1222/>Indigenous language technologies in Canada : Assessment, challenges, and successes<span class=acl-fixed-case>C</span>anada: Assessment, challenges, and successes</a></strong><br><a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/r/roland-kuhn/>Roland Kuhn</a>
|
<a href=/people/a/aidan-pine/>Aidan Pine</a>
|
<a href=/people/a/antti-arppe/>Antti Arppe</a>
|
<a href=/people/c/christopher-cox/>Christopher Cox</a>
|
<a href=/people/m/marie-odile-junker/>Marie-Odile Junker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1222><div class="card-body p-3 small">In this article, we discuss which text, speech, and image technologies have been developed, and would be feasible to develop, for the approximately 60 <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>Indigenous languages</a> spoken in Canada. In particular, we concentrate on technologies that may be feasible to develop for most or all of these <a href=https://en.wikipedia.org/wiki/Language>languages</a>, not just those that may be feasible for the few most-resourced of these. We assess past achievements and consider future horizons for Indigenous language transliteration, text prediction, spell-checking, approximate search, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Speaker_diarization>speaker diarization</a>, <a href=https://en.wikipedia.org/wiki/Speech_synthesis>speech synthesis</a>, <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition</a>, and computer-aided language learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1223/>Pluralizing Nouns across Agglutinating Bantu Languages<span class=acl-fixed-case>B</span>antu Languages</a></strong><br><a href=/people/j/joan-byamugisha/>Joan Byamugisha</a>
|
<a href=/people/c/c-maria-keet/>C. Maria Keet</a>
|
<a href=/people/b/brian-derenzi/>Brian DeRenzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1223><div class="card-body p-3 small">Text generation may require the pluralization of nouns, such as in <a href=https://en.wikipedia.org/wiki/Context-sensitive_user_interface>context-sensitive user interfaces</a> and in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> more broadly. While this has been solved for the widely-used languages, this is still a challenge for the languages in the <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu language family</a>. Pluralization results obtained for <a href=https://en.wikipedia.org/wiki/Zulu_language>isiZulu</a> and Runyankore showed there were similarities in approach, including the need to combine <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> with <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, despite belonging to different language zones. This suggests that <a href=https://en.wikipedia.org/wiki/Bootstrapping>bootstrapping</a> and <a href=https://en.wikipedia.org/wiki/Generalizability>generalizability</a> might be feasible. We investigated this systematically for seven languages across three different <a href=https://en.wikipedia.org/wiki/Guthrie_classification>Guthrie language zones</a>. The first outcome is that Meinhof&#8217;s 1948 specification of the noun classes are indeed inadequate for computational purposes for all examined languages, due to non-determinism in prefixes, and we thus redefined the characteristic noun class tables of 29 noun classes into 53. The second main result is that the generic pluralizer achieved over 93 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in coverage testing and over 94 % on a <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>random sample</a>. This is comparable to the language-specific isiZulu and Runyankore pluralizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1224/>Automatically Extracting Qualia Relations for the Rich Event Ontology</a></strong><br><a href=/people/g/ghazaleh-kazeminejad/>Ghazaleh Kazeminejad</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1224><div class="card-body p-3 small">Commonsense, real-world knowledge about the events that entities or things in the world are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we focus on automatically extracting information about (1) the events that typically bring about certain entities (origins), (2) the <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> that are the typical functions of entities, and (3) part-whole relationships in entities. These correspond to the agentive, telic and constitutive qualia central to the Generative Lexicon. We describe our motivations and methods for extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO) and show that human annotators overwhelmingly find the information extracted to be reasonable. Because <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> provide a way of structuring this information and making it accessible to agents and computational systems generally, efforts are underway to incorporate the extracted information to an ontology hub of Natural Language Processing semantic role labeling resources, the Rich Event Ontology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1227" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1227/>Using Formulaic Expressions in Writing Assistance Systems</a></strong><br><a href=/people/k/kenichi-iwatsuki/>Kenichi Iwatsuki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1227><div class="card-body p-3 small">Formulaic expressions (FEs) used in scholarly papers, such as &#8216;there has been little discussion about&#8217;, are helpful for non-native English speakers. However, it is time-consuming for users to manually search for an appropriate expression every time they want to consult FE dictionaries. For this reason, we tackle the task of semantic searches of FE dictionaries. At the start of our research, we identified two salient difficulties in this task. First, the paucity of example sentences in existing FE dictionaries results in a shortage of context information, which is necessary for acquiring semantic representation of FEs. Second, while a semantic category label is assigned to each FE in many FE dictionaries, it is difficult to predict the labels from user input, forcing users to manually designate the semantic category when searching. To address these difficulties, we propose a new framework for semantic searches of FEs and propose a new method to leverage both existing <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and domain sentence corpora. Further, we expand an existing FE dictionary to consider building a more comprehensive and domain-specific FE dictionary and to verify the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1228/>Whatâs in Your Embedding, And How It Predicts Task Performance</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/s/shashwath-hosur-ananthakrishna/>Shashwath Hosur Ananthakrishna</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1228><div class="card-body p-3 small">Attempts to find a single technique for general-purpose intrinsic evaluation of word embeddings have so far not been successful. We present a new approach based on scaled-up qualitative analysis of word vector neighborhoods that quantifies interpretable characteristics of a given <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (e.g. its preference for <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> or shared morphological forms as nearest neighbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally a more principled, hypothesis-driven approach to development of distributional semantic representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1229/>Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph</a></strong><br><a href=/people/d/dongsuk-o/>Dongsuk O</a>
|
<a href=/people/s/sunjae-kwon/>Sunjae Kwon</a>
|
<a href=/people/k/kyungsun-kim/>Kyungsun Kim</a>
|
<a href=/people/y/youngjoong-ko/>Youngjoong Ko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1229><div class="card-body p-3 small">Word sense disambiguation (WSD) is the task to determine the word sense according to its context. Many existing WSD studies have been using an external knowledge-based unsupervised approach because it has fewer word set constraints than supervised approaches requiring training data. In this paper, we propose a new WSD method to generate the context of an ambiguous word by using similarities between an ambiguous word and words in the input document. In addition, to leverage our WSD method, we further propose a new word similarity calculation method based on the semantic network structure of <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>. We evaluate the proposed methods on the SemEval-13 and SemEval-15 for English WSD dataset. Experimental results demonstrate that the proposed WSD method significantly improves the baseline WSD method. Furthermore, our WSD system outperforms the state-of-the-art WSD systems in the Semeval-13 dataset. Finally, it has higher performance than the state-of-the-art unsupervised knowledge-based WSD system in the average performance of both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1230/>Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator</a></strong><br><a href=/people/b/badri-narayana-patro/>Badri Narayana Patro</a>
|
<a href=/people/v/vinod-kumar-kurmi/>Vinod Kumar Kurmi</a>
|
<a href=/people/s/sandeep-kumar/>Sandeep Kumar</a>
|
<a href=/people/v/vinay-namboodiri/>Vinay Namboodiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1230><div class="card-body p-3 small">In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a>, we would like the generated <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a> to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> that is trained with a suitable <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>. Our <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> penalizes paraphrase sentence embedding distances from being too large. This <a href=https://en.wikipedia.org/wiki/Lossless_compression>loss</a> is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis task</a>. The proposed method results in semantic embeddings and outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1231/>A Reassessment of Reference-Based Grammatical Error Correction Metrics</a></strong><br><a href=/people/s/shamil-chollampatt/>Shamil Chollampatt</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1231><div class="card-body p-3 small">Several metrics have been proposed for evaluating grammatical error correction (GEC) systems based on <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, and adequacy of the output sentences. Previous studies of the correlation of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with human quality judgments were inconclusive, due to the lack of appropriate <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance tests</a>, discrepancies in the methods, and choice of datasets used. In this paper, we re-evaluate reference-based GEC metrics by measuring the system-level correlations with humans on a large dataset of human judgments of GEC outputs, and by properly conducting statistical significance tests. Our results show no significant advantage of GLEU over MaxMatch (M2), contradicting previous studies that claim GLEU to be superior. For a finer-grained analysis, we additionally evaluate these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for their agreement with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. Our sentence-level analysis indicates that comparing GLEU and M2, one <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> may be more useful than the other depending on the scenario. We further qualitatively analyze these metrics and our findings show that apart from being less interpretable and non-deterministic, GLEU also produces counter-intuitive scores in commonly occurring test examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1232 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1232/>Information Aggregation via <a href=https://en.wikipedia.org/wiki/Dynamic_routing>Dynamic Routing</a> for Sequence Encoding</a></strong><br><a href=/people/j/jingjing-gong/>Jingjing Gong</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/s/shaojing-wang/>Shaojing Wang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1232><div class="card-body p-3 small">While much progress has been made in how to encode a text sequence into a sequence of vectors, less attention has been paid to how to aggregate these preceding vectors (outputs of RNN / CNN) into fixed-size encoding vector. Usually, a simple max or average pooling is used, which is a bottom-up and passive way of aggregation and lack of guidance by task information. In this paper, we propose an aggregation mechanism to obtain a fixed-size encoding with a dynamic routing policy. The dynamic routing policy is dynamically deciding that what and how much information need be transferred from each word to the final encoding of the text sequence. Following the work of Capsule Network, we design two dynamic routing policies to aggregate the outputs of RNN / CNN encoding layer into a final encoding vector. Compared to the other aggregation methods, <a href=https://en.wikipedia.org/wiki/Dynamic_routing>dynamic routing</a> can refine the messages according to the state of final encoding vector. Experimental results on five text classification tasks show that our method outperforms other aggregating models by a significant margin. Related source code is released on our github page. Related source code is released on our github page.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1233/>A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?</a></strong><br><a href=/people/j/jiaxun-cai/>Jiaxun Cai</a>
|
<a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1233><div class="card-body p-3 small">Semantic role labeling (SRL) is to recognize the predicate-argument structure of a sentence, including subtasks of predicate disambiguation and argument labeling. Previous studies usually formulate the entire SRL problem into two or more subtasks. For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Using a biaffine scorer, our model directly predicts all semantic role labels for all given word pairs in the sentence without relying on any syntactic parse information. Specifically, we augment the BiLSTM encoder with a non-linear transformation to further distinguish the predicate and the argument in a given sentence, and model the semantic role labeling process as a word pair classification task by employing the biaffine attentional mechanism. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1236 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1236/>Challenges and Opportunities of Applying <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> in Business Process Management</a></strong><br><a href=/people/h/han-van-der-aa/>Han van der Aa</a>
|
<a href=/people/j/josep-carmona/>Josep Carmona</a>
|
<a href=/people/h/henrik-leopold/>Henrik Leopold</a>
|
<a href=/people/j/jan-mendling/>Jan Mendling</a>
|
<a href=/people/l/lluis-padro/>LluÃ­s PadrÃ³</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1236><div class="card-body p-3 small">The Business Process Management (BPM) field focuses in the coordination of labor so that organizational processes are smoothly executed in a way that products and services are properly delivered. At the same time, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has reached a maturity level that enables its widespread application in many contexts, thanks to publicly available frameworks. In this position paper, we show how <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> has potential in raising the benefits of <a href=https://en.wikipedia.org/wiki/Business_process_management>BPM practices</a> at different levels. Instead of being exhaustive, we show selected key challenges were a successful application of NLP techniques would facilitate the automation of particular tasks that nowadays require a significant effort to accomplish. Finally, we report on <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> that consider both the process perspective and its enhancement through <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1237/>Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection</a></strong><br><a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/v/vignesh-edithal/>Vignesh Edithal</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/george-tsatsaronis/>George Tsatsaronis</a>
|
<a href=/people/s/srinivasa-satya-sameer-kumar-chivukula/>Srinivasa Satya Sameer Kumar Chivukula</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1237><div class="card-body p-3 small">The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The <a href=https://en.wikipedia.org/wiki/System>system</a> is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a> which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on a document-level novelty detection dataset by a margin of 5 % in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1238 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1238/>What represents style in <a href=https://en.wikipedia.org/wiki/Authorship_attribution>authorship attribution</a>?</a></strong><br><a href=/people/k/kalaivani-sundararajan/>Kalaivani Sundararajan</a>
|
<a href=/people/d/damon-woodard/>Damon Woodard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1238><div class="card-body p-3 small">Authorship attribution typically uses all information representing both content and style whereas <a href=https://en.wikipedia.org/wiki/Attribution_(copyright)>attribution</a> based only on stylistic aspects may be robust in cross-domain settings. This paper analyzes different linguistic aspects that may help represent <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a>. Specifically, we study the role of <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and lexical words (nouns, verbs, adjectives and adverbs) in representing <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>style</a>. We use a purely syntactic language model to study the significance of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structures</a> in both single-domain and cross-domain attribution, i.e. cross-topic and cross-genre attribution. We show that <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> may be helpful for cross-genre attribution while cross-topic attribution and single-domain may benefit from additional lexical information. Further, pure syntactic models may not be effective by themselves and need to be used in combination with other robust models. To study the role of word choice, we perform attribution by masking all words or specific topic words corresponding to <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>, <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a>. Using a single-domain dataset, IMDB1 M reviews, we demonstrate the heavy influence of <a href=https://en.wikipedia.org/wiki/Proper_noun>common nouns</a> and <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a> in <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>attribution</a>, thereby highlighting topic interference. Using cross-domain Guardian10 dataset, we show that some common nouns, verbs, adjectives and adverbs may help with stylometric attribution as demonstrated by masking topic words corresponding to these <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts-of-speech</a>. As expected, it was observed that <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a> are heavily influenced by content and cross-domain attribution will benefit from completely masking them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1240/>Model-Free Context-Aware Word Composition</a></strong><br><a href=/people/b/bo-an/>Bo An</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1240><div class="card-body p-3 small">Word composition is a promising technique for representation learning of large linguistic units (e.g., phrases, sentences and documents). However, most of the current composition models do not take the ambiguity of words and the context outside of a linguistic unit into consideration for learning representations, and consequently suffer from the inaccurate representation of semantics. To address this issue, we propose a model-free context-aware word composition model, which employs the latent semantic information as global context for learning representations. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> attempts to resolve the <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> and <a href=https://en.wikipedia.org/wiki/Composition_(language)>word composition</a> in a unified framework. Extensive evaluation shows consistent improvements over various strong word representation / composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1241/>Learning Features from <a href=https://en.wikipedia.org/wiki/Co-occurrence>Co-occurrences</a> : A Theoretical Analysis</a></strong><br><a href=/people/y/yanpeng-li/>Yanpeng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1241><div class="card-body p-3 small">Representing a word by its co-occurrences with other words in context is an effective way to capture the meaning of the word. However, the theory behind remains a challenge. In this work, taking the example of a word classification task, we give a theoretical analysis of the approaches that represent a word X by a function f(P(C|X)), where C is a context feature, P(C|X) is the <a href=https://en.wikipedia.org/wiki/Conditional_probability>conditional probability</a> estimated from a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>, and the function f maps the co-occurrence measure to a prediction score. We investigate the impact of context feature C and the function f. We also explain the reasons why using the <a href=https://en.wikipedia.org/wiki/Co-occurrence>co-occurrences</a> with multiple context features may be better than just using a single one. In addition, based on the analysis, we propose a hypothesis about the <a href=https://en.wikipedia.org/wiki/Conditional_probability>conditional probability</a> on zero probability events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1242/>Towards a unified framework for bilingual terminology extraction of single-word and multi-word terms</a></strong><br><a href=/people/j/jingshu-liu/>Jingshu Liu</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/s/sebastian-pena-saldarriaga/>PeÃ±a Saldarriaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1242><div class="card-body p-3 small">Extracting a <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual terminology</a> for multi-word terms from comparable corpora has not been widely researched. In this work we propose a unified framework for aligning bilingual terms independently of the term lengths. We also introduce some enhancements to the context-based and the neural network based approaches. Our experiments show the effectiveness of our enhancements of previous works and the <a href=https://en.wikipedia.org/wiki/System>system</a> can be adapted in specialized domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1245" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1245/>Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1245><div class="card-body p-3 small">Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another <a href=https://en.wikipedia.org/wiki/One_(pronoun)>one</a>, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekman&#8217;s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1246/>Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning</a></strong><br><a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1246><div class="card-body p-3 small">Detection and classification of emotion categories expressed by a sentence is a challenging task due to <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity of emotion</a>. To date, most of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated on single genre and when used to predict emotion in different genre their performance drops by a large margin. To address the issue of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>, we model the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> within a joint multi-task learning framework. We train this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a multigenre emotion corpus to predict emotions across various genre. Each genre is represented as a separate task, we use soft parameter shared layers across the various tasks. our experimental results show that this model improves the results across the various genres, compared to a single genre training in the same neural net architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1247" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1247/>How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs</a></strong><br><a href=/people/d/devang-kulshreshtha/>Devang Kulshreshtha</a>
|
<a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1247><div class="card-body p-3 small">Social media based micro-blogging sites like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> have become a common source of real-time information (impacting organizations and their strategies, and are used for expressing emotions and opinions. Automated analysis of such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a> therefore rises in importance. To this end, we explore the viability of using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> on the specific task of emotion intensity prediction in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We propose a neural architecture combining convolutional and fully connected layers in a non-sequential manner-done for the first time in context of natural language based tasks. Combined with lexicon-based features along with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, our model achieves state-of-the-art performance, outperforming the previous <a href=https://en.wikipedia.org/wiki/System>system</a> by 0.044 or 4.4 % <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> on the WASSA&#8217;17 EmoInt shared task dataset. We investigate the performance of deep multi-task learning models trained for all <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> at once in a unified architecture and get encouraging results. Experiments performed on evaluating correlation between emotion pairs offer interesting insights into the relationship between them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1248" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1248/>Expressively vulgar : The socio-dynamics of <a href=https://en.wikipedia.org/wiki/Vulgarity>vulgarity</a> and its effects on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a></a></strong><br><a href=/people/i/isabel-cachola/>Isabel Cachola</a>
|
<a href=/people/e/eric-holgate/>Eric Holgate</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel PreoÅ£iuc-Pietro</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1248><div class="card-body p-3 small">Vulgarity is a common linguistic expression and is used to perform several linguistic functions. Understanding their usage can aid both linguistic and psychological phenomena as well as benefit downstream natural language processing applications such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. This study performs a large-scale, data-driven empirical analysis of vulgar words using social media data. We analyze the socio-cultural and pragmatic aspects of <a href=https://en.wikipedia.org/wiki/Vulgarity>vulgarity</a> using tweets from users with known demographics. Further, we collect <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment ratings</a> for vulgar tweets to study the relationship between the use of vulgar words and perceived sentiment and show that explicitly modeling vulgar words can boost <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1249 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1249/>Clausal Modifiers in the Grammar Matrix</a></strong><br><a href=/people/k/kristen-howell/>Kristen Howell</a>
|
<a href=/people/o/olga-zamaraeva/>Olga Zamaraeva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1249><div class="card-body p-3 small">We extend the coverage of an existing grammar customization system to clausal modifiers, also referred to as <a href=https://en.wikipedia.org/wiki/Adverbial_clause>adverbial clauses</a>. We present an analysis, taking a typologically-driven approach to account for this phenomenon across the world&#8217;s languages, which we implement in the Grammar Matrix customization system (Bender et al., 2002, 2010). Testing our <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> on testsuites from five genetically and geographically diverse languages that were not considered in development, we achieve 88.4 % coverage and 1.5 % overgeneration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1250 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1250/>Sliced Recurrent Neural Networks</a></strong><br><a href=/people/z/zeping-yu/>Zeping Yu</a>
|
<a href=/people/g/gongshen-liu/>Gongshen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1250><div class="card-body p-3 small">Recurrent neural networks have achieved great success in many NLP tasks. However, they have difficulty in <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelization</a> because of the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent structure</a>, so it takes much time to train <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNNs</a>. In this paper, we introduce sliced recurrent neural networks (SRNNs), which could be parallelized by slicing the sequences into many subsequences. SRNNs have the ability to obtain high-level information through multiple layers with few extra parameters. We prove that the standard RNN is a special case of the SRNN when we use linear activation functions. Without changing the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent units</a>, SRNNs are 136 times as fast as standard <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNNs</a> and could be even faster when we train longer sequences. Experiments on six large-scale sentiment analysis datasets show that SRNNs achieve better performance than standard RNNs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1252" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1252/>Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models<span class=acl-fixed-case>J</span>-<span class=acl-fixed-case>K</span>-fold Cross Validation To Reduce Variance When Tuning <span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/h/henry-moss/>Henry Moss</a>
|
<a href=/people/d/david-leslie/>David Leslie</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1252><div class="card-body p-3 small">K-fold cross validation (CV) is a popular method for estimating the true performance of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, allowing <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates can not be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues. Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature and instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies : sentiment classification (both general and target-specific), <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1253/>Incremental Natural Language Processing : Challenges, Strategies, and Evaluation</a></strong><br><a href=/people/a/arne-kohn/>Arne KÃ¶hn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1253><div class="card-body p-3 small">Incrementality is ubiquitous in human-human interaction and beneficial for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a>. It has been a topic of research in different parts of the NLP community, mostly with focus on the specific topic at hand even though incremental systems have to deal with similar challenges regardless of domain. In this survey, I consolidate and categorize the approaches, identifying similarities and differences in the computation and data, and show trade-offs that have to be considered. A focus lies on evaluating incremental systems because the standard <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a> often fail to capture the incremental properties of a system and coming up with a suitable evaluation scheme is non-trivial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1255/>Multi-layer Representation Fusion for Neural Machine Translation</a></strong><br><a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/f/fuxue-li/>Fuxue Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1255><div class="card-body p-3 small">Neural machine translation systems require a number of stacked layers for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a>. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and poses a risk of <a href=https://en.wikipedia.org/wiki/Information_loss>information loss</a> to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>fusion functions</a> to learn a better representation from the <a href=https://en.wikipedia.org/wiki/Call_stack>stack</a>. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1256/>Toward Better Loanword Identification in <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a> Using Cross-lingual Word Embeddings<span class=acl-fixed-case>U</span>yghur Using Cross-lingual Word Embeddings</a></strong><br><a href=/people/c/chenggang-mi/>Chenggang Mi</a>
|
<a href=/people/y/yating-yang/>Yating Yang</a>
|
<a href=/people/l/lei-wang/>Lei Wang</a>
|
<a href=/people/x/xi-zhou/>Xi Zhou</a>
|
<a href=/people/t/tonghai-jiang/>Tonghai Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1256><div class="card-body p-3 small">To enrich vocabulary of low resource settings, we proposed a novel method which identify <a href=https://en.wikipedia.org/wiki/Loanword>loanwords</a> in monolingual corpora. More specifically, we first use cross-lingual word embeddings as the core feature to generate semantically related candidates based on comparable corpora and a small bilingual lexicon ; then, a log-linear model which combines several shallow features such as pronunciation similarity and hybrid language model features to predict the final results. In this paper, we use <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a> as the receipt language and try to detect loanwords in four donor languages : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Persian_language>Persian</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We conduct two groups of experiments to evaluate the effectiveness of our proposed approach : loanword identification and OOV translation in four language pairs and eight translation directions (Uyghur-Arabic, Arabic-Uyghur, Uyghur-Chinese, Chinese-Uyghur, Uyghur-Persian, Persian-Uyghur, Uyghur-Russian, and Russian-Uyghur). Experimental results on loanword identification show that our method outperforms other baseline models significantly. Neural machine translation models integrating results of loanword identification experiments achieve the best results on OOV translation(with 0.5-0.9 BLEU improvements)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1257/>Adaptive Weighting for Neural Machine Translation</a></strong><br><a href=/people/y/yachao-li/>Yachao Li</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1257><div class="card-body p-3 small">In the popular sequence to sequence (seq2seq) neural machine translation (NMT), there exist many weighted sum models (WSMs), each of which takes a set of input and generates one output. However, the weights in a WSM are independent of each other and fixed for all inputs, suggesting that by ignoring different needs of inputs, the WSM lacks effective control on the influence of each input. In this paper, we propose adaptive weighting for WSMs to control the contribution of each input. Specifically, we apply adaptive weighting for both GRU and the output state in <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. Experimentation on Chinese-to-English translation and English-to-German translation demonstrates that the proposed adaptive weighting is able to much improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a> by achieving significant improvement of 1.49 and 0.92 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a> for the two translation tasks. Moreover, we discuss in-depth on what type of information is encoded in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and how information influences the generation of target words in the <a href=https://en.wikipedia.org/wiki/Code>decoder</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1258 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1258/>Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing</a></strong><br><a href=/people/k/kilian-gebhardt/>Kilian Gebhardt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1258><div class="card-body p-3 small">We formulate a generalization of Petrov et al. (2006)&#8217;s split / merge algorithm for interpreted regular tree grammars (Koller and Kuhlmann, 2011), which capture a large class of grammar formalisms. We evaluate its effectiveness empirically on the task of discontinuous constituent parsing with two mildly context-sensitive grammar formalisms : linear context-free rewriting systems (Vijay-Shanker et al., 1987) as well as hybrid grammars (Nederhof and Vogler, 2014).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1259 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1259/>Double Path Networks for Sequence to Sequence Learning</a></strong><br><a href=/people/k/kaitao-song/>Kaitao Song</a>
|
<a href=/people/x/xu-tan/>Xu Tan</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/j/jianfeng-lu/>Jianfeng Lu</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1259><div class="card-body p-3 small">Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable progress in recent years. Different <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a> have been used in the encoder / decoder. Among them, <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks (CNN)</a> and Self Attention Networks (SAN) are the prominent ones. The two architectures achieve similar performances but use very different ways to encode and decode context : CNN use <a href=https://en.wikipedia.org/wiki/Convolutional_layer>convolutional layers</a> to focus on the local connectivity of the sequence, while SAN uses self-attention layers to focus on global semantics. In this work we propose Double Path Networks for Sequence to Sequence learning (DPN-S2S), which leverage the advantages of both models by using double path information fusion. During the encoding step, we develop a double path architecture to maintain the information coming from different paths with convolutional layers and self-attention layers separately. To effectively use the encoded context, we develop a gated attention fusion module and use it to automatically pick up the information needed during the decoding step, which is also a double path network. By deeply integrating the two <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>paths</a>, both types of information are combined and well exploited. Experiments show that our proposed method can significantly improve the performance of sequence to sequence learning over state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1262 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1262/>Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>E</span>thiopian Languages Statistical Machine Translation</a></strong><br><a href=/people/s/solomon-teferra-abate/>Solomon Teferra Abate</a>
|
<a href=/people/m/michael-melese/>Michael Melese</a>
|
<a href=/people/m/martha-yifiru-tachbelie/>Martha Yifiru Tachbelie</a>
|
<a href=/people/m/million-meshesha/>Million Meshesha</a>
|
<a href=/people/s/solomon-atinafu/>Solomon Atinafu</a>
|
<a href=/people/w/wondwossen-mulugeta/>Wondwossen Mulugeta</a>
|
<a href=/people/y/yaregal-assabie/>Yaregal Assabie</a>
|
<a href=/people/h/hafte-abera/>Hafte Abera</a>
|
<a href=/people/b/binyam-ephrem-seyoum/>Binyam Ephrem</a>
|
<a href=/people/t/tewodros-abebe/>Tewodros Abebe</a>
|
<a href=/people/w/wondimagegnhue-tsegaye/>Wondimagegnhue Tsegaye</a>
|
<a href=/people/a/amanuel-lemma/>Amanuel Lemma</a>
|
<a href=/people/t/tsegaye-andargie/>Tsegaye Andargie</a>
|
<a href=/people/s/seifedin-shifaw/>Seifedin Shifaw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1262><div class="card-body p-3 small">In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a>, <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna</a>, <a href=https://en.wikipedia.org/wiki/Oromo_language>Afan-Oromo</a>, <a href=https://en.wikipedia.org/wiki/Wolaytta_language>Wolaytta</a> and <a href=https://en.wikipedia.org/wiki/Ge&#701;ez>Ge&#8217;ez</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> are used for conducting a bi-directional statistical machine translation experiments. The BLEU scores of the bi-directional Statistical Machine Translation (SMT) systems show a promising result. The morphological richness of the <a href=https://en.wikipedia.org/wiki/Ethiopian_Semitic_languages>Ethiopian languages</a> has a great impact on the performance of SMT specially when the targets are <a href=https://en.wikipedia.org/wiki/Ethiopian_Semitic_languages>Ethiopian languages</a>. Now we are working towards an optimal alignment for a bi-directional English-Ethiopian languages SMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1265 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1265/>Tailoring Neural Architectures for Translating from Morphologically Rich Languages</a></strong><br><a href=/people/p/peyman-passban/>Peyman Passban</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1265><div class="card-body p-3 small">A morphologically complex word (MCW) is a hierarchical constituent with meaning-preserving subunits, so word-based models which rely on surface forms might not be powerful enough to translate such structures. When translating from morphologically rich languages (MRLs), a source word could be mapped to several words or even a full sentence on the target side, which means an MCW should not be treated as an atomic unit. In order to provide better translations for <a href=https://en.wikipedia.org/wiki/Machine_translation>MRLs</a>, we boost the existing neural machine translation (NMT) architecture with a double- channel encoder and a double-attentive decoder. The main goal targeted in this research is to provide richer information on the encoder side and redesign the decoder accordingly to benefit from such information. Our experimental results demonstrate that we could achieve our goal as the proposed model outperforms existing subword- and character-based architectures and showed significant improvements on translating from <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> into <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1267/>Butterfly Effects in Frame Semantic Parsing : impact of <a href=https://en.wikipedia.org/wiki/Data_processing>data processing</a> on model ranking</a></strong><br><a href=/people/a/alexandre-kabbach/>Alexandre Kabbach</a>
|
<a href=/people/c/corentin-ribeyre/>Corentin Ribeyre</a>
|
<a href=/people/a/aurelie-herbelot/>AurÃ©lie Herbelot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1267><div class="card-body p-3 small">Knowing the state-of-the-art for a particular task is an essential component of any computational linguistics investigation. But can we be truly confident that the current state-of-the-art is indeed the best performing <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a>? In this paper, we study the case of frame semantic parsing, a well-established task with multiple shared datasets. We show that in spite of all the care taken to provide a standard evaluation resource, small variations in <a href=https://en.wikipedia.org/wiki/Data_processing>data processing</a> can have dramatic consequences for ranking parser performance. This leads us to propose an open-source standardized processing pipeline, which can be shared and reused for robust model comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1268 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1268/>Sensitivity to Input Order : Evaluation of an Incremental and Memory-Limited Bayesian Cross-Situational Word Learning Model<span class=acl-fixed-case>B</span>ayesian Cross-Situational Word Learning Model</a></strong><br><a href=/people/s/sepideh-sadeghi/>Sepideh Sadeghi</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1268><div class="card-body p-3 small">We present a variation of the incremental and memory-limited algorithm in (Sadeghi et al., 2017) for Bayesian cross-situational word learning and evaluate the model in terms of its functional performance and its sensitivity to input order. We show that the functional performance of our sub-optimal model on corpus data is close to that of its optimal counterpart (Frank et al., 2009), while only the sub-optimal model is capable of predicting the input order effects reported in experimental studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1269/>Sentence Weighting for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/s/shiqi-zhang/>Shiqi Zhang</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1269><div class="card-body p-3 small">In this paper, we propose a new sentence weighting method for the domain adaptation of neural machine translation. We introduce a domain similarity metric to evaluate the relevance between a sentence and an available entire domain dataset. The similarity of each sentence to the target domain is calculated with various methods. The computed <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> is then integrated into the training objective to weight sentences. The adaptation results on both IWSLT Chinese-English TED task and a task with only synthetic training parallel data show that our sentence weighting method is able to achieve an significant improvement over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1271/>Seq2seq Dependency Parsing</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/j/jiaxun-cai/>Jiaxun Cai</a>
|
<a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1271><div class="card-body p-3 small">This paper presents a sequence to sequence (seq2seq) dependency parser by directly predicting the relative position of head for each given word, which therefore results in a truly end-to-end seq2seq dependency parser for the first time. Enjoying the advantage of seq2seq modeling, we enrich a series of embedding enhancement, including firstly introduced subword and node2vec augmentation. Meanwhile, we propose a beam search decoder with <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree constraint</a> and subroot decomposition over the sequence to furthermore enhance our seq2seq parser. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is evaluated on benchmark treebanks, being on par with the state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> by achieving 94.11 % <a href=https://en.wikipedia.org/wiki/Unit_of_analysis>UAS</a> on <a href=https://en.wikipedia.org/wiki/Tree_traversal>PTB</a> and 88.78 % <a href=https://en.wikipedia.org/wiki/Unit_of_analysis>UAS</a> on <a href=https://en.wikipedia.org/wiki/Tree_traversal>CTB</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1272.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1272 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1272 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1272/>Revisiting the Hierarchical Multiscale LSTM<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/a/akos-kadar/>Ãkos KÃ¡dÃ¡r</a>
|
<a href=/people/m/marc-alexandre-cote/>Marc-Alexandre CÃ´tÃ©</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz ChrupaÅa</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1272><div class="card-body p-3 small">Hierarchical Multiscale LSTM (Chung et. al., 2016) is a state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> that learns interpretable structure from character-level input. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can provide fertile ground for (cognitive) computational linguistics studies. However, the high complexity of the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, training and implementations might hinder its applicability. We provide a detailed reproduction and ablation study of the <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>, shedding light on some of the potential caveats of re-purposing complex deep-learning architectures. We further show that simplifying certain aspects of the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> can in fact improve its performance. We also investigate the linguistic units (segments) learned by various levels of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, and argue that their quality does not correlate with the overall performance of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1273/>Character-Level Feature Extraction with Densely Connected Networks</a></strong><br><a href=/people/c/chanhee-lee/>Chanhee Lee</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/d/dongyub-lee/>Dongyub Lee</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1273><div class="card-body p-3 small">Generating character-level features is an important step for achieving good results in various natural language processing tasks. To alleviate the need for human labor in generating hand-crafted features, methods that utilize neural architectures such as <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network (CNN)</a> or <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network (RNN)</a> to automatically extract such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> have been proposed and have shown great results. However, <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> generates position-independent features, and RNN is slow since it needs to process the characters sequentially. In this paper, we propose a novel method of using a densely connected network to automatically extract character-level features. The proposed method does not require any language or task specific assumptions, and shows robustness and <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a> while being faster than CNN- or RNN-based methods. Evaluating this method on three sequence labeling tasks-slot tagging, Part-of-Speech (POS) tagging, and Named-Entity Recognition (NER)-we obtain state-of-the-art performance with a 96.62 F1-score and 97.73 % accuracy on slot tagging and POS tagging, respectively, and comparable performance to the state-of-the-art 91.13 F1-score on NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1274 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1274/>Neural Machine Translation Incorporating Named Entity</a></strong><br><a href=/people/a/arata-ugawa/>Arata Ugawa</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/t/takashi-ninomiya/>Takashi Ninomiya</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1274><div class="card-body p-3 small">This study proposes a new neural machine translation (NMT) model based on the encoder-decoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows : (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these models&#8217;abilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword, a part of the compound word, at each time step. To alleviate these problems, the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore, the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves up to 3.11 point improvement in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1275/>Semantic Parsing for Technical Support Questions</a></strong><br><a href=/people/a/abhirut-gupta/>Abhirut Gupta</a>
|
<a href=/people/a/anupama-ray/>Anupama Ray</a>
|
<a href=/people/g/gargi-dasgupta/>Gargi Dasgupta</a>
|
<a href=/people/g/gautam-singh/>Gautam Singh</a>
|
<a href=/people/p/pooja-aggarwal/>Pooja Aggarwal</a>
|
<a href=/people/p/prateeti-mohapatra/>Prateeti Mohapatra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1275><div class="card-body p-3 small">Technical support problems are very complex. In contrast to regular web queries (that contain few keywords) or factoid questions (which are a few sentences), these problems usually include attributes like a detailed description of what is failing (symptom), steps taken in an effort to remediate the failure (activity), and sometimes a specific request or ask (intent). Automating support is the task of automatically providing answers to these problems given a corpus of solution documents. Traditional approaches to this task rely on <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and are keyword based ; looking for keyword overlap between the question and solution documents and ignoring these attributes. We present an approach for semantic parsing of technical questions that uses grammatical structure to extract these attributes as a baseline, and a CRF based model that can improve performance considerably in the presence of annotated data for training. We also demonstrate that combined with <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>, these <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>attributes</a> help outperform retrieval baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1276 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1276" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1276/>Deconvolution-Based Global Decoding for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1276><div class="card-body p-3 small">A great proportion of sequence-to-sequence (Seq2Seq) models for Neural Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate translation word by word following a sequential order. As the studies of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> have proved that language is not linear word sequence but sequence of complex structure, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> at each step should be conditioned on the whole target-side context. To tackle the problem, we propose a new NMT model that decodes the sequence with the guidance of its structural prediction of the context of the target sequence. Our model generates <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> based on the structural prediction of the target-side context so that the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> can be freed from the bind of sequential order. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more competitive compared with the state-of-the-art methods, and the analysis reflects that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also robust to translating sentences of different lengths and it also reduces <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> with the instruction from the target-side context for decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1277/>Pattern-revising Enhanced Simple Question Answering over Knowledge Bases</a></strong><br><a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1277><div class="card-body p-3 small">Question Answering over Knowledge Bases (KB-QA), which automatically answer natural language questions based on the facts contained by a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, is one of the most important natural language processing (NLP) tasks. Simple questions constitute a large part of questions queried on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, still being a challenge to <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>. In this work, we propose to conduct pattern extraction and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> first, and put forward pattern revising procedure to mitigate the error propagation problem. In order to learn to rank candidate subject-predicate pairs to enable the relevant facts retrieval given a question, we propose to do joint fact selection enhanced by relation detection. Multi-level encodings and multi-dimension information are leveraged to strengthen the whole procedure. The experimental results demonstrate that our approach sets a new record in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, outperforming the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by an absolute large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1278/>Integrating Question Classification and <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for improved Answer Selection</a></strong><br><a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a>
|
<a href=/people/j/john-barnden/>John Barnden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1278><div class="card-body p-3 small">We present a system for Answer Selection that integrates fine-grained Question Classification with a Deep Learning model designed for Answer Selection. We detail the necessary changes to the Question Classification taxonomy and <a href=https://en.wikipedia.org/wiki/System>system</a>, the creation of a new Entity Identification system and methods of highlighting entities to achieve this objective. Our experiments show that Question Classes are a strong signal to Deep Learning models for Answer Selection, and enable us to outperform the current state of the art in all variations of our experiments except one. In the best configuration, our MRR and MAP scores outperform the current state of the art by between 3 and 5 points on both versions of the TREC Answer Selection test set, a standard dataset for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1280/>Modeling <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> with Gated Graph Neural Networks for Knowledge Base Question Answering</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1280><div class="card-body p-3 small">The most approaches to Knowledge Base Question Answering are based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In this paper, we address the problem of learning vector representations for complex semantic parses that consist of multiple entities and relations. Previous work largely focused on selecting the correct semantic relations for a question and disregarded the structure of the semantic parse : the connections between entities and the directions of the relations. We propose to use Gated Graph Neural Networks to encode the graph structure of the semantic parse. We show on two <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> that the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph networks</a> outperform all baseline models that do not explicitly model the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>structure</a>. The <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> confirms that our approach can successfully process complex semantic parses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1283/>Automated Fact Checking : Task Formulations, Methods and Future Directions</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1283><div class="card-body p-3 small">The recently increased focus on <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> has stimulated research in <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a>, <a href=https://en.wikipedia.org/wiki/Database>databases</a>, and <a href=https://en.wikipedia.org/wiki/Journalism>journalism</a>. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1286/>Predicting Stances from Social Media Posts using <a href=https://en.wikipedia.org/wiki/Factorization>Factorization Machines</a></a></strong><br><a href=/people/a/akira-sasaki/>Akira Sasaki</a>
|
<a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1286><div class="card-body p-3 small">Social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. An important step to analyze the discussions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and to assist in healthy decision-making is stance detection. This paper presents an approach to detect the stance of a user toward a topic based on their stances toward other topics and the social media posts of the user. We apply factorization machines, a widely used method in item recommendation, to model user preferences toward topics from the social media data. The experimental results demonstrate that users&#8217; posts are useful to model topic preferences and therefore predict stances of silent users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1287 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1287/>Automatic Detection of Fake News</a></strong><br><a href=/people/v/veronica-perez-rosas/>VerÃ³nica PÃ©rez-Rosas</a>
|
<a href=/people/b/bennett-kleinberg/>Bennett Kleinberg</a>
|
<a href=/people/a/alexandra-lefevre/>Alexandra Lefevre</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1287><div class="card-body p-3 small">The proliferation of misleading information in everyday access media outlets such as <a href=https://en.wikipedia.org/wiki/Social_media>social media feeds</a>, <a href=https://en.wikipedia.org/wiki/Blog>news blogs</a>, and <a href=https://en.wikipedia.org/wiki/Online_newspaper>online newspapers</a> have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news</a>. Our contribution is twofold. First, we introduce two novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for the task of fake news detection, covering seven different <a href=https://en.wikipedia.org/wiki/News_media>news domains</a>. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76 %. In addition, we provide comparative analyses of the automatic and manual identification of fake news.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1288/>All-in-one : Multi-task Learning for Rumour Verification</a></strong><br><a href=/people/e/elena-kochkina/>Elena Kochkina</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1288><div class="card-body p-3 small">Automatic resolution of rumours is a challenging task that can be broken down into smaller components that make up a pipeline, including rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour. In previous work, these steps in the process of rumour verification have been developed as separate components where the output of one feeds into the next. We propose a multi-task learning approach that allows joint training of the main and auxiliary tasks, improving the performance of rumour verification. We examine the connection between the dataset properties and the outcomes of the multi-task learning models used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1289 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1289/>Open Information Extraction on <a href=https://en.wikipedia.org/wiki/Scientific_literature>Scientific Text</a> : An Evaluation</a></strong><br><a href=/people/p/paul-groth/>Paul Groth</a>
|
<a href=/people/m/mike-lauruhn/>Mike Lauruhn</a>
|
<a href=/people/a/antony-scerri/>Antony Scerri</a>
|
<a href=/people/r/ron-daniel-jr/>Ron Daniel Jr.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1289><div class="card-body p-3 small">Open Information Extraction (OIE) is the task of the unsupervised creation of structured information from text. OIE is often used as a starting point for a number of downstream tasks including <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base construction</a>, <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. While OIE methods are targeted at being domain independent, they have been evaluated primarily on newspaper, encyclopedic or general web text. In this article, we evaluate the performance of OIE on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific texts</a> originating from 10 different disciplines. To do so, we use two state-of-the-art OIE systems using a crowd-sourcing approach. We find that OIE systems perform significantly worse on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific text</a> than <a href=https://en.wikipedia.org/wiki/Encyclopedia>encyclopedic text</a>. We also provide an error analysis and suggest areas of work to reduce errors. Our corpus of sentences and judgments are made available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1290/>Simple <a href=https://en.wikipedia.org/wiki/Algorithm>Algorithms</a> For <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> On Sentiment Rich, Data Poor Domains.</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha K Sarma</a>
|
<a href=/people/w/william-sethares/>William Sethares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1290><div class="card-body p-3 small">Standard word embedding algorithms learn vector representations from large corpora of text documents in an unsupervised fashion. However, the quality of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned from these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> is affected by the size of training data sets. Thus, applications of these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in domains with only moderate amounts of available data is limited. In this paper we introduce an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that learns <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> jointly with a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is called SWESA (Supervised Word Embeddings for Sentiment Analysis). SWESA leverages document label information to learn vector representations of words from a modest corpus of text documents by solving an <a href=https://en.wikipedia.org/wiki/Optimization_problem>optimization problem</a> that minimizes a cost function with respect to both word embeddings and the weight vector used for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Experiments on several real world data sets show that SWESA has superior performance on domains with limited data, when compared to previously suggested approaches to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and sentiment analysis tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1291 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1291/>Word-Level Loss Extensions for Neural Temporal Relation Classification</a></strong><br><a href=/people/a/artuur-leeuwenberg/>Artuur Leeuwenberg</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1291><div class="card-body p-3 small">Unsupervised pre-trained word embeddings are used effectively for many tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> to leverage unlabeled textual data. Often these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are either used as initializations or as fixed word representations for task-specific classification models. In this work, we extend our classification model&#8217;s task loss with an unsupervised auxiliary loss on the word-embedding level of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. This is to ensure that the learned word representations contain both task-specific features, learned from the supervised loss component, and more general features learned from the unsupervised loss component. We evaluate our approach on the task of temporal relation extraction, in particular, narrative containment relation extraction from clinical records, and show that continued training of the embeddings on the unsupervised objective together with the task objective gives better task-specific embeddings, and results in an improvement over the state of the art on the THYME dataset, using only a general-domain part-of-speech tagger as linguistic resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1293/>Punctuation as Native Language Interference</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1293><div class="card-body p-3 small">In this paper, we describe experiments designed to explore and evaluate the impact of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation marks</a> on the task of native language identification. Punctuation is specific to each language, and is part of the indicators that overtly represent the manner in which each language organizes and conveys information. Our experiments are organized in various set-ups : the usual multi-class classification for individual languages, also considering classification by language groups, across different proficiency levels, topics and even cross-corpus. The results support our hypothesis that <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation marks</a> are persistent and robust indicators of the native language of the author, which do not diminish in influence even when a high proficiency level in a non-native language is achieved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1294 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1294/>Investigating Productive and Receptive Knowledge : A Profile for Second Language Learning</a></strong><br><a href=/people/l/leonardo-zilio/>Leonardo Zilio</a>
|
<a href=/people/r/rodrigo-wilkens/>Rodrigo Wilkens</a>
|
<a href=/people/c/cedrick-fairon/>CÃ©drick Fairon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1294><div class="card-body p-3 small">The literature frequently addresses the differences in receptive and productive vocabulary, but <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> is often left unacknowledged in second language acquisition studies. In this paper, we used two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> to investigate the divergences in the behavior of pedagogically relevant grammatical structures in reception and production texts. We further improved the divergence scores observed in this investigation by setting a polarity to them that indicates whether there is overuse or underuse of a <a href=https://en.wikipedia.org/wiki/Grammatical_structure>grammatical structure</a> by language learners. This led to the compilation of a language profile that was later combined with vocabulary and readability features for classifying reception and production texts in three classes : beginner, intermediate, and advanced. The results of the automatic classification task in both <a href=https://en.wikipedia.org/wiki/Production_(economics)>production</a> (0.872 of F-measure) and reception (0.942 of F-measure) were comparable to the current state of the art. We also attempted to automatically attribute a score to texts produced by learners, and the correlation results were encouraging, but there is still a good amount of room for improvement in this task. The developed language profile will serve as input for a <a href=https://en.wikipedia.org/wiki/System>system</a> that helps <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a> to activate more of their passive knowledge in writing texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1297 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1297/>Corpus-based Content Construction</a></strong><br><a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/p/pranav-maneriker/>Pranav Maneriker</a>
|
<a href=/people/k/kundan-krishna/>Kundan Krishna</a>
|
<a href=/people/n/natwar-modani/>Natwar Modani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1297><div class="card-body p-3 small">Enterprise content writers are engaged in writing <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a> for various purposes. Often, the text being written may already be present in the enterprise corpus in the form of past articles and can be re-purposed for the current needs. In the absence of suitable tools, authors manually curate / create such content (sometimes from scratch) which reduces their productivity. To address this, we propose an automatic approach to generate an initial version of the author&#8217;s intended text based on an input content snippet. Starting with a set of extracted textual fragments related to the snippet based on the query words in it, the proposed approach builds the desired text from these fragment by simultaneously optimizing the information coverage, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, diversity and <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> in the generated content. Evaluations on standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> shows improved performance against existing <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on several <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1300 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1300" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1300/>ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents<span class=acl-fixed-case>ISO</span>-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents</a></strong><br><a href=/people/s/stefano-mezza/>Stefano Mezza</a>
|
<a href=/people/a/alessandra-cervone/>Alessandra Cervone</a>
|
<a href=/people/e/evgeny-stepanov/>Evgeny Stepanov</a>
|
<a href=/people/g/giuliano-tortoreto/>Giuliano Tortoreto</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1300><div class="card-body p-3 small">Dialogue Act (DA) tagging is crucial for spoken language understanding systems, as it provides a general representation of speakers&#8217; intents, not bound to a particular dialogue system. Unfortunately, publicly available data sets with DA annotation are all based on different annotation schemes and thus incompatible with each other. Moreover, their schemes often do not cover all aspects necessary for open-domain human-machine interaction. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to map several publicly available corpora to a subset of the <a href=https://en.wikipedia.org/wiki/International_Organization_for_Standardization>ISO standard</a>, in order to create a large task-independent training corpus for DA classification. We show the feasibility of using this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to train a domain-independent DA tagger testing it on out-of-domain conversational data, and argue the importance of training on multiple corpora to achieve robustness across different DA categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1301/>Arrows are the Verbs of Diagrams</a></strong><br><a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/m/matthew-stone/>Matthew Stone</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1301><div class="card-body p-3 small">Arrows are a key ingredient of schematic pictorial communication. This paper investigates the interpretation of <a href=https://en.wikipedia.org/wiki/Arrow_(symbol)>arrows</a> through linguistic, crowdsourcing and machine-learning methodology. Our work establishes a novel analogy between arrows and verbs : we advocate representing arrows in terms of qualitatively different structural and semantic frames, and resolving frames to specific interpretations using shallow world knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1303/>Bridge Video and Text with Cascade Syntactic Structure</a></strong><br><a href=/people/g/guolong-wang/>Guolong Wang</a>
|
<a href=/people/z/zheng-qin/>Zheng Qin</a>
|
<a href=/people/k/kaiping-xu/>Kaiping Xu</a>
|
<a href=/people/k/kai-huang/>Kai Huang</a>
|
<a href=/people/s/shuxiong-ye/>Shuxiong Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1303><div class="card-body p-3 small">We present a video captioning approach that encodes features by progressively completing syntactic structure (LSTM-CSS). To construct basic <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic structure</a> (i.e., <a href=https://en.wikipedia.org/wiki/Subject_(grammar)>subject</a>, predicate, and object), we use a <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Field</a> to label semantic representations (i.e., motions, objects). We argue that in order to improve the comprehensiveness of the description, the local features within object regions can be used to generate complementary syntactic elements (e.g., <a href=https://en.wikipedia.org/wiki/Attribute_(grammar)>attribute</a>, adverbial). Inspired by redundancy of human receptors, we utilize a Region Proposal Network to focus on the object regions. To model the final <a href=https://en.wikipedia.org/wiki/Temporal_logic>temporal dynamics</a>, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network</a> with Path Embeddings is adopted. We demonstrate the effectiveness of LSTM-CSS on generating <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>natural sentences</a> : 42.3 % and 28.5 % in terms of BLEU@4 and <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>. Superior performance when compared to <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a> are reported on a large video description dataset (i.e., MSR-VTT-2016).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1304/>Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling</a></strong><br><a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/t/tomohiro-tanaka/>Tomohiro Tanaka</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/h/hirokazu-masataki/>Hirokazu Masataki</a>
|
<a href=/people/y/yushi-aono/>Yushi Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1304><div class="card-body p-3 small">This paper is an initial study on multi-task and multi-lingual joint learning for lexical utterance classification. A major problem in constructing lexical utterance classification modules for spoken dialogue systems is that individual data resources are often limited or unbalanced among tasks and/or languages. Various studies have examined joint learning using neural-network based shared modeling ; however, previous joint learning studies focused on either cross-task or cross-lingual knowledge transfer. In order to simultaneously support both multi-task and multi-lingual joint learning, our idea is to explicitly divide state-of-the-art neural lexical utterance classification into language-specific components that can be shared between different tasks and task-specific components that can be shared between different languages. In addition, in order to effectively transfer knowledge between different task data sets and different language data sets, this paper proposes a partially-shared modeling method that possesses both shared components and components specific to individual data sets. We demonstrate the effectiveness of proposed method using Japanese and English data sets with three different lexical utterance classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1305 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1305/>Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language</a></strong><br><a href=/people/h/he-bai/>He Bai</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/liang-zhao/>Liang Zhao</a>
|
<a href=/people/m/mei-yuh-hwang/>Mei-Yuh Hwang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1305><div class="card-body p-3 small">To deploy a spoken language understanding (SLU) model to a new language, language transferring is desired to avoid the trouble of acquiring and labeling a new big SLU corpus. An SLU corpus is a monolingual corpus with domain / intent / slot labels. Translating the original SLU corpus into the target language is an attractive strategy. However, SLU corpora consist of plenty of semantic labels (slots), which general-purpose translators can not handle well, not to mention additional culture differences. This paper focuses on the language transferring task given a small in-domain parallel SLU corpus. The in-domain parallel corpus can be used as the first adaptation on the general translator. But more importantly, we show how to use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> to further adapt the adapted <a href=https://en.wikipedia.org/wiki/Translation>translator</a>, where translated sentences with more proper slot tags receive higher rewards. Our <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> is derived from the source input sentence exclusively, unlike <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> via actor-critical methods or computing <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> with a ground truth target sentence. Hence we can adapt the <a href=https://en.wikipedia.org/wiki/Translation>translator</a> the second time, using the big monolingual SLU corpus from the source language. We evaluate our approach on Chinese to English language transferring for SLU systems. The experimental results show that the generated English SLU corpus via adaptation and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> gives us over 97 % in the slot F1 score and over 84 % accuracy in domain classification. It demonstrates the effectiveness of the proposed language transferring method. Compared with naive translation, our proposed method improves domain classification accuracy by relatively 22 %, and the slot filling F1 score by relatively more than 71 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1309/>Graph Based Decoding for Event Sequencing and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1309><div class="card-body p-3 small">Events in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text documents</a> are interrelated in complex ways. In this paper, we study two types of <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> : Event Coreference and Event Sequencing. We show that the popular tree-like decoding structure for automated Event Coreference is not suitable for Event Sequencing. To this end, we propose a graph-based decoding algorithm that is applicable to both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. The new decoding algorithm supports flexible feature sets for both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. Empirically, our event coreference system has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference task and our event sequencing system beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event relations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1312" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1312/>NIPS Conversational Intelligence Challenge 2017 Winner System : Skill-based Conversational Agent with Supervised Dialog Manager<span class=acl-fixed-case>NIPS</span> Conversational Intelligence Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised Dialog Manager</a></strong><br><a href=/people/i/idris-yusupov/>Idris Yusupov</a>
|
<a href=/people/y/yurii-kuratov/>Yurii Kuratov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1312><div class="card-body p-3 small">We present bot#1337 : a dialog system developed for the 1st NIPS Conversational Intelligence Challenge 2017 (ConvAI). The aim of the competition was to implement a bot capable of conversing with humans based on a given passage of text. To enable conversation, we implemented a set of skills for our <a href=https://en.wikipedia.org/wiki/Internet_bot>bot</a>, including <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a>, topic detection, text summarization, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question generation</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> has been trained in a supervised setting using a <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a> to select an appropriate skill for generating a response. The latter allows a developer to focus on the skill implementation rather than the finite state machine based dialog manager. The proposed system bot#1337 won the competition with an average dialogue quality score of 2.78 out of 5 given by human evaluators. Source code and trained models for the bot#1337 are available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1313/>AMR Beyond the Sentence : the Multi-sentence AMR corpus<span class=acl-fixed-case>AMR</span> Beyond the Sentence: the Multi-sentence <span class=acl-fixed-case>AMR</span> corpus</a></strong><br><a href=/people/t/tim-ogorman/>Tim OâGorman</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/k/kira-griffitt/>Kira Griffitt</a>
|
<a href=/people/u/ulf-hermjakob/>Ulf Hermjakob</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1313><div class="card-body p-3 small">There are few <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> that endeavor to represent the semantic content of entire documents. We present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that accomplishes one way of capturing document level semantics, by annotating <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and similar phenomena (bridging and implicit roles) on top of gold Abstract Meaning Representations of sentence-level semantics. We present a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of this <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, with analysis of its quality, alongside a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) may become a feasible method for developing rich representations of document meaning, useful for tasks such as <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1315/>Learning Visually-Grounded Semantics from Contrastive Adversarial Samples</a></strong><br><a href=/people/h/haoyue-shi/>Haoyue Shi</a>
|
<a href=/people/j/jiayuan-mao/>Jiayuan Mao</a>
|
<a href=/people/t/tete-xiao/>Tete Xiao</a>
|
<a href=/people/y/yuning-jiang/>Yuning Jiang</a>
|
<a href=/people/j/jian-sun/>Jian Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1315><div class="card-body p-3 small">We study the problem of grounding distributional representations of texts on the <a href=https://en.wikipedia.org/wiki/Visual_system>visual domain</a>, namely visual-semantic embeddings (VSE for short). Begin with an insightful adversarial attack on VSE embeddings, we show the limitation of current frameworks and image-text datasets (e.g., MS-COCO) both quantitatively and qualitatively. The large gap between the number of possible constitutions of real-world semantics and the size of parallel data, to a large extent, restricts the model to establish a strong link between textual semantics and visual concepts. We alleviate this problem by augmenting the MS-COCO image captioning datasets with textual contrastive adversarial samples. These samples are synthesized using language priors of human and the WordNet knowledge base, and enforce the model to ground learned embeddings to concrete concepts within the image. This simple but powerful technique brings a noticeable improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on a diverse set of downstream tasks, in addition to defending known-type adversarial attacks. Codes are available at https://github.com/ExplorerFreda/VSE-C.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1316/>Structured Representation Learning for Online Debate Stance Prediction</a></strong><br><a href=/people/c/chang-li/>Chang Li</a>
|
<a href=/people/a/aldo-porco/>Aldo Porco</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1316><div class="card-body p-3 small">Online debates can help provide valuable information about various perspectives on a wide range of issues. However, understanding the stances expressed in these <a href=https://en.wikipedia.org/wiki/Debate>debates</a> is a highly challenging task, which requires modeling both textual content and users&#8217; conversational interactions. Current approaches take a collective classification approach, which ignores the relationships between different debate topics. In this work, we suggest to view this task as a representation learning problem, and embed the text and authors jointly based on their interactions. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the Internet Argumentation Corpus, and compare different approaches for structural information embedding. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve significantly better results compared to previous competitive models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1318/>Argumentation Synthesis following <a href=https://en.wikipedia.org/wiki/Rhetorical_techniques>Rhetorical Strategies</a></a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/r/roxanne-el-baff/>Roxanne El Baff</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1318><div class="card-body p-3 small">Persuasion is rarely achieved through a loose set of arguments alone. Rather, an effective delivery of arguments follows a rhetorical strategy, combining <a href=https://en.wikipedia.org/wiki/Logical_reasoning>logical reasoning</a> with appeals to <a href=https://en.wikipedia.org/wiki/Ethics>ethics</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. We argue that such a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> means to select, arrange, and phrase a set of argumentative discourse units. In this paper, we model rhetorical strategies for the computational synthesis of effective argumentation. In a study, we let 26 experts synthesize argumentative texts with different <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> for 10 topics. We find that the experts agree in the selection significantly more when following the same <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>. While the texts notably vary for different strategies, especially their arrangement remains stable. The results suggest that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> enables a strategical synthesis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1320/>Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation</a></strong><br><a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1320><div class="card-body p-3 small">Classic <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline models</a> for task-oriented dialogue system require explicit modeling the dialogue states and hand-crafted action spaces to query a domain-specific knowledge base. Conversely, sequence-to-sequence models learn to map dialogue history to the response in current turn without explicit knowledge base querying. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> that leverages the advantages of classic pipeline and sequence-to-sequence models. Our framework models a dialogue state as a fixed-size distributed representation and use this <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to query a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> via an attention mechanism. Experiment on Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly outperforms other sequence-to-sequence based baseline models on both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1321/>Incorporating <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Visual Features</a> into Multiobjective based Multi-view Search Results Clustering</a></strong><br><a href=/people/s/sayantan-mitra/>Sayantan Mitra</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1321><div class="card-body p-3 small">Current paper explores the use of multi-view learning for search result clustering. A web-snippet can be represented using multiple views. Apart from textual view cued by both the semantic and syntactic information, a complimentary view extracted from images contained in the web-snippets is also utilized in the current framework. A single consensus partitioning is finally obtained after consulting these two individual views by the deployment of a multiobjective based clustering technique. Several objective functions including the values of a cluster quality measure measuring the goodness of partitionings obtained using different views and an agreement-disagreement index, quantifying the amount of oneness among multiple views in generating <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partitionings</a> are optimized simultaneously using AMOSA. In order to detect the number of clusters automatically, concepts of variable length solutions and a vast range of permutation operators are introduced in the clustering process. Finally, a set of alternative partitioning are obtained on the final Pareto front by the proposed multi-view based multiobjective technique. Experimental results by the proposed approach on several benchmark test datasets of SRC with respect to different performance metrics evidently establish the power of visual and text-based views in achieving better search result clustering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1323 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1323/>AnlamVer : Semantic Model Evaluation Dataset for Turkish-Word Similarity and Relatedness<span class=acl-fixed-case>A</span>nlam<span class=acl-fixed-case>V</span>er: Semantic Model Evaluation Dataset for <span class=acl-fixed-case>T</span>urkish - Word Similarity and Relatedness</a></strong><br><a href=/people/g/gokhan-ercan/>GÃ¶khan Ercan</a>
|
<a href=/people/o/olcay-taner-yildiz/>Olcay Taner YÄ±ldÄ±z</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1323><div class="card-body p-3 small">In this paper, we present AnlamVer, which is a semantic model evaluation dataset for <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a>. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to <a href=https://en.wikipedia.org/wiki/Semantic_model>semantic model</a> researchers by evaluating <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1324 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1324/>Arguments and Adjuncts in Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/a/adam-przepiorkowski/>Adam PrzepiÃ³rkowski</a>
|
<a href=/people/a/agnieszka-patejuk/>Agnieszka Patejuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1324><div class="card-body p-3 small">The aim of this paper is to argue for a coherent Universal Dependencies approach to the core vs. non-core distinction. We demonstrate inconsistencies in the current version 2 of UD in this respect mostly resulting from the preservation of the argumentadjunct dichotomy despite the declared avoidance of this distinction and propose a relatively conservative modification of UD that is free from these problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1325/>Distinguishing affixoid formations from compounds</a></strong><br><a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/r/rebecca-wilm/>Rebecca Wilm</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1325><div class="card-body p-3 small">We study German affixoids, a type of <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> in between <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> and free stems. Several properties have been associated with them increased productivity ; a bleached semantics, which is often evaluative and/or intensifying and thus of relevance to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> ; and the existence of a free morpheme counterpart but not been validated empirically. In experiments on a new data set that we make available, we put these key assumptions from the morphological literature to the test and show that despite the fact that affixoids generate many low-frequency formations, we can classify these as affixoid or non-affixoid instances with a best F1-score of 74 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1326 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1326/>A Survey on Open Information Extraction</a></strong><br><a href=/people/c/christina-niklaus/>Christina Niklaus</a>
|
<a href=/people/m/matthias-cetto/>Matthias Cetto</a>
|
<a href=/people/a/andre-freitas/>AndrÃ© Freitas</a>
|
<a href=/people/s/siegfried-handschuh/>Siegfried Handschuh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1326><div class="card-body p-3 small">We provide a detailed overview of the various approaches that were proposed to date to solve the task of <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a>. We present the major challenges that such <a href=https://en.wikipedia.org/wiki/System>systems</a> face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1327 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1327" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1327/>Design Challenges and Misconceptions in Neural Sequence Labeling</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/s/shuailong-liang/>Shuailong Liang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1327><div class="card-body p-3 small">We investigate the design challenges of constructing effective and efficient neural sequence labeling systems, by reproducing twelve neural sequence labeling models, which include most of the state-of-the-art structures, and conduct a systematic model comparison on three benchmarks (i.e. NER, <a href=https://en.wikipedia.org/wiki/Chunk_(information)>Chunking</a>, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1328/>Neural Network Models for Paraphrase Identification, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Textual Similarity</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a></a></strong><br><a href=/people/w/wuwei-lan/>Wuwei Lan</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1328><div class="card-body p-3 small">In this paper, we analyze several neural network designs (and their variations) for sentence pair modeling and compare their performance extensively across eight datasets, including paraphrase identification, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic textual similarity</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering tasks</a>. Although most of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have claimed state-of-the-art performance, the original papers often reported on only one or two selected datasets. We provide a systematic study and show that (i) encoding contextual information by LSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference Model is the best so far for larger datasets, while the Pairwise Word Interaction Model achieves the best performance when less data is available. We release our <a href=https://en.wikipedia.org/wiki/Implementation>implementations</a> as an open-source toolkit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1329 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1329/>Authorless Topic Models : Biasing Models Away from Known Structure</a></strong><br><a href=/people/l/laure-thompson/>Laure Thompson</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1329><div class="card-body p-3 small">Most previous work in unsupervised semantic modeling in the presence of <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> has assumed that our goal is to make latent dimensions more correlated with <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, but in practice the exact opposite is often true. Some users want <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> that highlight differences between, for example, authors, but others seek more subtle connections across authors. We introduce three <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for identifying topics that are highly correlated with <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, and demonstrate that this problem affects between 30 and 50 % of the topics in <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on two real-world collections, regardless of the size of the model. We find that we can predict which words cause this phenomenon and that by selectively subsampling these words we dramatically reduce topic-metadata correlation, improve topic stability, and maintain or even improve model quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1330/>SGM : Sequence Generation Model for Multi-label Classification<span class=acl-fixed-case>SGM</span>: Sequence Generation Model for Multi-label Classification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1330><div class="card-body p-3 small">Multi-label classification is an important yet challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform previous <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.</div></div></div><hr><div id=c18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/C18-2/>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2000/>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2001/>Abbreviation Expander-a Web-based System for Easy Reading of Technical Documents</a></strong><br><a href=/people/m/manuel-r-ciosici/>Manuel R. Ciosici</a>
|
<a href=/people/i/ira-assent/>Ira Assent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2001><div class="card-body p-3 small">Abbreviations and <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> are a part of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual communication</a> in most domains. However, <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> are not necessarily defined in documents that employ them. Understanding all <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> used in a given document often requires extensive knowledge of the target domain and the ability to disambiguate based on context. This creates considerable entry barriers to newcomers and difficulties in automated document processing. Existing abbreviation expansion systems or tools require substantial technical knowledge for set up or make strong assumptions which limit their use in practice. Here, we present Abbreviation Expander, a system that builds on state of the art methods for identification of abbreviations, acronyms and their definitions and a novel disambiguator for abbreviation expansion in an easily accessible web-based solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2002/>The INCEpTION Platform : Machine-Assisted and Knowledge-Oriented Interactive Annotation<span class=acl-fixed-case>INCE</span>p<span class=acl-fixed-case>TION</span> Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation</a></strong><br><a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/m/michael-bugert/>Michael Bugert</a>
|
<a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2002><div class="card-body p-3 small">We introduce INCEpTION, a new annotation platform for tasks including <a href=https://en.wikipedia.org/wiki/Semantic_annotation>interactive and semantic annotation</a> (e.g., concept linking, fact linking, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base population</a>, semantic frame annotation). These <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are very time consuming and demanding for annotators, especially when <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> is both generic and modular. It targets a range of research domains in need of semantic annotation, such as <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, <a href=https://en.wikipedia.org/wiki/Bioinformatics>bioinformatics</a>, or <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. INCEpTION is publicly available as <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source software</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2003/>JeSemE : Interleaving Semantics and Emotions in a Web Service for the Exploration of Language Change Phenomena<span class=acl-fixed-case>J</span>e<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>: Interleaving Semantics and Emotions in a Web Service for the Exploration of Language Change Phenomena</a></strong><br><a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2003><div class="card-body p-3 small">We here introduce a substantially extended version of JeSemE, an interactive website for visually exploring computationally derived time-variant information on word meanings and lexical emotions assembled from five large diachronic text corpora. JeSemE is designed for scholars in the (digital) humanities as an alternative to consulting manually compiled, printed dictionaries for such information (if available at all). This tool uniquely combines state-of-the-art <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> with a nuanced model of <a href=https://en.wikipedia.org/wiki/Emotion>human emotions</a>, two information streams we deem beneficial for a data-driven interpretation of texts in the <a href=https://en.wikipedia.org/wiki/Humanities>humanities</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2004/>T-Know : a Knowledge Graph-based Question Answering and Infor-mation Retrieval System for Traditional Chinese Medicine<span class=acl-fixed-case>T</span>-Know: a Knowledge Graph-based Question Answering and Infor-mation Retrieval System for Traditional <span class=acl-fixed-case>C</span>hinese Medicine</a></strong><br><a href=/people/z/ziqing-liu/>Ziqing Liu</a>
|
<a href=/people/e/enwei-peng/>Enwei Peng</a>
|
<a href=/people/s/shixing-yan/>Shixing Yan</a>
|
<a href=/people/g/guozheng-li/>Guozheng Li</a>
|
<a href=/people/t/tianyong-hao/>Tianyong Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2004><div class="card-body p-3 small">T-Know is a knowledge service system based on the constructed <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> of <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine (TCM)</a>. Using authorized and anonymized clinical records, medicine clinical guidelines, teaching materials, classic medical books, academic publications, etc., as data resources, the system extracts triples from free texts to build a TCM knowledge graph by our developed natural language processing methods. On the basis of the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>, a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithm</a> is implemented for single-round question understanding and multiple-round dialogue. In addition, the TCM knowledge graph also is used to support <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interactive knowledge retrieval</a> by normalizing search keywords to <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terminology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2008/>HiDE : a Tool for Unrestricted Literature Based Discovery<span class=acl-fixed-case>H</span>i<span class=acl-fixed-case>DE</span>: a Tool for Unrestricted Literature Based Discovery</a></strong><br><a href=/people/j/judita-preiss/>Judita Preiss</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2008><div class="card-body p-3 small">As the quantity of publications increases daily, researchers are forced to narrow their attention to their own specialism and are therefore less likely to make new connections with other areas. Literature based discovery (LBD) supports the identification of such connections. A number of LBD tools are available, however, they often suffer from limitations such as constraining possible searches or not producing results in real-time. We introduce HiDE (Hidden Discovery Explorer), an online knowledge browsing tool which allows fast access to hidden knowledge generated from all abstracts in Medline. HiDE is fast enough to allow users to explore the full range of hidden connections generated by an LBD system. The tool employs a novel combination of two approaches to LBD : a graph-based approach which allows hidden knowledge to be generated on a large scale and an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference algorithm</a> to identify the most promising (most likely to be non trivial) information. Available at https://skye.shef.ac.uk/kdisc</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2011/>Utilizing Graph Measure to Deduce Omitted Entities in Paragraphs</a></strong><br><a href=/people/e/eun-kyung-kim/>Eun-kyung Kim</a>
|
<a href=/people/k/kijong-han/>Kijong Han</a>
|
<a href=/people/j/jiho-kim/>Jiho Kim</a>
|
<a href=/people/k/key-sun-choi/>Key-Sun Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2011><div class="card-body p-3 small">This demo deals with the problem of capturing omitted arguments in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> given a proper <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> for entities of interest. This paper introduces the concept of a salient entity and use this <a href=https://en.wikipedia.org/wiki/Information>information</a> to deduce omitted entities in the paragraph which allows improving the relation extraction quality. The main idea to compute salient entities is to construct a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> on the given information (by identifying the entities but without parsing it), rank it with standard <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph measures</a> and embed it in the context of the sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2013/>SetExpander : End-to-end Term Set Expansion Based on Multi-Context Term Embeddings<span class=acl-fixed-case>S</span>et<span class=acl-fixed-case>E</span>xpander: End-to-end Term Set Expansion Based on Multi-Context Term Embeddings</a></strong><br><a href=/people/j/jonathan-mamou/>Jonathan Mamou</a>
|
<a href=/people/o/oren-pereg/>Oren Pereg</a>
|
<a href=/people/m/moshe-wasserblat/>Moshe Wasserblat</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/a/alon-eirew/>Alon Eirew</a>
|
<a href=/people/y/yael-green/>Yael Green</a>
|
<a href=/people/s/shira-guskin/>Shira Guskin</a>
|
<a href=/people/p/peter-izsak/>Peter Izsak</a>
|
<a href=/people/d/daniel-korat/>Daniel Korat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2013><div class="card-body p-3 small">We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic class</a>. SetExpander implements an <a href=https://en.wikipedia.org/wiki/Workflow>iterative end-to end workflow</a> for term set expansion. It enables users to easily select a seed set of terms, expand <a href=https://en.wikipedia.org/wiki/Information_technology>it</a>, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2015/>Simulating Language Evolution : a Tool for <a href=https://en.wikipedia.org/wiki/Historical_linguistics>Historical Linguistics</a></a></strong><br><a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2015><div class="card-body p-3 small">Language change across space and time is one of the main concerns in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. In this paper, we develop a language evolution simulator : a web-based tool for word form production to assist in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>, in studying the <a href=https://en.wikipedia.org/wiki/Evolutionary_linguistics>evolution of the languages</a>. Given a word in a source language, the <a href=https://en.wikipedia.org/wiki/System>system</a> automatically predicts how the word evolves in a target language. The method that we propose is language-agnostic and does not use any external knowledge, except for the training word pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2018/>Cool English : a Grammatical Error Correction System Based on Large Learner Corpora<span class=acl-fixed-case>E</span>nglish: a Grammatical Error Correction System Based on Large Learner Corpora</a></strong><br><a href=/people/y/yu-chun-lo/>Yu-Chun Lo</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/c/chingyu-yang/>Chingyu Yang</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2018><div class="card-body p-3 small">This paper presents a grammatical error correction (GEC) system that provides corrective feedback for <a href=https://en.wikipedia.org/wiki/Essay>essays</a>. We apply the sequence-to-sequence model, which is frequently used in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, to this GEC task. The model is trained by EF-Cambridge Open Language Database (EFCAMDAT), a large learner corpus annotated with grammatical errors and corrections. Evaluation shows that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves competitive performance on a number of publicly available testsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2020/>KIT Lecture Translator : Multilingual Speech Translation with One-Shot Learning<span class=acl-fixed-case>KIT</span> Lecture Translator: Multilingual Speech Translation with One-Shot Learning</a></strong><br><a href=/people/f/florian-dessloch/>Florian Dessloch</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/m/markus-muller/>Markus MÃ¼ller</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/t/thai-son-nguyen/>Thai-Son Nguyen</a>
|
<a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian StÃ¼ker</a>
|
<a href=/people/t/thomas-zenkel/>Thomas Zenkel</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2020><div class="card-body p-3 small">In today&#8217;s globalized world we have the ability to communicate with people across the world. However, in many situations the <a href=https://en.wikipedia.org/wiki/Language_barrier>language barrier</a> still presents a major issue. For example, many foreign students coming to KIT to study are initially unable to follow a lecture in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Therefore, we offer an automatic simultaneous interpretation service for students. To fulfill this task, we have developed a low-latency translation system that is adapted to lectures and covers several language pairs. While the switch from traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a> to Neural Machine Translation (NMT) significantly improved performance, to integrate NMT into the speech translation framework required several adjustments. We have addressed the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time constraints</a> and different types of input. Furthermore, we utilized <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> to easily add new topic-specific terms to the <a href=https://en.wikipedia.org/wiki/System>system</a>. Besides better performance, <a href=https://en.wikipedia.org/wiki/Non-verbal_communication>NMT</a> also enabled us increase our covered languages through multilingual NMT. % Combining these techniques, we are able to provide an adapted speech translation system for several <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2022/>LanguageNet : Learning to Find Sense Relevant Example Sentences<span class=acl-fixed-case>L</span>anguage<span class=acl-fixed-case>N</span>et: Learning to Find Sense Relevant Example Sentences</a></strong><br><a href=/people/s/shang-chien-cheng/>Shang-Chien Cheng</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/c/chingyu-yang/>Chingyu Yang</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2022><div class="card-body p-3 small">In this paper, we present a system, LanguageNet, which can help second language learners to search for different meanings and usages of a word. We disambiguate word senses based on the pairs of an English word and its corresponding Chinese translations in a parallel corpus, UM-Corpus. The process involved performing <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>, learning <a href=https://en.wikipedia.org/wiki/Word_formation>vector space representations of words</a> and training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to distinguish words into groups of senses. LanguageNet directly shows the definition of a sense, bilingual synonyms and sense relevant examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2024/>Lingke : a Fine-grained Multi-turn Chatbot for Customer Service<span class=acl-fixed-case>L</span>ingke: a Fine-grained Multi-turn Chatbot for Customer Service</a></strong><br><a href=/people/p/pengfei-zhu/>Pengfei Zhu</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/j/jiangtong-li/>Jiangtong Li</a>
|
<a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2024><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> usually need a mass of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human dialogue data</a>, especially when using supervised machine learning method. Though they can easily deal with single-turn question answering, for <a href=https://en.wikipedia.org/wiki/Turns,_rounds_and_time-keeping_systems_in_games>multi-turn</a> the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a fine-grained pipeline processing to distill responses based on <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured documents</a>, and attentive sequential context-response matching for multi-turn conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2025/>Writing Mentor : Self-Regulated Writing Feedback for Struggling Writers</a></strong><br><a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/n/norbert-elliot/>Norbert Elliot</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/d/diane-napolitano/>Diane Napolitano</a>
|
<a href=/people/s/slava-andreyev/>Slava Andreyev</a>
|
<a href=/people/m/maxwell-schwartz/>Maxwell Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2025><div class="card-body p-3 small">Writing Mentor is a free Google Docs add-on designed to provide feedback to struggling writers and help them improve their writing in a self-paced and self-regulated fashion. Writing Mentor uses natural language processing (NLP) methods and resources to generate feedback in terms of features that research into post-secondary struggling writers has classified as developmental (Burstein et al., 2016b). These features span many writing sub-constructs (use of sources, claims, and evidence ; topic development ; <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> ; and knowledge of English conventions). Prelimi- nary analysis indicates that users have a largely positive impression of Writing Mentor in terms of <a href=https://en.wikipedia.org/wiki/Usability>usability</a> and potential impact on their writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2027/>Sensala : a Dynamic Semantics System for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>S</span>ensala: a Dynamic Semantics System for Natural Language Processing</a></strong><br><a href=/people/d/daniyar-itegulov/>Daniyar Itegulov</a>
|
<a href=/people/e/ekaterina-lebedeva/>Ekaterina Lebedeva</a>
|
<a href=/people/b/bruno-woltzenlogel-paleo/>Bruno Woltzenlogel Paleo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2027><div class="card-body p-3 small">Here we describe Sensala, an open source framework for the semantic interpretation of natural language that provides the logical meaning of a given text. The framework&#8217;s theory is based on a <a href=https://en.wikipedia.org/wiki/Lambda_calculus>lambda calculus</a> with exception handling and uses contexts, continuations, events and dependent types to handle a wide range of complex linguistic phenomena, such as <a href=https://en.wikipedia.org/wiki/Donkey_anaphora>donkey anaphora</a>, verb phrase anaphora, propositional anaphora, presuppositions and implicatures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-2028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-2028/>On-Device Neural Language Model Based Word Prediction</a></strong><br><a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/n/nilesh-kulkarni/>Nilesh Kulkarni</a>
|
<a href=/people/h/haejun-lee/>Haejun Lee</a>
|
<a href=/people/j/jihie-kim/>Jihie Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2028><div class="card-body p-3 small">Recent developments in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> with application to <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> have led to success in tasks of <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, summarizing and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, deploying huge language models for the <a href=https://en.wikipedia.org/wiki/Mobile_device>mobile device</a> such as <a href=https://en.wikipedia.org/wiki/Computer_keyboard>on-device keyboards</a> poses computation as a bottle-neck due to their puny computation capacities. In this work, we propose an on-device neural language model based word prediction method that optimizes run-time memory and also provides a real-time prediction environment. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> size is 7.40 MB and has average prediction time of 6.47 ms. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the existing methods for <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a> in terms of keystroke savings and <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction rate</a> and has been successfully commercialized.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-2029" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-2029/>WARP-Text : a Web-Based Tool for Annotating Relationships between Pairs of Texts<span class=acl-fixed-case>WARP</span>-Text: a Web-Based Tool for Annotating Relationships between Pairs of Texts</a></strong><br><a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/m/m-antonia-marti/>M. AntÃ²nia MartÃ­</a>
|
<a href=/people/m/maria-salamo/>Maria SalamÃ³</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2029><div class="card-body p-3 small">We present WARP-Text, an open-source web-based tool for annotating relationships between pairs of texts. WARP-Text supports multi-layer annotation and custom definitions of inter-textual and intra-textual relationships. Annotation can be performed at different granularity levels (such as sentences, phrases, or tokens). WARP-Text has an intuitive user-friendly interface both for project managers and annotators. WARP-Text fills a gap in the currently available NLP toolbox, as open-source alternatives for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of pairs of text are not readily available. WARP-Text has already been used in several annotation tasks and can be of interest to the researchers working in the areas of <a href=https://en.wikipedia.org/wiki/Paraphrase>Paraphrasing</a>, Entailment, <a href=https://en.wikipedia.org/wiki/Simplification>Simplification</a>, and <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a>, among others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2030/>A <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Writing Correction System</a> for Learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a Foreign Language<span class=acl-fixed-case>C</span>hinese Writing Correction System for Learning <span class=acl-fixed-case>C</span>hinese as a Foreign Language</a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2030><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese writing correction system</a> for learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language. The <a href=https://en.wikipedia.org/wiki/System>system</a> takes a wrong input sentence and generates several correction suggestions. It also retrieves example <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese sentences</a> with English translations, helping users understand the correct usages of certain <a href=https://en.wikipedia.org/wiki/Chinese_grammar>grammar patterns</a>. This is the first available Chinese writing error correction system based on the neural machine translation framework. We discuss several design choices and show empirical results to support our decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2031/>LTV : Labeled Topic Vector<span class=acl-fixed-case>LTV</span>: Labeled Topic Vector</a></strong><br><a href=/people/d/daniel-baumartz/>Daniel Baumartz</a>
|
<a href=/people/t/tolga-uslu/>Tolga Uslu</a>
|
<a href=/people/a/alexander-mehler/>Alexander Mehler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2031><div class="card-body p-3 small">In this paper we present LTV, a website and <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> that generates labeled topic classifications based on the Dewey Decimal Classification (DDC), an international standard for topic classification in libraries. We introduce nnDDC, a largely language-independent natural network-based classifier for DDC, which we optimized using a wide range of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to achieve an F-score of 87.4 %. To show that our approach is language-independent, we evaluate nnDDC using up to 40 different languages. We derive a <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> based on nnDDC, which generates probability distributions over semantic units for any input on sense-, word- and text-level. Unlike related approaches, however, these probabilities are estimated by means of <a href=https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction>nnDDC</a> so that each dimension of the resulting <a href=https://en.wikipedia.org/wiki/Vector_space>vector representation</a> is uniquely labeled by a <a href=https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction>DDC class</a>. In this way, we introduce a neural network-based Classifier-Induced Semantic Space (nnCISS).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-2033/>A Cross-lingual Messenger with Keyword Searchable Phrases for the Travel Domain</a></strong><br><a href=/people/s/shehroze-khan/>Shehroze Khan</a>
|
<a href=/people/j/jihyun-kim/>Jihyun Kim</a>
|
<a href=/people/t/tarik-zulfikarpasic/>Tarik Zulfikarpasic</a>
|
<a href=/people/y/yuanzhu-peter-chen/>Peter Chen</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-2033><div class="card-body p-3 small">We present Qutr (Query Translator), a smart cross-lingual communication application for the travel domain. Qutr is a real-time messaging app that automatically translates conversations while supporting keyword-to-sentence matching. Qutr relies on querying a <a href=https://en.wikipedia.org/wiki/Database>database</a> that holds commonly used pre-translated travel-domain phrases and phrase templates in different languages with the use of <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>. The query matching supports <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>, incomplete keywords and some input spelling errors. The <a href=https://en.wikipedia.org/wiki/Application_software>application</a> addresses common cross-lingual communication issues such as translation accuracy, speed, <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a>, and <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>.</div></div></div><hr><div id=c18-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/C18-3/>Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-3000/>Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/d/donia-scott/>Donia Scott</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p></div><hr><div id=w18-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-38/>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3800/>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</a></strong><br><a href=/people/p/peter-machonis/>Peter Machonis</a>
|
<a href=/people/a/anabela-barreiro/>Anabela Barreiro</a>
|
<a href=/people/k/kristina-kocijan/>Kristina Kocijan</a>
|
<a href=/people/m/max-silberztein/>Max Silberztein</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3804/>Linguistic Resources for Phrasal Verb Identification</a></strong><br><a href=/people/p/peter-machonis/>Peter Machonis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3804><div class="card-body p-3 small">This paper shows how a Lexicon-Grammar dictionary of English phrasal verbs (PV) can be transformed into an electronic dictionary, and with the help of multiple grammars, dictionaries, and filters within the linguistic development environment, NooJ, how to accurately identify PV in large corpora. The NooJ program is an alternative to statistical methods commonly used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : all PV are listed in a dictionary and then located by means of a PV grammar in both continuous and discontinuous format. Results are then refined with a series of <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>, disambiguating grammars, and other linguistics recourses. The main advantage of such a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> is that all PV can be identified in any corpus. The only drawback is that PV not listed in the dictionary (e.g., archaic forms, recent neologisms) are not identified ; however, new PV can easily be added to the electronic dictionary, which is freely available to all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3805/>Designing a Croatian Aspectual Derivatives Dictionary : Preliminary Stages<span class=acl-fixed-case>C</span>roatian Aspectual Derivatives Dictionary: Preliminary Stages</a></strong><br><a href=/people/k/kristina-kocijan/>Kristina Kocijan</a>
|
<a href=/people/k/kresimir-sojat/>KreÅ¡imir Å ojat</a>
|
<a href=/people/d/dario-poljak/>Dario Poljak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3805><div class="card-body p-3 small">The paper focusses on <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivationally connected verbs</a> in <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, i.e. on verbs that share the same lexical morpheme and are derived from other verbs via <a href=https://en.wikipedia.org/wiki/Prefix>prefixation</a>, <a href=https://en.wikipedia.org/wiki/Suffix>suffixation</a> and/or <a href=https://en.wikipedia.org/wiki/Alternation_(linguistics)>stem alternations</a>. As in other <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a> with rich derivational morphology, each verb is marked for <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, either perfective or imperfective. Some verbs, mostly of foreign origin, are marked as bi-aspectual verbs. The main objective of this paper is to detect and to describe major derivational processes and affixes used in the derivation of aspectually connected verbs with NooJ. Annotated chains are exported into a format adequate for web database system and further used to enhance the aspectual and derivational information for each verb.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3806/>A Rule-Based System for Disambiguating French Locative Verbs and Their Translation into Arabic<span class=acl-fixed-case>F</span>rench Locative Verbs and Their Translation into <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/s/safa-boudhina/>Safa Boudhina</a>
|
<a href=/people/h/hela-fehri/>HÃ©la Fehri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3806><div class="card-body p-3 small">This paper presents a rule-based system for disambiguating frensh locative verbs and their translation to <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>. The disambiguation phase is based on the use of the French Verb dictionary (LVF) of Dubois and Dubois Charlier as a linguistic resource, from which a base of disambiguation rules is extracted. The extracted <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> thus take the form of <a href=https://en.wikipedia.org/wiki/Transducer>transducers</a> which will be subsequently applied to texts. The translation phase consists in translating the disambiguated locative verbs returned by the disambiguation phase. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> takes into account the verb&#8217;s tense used as well as the <a href=https://en.wikipedia.org/wiki/Inflection>inflected form</a> of the verb. This phase is based on <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionaries</a> that contain the different French locative verbs and their translation into the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>. The experimentation and the evaluation are done in the linguistic platform NooJ. The obtained results are satisfactory.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3807/>A Pedagogical Application of NooJ in Language Teaching : The Adjective in Spanish and Italian<span class=acl-fixed-case>N</span>oo<span class=acl-fixed-case>J</span> in Language Teaching: The Adjective in <span class=acl-fixed-case>S</span>panish and <span class=acl-fixed-case>I</span>talian</a></strong><br><a href=/people/a/andrea-rodrigo/>Andrea Rodrigo</a>
|
<a href=/people/m/mario-monteleone/>Mario Monteleone</a>
|
<a href=/people/s/silvia-reyes/>Silvia Reyes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3807><div class="card-body p-3 small">In this paper, a pedagogical application of NooJ to the teaching and learning of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> as a foreign language is presented, which is directed to a specific addressee : learners whose mother tongue is <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. The category &#8216;adjective&#8217; has been chosen on account of its lower frequency of occurrence in texts written in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and particularly in the Argentine Rioplatense variety, and with the aim of developing strategies to increase its use. In addition, the features that the <a href=https://en.wikipedia.org/wiki/Adjective>adjective</a> shares with other <a href=https://en.wikipedia.org/wiki/Grammatical_category>grammatical categories</a> render it extremely productive and provide elements that enrich the learners&#8217; proficiency. The reference corpus contains the front pages of the Argentinian newspaper Clarn related to an emblematic historical moment, whose starting point is 24 March 1976, when a military coup began, and covers a thirty year period until 24 March 2006. It can be seen how the term desaparecido emerges with all its cultural and social charge, providing a context which allows an approach to Rioplatense Spanish from a more comprehensive perspective. Finally, a pedagogical proposal accounting for the application of the NooJ platform in <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> is included.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3808/>STYLUS : A Resource for Systematically Derived Language Usage<span class=acl-fixed-case>STYLUS</span>: A Resource for Systematically Derived Language Usage</a></strong><br><a href=/people/b/bonnie-dorr/>Bonnie Dorr</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3808><div class="card-body p-3 small">We describe a resource derived through extraction of a set of argument realizations from an existing lexical-conceptual structure (LCS) Verb Database of 500 verb classes (containing a total of 9525 verb entries) to include information about realization of arguments for a range of different verb classes. We demonstrate that our extended resource, called STYLUS (SysTematicallY Derived Language USe), enables systematic derivation of regular patterns of language usage without requiring manual annotation. We posit that both spatially oriented applications such as robot navigation and more general applications such as narrative generation require a layered representation scheme where a set of primitives (often grounded in space / motion such as GO) is coupled with a representation of constraints at the syntax-semantics interface. We demonstrate that the resulting resource covers three cases of lexico-semantic operations applicable to both <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3813/>Using <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a> to Compare FrameNet Frames Across Languages<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Frames Across Languages</a></strong><br><a href=/people/j/jennifer-sikos/>Jennifer Sikos</a>
|
<a href=/people/s/sebastian-pado/>Sebastian PadÃ³</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3813><div class="card-body p-3 small">Much interest in <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame Semantics</a> is fueled by the substantial extent of its applicability across languages. At the same time, lexicographic studies have found that the applicability of individual frames can be diminished by cross-lingual divergences regarding <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, <a href=https://en.wikipedia.org/wiki/Valency_(linguistics)>syntactic valency</a>, and <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a>. Due to the large effort involved in manual investigations, there are so far no broad-coverage resources with problematic frames for any language pair. Our study investigates to what extent multilingual vector representations of frames learned from manually annotated corpora can address this need by serving as a wide coverage source for such divergences. We present a case study for the language pair English German using the FrameNet and SALSA corpora and find that inferences can be made about cross-lingual frame applicability using a vector space model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3814/>Construction of a Multilingual Corpus Annotated with Translation Relations</a></strong><br><a href=/people/y/yuming-zhai/>Yuming Zhai</a>
|
<a href=/people/a/aurelien-max/>AurÃ©lien Max</a>
|
<a href=/people/a/anne-vilnat/>Anne Vilnat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3814><div class="card-body p-3 small">Translation relations, which distinguish <a href=https://en.wikipedia.org/wiki/Literal_translation>literal translation</a> from other <a href=https://en.wikipedia.org/wiki/Translation>translation techniques</a>, constitute an important subject of study for <a href=https://en.wikipedia.org/wiki/Translation>human translators</a> (Chuquet and Paillard, 1989). However, automatic processing techniques based on interlingual relations, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> exploiting translational equivalence, have not exploited these relations explicitly until now. In this work, we present a categorisation of translation relations and annotate them in a parallel multilingual (English, French, Chinese) corpus of oral presentations, the <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED Talks</a>. Our long term objective will be to automatically detect these relations in order to integrate them as important characteristics for the search of monolingual segments in relation of equivalence (paraphrases) or of <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>. The annotated corpus resulting from our work will be made available to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3817/>Enabling Code-Mixed Translation : Parallel Corpus Creation and MT Augmentation Approach<span class=acl-fixed-case>MT</span> Augmentation Approach</a></strong><br><a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3817><div class="card-body p-3 small">Code-mixing, use of two or more languages in a single sentence, is ubiquitous ; generated by multi-lingual speakers across the world. The phenomenon presents itself prominently in social media discourse. Consequently, there is a growing need for translating code-mixed hybrid language into standard languages. However, due to the lack of gold parallel data, existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> fail to properly translate code-mixed text. In an effort to initiate the task of machine translation of code-mixed content, we present a newly created <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> of code-mixed English-Hindi and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We selected previously available English-Hindi code-mixed data as a starting point for the creation of our parallel corpus. We then chose 4 human translators, fluent in both English and Hindi, for translating the 6088 code-mixed English-Hindi sentences to English. With the help of the created parallel corpus, we analyzed the structure of English-Hindi code-mixed data and present a technique to augment run-of-the-mill machine translation (MT) approaches that can help achieve superior translations without the need for specially designed translation systems. We present an augmentation pipeline for existing MT approaches, like Phrase Based MT (Moses) and Neural MT, to improve the translation of code-mixed text.</div></div></div><hr><div id=w18-39><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-39.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-39/>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3900/>Proceedings of the Fifth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2018)</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola LjubeÅ¡iÄ</a>
|
<a href=/people/j/jorg-tiedemann/>JÃ¶rg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3902/>Encoder-Decoder Methods for Text Normalization</a></strong><br><a href=/people/m/massimo-lusetti/>Massimo Lusetti</a>
|
<a href=/people/t/tatyana-ruzsics/>Tatyana Ruzsics</a>
|
<a href=/people/a/anne-gohring/>Anne GÃ¶hring</a>
|
<a href=/people/t/tanja-samardzic/>Tanja SamardÅ¾iÄ</a>
|
<a href=/people/e/elisabeth-stark/>Elisabeth Stark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3902><div class="card-body p-3 small">Text normalization is the task of mapping non-canonical language, typical of <a href=https://en.wikipedia.org/wiki/Speech_transcription>speech transcription</a> and <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>computer-mediated communication</a>, to a standardized writing. It is an up-stream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss German</a>, with strong regional variation and no written standard. Text normalization has been addressed with a variety of methods, most successfully with character-level statistical machine translation (CSMT). In the meantime, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has changed and the new methods, known as neural encoder-decoder (ED) models, resulted in remarkable improvements. Text normalization, however, has not yet followed. A number of <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> have been tried, but <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>CSMT</a> remains the state-of-the-art. In this work, we normalize Swiss German WhatsApp messages using the ED framework. We exploit the flexibility of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, which allows us to learn from the same training data in different ways. In particular, we modify the decoding stage of a plain ED model to include target-side language models operating at different levels of granularity : <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words. Our systematic comparison shows that our approach results in an improvement over the CSMT state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3903.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3903" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3903/>A High Coverage Method for Automatic False Friends Detection for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and Portuguese<span class=acl-fixed-case>F</span>riends Detection for <span class=acl-fixed-case>S</span>panish and <span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/s/santiago-castro/>Santiago Castro</a>
|
<a href=/people/j/jairo-bonanata/>Jairo Bonanata</a>
|
<a href=/people/a/aiala-rosa/>Aiala RosÃ¡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3903><div class="card-body p-3 small">False friends are words in two languages that look or sound similar, but have different meanings. They are a common source of confusion among <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a>. Methods to detect them automatically do exist, however they make use of large aligned bilingual corpora, which are hard to find and expensive to build, or encounter problems dealing with infrequent words. In this work we propose a high coverage method that uses word vector representations to build a false friends classifier for any pair of languages, which we apply to the particular case of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>. The required resources are a large corpus for each language and a small bilingual lexicon for the pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3905/>Part of Speech Tagging in <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia</a> : A Bantu Macrolanguage<span class=acl-fixed-case>L</span>uyia: A <span class=acl-fixed-case>B</span>antu Macrolanguage</a></strong><br><a href=/people/k/kenneth-steimel/>Kenneth Steimel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3905><div class="card-body p-3 small">Luyia is a <a href=https://en.wikipedia.org/wiki/ISO_639_macrolanguage>macrolanguage</a> in central Kenya. The <a href=https://en.wikipedia.org/wiki/Luyia_languages>Luyia languages</a>, like other <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu languages</a>, have a complex morphological system. This system can be leveraged to aid in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tagging</a>. Bag-of-characters taggers trained on a source <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia language</a> can be applied directly to another <a href=https://en.wikipedia.org/wiki/Luyia_language>Luyia language</a> with some degree of success. In addition, mixing data from the target language with data from the source language does produce more accurate predictive models compared to <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on just the target language data when the training set size is small. However, for both of these tagging tasks, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> involving the more distantly related language, <a href=https://en.wikipedia.org/wiki/Tiriki_language>Tiriki</a>, are better at predicting <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tags</a> for Wanga data. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> incorporating <a href=https://en.wikipedia.org/wiki/Bukusu>Bukusu data</a> are not as successful despite the closer relationship between <a href=https://en.wikipedia.org/wiki/Bukusu>Bukusu</a> and <a href=https://en.wikipedia.org/wiki/Wanga_language>Wanga</a>. Overlapping vocabulary between the Wanga and Tiriki corpora as well as a bias towards open class words help <a href=https://en.wikipedia.org/wiki/Tiriki_language>Tiriki</a> outperform <a href=https://en.wikipedia.org/wiki/Bukusu_language>Bukusu</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3907/>Iterative Language Model Adaptation for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan Language Identification</a><span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan Language Identification</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister LindÃ©n</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3907><div class="card-body p-3 small">This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan Language Identification shared task of the VarDial 2018 Evaluation Campaign. The shared task was an open one, but we did not use any corpora other than what was distributed by the organizers. A total of eight teams provided results for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>. Our submission using a HeLI-method based language identifier with iterative language model adaptation obtained the best results in the shared task with a macro F1-score of 0.958.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3910" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3910/>Varying image description tasks : spoken versus written descriptions</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/r/ruud-koolen/>Ruud Koolen</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3910><div class="card-body p-3 small">Automatic image description systems are commonly trained and evaluated on written image descriptions. At the same time, these systems are often used to provide <a href=https://en.wikipedia.org/wiki/Linguistic_description>spoken descriptions</a> (e.g. for visually impaired users) through <a href=https://en.wikipedia.org/wiki/Mobile_app>apps</a> like TapTapSee or Seeing AI. This is not a problem, as long as spoken and written descriptions are very similar. However, linguistic research suggests that <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> often differs from <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. These differences are not regular, and vary from context to context. Therefore, this paper investigates whether there are differences between written and spoken image descriptions, even if they are elicited through similar tasks. We compare descriptions produced in two <a href=https://en.wikipedia.org/wiki/Language>languages</a> (English and Dutch), and in both <a href=https://en.wikipedia.org/wiki/Language>languages</a> observe substantial differences between spoken and written descriptions. Future research should see if users prefer the spoken over the written style and, if so, aim to emulate spoken descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3911/>Transfer Learning for British Sign Language Modelling<span class=acl-fixed-case>B</span>ritish <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Modelling</a></strong><br><a href=/people/b/boris-mocialov/>Boris Mocialov</a>
|
<a href=/people/h/helen-hastie/>Helen Hastie</a>
|
<a href=/people/g/graham-turner/>Graham Turner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3911><div class="card-body p-3 small">Automatic speech recognition and <a href=https://en.wikipedia.org/wiki/Speech_recognition>spoken dialogue systems</a> have made great advances through the use of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep machine learning methods</a>. This is partly due to greater computing power but also through the large amount of data available in <a href=https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers>common languages</a>, such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Conversely, research in <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a>, including <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> developed for one language is reused as the starting point for a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on a <a href=https://en.wikipedia.org/wiki/Second_language>second language</a>, which is less resourced. In this paper, we examine two transfer learning techniques of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and layer substitution for language modelling of <a href=https://en.wikipedia.org/wiki/British_Sign_Language>British Sign Language</a>. Our results show improvement in perplexity when using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3913/>Character Level Convolutional Neural Network for Arabic Dialect Identification<span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/m/mohamed-ali/>Mohamed Ali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3913><div class="card-body p-3 small">This submission is for the description paper for our <a href=https://en.wikipedia.org/wiki/System>system</a> in the ADI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3918/>Computationally efficient discrimination between <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> with large feature vectors and regularized classifiers</a></strong><br><a href=/people/a/adrien-barbaresi/>Adrien Barbaresi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3918><div class="card-body p-3 small">The present contribution revolves around efficient approaches to <a href=https://en.wikipedia.org/wiki/Language_classification>language classification</a> which have been field-tested in the Vardial evaluation campaign. The methods used in several language identification tasks comprising different language types are presented and their results are discussed, giving insights on real-world application of <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>, <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifiers</a> and corresponding <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. The use of a specially adapted Ridge classifier proved useful in 2 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> out of 3. The overall approach (XAC) has slightly outperformed most of the other systems on the DFS task (Dutch and Flemish) and on the ILI task (Indo-Aryan languages), while its comparative performance was poorer in on the GDI task (Swiss German dialects).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3922/>Exploring Classifier Combinations for Language Variety Identification</a></strong><br><a href=/people/t/tim-kreutz/>Tim Kreutz</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3922><div class="card-body p-3 small">This paper describes CLiPS&#8217;s submissions for the Discriminating between Dutch and Flemish in Subtitles (DFS) shared task at VarDial 2018. We explore different ways to combine <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature groups</a>. Our best system uses two Linear SVM classifiers ; one trained on lexical features (word n-grams) and one trained on syntactic features (PoS n-grams). The final prediction for a document to be in Flemish Dutch or Netherlandic Dutch is made by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that outputs the highest probability for one of the two labels. This confidence vote approach outperforms a meta-classifier on the <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> and on the <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3927.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3927 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3927 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3927/>Using Neural Transfer Learning for Morpho-syntactic Tagging of South-Slavic Languages Tweets<span class=acl-fixed-case>S</span>outh-<span class=acl-fixed-case>S</span>lavic Languages Tweets</a></strong><br><a href=/people/s/sara-meftah/>Sara Meftah</a>
|
<a href=/people/n/nasredine-semmar/>Nasredine Semmar</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a>
|
<a href=/people/s/stephan-raaijmakers/>Stephan Raaijmakers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3927><div class="card-body p-3 small">In this paper, we describe a morpho-syntactic tagger of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, an important component of the CEA List DeepLIMA tool which is a multilingual text analysis platform based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. This tagger is built for the Morpho-syntactic Tagging of Tweets (MTT) Shared task of the 2018 VarDial Evaluation Campaign. The MTT task focuses on morpho-syntactic annotation of non-canonical Twitter varieties of three <a href=https://en.wikipedia.org/wiki/South_Slavic_languages>South-Slavic languages</a> : <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, Croatian and <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a>. We propose to use a neural network model trained in an end-to-end manner for the three languages without any need for task or domain specific features engineering. The proposed approach combines both character and word level representations. Considering the lack of annotated data in the social media domain for South-Slavic languages, we have also implemented a cross-domain Transfer Learning (TL) approach to exploit any available related out-of-domain annotated data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3928 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3928/>When Simple n-gram Models Outperform Syntactic Approaches : Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and Flemish<span class=acl-fixed-case>D</span>utch and <span class=acl-fixed-case>F</span>lemish</a></strong><br><a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/m/masha-medvedeva/>Masha Medvedeva</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3928><div class="card-body p-3 small">In this paper we present the results of our participation in the Discriminating between <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> and <a href=https://en.wikipedia.org/wiki/Flemish>Flemish</a> in Subtitles VarDial 2018 shared task. We try techniques proven to work well for discriminating between language varieties as well as explore the potential of using <a href=https://en.wikipedia.org/wiki/Syntax_(linguistics)>syntactic features</a>, i.e. hierarchical syntactic subtrees. We experiment with different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Discriminating between these two languages turned out to be a very hard task, not only for a machine : human performance is only around 0.51 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> ; our best system is still a simple Naive Bayes model with <a href=https://en.wikipedia.org/wiki/Unigram>word unigrams</a> and <a href=https://en.wikipedia.org/wiki/Bigram>bigrams</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an F1 score (macro) of 0.62, which ranked us 4th in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3930 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3930/>Deep Models for Arabic Dialect Identification on Benchmarked Data<span class=acl-fixed-case>A</span>rabic Dialect Identification on Benchmarked Data</a></strong><br><a href=/people/m/mohamed-elaraby/>Mohamed Elaraby</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3930><div class="card-body p-3 small">The Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011) is a large-scale repos-itory of <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> with manual labels for4varieties of the language. Existing dialect iden-tification models exploiting the dataset pre-date the recent boost <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> brought to NLPand hence the data are not benchmarked for use with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, nor is it clear how much neural networks can help tease the categories in the data apart. We treat these two limitations : We (1) benchmark the data, and (2) empirically test6different deep learning methods on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>thetask</a>, comparing peformance to several classical machine learning models under different condi-tions (i.e., both binary and multi-way classification). Our experimental results show that variantsof (attention-based) bidirectional recurrent neural networks achieve best accuracy (acc) on thetask, significantly outperforming all competitive baselines. On <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test data</a>, our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> reach87.65%acc on the binary task (MSA vs. dialects),87.4%acc on the 3-way dialect task (Egyptianvs. Gulf vs. Levantine), and82.45%acc on the 4-way variants task (MSA vs. Egyptian vs. Gulfvs. Levantine). We release our benchmark for future work on the dataset</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3931 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3931/>A Neural Approach to Language Variety Translation</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussÃ </a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3931><div class="card-body p-3 small">In this paper we present the first neural-based machine translation system trained to translate between standard national varieties of the same language. We take the pair <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian-European Portuguese</a> as an example and compare the performance of this method to a phrase-based statistical machine translation system. We report a performance improvement of 0.9 BLEU points in translating from European to Brazilian Portuguese and 0.2 BLEU points when translating in the opposite direction. We also carried out a human evaluation experiment with native speakers of <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> which indicates that humans prefer the output produced by the neural-based system in comparison to the statistical system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3932/>Character Level Convolutional Neural Network for Indo-Aryan Language Identification<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan Language Identification</a></strong><br><a href=/people/m/mohamed-ali/>Mohamed Ali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3932><div class="card-body p-3 small">This submission is a description paper for our <a href=https://en.wikipedia.org/wiki/System>system</a> in ILI shared task</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3933/>German Dialect Identification Using Classifier Ensembles<span class=acl-fixed-case>G</span>erman Dialect Identification Using Classifier Ensembles</a></strong><br><a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3933><div class="card-body p-3 small">In this paper we present the GDI classification entry to the second German Dialect Identification (GDI) shared task organized within the scope of the VarDial Evaluation Campaign 2018. We present a system based on SVM classifier ensembles trained on <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words. The <a href=https://en.wikipedia.org/wiki/System>system</a> was trained on a collection of speech transcripts of five <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss-German dialects</a> provided by the organizers. The transcripts included in the dataset contained speakers from <a href=https://en.wikipedia.org/wiki/Canton_of_Basel-Stadt>Basel</a>, <a href=https://en.wikipedia.org/wiki/Canton_of_Bern>Bern</a>, Lucerne, and <a href=https://en.wikipedia.org/wiki/Canton_of_Z&#252;rich>Zurich</a>. Our entry in the <a href=https://en.wikipedia.org/wiki/Challenge_(competition)>challenge</a> reached 62.03 % F1 score and was ranked third out of eight teams.</div></div></div><hr><div id=w18-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-40.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-40/>Proceedings of the Third Workshop on Semantic Deep Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4000/>Proceedings of the Third Workshop on Semantic Deep Learning</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4002/>Word-Embedding based Content Features for Automated Oral Proficiency Scoring</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/c/chungmin-lee/>Chong Min Lee</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/x/xinhao-wang/>Xinhao Wang</a>
|
<a href=/people/i/ikkyu-choi/>Ikkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4002><div class="card-body p-3 small">In this study, we develop content features for an <a href=https://en.wikipedia.org/wiki/Score_(statistics)>automated scoring system</a> of non-native English speakers&#8217; spontaneous speech. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> calculate the <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure. We explore the impact of these new features on the automated scoring of two different question types : (a) providing opinions on familiar topics and (b) answering a question about a stimulus material. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> showed statistically significant correlations with the oral proficiency scores, and the combination of new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> with the speech-driven features achieved a small but significant further improvement for the latter question type. Further analyses suggested that the new <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were effective in assigning more accurate scores for responses with serious content issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4003/>Automatically Linking Lexical Resources with Word Sense Embedding Models</a></strong><br><a href=/people/l/luis-nieto-pina/>Luis Nieto-PiÃ±a</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4003><div class="card-body p-3 small">Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings. The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to formally defined <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>. We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts. We evaluate the applicability of these <a href=https://en.wikipedia.org/wiki/Hyperlink>links</a> in a task to retrieve instances of word sense unlisted in the lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4004/>Transferred Embeddings for Igbo Similarity, <a href=https://en.wikipedia.org/wiki/Analogy>Analogy</a>, and Diacritic Restoration Tasks<span class=acl-fixed-case>I</span>gbo Similarity, Analogy, and Diacritic Restoration Tasks</a></strong><br><a href=/people/i/ignatius-ezeani/>Ignatius Ezeani</a>
|
<a href=/people/i/ikechukwu-onyenwe/>Ikechukwu Onyenwe</a>
|
<a href=/people/m/mark-hepple/>Mark Hepple</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4004><div class="card-body p-3 small">Existing NLP models are mostly trained with data from well-resourced languages. Most <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a> face the challenge of lack of resources-data and technologies-for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>. Building these resources from scratch for each <a href=https://en.wikipedia.org/wiki/Minority_language>minority language</a> will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of <a href=https://en.wikipedia.org/wiki/Analogy>analogy</a>, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4007/>Knowledge Representation and Extraction at Scale</a></strong><br><a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4007><div class="card-body p-3 small">These days, most general knowledge question-answering systems rely on large-scale knowledge bases comprising billions of facts about millions of entities. Having a structured source of semantic knowledge means that we can answer questions involving single static facts (e.g. Who was the 8th president of the US?) or dynamically generated ones (e.g. How old is Donald Trump?). More importantly, we can answer questions involving multiple inference steps (Is the queen older than the president of the US?). In this talk, I&#8217;m going to be discussing some of the unique challenges that are involved with building and maintaining a consistent <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> for <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, extending <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with new facts and using <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to serve answers in multiple languages. I will focus on three recent projects from our group. First, a way of measuring the completeness of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, that is based on usage patterns. The definition of the usage of the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> is done in terms of the relation distribution of entities seen in question-answer logs. Instead of directly estimating the <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation distribution</a> of individual entities, it is generalized to the class signature of each entity. For example, users ask for baseball players&#8217; height, age, and batting average, so a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> is complete (with respect to baseball players) if every entity has facts for those three relations. Second, an investigation into fact extraction from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. I will present a method for creating distant (weak) supervision labels for training a large-scale relation extraction system. I will also discuss the effectiveness of neural network approaches by decoupling the model architecture from the feature design of a state-of-the-art neural network system. Surprisingly, a much simpler <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on similar <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> performs on par with the highly complex neural network system (at 75x reduction to the training time), suggesting that the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are a bigger contributor to the final performance. Finally, I will present the Fact Extraction and VERification (FEVER) dataset and challenge. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprises more than 185,000 human-generated claims extracted from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia pages</a>. False claims were generated by mutating true claims in a variety of ways, some of which were meaningaltering. During the verification step, annotators were required to label a claim for its validity and also supply full-sentence textual evidence from (potentially multiple) Wikipedia articles for the label. With <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a>, we aim to help create a new generation of transparent and interprable knowledge extraction systems.</div></div></div><hr><div id=w18-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-41/>Proceedings of the First International Workshop on Language Cognition and Computational Models</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4100/>Proceedings of the First International Workshop on Language Cognition and Computational Models</a></strong><br><a href=/people/m/manjira-sinha/>Manjira Sinha</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4102/>Detecting Linguistic Traces of Depression in Topic-Restricted Text : Attending to Self-Stigmatized Depression with <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a><span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/jt-wolohan/>JT Wolohan</a>
|
<a href=/people/m/misato-hiraga/>Misato Hiraga</a>
|
<a href=/people/a/atreyee-mukherjee/>Atreyee Mukherjee</a>
|
<a href=/people/z/zeeshan-ali-sayyed/>Zeeshan Ali Sayyed</a>
|
<a href=/people/m/matthew-millard/>Matthew Millard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4102><div class="card-body p-3 small">Natural language processing researchers have proven the ability of machine learning approaches to detect depression-related cues from language ; however, to date, these efforts have primarily assumed it was acceptable to leave depression-related texts in the data. Our concerns with this are twofold : first, that the models may be overfitting on depression-related signals, which may not be present in all depressed users (only those who talk about depression on social media) ; and second, that these models would under-perform for users who are sensitive to the public stigma of depression. This study demonstrates the validity to those concerns. We construct a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of texts</a> from 12,106 Reddit users and perform lexical and predictive analyses under two conditions : one where all text produced by the users is included and one where the depression data is withheld. We find significant differences in the language used by depressed users under the two conditions as well as a difference in the ability of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to correctly detect <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>. However, despite the lexical differences and reduced classification performanceeach of which suggests that users may be able to fool algorithms by avoiding direct discussion of depressiona still respectable overall performance suggests lexical models are reasonably robust and well suited for a role in a diagnostic or monitoring capacity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4103/>An OpenNMT Model to Arabic Broken Plurals<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>NMT</span> Model to <span class=acl-fixed-case>A</span>rabic Broken Plurals</a></strong><br><a href=/people/e/elsayed-issa/>Elsayed Issa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4103><div class="card-body p-3 small">Arabic Broken Plurals show an interesting phenomenon in Arabic morphology as they are formed by shifting the consonants of the syllables into different syllable patterns, and subsequently, the pattern of the word changes. The present paper, therefore, attempts to look at Arabic broken plurals from the perspective of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> by implementing an OpenNMT experiment to better understand and interpret the behavior of these plurals, especially when it comes to L2 acquisition. The results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is successful in predicting the <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic template</a>. However, it fails to predict certain <a href=https://en.wikipedia.org/wiki/Consonant>consonants</a> such as the <a href=https://en.wikipedia.org/wiki/Emphatic_consonant>emphatics</a> and the <a href=https://en.wikipedia.org/wiki/Guttural>gutturals</a>. This reinforces the fact that these <a href=https://en.wikipedia.org/wiki/Consonant>consonants</a> or sounds are the most difficult for L2 learners to acquire.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4104/>Enhancing Cohesion and Coherence of Fake Text to Improve Believability for Deceiving Cyber Attackers</a></strong><br><a href=/people/p/prakruthi-karuna/>Prakruthi Karuna</a>
|
<a href=/people/h/hemant-purohit/>Hemant Purohit</a>
|
<a href=/people/o/ozlem-uzuner/>Ãzlem Uzuner</a>
|
<a href=/people/s/sushil-jajodia/>Sushil Jajodia</a>
|
<a href=/people/r/rajesh-ganesan/>Rajesh Ganesan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4104><div class="card-body p-3 small">Ever increasing <a href=https://en.wikipedia.org/wiki/Ransomware>ransomware attacks</a> and thefts of intellectual property demand <a href=https://en.wikipedia.org/wiki/Computer_security>cybersecurity solutions</a> to protect critical documents. One emerging solution is to place fake text documents in the repository of critical documents for deceiving and catching cyber attackers. We can generate fake text documents by obscuring the salient information in legit text documents. However, the obscuring process can result in linguistic inconsistencies, such as broken co-references and illogical flow of ideas across the sentences, which can discern the fake document and render it unbelievable. In this paper, we propose a novel method to generate believable fake text documents by automatically improving the linguistic consistency of computer-generated fake text. Our method focuses on enhancing syntactic cohesion and semantic coherence across <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse segments</a>. We conduct experiments with <a href=https://en.wikipedia.org/wiki/Human_subject_research>human subjects</a> to evaluate the effect of believability improvements in distinguishing legit texts from fake texts. Results show that the probability to distinguish legit texts from believable fake texts is consistently lower than from fake texts that have not been improved in believability. This indicates the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in generating believable fake text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4106/>Finite State Reasoning for Presupposition Satisfaction</a></strong><br><a href=/people/j/jacob-collard/>Jacob Collard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4106><div class="card-body p-3 small">Sentences with presuppositions are often treated as uninterpretable or unvalued (neither true nor false) if their presuppositions are not satisfied. However, there is an open question as to how this satisfaction is calculated. In some cases, determining whether a <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition</a> is satisfied is not a trivial task (or even a decidable one), yet native speakers are able to quickly and confidently identify instances of <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition failure</a>. I propose that this can be accounted for with a form of possible world semantics that encapsulates some reasoning abilities, but is limited in its computational power, thus circumventing the need to solve computationally difficult problems. This can be modeled using a variant of the framework of finite state semantics proposed by Rooth (2017). A few modifications to this <a href=https://en.wikipedia.org/wiki/Formal_system>system</a> are necessary, including its extension into a <a href=https://en.wikipedia.org/wiki/Three-valued_logic>three-valued logic</a> to account for <a href=https://en.wikipedia.org/wiki/Presupposition>presupposition</a>. Within this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, the logic necessary to calculate presupposition satisfaction is readily available, but there is no risk of needing exceptional computational power. This correctly predicts that certain presuppositions will not be calculated intuitively, while others can be easily evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4107/>Language-Based Automatic Assessment of Cognitive and Communicative Functions Related to Parkinsonâs Disease<span class=acl-fixed-case>P</span>arkinsonâs Disease</a></strong><br><a href=/people/l/lesley-jessiman/>Lesley Jessiman</a>
|
<a href=/people/g/gabriel-murray/>Gabriel Murray</a>
|
<a href=/people/m/mckenzie-braley/>McKenzie Braley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4107><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for detecting evidence of Parkinson&#8217;s disease from transcribed speech of subjects who are describing everyday tasks. Experiments reveal the difficulty of treating this as a binary classification task, and a multi-class approach yields superior results. We also show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can be used to predict <a href=https://en.wikipedia.org/wiki/Cognitive_skill>cognitive abilities</a> across all subjects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4109/>Word-word Relations in Dementia and Typical Aging</a></strong><br><a href=/people/n/natalia-arias-trejo/>Natalia Arias-Trejo</a>
|
<a href=/people/a/aline-minto-garcia/>Aline Minto-GarcÃ­a</a>
|
<a href=/people/d/diana-i-luna-umanzor/>Diana I. Luna-Umanzor</a>
|
<a href=/people/a/alma-e-rios-ponce/>Alma E. RÃ­os-Ponce</a>
|
<a href=/people/b/balderas-pliego-mariana/>Balderas-Pliego Mariana</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4109><div class="card-body p-3 small">Older adults tend to suffer a decline in some of their cognitive capabilities, being language one of least affected processes. Word association norms (WAN) also known as free word associations reflect word-word relations, the participant reads or hears a word and is asked to write or say the first word that comes to mind. Free word associations show how the organization of <a href=https://en.wikipedia.org/wiki/Semantic_memory>semantic memory</a> remains almost unchanged with age. We have performed a WAN task with very small samples of older adults with Alzheimer&#8217;s disease (AD), vascular dementia (VaD) and mixed dementia (MxD), and also with a control group of typical aging adults, matched by age, sex and education. All of them are native speakers of <a href=https://en.wikipedia.org/wiki/Mexican_Spanish>Mexican Spanish</a>. The results show, as expected, that <a href=https://en.wikipedia.org/wiki/Alzheimer&#8217;s_disease>Alzheimer disease</a> has a very important impact in lexical retrieval, unlike vascular and mixed dementia. This suggests that linguistic tests elaborated from <a href=https://en.wikipedia.org/wiki/Wide_area_network>WAN</a> can be also used for detecting <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a> at early stages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4110/>Part-of-Speech Annotation of English-Assamese code-mixed texts : Two Approaches<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>A</span>ssamese code-mixed texts: Two Approaches</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/m/manas-jyoti-bora/>Manas Jyoti Bora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4110><div class="card-body p-3 small">In this paper, we discuss the development of a <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagger</a> for English-Assamese code-mixed texts. We provide a comparison of 2 approaches to annotating code-mixed data a) annotation of the texts from the two languages using monolingual resources from each language and b) annotation of the text through a different resource created specifically for code-mixed data. We present a comparative study of the efforts required in each <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> and the final performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. Based on this, we argue that it might be a better approach to develop new technologies using code-mixed data instead of monolingual, &#8216;clean&#8217; data, especially for those languages where we do not have significant tools and technologies available till now.</div></div></div><hr><div id=w18-42><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-42.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-42/>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4200/>Proceedings of the First Workshop on Natural Language Processing for <span class=acl-fixed-case>I</span>nternet Freedom</a></strong><br><a href=/people/c/chris-brew/>Chris Brew</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4203/>Creative Language Encoding under Censorship</a></strong><br><a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4203><div class="card-body p-3 small">People often create obfuscated language for online communication to avoid <a href=https://en.wikipedia.org/wiki/Internet_censorship>Internet censorship</a>, share sensitive information, express strong sentiment or emotion, plan for secret actions, trade illegal products, or simply hold interesting conversations. In this position paper we systematically categorize human-created obfuscated language on various levels, investigate their basic mechanisms, give an overview on automated techniques needed to simulate human encoding. These encoders have potential to frustrate and evade, co-evolve with dynamic human or automated decoders, and produce interesting and adoptable code words. We also summarize remaining challenges for future research on the interaction between Natural Language Processing (NLP) and <a href=https://en.wikipedia.org/wiki/Encryption>encryption</a>, and leveraging NLP techniques for encoding and decoding.</div></div></div><hr><div id=w18-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-43/>Proceedings of the Workshop Events and Stories in the News 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4300/>Proceedings of the Workshop Events and Stories in the News 2018</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/b/ben-miller/>Ben Miller</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/d/david-caswell/>David Caswell</a>
|
<a href=/people/s/susan-windisch-brown/>Susan W. Brown</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4301/>Every Object Tells a Story</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4301><div class="card-body p-3 small">Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with <a href=https://en.wikipedia.org/wiki/Verb>verbs</a> and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of <a href=https://en.wikipedia.org/wiki/Image>images</a>. By semantically grounding <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event descriptions</a> in their visualization, the importance of <a href=https://en.wikipedia.org/wiki/Object-oriented_programming>object-based attributes</a> becomes more apparent. In this position paper, we look at the narrative structure of objects : that is, how objects reference events through their intrinsic attributes, such as <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, purposes, and <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4302/>A Rich Annotation Scheme for Mental Events</a></strong><br><a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/p/pavlina-peskova/>PavlÃ­na PeÅ¡kovÃ¡</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/s/sook-kyung-lee/>Sook-kyung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4302><div class="card-body p-3 small">We present a rich annotation scheme for the structure of mental events. Mental events are those in which the verb describes a mental state or process, usually oriented towards an external situation. While <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>physical events</a> have been described in detail and there are numerous studies of their <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, <a href=https://en.wikipedia.org/wiki/Mental_event>mental events</a> are less thoroughly studied. The annotation scheme proposed here is based on decompositional analyses in the semantic and typological linguistic literature. The scheme was applied to the <a href=https://en.wikipedia.org/wiki/Text_corpus>news corpus</a> from the 2016 Events workshop, and <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> of the test annotation provides suggestions for refinement and clarification of the annotation scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4304/>Identifying the Discourse Function of News Article Paragraphs</a></strong><br><a href=/people/w/w-victor-yarlott/>W. Victor Yarlott</a>
|
<a href=/people/c/cristina-cornelio/>Cristina Cornelio</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4304><div class="card-body p-3 small">Discourse structure is a key aspect of all forms of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, providing valuable information both to humans and machines. We applied the hierarchical theory of news discourse developed by van Dijk to examine how paragraphs operate as units of discourse structure within news articleswhat we refer to here as document-level discourse. This document-level discourse provides a characterization of the content of each paragraph that describes its relation to the events presented in the article (such as main events, backgrounds, and consequences) as well as to other components of the story (such as commentary and evaluation). The purpose of a news discourse section is of great utility to story understanding as it affects both the importance and temporal order of items introduced in the texttherefore, if we know the news discourse purpose for different sections, we should be able to better rank events for their importance and better construct timelines. We test two hypotheses : first, that people can reliably annotate news articles with van Dijk&#8217;s theory ; second, that we can reliably predict these labels using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We show that people have a high degree of agreement with each other when annotating the theory (F1 0.8, Cohen&#8217;s kappa 0.6), demonstrating that it can be both learned and reliably applied by human annotators. Additionally, we demonstrate first steps toward <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> of the <a href=https://en.wikipedia.org/wiki/Theory>theory</a>, achieving a performance of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> = 0.54, which is 65 % of human performance. Moreover, we have generated a gold-standard, adjudicated corpus of 50 documents for document-level discourse annotation based on the ACE Phase 2 corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4305/>An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines</a></strong><br><a href=/people/s/shi-yuan/>Shi Yuan</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4305><div class="card-body p-3 small">This study evaluates the performance of four information extraction tools (extractors) on identifying health claims in health news headlines. A <a href=https://en.wikipedia.org/wiki/Health_claim>health claim</a> is defined as a triplet : IV (what is being manipulated), DV (what is being measured) and their relation. Tools that can identify <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a> provide the foundation for evaluating the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of these <a href=https://en.wikipedia.org/wiki/Health_claim>claims</a> against authoritative resources. The evaluation result shows that 26 % headlines do not in-clude <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, and all extractors face difficulty separating them from the rest. For those with <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, OPENIE-5.0 performed the best with <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> at 0.6 level for ex-tracting IV-relation-DV. However, the characteristic linguistic structures in health news headlines, such as <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>incomplete sentences</a> and non-verb relations, pose particular challenge to existing tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4307/>Can You Spot the Semantic Predicate in this Video?</a></strong><br><a href=/people/c/christopher-reale/>Christopher Reale</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/h/heesung-kwon/>Heesung Kwon</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4307><div class="card-body p-3 small">We propose a method to improve human activity recognition in video by leveraging semantic information about the target activities from an expert-defined linguistic resource, <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>. Our hypothesis is that activities that share similar event semantics, as defined by the semantic predicates of <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>, will be more likely to share some visual components. We use a deep convolutional neural network approach as a baseline and incorporate linguistic information from <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We present results of experiments showing the added information has negligible impact on <a href=https://en.wikipedia.org/wiki/Computer_vision>recognition</a> performance. We discuss how this may be because the lexical semantic information defined by <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> is generally not visually salient given the video processing approach used here, and how we may handle this in future approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4309/>On Training Classifiers for Linking Event Templates</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/f/fredi-saric/>Fredi Å ariÄ</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/m/martin-atkinson/>Martin Atkinson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4309><div class="card-body p-3 small">The paper reports on exploring various machine learning techniques and a range of textual and meta-data features to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> for linking related event templates automatically extracted from <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news</a>. With the best <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>textual features</a> only we achieved 94.7 % (92.9 %) <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> on GOLD (SILVER) dataset. These figures were further improved to 98.6 % (GOLD) and 97 % (SILVER) F1 score by adding meta-data features, mainly thanks to the strong discriminatory power of automatically extracted geographical information related to <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4310/>HEI : Hunter Events Interface A <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> based on <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the detection and reasoning about events<span class=acl-fixed-case>HEI</span>: Hunter Events Interface A platform based on services for the detection and reasoning about events</a></strong><br><a href=/people/a/antonio-sorgente/>Antonio Sorgente</a>
|
<a href=/people/a/antonio-calabrese/>Antonio Calabrese</a>
|
<a href=/people/g/gianluca-coda/>Gianluca Coda</a>
|
<a href=/people/p/paolo-vanacore/>Paolo Vanacore</a>
|
<a href=/people/f/francesco-mele/>Francesco Mele</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4310><div class="card-body p-3 small">In this paper we present the definition and implementation of the Hunter Events Interface (HEI) System. The HEI System is a system for events annotation and temporal reasoning in Natural Language Texts and media, mainly oriented to texts of historical and cultural contents available on the Web. In this work we assume that <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> are defined through various components : <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>actions</a>, participants, <a href=https://en.wikipedia.org/wiki/Location>locations</a>, and occurrence intervals. The HEI system, through independent services, locates (annotates) the various components, and successively associates them to a specific event. The objective of this work is to build a system integrating <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the identification of events, the discovery of their connections, and the evaluation of their consistency. We believe this interface is useful to develop applications that use the notion of story, to integrate data of digital cultural archives, and to build systems of fruition in the same field. The HEI system has been partially developed within the TrasTest project</div></div></div><hr><div id=w18-44><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-44.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-44/>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4400/>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (<span class=acl-fixed-case>TRAC</span>-2018)</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4401/>Benchmarking Aggression Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4401><div class="card-body p-3 small">In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-1) at COLING 2018. The task was to develop a <a href=https://en.wikipedia.org/wiki/Social_class>classifier</a> that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> (in both Roman and Devanagari script) and <a href=https://en.wikipedia.org/wiki/English_language>English</a> for training and validation. For testing, two different sets-one from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and another from a different social media-were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> respectively. The results presented in this report depict how challenging the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4402/>RiTUAL-UH at TRAC 2018 Shared Task : Aggression Identification<span class=acl-fixed-case>R</span>i<span class=acl-fixed-case>TUAL</span>-<span class=acl-fixed-case>UH</span> at <span class=acl-fixed-case>TRAC</span> 2018 Shared Task: Aggression Identification</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/d/deepthi-mave/>Deepthi Mave</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4402><div class="card-body p-3 small">This paper presents our <a href=https://en.wikipedia.org/wiki/System>system</a> for TRAC 2018 Shared Task on Aggression Identification. Our best systems for the English dataset use a combination of lexical and semantic features. However, for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi data</a> using only lexical features gave us the best results. We obtained weighted F1-measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4405/>Cyberbullying Intervention Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/j/jianhong-zhang/>Jianhong Zhang</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4405><div class="card-body p-3 small">This paper describes the process of building a cyberbullying intervention interface driven by a machine-learning based text-classification service. We make two main contributions. First, we show that <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> can be identified in real-time before it takes place, with available machine learning and natural language processing tools. Second, we present a mechanism that provides individuals with early feedback about how other people would feel about wording choices in their messages before they are sent out. This <a href=https://en.wikipedia.org/wiki/User_interface>interface</a> not only gives a chance for the user to revise the text, but also provides a system-level flagging / intervention in a situation related to <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4406/>LSTMs with Attention for Aggression Detection<span class=acl-fixed-case>LSTM</span>s with Attention for Aggression Detection</a></strong><br><a href=/people/n/nishant-nikhil/>Nishant Nikhil</a>
|
<a href=/people/r/ramit-pahwa/>Ramit Pahwa</a>
|
<a href=/people/m/mehul-kumar-nirala/>Mehul Kumar Nirala</a>
|
<a href=/people/r/rohan-khilnani/>Rohan Khilnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4406><div class="card-body p-3 small">In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted for the shared task on Aggression Identification in <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments by the team Nishnik. Previous works demonstrate that <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> ranks 17th and 10th in the corresponding English subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4407/>TRAC-1 Shared Task on Aggression Identification : IIT(ISM)@COLINGâ18<span class=acl-fixed-case>TRAC</span>-1 Shared Task on Aggression Identification: <span class=acl-fixed-case>IIT</span>(<span class=acl-fixed-case>ISM</span>)@<span class=acl-fixed-case>COLING</span>â18</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/g/guggilla-bhanodai/>Guggilla Bhanodai</a>
|
<a href=/people/r/rajendra-pamula/>Rajendra Pamula</a>
|
<a href=/people/m/maheshwar-reddy-chennuru/>Maheshwar Reddy Chennuru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4407><div class="card-body p-3 small">This paper describes the work that our team bhanodaig did at Indian Institute of Technology (ISM) towards TRAC-1 Shared Task on Aggression Identification in Social Media for COLING 2018. In this paper we label aggression identification into three categories : Overtly Aggressive, Covertly Aggressive and Non-aggressive. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. We participated in two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> named as English (Facebook) task and English (Social Media) task. For English (Facebook) task System 05 was our best run (i.e. 0.3572) above the Random Baseline (i.e. 0.3535). For English (Social Media) task our <a href=https://en.wikipedia.org/wiki/System>system</a> 02 got the value (i.e. 0.1960) below the Random Bseline (i.e. 0.3477). For all of our runs we used Long Short-Term Memory model. Overall, our performance is not satisfactory. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4408/>An Ensemble Approach for Aggression Identification in English and Hindi Text<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>H</span>indi Text</a></strong><br><a href=/people/a/arjun-roy/>Arjun Roy</a>
|
<a href=/people/p/prashant-kapil/>Prashant Kapil</a>
|
<a href=/people/k/kingshuk-basak/>Kingshuk Basak</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4408><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted in the shared task at COLING 2018 TRAC-1 : Aggression Identification. The objective of this task was to predict online aggression spread through online textual post or comment. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was released in two languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We submitted a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Both the systems are based on an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble architecture</a> where the individual models are based on <a href=https://en.wikipedia.org/wiki/Convoluted_neural_network>Convoluted Neural Network</a> and <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machine</a>. Evaluation shows promising results for both the languages. The total submission for <a href=https://en.wikipedia.org/wiki/English_language>English</a> was 30 and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> was 15. Our system on <a href=https://en.wikipedia.org/wiki/Facebook>English facebook</a> and social media obtained F1 score of 0.5151 and 0.5099 respectively where <a href=https://en.wikipedia.org/wiki/Facebook>Hindi facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> obtained F1 score of 0.5599 and 0.3790 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4409/>Aggression Identification and Multi Lingual Word Embeddings</a></strong><br><a href=/people/t/thiago-galery/>Thiago Galery</a>
|
<a href=/people/e/efstathios-charitos/>Efstathios Charitos</a>
|
<a href=/people/y/ye-tian/>Ye Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4409><div class="card-body p-3 small">The system presented here took part in the 2018 Trolling, Aggression and Cyberbullying shared task (Forest and Trees team) and uses a Gated Recurrent Neural Network architecture (Cho et al., 2014) in an attempt to assess whether combining pre-trained English and Hindi fastText (Mikolov et al., 2018) word embeddings as a representation of the sequence input would improve classification performance. The motivation for this comes from the fact that the shared task data for <a href=https://en.wikipedia.org/wiki/English_language>English</a> contained many Hindi tokens and therefore some users might be doing <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> : the alternation between two or more languages in communication. To test this hypothesis, we also aligned Hindi and English vectors using pre-computed SVD matrices that pulls representations from different languages into a common space (Smith et al., 2017). Two conditions were tested : (i) one with standard pre-trained fastText word embeddings where each <a href=https://en.wikipedia.org/wiki/Hindi>Hindi word</a> is treated as an OOV token, and (ii) another where word embeddings for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> are loaded in a common vector space, so <a href=https://en.wikipedia.org/wiki/Hindi>Hindi tokens</a> can be assigned a meaningful representation. We submitted the second (i.e., multilingual) <a href=https://en.wikipedia.org/wiki/System>system</a> and obtained the scores of 0.531 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-FB dataset and 0.438 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-TW dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4410/>A K-Competitive Autoencoder for Aggression Detection in Social Media Text</a></strong><br><a href=/people/p/promita-maitra/>Promita Maitra</a>
|
<a href=/people/r/ritesh-sarkhel/>Ritesh Sarkhel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4410><div class="card-body p-3 small">We present an approach to detect <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from social media text in this work. A winner-takes-all autoencoder, called Emoti-KATE is proposed for this purpose. Using a log-normalized, weighted word-count vector at input dimensions, the <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> simulates a competition between neurons in the hidden layer to minimize the reconstruction loss between the input and final output layers. We have evaluated the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> provided by the organizers of TRAC workshop, 2018. Using the <a href=https://en.wikipedia.org/wiki/Code>encoding</a> generated by Emoti-KATE, a 3-way classification is performed for every social media text in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Each data point is classified as &#8216;Overtly Aggressive&#8217;, &#8216;Covertly Aggressive&#8217; or &#8216;Non-aggressive&#8217;. Results show that our (team name : PMRS) proposed method is able to achieve promising results on some of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, we have described the effects of introducing an winner-takes-all autoencoder for the task of aggression detection, reported its performance on four different datasets, analyzed some of its limitations and how to improve its performance in future works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4413/>Degree based Classification of Harmful Speech using Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/s/sanjana-sharma/>Sanjana Sharma</a>
|
<a href=/people/s/saksham-agrawal/>Saksham Agrawal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4413><div class="card-body p-3 small">Harmful speech has various forms and it has been plaguing the <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in different ways. If we need to crackdown different degrees of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets we created based on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological classes</a> and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it&#8217;s automatic detection more robust and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4414/>Aggressive Language Identification Using Word Embeddings and Sentiment Features</a></strong><br><a href=/people/c/constantin-orasan/>Constantin OrÄsan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4414><div class="card-body p-3 small">This paper describes our participation in the First Shared Task on Aggression Identification. The method proposed relies on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to identify social media texts which contain <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a>. The main <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> employed by our method are information extracted from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and the output of a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyser</a>. Several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> and different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were tried. The official submissions used <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a> and <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forests</a>. The official evaluation showed that for texts similar to the ones in the training dataset Random Forests work best, whilst for texts which are different SVMs are a better choice. The evaluation also showed that despite its simplicity the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well when compared with more elaborated <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4415/>Aggression Detection in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> using Deep Neural Networks</a></strong><br><a href=/people/s/sreekanth-madisetty/>Sreekanth Madisetty</a>
|
<a href=/people/m/maunendra-sankar-desarkar/>Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4415><div class="card-body p-3 small">With the rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> coupled with almost non-existent <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a> in many such systems, aggressive contents have been observed to rise in such <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>. In this paper, we work on the problem of aggression detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Aggression can sometimes be expressed directly or overtly or it can be hidden or covert in the text. On the other hand, most of the content in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is non-aggressive in nature. We propose an ensemble based system to classify an input post to into one of three classes, namely, Overtly Aggressive, Covertly Aggressive, and Non-aggressive. Our approach uses three deep learning methods, namely, Convolutional Neural Networks (CNN) with five layers (input, convolution, pooling, hidden, and output), Long Short Term Memory networks (LSTM), and Bi-directional Long Short Term Memory networks (Bi-LSTM). A majority voting based ensemble method is used to combine these classifiers (CNN, LSTM, and Bi-LSTM). We trained our method on Facebook comments dataset and tested on Facebook comments (in-domain) and other social media posts (cross-domain). Our system achieves the F1-score (weighted) of 0.604 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and 0.508 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>social media posts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4416/>Merging Datasets for Aggressive Text Identification</a></strong><br><a href=/people/p/paula-fortuna/>Paula Fortuna</a>
|
<a href=/people/j/jose-ferreira/>JosÃ© Ferreira</a>
|
<a href=/people/l/luiz-pires/>Luiz Pires</a>
|
<a href=/people/g/guilherme-routar/>Guilherme Routar</a>
|
<a href=/people/s/sergio-nunes/>SÃ©rgio Nunes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4416><div class="card-body p-3 small">This paper presents the approach of the team groutar to the shared task on Aggression Identification, considering the test sets in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, both from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and general Social Media. This experiment aims to test the effect of merging new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and apply <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> to data from different <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>, indicating that more generalized models can be an advantage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4417/>Cyberbullying Detection Task : the EBSI-LIA-UNAM System (ELU) at COLINGâ18 TRAC-1<span class=acl-fixed-case>EBSI</span>-<span class=acl-fixed-case>LIA</span>-<span class=acl-fixed-case>UNAM</span> System (<span class=acl-fixed-case>ELU</span>) at <span class=acl-fixed-case>COLING</span>â18 <span class=acl-fixed-case>TRAC</span>-1</a></strong><br><a href=/people/i/ignacio-arroyo-fernandez/>Ignacio Arroyo-FernÃ¡ndez</a>
|
<a href=/people/d/dominic-forest/>Dominic Forest</a>
|
<a href=/people/j/juan-manuel-torres-moreno/>Juan-Manuel Torres-Moreno</a>
|
<a href=/people/m/mauricio-carrasco-ruiz/>Mauricio Carrasco-Ruiz</a>
|
<a href=/people/t/thomas-legeleux/>Thomas Legeleux</a>
|
<a href=/people/k/karen-joannette/>Karen Joannette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4417><div class="card-body p-3 small">The phenomenon of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> has growing in worrying proportions with the development of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Forums and <a href=https://en.wikipedia.org/wiki/Chat_room>chat rooms</a> are spaces where serious damage can now be done to others, while the tools for avoiding on-line spills are still limited. This study aims to assess the ability that both classical and state-of-the-art vector space modeling methods provide to well known <a href=https://en.wikipedia.org/wiki/Machine_learning>learning machines</a> to identify aggression levels in social network cyberbullying (i.e. social network posts manually labeled as Overtly Aggressive, Covertly Aggressive and Non-aggressive). To this end, an exploratory stage was performed first in order to find relevant settings to test, i.e. by using training and development samples, we trained multiple learning machines using multiple vector space modeling methods and discarded the less informative configurations. Finally, we selected the two best settings and their <a href=https://en.wikipedia.org/wiki/Electoral_system>voting combination</a> to form three competing <a href=https://en.wikipedia.org/wiki/Electoral_system>systems</a>. These systems were submitted to the competition of the TRACK-1 task of the Workshop on Trolling, Aggression and Cyberbullying. Our voting combination system resulted second place in predicting Aggression levels on a test set of untagged social network posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4418/>Aggression Identification Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Data Augmentation</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4418><div class="card-body p-3 small">Social media platforms allow users to share and discuss their opinions online. However, a minority of user posts is aggressive, thereby hinders respectful discussion, and at an extreme level is liable to prosecution. The automatic identification of such harmful posts is important, because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can support the costly manual moderation of online discussions. Further, the <a href=https://en.wikipedia.org/wiki/Automation>automation</a> allows unprecedented analyses of discussion datasets that contain millions of posts. This system description paper presents our submission to the First Shared Task on Aggression Identification. We propose to augment the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to increase the number of labeled comments from 15,000 to 60,000. Thereby, we introduce <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>linguistic variety</a> into the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. As a consequence of the larger amount of training data, we are able to train a special <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural net</a>, which generalizes especially well to unseen data. To further boost the performance, we combine this <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural net</a> with three logistic regression classifiers trained on character and word n-grams, and hand-picked syntactic features. This <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> is more robust than the individual single models. Our team named Julian achieves an F1-score of 60 % on both English datasets, 63 % on the Hindi Facebook dataset, and 38 % on the Hindi Twitter dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4419 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4419/>Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text</a></strong><br><a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4419><div class="card-body p-3 small">In this paper, we propose a novel deep-learning architecture for text classification, named cross segment-and-concatenate multi-task learning (CSC-MTL). We use CSC-MTL to improve the performance of cyber-aggression detection from text. Our approach provides a robust shared feature representation for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> by detecting contrasts and similarities among polarity and neutral classes. We participated in the cyber-aggression shared task under the team name uOttawa. We report 59.74 % F1 performance for the Facebook test set and 56.9 % for the Twitter test set, for detecting <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4420/>Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4420><div class="card-body p-3 small">Comment sections of online news providers have enabled millions to share and discuss their opinions on news topics. Today, moderators ensure respectful and informative discussions by deleting not only insults, <a href=https://en.wikipedia.org/wiki/Defamation>defamation</a>, and hate speech, but also unverifiable facts. This process has to be transparent and comprehensive in order to keep the community engaged. Further, news providers have to make sure to not give the impression of <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> or dissemination of fake news. Yet <a href=https://en.wikipedia.org/wiki/Moderation_system>manual moderation</a> is very expensive and becomes more and more unfeasible with the increasing amount of comments. Hence, we propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles. For evaluation, we present experiments on a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 3 million news comments annotated by a team of professional moderators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4421/>Textual Aggression Detection through <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/a/antonela-tommasel/>Antonela Tommasel</a>
|
<a href=/people/j/juan-manuel-rodriguez/>Juan Manuel Rodriguez</a>
|
<a href=/people/d/daniela-godoy/>Daniela Godoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4421><div class="card-body p-3 small">Cyberbullying and cyberaggression are serious and widespread issues increasingly affecting Internet users. With the widespread of <a href=https://en.wikipedia.org/wiki/Social_media>social media networks</a>, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a>, once limited to particular places, can now occur anytime and anywhere. Cyberaggression refers to aggressive online behaviour that aims at harming other individuals, and involves rude, insulting, offensive, teasing or demoralising comments through <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Considering the dangerous consequences that cyberaggression has on its victims and its rapid spread amongst internet users (specially kids and teens), it is crucial to understand how <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> occurs to prevent <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> from escalating. Given the massive information overload on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, there is an imperious need to develop intelligent techniques to automatically detect harmful content, which would allow the large-scale social media monitoring and early detection of undesired situations. This paper presents the Isistanitos&#8217;s approach for detecting aggressive content in multiple social media sites. The approach is based on combining Support Vector Machines and Recurrent Neural Network models for analysing a wide-range of <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character</a>, <a href=https://en.wikipedia.org/wiki/Word>word</a>, word embeddings, sentiment and irony features. Results confirmed the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> (particularly for detecting covert aggressions), showing the limitations of traditionally used <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4422/>Combining Shallow and Deep Learning for Aggressive Text Detection</a></strong><br><a href=/people/v/viktor-golem/>Viktor Golem</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Å najder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4422><div class="card-body p-3 small">We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Aggression manifests in a variety of ways. Unlike some forms of <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> that are impossible to prevent in day-to-day life, aggressive speech abounding on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages. The first step in achieving this is to detect such <a href=https://en.wikipedia.org/wiki/Message>messages</a>. The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is further complicated by the noisy nature of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a> on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Our system learns to distinguish between <a href=https://en.wikipedia.org/wiki/Aggression>open aggression</a>, <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>covert aggression</a>, and <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>non-aggression</a> in social media texts. We tried different machine learning approaches, including traditional (shallow) machine learning models, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, and a combination of both. We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4423 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4423/>Filtering Aggression from the Multilingual Social Media Feed</a></strong><br><a href=/people/s/sandip-modha/>Sandip Modha</a>
|
<a href=/people/p/prasenjit-majumder/>Prasenjit Majumder</a>
|
<a href=/people/t/thomas-mandl/>Thomas Mandl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4423><div class="card-body p-3 small">This paper describes the participation of team DA-LD-Hildesheim from the Information Retrieval Lab(IRLAB) at DA-IICT Gandhinagar, India in collaboration with the University of Hildesheim, Germany and LDRP-ITR, Gandhinagar, India in a shared task on Aggression Identification workshop in COLING 2018. The objective of the shared task is to identify the level of aggression from the User-Generated contents within <a href=https://en.wikipedia.org/wiki/Social_media>Social media</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Devanagari>Devnagiri Hindi</a> and Romanized Hindi. Aggression levels are categorized into three predefined classes namely : &#8216;Overtly Aggressive &#8216;, &#8216;Covertly Aggressive &#8216;and &#8216;Non-aggressive &#8216;. The participating teams are required to develop a multi-class classifier which classifies <a href=https://en.wikipedia.org/wiki/User-generated_content>User-generated content</a> into these pre-defined classes. Instead of relying on a <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a>, we have used pre-trained vectors for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. We have performed experiments with standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning classifiers</a>. In addition, we have developed various deep learning models for the multi-class classification problem. Using the <a href=https://en.wikipedia.org/wiki/Data_validation>validation data</a>, we found that validation accuracy of our deep learning models outperform all standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> and voting based ensemble techniques and results on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a> support these findings. We have also found that <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyper-parameters</a> of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> are the keys to improve the results.</div></div></div><hr><div id=w18-45><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-45.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-45/>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4500/>Proceedings of the Second Joint <span class=acl-fixed-case>SIGHUM</span> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4501/>Learning Diachronic Analogies to Analyze Concept Change</a></strong><br><a href=/people/m/matthias-orlikowski/>Matthias Orlikowski</a>
|
<a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4501><div class="card-body p-3 small">We propose to study the evolution of concepts by learning to complete diachronic analogies between lists of terms which relate to the same concept at different points in time. We present a number of models based on operations on word embedddings that correspond to different assumptions about the characteristics of diachronic analogies and change in concept vocabularies. These are tested in a quantitative evaluation for nine different <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> on a <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_the_Netherlands>corpus of Dutch newspapers</a> from the 1950s and 1980s. We show that a model which treats the concept terms as analogous and learns weights to compensate for diachronic changes (weighted linear combination) is able to more accurately predict the missing term than a learned transformation and two baselines for most of the evaluated concepts. We also find that all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> tend to be coherent in relation to the represented concept, but less discriminative in regard to other concepts. Additionally, we evaluate the effect of aligning the time-specific embedding spaces using orthogonal Procrustes, finding varying effects on performance, depending on the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, concept and evaluation metric. For the weighted linear combination, however, results improve with alignment in a majority of cases. All related code is released publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4502/>A Linked Coptic Dictionary Online<span class=acl-fixed-case>C</span>optic Dictionary Online</a></strong><br><a href=/people/f/frank-feder/>Frank Feder</a>
|
<a href=/people/m/maxim-kupreyev/>Maxim Kupreyev</a>
|
<a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/c/caroline-t-schroeder/>Caroline T. Schroeder</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4502><div class="card-body p-3 small">We describe a new project publishing a freely available <a href=https://en.wikipedia.org/wiki/Online_dictionary>online dictionary</a> for <a href=https://en.wikipedia.org/wiki/Coptic_language>Coptic</a>. The dictionary encompasses comprehensive cross-referencing mechanisms, including linking entries to an online scanned edition of Crum&#8217;s Coptic Dictionary, internal cross-references and etymological information, translated searchable definitions in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and linked corpus data which provides frequencies and corpus look-up for headwords and multiword expressions. Headwords are available for linking in external projects using a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST API</a>. We describe the challenges in encoding our <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> using <a href=https://en.wikipedia.org/wiki/Text_Encoding_Initiative>TEI XML</a> and implementing linking mechanisms to construct a Web interface querying frequency information, which draw on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a> to recognize inflected forms in context. We evaluate our <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>&#8217;s coverage using digital corpora of Coptic available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4505/>Analysis of Rhythmic Phrasing : <a href=https://en.wikipedia.org/wiki/Feature_engineering>Feature Engineering</a> vs. <a href=https://en.wikipedia.org/wiki/Representation_learning>Representation Learning</a> for Classifying Readout Poetry</a></strong><br><a href=/people/t/timo-baumann/>Timo Baumann</a>
|
<a href=/people/h/hussein-hussein/>Hussein Hussein</a>
|
<a href=/people/b/burkhard-meyer-sickendiek/>Burkhard Meyer-Sickendiek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4505><div class="card-body p-3 small">We show how to classify the phrasing of readout poems with the help of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> that use manually engineered features or automatically learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We investigate modern and postmodern poems from the webpage lyrikline, and focus on two exemplary rhythmical patterns in order to detect the rhythmic phrasing : The Parlando and the Variable Foot. These rhythmical patterns have been compared by using two important theoretical works : The Generative Theory of Tonal Music and the Rhythmic Phrasing in English Verse. Using both, we focus on a combination of four different features : The grouping structure, the <a href=https://en.wikipedia.org/wiki/Metre_(music)>metrical structure</a>, the time-span-variation, and the prolongation in order to detect the rhythmic phrasing in the two rhythmical types. We use manually engineered features based on <a href=https://en.wikipedia.org/wiki/Speech_recognition>text-speech alignment</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. We also train a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to learn its own <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> based on text, speech and audio during pauses. The <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> outperforms manual feature engineering, reaching an <a href=https://en.wikipedia.org/wiki/F-measure>f-measure</a> of 0.85.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4507/>The Historical Significance of Textual Distances</a></strong><br><a href=/people/t/ted-underwood/>Ted Underwood</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4507><div class="card-body p-3 small">Measuring similarity is a basic task in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, and now often a building-block for more complex arguments about <a href=https://en.wikipedia.org/wiki/Cultural_change>cultural change</a>. But do measures of textual similarity and <a href=https://en.wikipedia.org/wiki/Distance>distance</a> really correspond to evidence about cultural proximity and differentiation? To explore that question empirically, this paper compares textual and social measures of the similarities between genres of English-language fiction. Existing measures of textual similarity (cosine similarity on tf-idf vectors or topic vectors) are also compared to new strategies that strive to anchor textual measurement in a social context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4510/>Normalizing Early English Letters to Present-day English Spelling<span class=acl-fixed-case>E</span>nglish Letters to Present-day <span class=acl-fixed-case>E</span>nglish Spelling</a></strong><br><a href=/people/m/mika-hamalainen/>Mika HÃ¤mÃ¤lÃ¤inen</a>
|
<a href=/people/t/tanja-saily/>Tanja SÃ¤ily</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a>
|
<a href=/people/j/jorg-tiedemann/>JÃ¶rg Tiedemann</a>
|
<a href=/people/e/eetu-makela/>Eetu MÃ¤kelÃ¤</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4510><div class="card-body p-3 small">This paper presents multiple methods for normalizing the most deviant and infrequent historical spellings in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of <a href=https://en.wikipedia.org/wiki/Letter_(message)>personal correspondence</a> from the 15th to the 19th century. The methods include machine translation (neural and statistical), <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> and rule-based FST. Different <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization methods</a> are compared and evaluated. All of the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> have their own strengths in <a href=https://en.wikipedia.org/wiki/Word_normalization>word normalization</a>. This calls for finding ways of combining the results from these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to leverage their individual strengths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4513/>A Method for Human-Interpretable Paraphrasticality Prediction</a></strong><br><a href=/people/m/maria-moritz/>Maria Moritz</a>
|
<a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/s/sven-buechel/>Sven BÃ¼chel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4513><div class="card-body p-3 small">The detection of reused text is important in a wide range of disciplines. However, even as research in the field of <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a> is constantly improving, heavily modified or paraphrased text is still challenging for current <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a>. For historical texts, these problems are even more severe, since <a href=https://en.wikipedia.org/wiki/Source_text>text sources</a> were often subject to stronger and more frequent modifications. Despite the need for tools to automate <a href=https://en.wikipedia.org/wiki/Textual_criticism>text criticism</a>, e.g., tracing modifications in historical text, algorithmic support is still limited. While current techniques can tell if and how frequently a text has been modified, very little work has been done on determining the degree and kind of paraphrastic modificationdespite such information being of substantial interest to scholars. We present a human-interpretable, feature-based method to measure paraphrastic modification. Evaluating our technique on three data sets, we find that our approach performs competitive to text similarity scores borrowed from machine translation evaluation, being much harder to interpret.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4514/>Exploring <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and phonological similarity for the unsupervised correction of language learner errors</a></strong><br><a href=/people/i/ildiko-pilan/>IldikÃ³ PilÃ¡n</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4514><div class="card-body p-3 small">The presence of misspellings and other errors or non-standard word forms poses a considerable challenge for NLP systems. Although several <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approaches</a> have been proposed previously to normalize these, annotated training data is scarce for many languages. We investigate, therefore, an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> where correction candidates for Swedish language learners&#8217; errors are retrieved from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Furthermore, we compare the usefulness of combining <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> with orthographic and phonological similarity based on a neural grapheme-to-phoneme conversion system we train for this purpose. Although combinations of similarity measures have been explored for finding error correction candidates, it remains unclear how these <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> relate to each other and how much they contribute individually to identifying the correct alternative. We experiment with different combinations of these and find that integrating <a href=https://en.wikipedia.org/wiki/Phonology>phonological information</a> is especially useful when the majority of learner errors are related to <a href=https://en.wikipedia.org/wiki/Misspelling>misspellings</a>, but less so when errors are of a variety of types including, e.g. grammatical errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4515/>Towards Coreference for <a href=https://en.wikipedia.org/wiki/Literature>Literary Text</a> : Analyzing Domain-Specific Phenomena</a></strong><br><a href=/people/i/ina-roesiger/>Ina Roesiger</a>
|
<a href=/people/s/sarah-schulz/>Sarah Schulz</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4515><div class="card-body p-3 small">Coreference resolution is the task of grouping together references to the same discourse entity. Resolving <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> in literary texts could benefit a number of Digital Humanities (DH) tasks, such as analyzing the depiction of characters and/or their relations. Domain-dependent training data has shown to improve <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> for many domains, e.g. the biomedical domain, as its properties differ significantly from <a href=https://en.wikipedia.org/wiki/News>news text</a> or <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, on which <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> are typically trained. Literary texts could also benefit from corpora annotated with <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>. We therefore analyze the specific properties of coreference-related phenomena on a number of texts and give directions for the adaptation of annotation guidelines. As some of the adaptations have profound impact, we also present a new annotation tool for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, with a focus on enabling annotation of long texts with many discourse entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4516/>An Evaluation of Lexicon-based Sentiment Analysis Techniques for the Plays of Gotthold Ephraim Lessing<span class=acl-fixed-case>G</span>otthold <span class=acl-fixed-case>E</span>phraim <span class=acl-fixed-case>L</span>essing</a></strong><br><a href=/people/t/thomas-schmidt/>Thomas Schmidt</a>
|
<a href=/people/m/manuel-burghardt/>Manuel Burghardt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4516><div class="card-body p-3 small">We present results from a project in the research area of sentiment analysis of drama texts, more concretely the plays of Gotthold Ephraim Lessing. We conducted an annotation study to create a gold standard for a systematic evaluation. The <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> consists of 200 speeches of Lessing&#8217;s plays manually annotated with <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>. We explore the performance of different German sentiment lexicons and processing configurations like <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>, the extension of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> with historical linguistic variants or stop words elimination to explore the influence of these parameters and find best practices for our domain of application. The best performing configuration accomplishes an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 70 %. We discuss the problems and challenges for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in this area and describe our next steps toward further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4518/>Induction of a Large-Scale Knowledge Graph from the Regesta Imperii<span class=acl-fixed-case>R</span>egesta <span class=acl-fixed-case>I</span>mperii</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/l/leo-born/>Leo Born</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4518><div class="card-body p-3 small">We induce and visualize a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graph</a> over the Regesta Imperii (RI), an important large-scale resource for medieval history research. The RI comprise more than 150,000 digitized abstracts of medieval charters issued by the Roman-German kings and popes distributed over many European locations and a time span of more than 700 years. Our goal is to provide a resource for historians to visualize and query the RI, possibly aiding medieval history research. The resulting medieval graph and visualization tools are shared publicly.</div></div></div><hr><div id=w18-47><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-47.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-47/>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4700/>Proceedings 14th Joint <span class=acl-fixed-case>ACL</span> - <span class=acl-fixed-case>ISO</span> Workshop on Interoperable Semantic Annotation</a></strong><br><a href=/people/h/harry-bunt/>Harry Bunt</a></span></p></div><hr><div id=w18-48><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-48.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-48/>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4800/>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</a></strong><br><a href=/people/j/judith-l-klavans/>Judith L. Klavans</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4802/>A Neural Morphological Analyzer for Arapaho Verbs Learned from a <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite State Transducer</a><span class=acl-fixed-case>A</span>rapaho Verbs Learned from a Finite State Transducer</a></strong><br><a href=/people/s/sarah-moeller/>Sarah Moeller</a>
|
<a href=/people/g/ghazaleh-kazeminejad/>Ghazaleh Kazeminejad</a>
|
<a href=/people/a/andrew-cowell/>Andrew Cowell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4802><div class="card-body p-3 small">We experiment with training an encoder-decoder neural model for mimicking the behavior of an existing hand-written finite-state morphological grammar for <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho verbs</a>, a <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic language</a> with a highly complex verbal inflection system. After adjusting for ambiguous parses, we find that the <a href=https://en.wikipedia.org/wiki/System>system</a> is able to generalize to unseen forms with accuracies of 98.68 % (unambiguous verbs) and 92.90 % (all verbs).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4805/>Natural Language Generation for Polysynthetic Languages : Language Teaching and Learning Software for Kanyenâkha (Mohawk)<span class=acl-fixed-case>K</span>anyenâkÃ©ha (<span class=acl-fixed-case>M</span>ohawk)</a></strong><br><a href=/people/g/greg-lessard/>Greg Lessard</a>
|
<a href=/people/n/nathan-brinklow/>Nathan Brinklow</a>
|
<a href=/people/m/michael-levison/>Michael Levison</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4805><div class="card-body p-3 small">Kanyen&#8217;kha (in English, Mohawk) is an <a href=https://en.wikipedia.org/wiki/Iroquoian_languages>Iroquoian language</a> spoken primarily in Eastern Canada (Ontario, Qubec). Classified as endangered, <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> has only a small number of speakers and very few younger native speakers. Consequently, teachers and courses, teaching materials and <a href=https://en.wikipedia.org/wiki/Software>software</a> are urgently needed. In the case of <a href=https://en.wikipedia.org/wiki/Software>software</a>, the polysynthetic nature of Kanyen&#8217;kha means that the number of possible combinations grows exponentially and soon surpasses attempts to capture variant forms by hand. It is in this context that we describe an attempt to produce <a href=https://en.wikipedia.org/wiki/Language_pedagogy>language teaching materials</a> based on a generative approach. A natural language generation environment (ivi / Vinci) embedded in a web environment (VinciLingua) makes it possible to produce, by rule, variant forms of indefinite complexity. These may be used as models to explore, or as materials to which learners respond. Generated materials may take the form of written text, oral utterances, or images ; responses may be typed on a keyboard, gestural (using a mouse) or, to a limited extent, oral. The <a href=https://en.wikipedia.org/wiki/Software>software</a> also provides complex orthographic, morphological and syntactic analysis of learner productions. We describe the trajectory of development of materials for a suite of four courses on Kanyen&#8217;kha, the first of which will be taught in the fall of 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4808/>Lost in Translation : Analysis of Information Loss During <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> Between Polysynthetic and Fusional Languages</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/e/elisabeth-maier/>Elisabeth Mager</a>
|
<a href=/people/a/alfonso-medina-urrea/>Alfonso Medina-Urrea</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4808><div class="card-body p-3 small">Machine translation from polysynthetic to fusional languages is a challenging task, which gets further complicated by the limited amount of parallel text available. Thus, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance is far from the state of the art for high-resource and more intensively studied language pairs. To shed light on the phenomena which hamper automatic translation to and from <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a>, we study translations from three low-resource, polysynthetic languages (Nahuatl, Wixarika and Yorem Nokki) into <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and vice versa. Doing so, we find that in a morpheme-to-morpheme alignment an important amount of information contained in polysynthetic morphemes has no Spanish counterpart, and its translation is often omitted. We further conduct a qualitative analysis and, thus, identify morpheme types that are commonly hard to align or ignored in the translation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4809/>Automatic Glossing in a Low-Resource Setting for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a></a></strong><br><a href=/people/s/sarah-moeller/>Sarah Moeller</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4809><div class="card-body p-3 small">Morphological analysis of morphologically rich and low-resource languages is important to both <a href=https://en.wikipedia.org/wiki/Linguistic_description>descriptive linguistics</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such <a href=https://en.wikipedia.org/wiki/Data>data</a> is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a <a href=https://en.wikipedia.org/wiki/Nakh_languages>Nakh-Daghestanian language</a>, <a href=https://en.wikipedia.org/wiki/Lezgi_language>Lezgi</a>, from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3,000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich, endangered languages.</div></div></div><hr><div id=w18-49><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-49.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-49/>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4900/>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (<span class=acl-fixed-case>LAW</span>-<span class=acl-fixed-case>MWE</span>-<span class=acl-fixed-case>C</span>x<span class=acl-fixed-case>G</span>-2018)</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/m/melanie-andresen/>Melanie Andresen</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/m/miriam-r-l-petruck/>Miriam R. L. Petruck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4902/>From <a href=https://en.wikipedia.org/wiki/Lexical_functional_grammar>Lexical Functional Grammar</a> to Enhanced Universal Dependencies<span class=acl-fixed-case>L</span>exical <span class=acl-fixed-case>F</span>unctional <span class=acl-fixed-case>G</span>rammar to Enhanced <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/a/adam-przepiorkowski/>Adam PrzepiÃ³rkowski</a>
|
<a href=/people/a/agnieszka-patejuk/>Agnieszka Patejuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4902><div class="card-body p-3 small">This is a summary of an invited talk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4906/>Discourse and Lexicons : Lexemes, MWEs, Grammatical Constructions and Compositional Word Combinations to Signal Discourse Relations<span class=acl-fixed-case>MWE</span>s, Grammatical Constructions and Compositional Word Combinations to Signal Discourse Relations</a></strong><br><a href=/people/l/laurence-danlos/>Laurence Danlos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4906><div class="card-body p-3 small">Lexicons generally record a list of <a href=https://en.wikipedia.org/wiki/Lexeme>lexemes</a> or non-compositional multiword expressions. We propose to build <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> for compositional word combinations, namely secondary discourse connectives. Secondary discourse connectives play the same function as primary discourse connectives but the latter are either lexemes or non-compositional multiword expressions. The paper defines primary and secondary connectives, and explains why it is possible to build a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> for the compositional ones and how <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> could be organized. It also puts forward the utility of such a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> in discourse annotation and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Finally, it opens the discussion on the constructions that signal a <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> between two spans of text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4907/>From Chinese Word Segmentation to Extraction of Constructions : Two Sides of the Same Algorithmic Coin<span class=acl-fixed-case>C</span>hinese Word Segmentation to Extraction of Constructions: Two Sides of the Same Algorithmic Coin</a></strong><br><a href=/people/j/jean-pierre-colson/>Jean-Pierre Colson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4907><div class="card-body p-3 small">This paper presents the results of two experiments carried out within the framework of computational construction grammar. Starting from the constructionist point of view that there are just constructions in language, including lexical ones, we tested the validity of a clustering algorithm that was primarily designed for MWE extraction, the cpr-score (Colson, 2017), on Chinese word segmentation. Our results indicate a striking recall rate of 75 percent without any special adaptation to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> or to the lexicon, which confirms that there is some similarity between extracting MWEs and CWS. Our second experiment also suggests that the same <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> might be used for extracting more schematic or abstract constructions, thereby providing evidence for the statistical foundation of <a href=https://en.wikipedia.org/wiki/Construction_grammar>construction grammar</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4908/>Fixed Similes : Measuring aspects of the relation between MWE idiomatic semantics and syntactic flexibility<span class=acl-fixed-case>MWE</span> idiomatic semantics and syntactic flexibility</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/p/panagiotis-kouris/>Panagiotis Kouris</a>
|
<a href=/people/y/yanis-maistros/>Yanis Maistros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4908><div class="card-body p-3 small">We shed light on aspects of the relation between the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and the syntactic flexibility of multiword expressions by investigating fixed adjective similes (FS), a predicative multiword expression class not studied in this respect before. We find that only a subset of the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> observed in the data are related with <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>. We identify and measure two aspects of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>, one of which seems to allow for predictions about FS syntactic flexibility. Our research draws on a resource developed with the semantic and detailed syntactic annotation of web-retrieved Modern Greek material, indicating frequency of use of the individual similes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4909/>Fine-Grained Termhood Prediction for German Compound Terms Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a><span class=acl-fixed-case>G</span>erman Compound Terms Using Neural Networks</a></strong><br><a href=/people/a/anna-hatty/>Anna HÃ¤tty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4909><div class="card-body p-3 small">Automatic term identification and investigating the understandability of terms in a specialized domain are often treated as two separate lines of research. We propose a combined approach for this matter, by defining fine-grained classes of termhood and framing a classification task. The <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> reflect tiers of a term&#8217;s association to a domain. The new setup is applied to German closed compounds as term candidates in the domain of cooking. For the prediction of the classes, we compare several neural network architectures and also take salient information about the compounds&#8217; components into account. We show that applying a similar class distinction to the compounds&#8217; components and propagating this information within the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> improves the compound class prediction results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4910/>Towards a Computational Lexicon for Moroccan Darija : <a href=https://en.wikipedia.org/wiki/Word>Words</a>, <a href=https://en.wikipedia.org/wiki/Idiom>Idioms</a>, and Constructions<span class=acl-fixed-case>M</span>oroccan <span class=acl-fixed-case>D</span>arija: Words, Idioms, and Constructions</a></strong><br><a href=/people/j/jamal-laoudi/>Jamal Laoudi</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/s/stephen-tratz/>Stephen Tratz</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4910><div class="card-body p-3 small">In this paper, we explore the challenges of building a computational lexicon for Moroccan Darija (MD), an <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialect</a> spoken by over 32 million people worldwide but which only recently has begun appearing frequently in written form in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We raise the question of what belongs in such a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> and start by describing our work building traditional word-level lexicon entries with their English translations. We then discuss challenges in translating idiomatic MD text that led to creating multi-word expression lexicon entries whose meanings could not be fully derived from the individual words. Finally, we provide a preliminary exploration of <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructions</a> to be considered for inclusion in an MD constructicon by translating examples of English constructions and examining their MD counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4911/>Verbal Multiword Expressions in <a href=https://en.wikipedia.org/wiki/Basque_language>Basque Corpora</a><span class=acl-fixed-case>B</span>asque Corpora</a></strong><br><a href=/people/u/uxoa-inurrieta/>Uxoa IÃ±urrieta</a>
|
<a href=/people/i/itziar-aduriz/>Itziar Aduriz</a>
|
<a href=/people/a/ainara-estarrona/>Ainara Estarrona</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a>
|
<a href=/people/a/antton-gurrutxaga/>Antton Gurrutxaga</a>
|
<a href=/people/r/ruben-urizar/>Ruben Urizar</a>
|
<a href=/people/i/inaki-alegria/>IÃ±aki Alegria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4911><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Basque_language>Basque corpus</a> where Verbal Multiword Expressions (VMWEs) were annotated following universal guidelines. Information on the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> is given, and some ideas for discussion upon the guidelines are also proposed. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is useful not only for NLP-related research, but also to draw conclusions on Basque phraseology in comparison with other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4912/>Annotation of Tense and Aspect Semantics for Sentential AMR<span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4912><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/English_grammar>English grammar</a> encodes a number of semantic contrasts with <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect marking</a>, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a> and applications that require sophisticated reasoning about time and <a href=https://en.wikipedia.org/wiki/Event_(computing)>event structure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4913/>A Syntax-Based Scheme for the Annotation and Segmentation of German Spoken Language Interactions<span class=acl-fixed-case>G</span>erman Spoken Language Interactions</a></strong><br><a href=/people/s/swantje-westpfahl/>Swantje Westpfahl</a>
|
<a href=/people/j/jan-gorisch/>Jan Gorisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4913><div class="card-body p-3 small">Unlike corpora of written language where segmentation can mainly be derived from orthographic punctuation marks, the basis for segmenting spoken language corpora is not predetermined by the primary data, but rather has to be established by the corpus compilers. This impedes consistent querying and visualization of such <a href=https://en.wikipedia.org/wiki/Data>data</a>. Several ways of <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segmenting</a> have been proposed, some of which are based on <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. In this study, we developed and evaluated annotation and segmentation guidelines in reference to the topological field model for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We can show that these <a href=https://en.wikipedia.org/wiki/Guideline>guidelines</a> are used consistently across annotators. We also investigated the influence of various interactional settings with a rather simple <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>, the word-count per segment and unit-type. We observed that the <a href=https://en.wikipedia.org/wiki/Word_count>word count</a> and the distribution of each unit type differ in varying interactional settings and that our developed segmentation and annotation guidelines are used consistently across annotators. In conclusion, our syntax-based segmentations reflect interactional properties that are intrinsic to the <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a> that participants are involved in. This can be used for further analysis of <a href=https://en.wikipedia.org/wiki/Social_relation>social interaction</a> and opens the possibility for automatic segmentation of transcripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4916/>A Treebank for the Healthcare Domain</a></strong><br><a href=/people/n/nganthoibi-oinam/>Nganthoibi Oinam</a>
|
<a href=/people/d/diwakar-mishra/>Diwakar Mishra</a>
|
<a href=/people/p/pinal-patel/>Pinal Patel</a>
|
<a href=/people/n/narayan-choudhary/>Narayan Choudhary</a>
|
<a href=/people/h/hitesh-desai/>Hitesh Desai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4916><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> for the healthcare domain developed at ezDI. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is created from a wide array of clinical health record documents across hospitals. The <a href=https://en.wikipedia.org/wiki/Data>data</a> has been de-identified and annotated for constituent syntactic structure. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> contains a total of 52053 sentences that have been sampled for subdomains as well as <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variations</a>. The paper outlines the sampling process followed to ensure a better domain representation in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, the annotation process and challenges, and <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus statistics</a>. The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines. The <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> created was used to re-train the Berkeley parser and the Stanford parser. These <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> were also trained with the GENIA treebank for comparative quality assessment. Our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> yielded great-er accuracy on both <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. Berkeley parser performed better on our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> with an average F1 measure of 91 across 5-folds. This was a significant jump from the out-of-the-box F1 score of 70 on Berkeley parser&#8217;s default grammar.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4918/>All Roads Lead to UD : Converting Stanford and Penn Parses to English Universal Dependencies with Multilayer Annotations<span class=acl-fixed-case>UD</span>: Converting <span class=acl-fixed-case>S</span>tanford and <span class=acl-fixed-case>P</span>enn Parses to <span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies with Multilayer Annotations</a></strong><br><a href=/people/s/siyao-peng/>Siyao Peng</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4918><div class="card-body p-3 small">We describe and evaluate different approaches to the conversion of gold standard corpus data from Stanford Typed Dependencies (SD) and Penn-style constituent trees to the latest English Universal Dependencies representation (UD 2.2). Our results indicate that pure SD to UD conversion is highly accurate across multiple genres, resulting in around 1.5 % errors, but can be improved further to fewer than 0.5 % errors given access to annotations beyond the pure syntax tree, such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a> and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, which are necessary for correct generation of several UD relations. We show that constituent-based conversion using CoreNLP (with automatic NER) performs substantially worse in all genres, including when using gold constituent trees, primarily due to underspecification of phrasal grammatical functions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4921.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4921 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4921 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4921/>Constructing an Annotated Corpus of Verbal MWEs for English<span class=acl-fixed-case>MWE</span>s for <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/abigail-walsh/>Abigail Walsh</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/k/kristina-geeraert/>Kristina Geeraert</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/c/clarissa-somers/>Clarissa Somers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4921><div class="card-body p-3 small">This paper describes the construction and annotation of a corpus of verbal MWEs for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, as part of the PARSEME Shared Task 1.1 on automatic identification of verbal MWEs. The criteria for corpus selection, the categories of MWEs used, and the training process are discussed, along with the particular issues that led to revisions in edition 1.1 of the annotation guidelines. Finally, an overview of the characteristics of the final annotated corpus is presented, as well as some discussion on <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4922/>Cooperating Tools for MWE Lexicon Management and Corpus Annotation<span class=acl-fixed-case>MWE</span> Lexicon Management and Corpus Annotation</a></strong><br><a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/a/akihiko-kato/>Akihiko Kato</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/t/toshio-morita/>Toshio Morita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4922><div class="card-body p-3 small">We present tools for lexicon and corpus management that offer cooperating functionality in corpus annotation. The former, named Cradle, stores a set of words and expressions where multi-word expressions are defined with their own part-of-speech information and internal syntactic structures. The latter, named ChaKi, manages <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> with part-of-speech (POS) and syntactic dependency structure annotations. Those two tools cooperate so that the words and multi-word expressions stored in Cradle are directly referred to by ChaKi in conducting corpus annotation, and the words and expressions annotated in ChaKi can be output as a list of lexical entities that are to be stored in Cradle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4923 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4923/>Fingers in the Nose : Evaluating Speakersâ Identification of Multi-Word Expressions Using a Slightly Gamified Crowdsourcing Platform</a></strong><br><a href=/people/k/karen-fort/>KarÃ«n Fort</a>
|
<a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/m/matthieu-constant/>Matthieu Constant</a>
|
<a href=/people/n/nicolas-lefebvre/>Nicolas LefÃ¨bvre</a>
|
<a href=/people/y/yann-alan-pilatte/>Yann-Alan Pilatte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4923><div class="card-body p-3 small">This article presents the results we obtained in crowdsourcing French speakers&#8217; intuition concerning multi-work expressions (MWEs). We developed a slightly gamified crowdsourcing platform, part of which is designed to test users&#8217; ability to identify MWEs with no prior training. The participants perform relatively well at the task, with a <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> reaching 65 % for MWEs that do not behave as <a href=https://en.wikipedia.org/wiki/Function_word>function words</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4925.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4925 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4925 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4925/>Edition 1.1 of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions<span class=acl-fixed-case>PARSEME</span> Shared Task on Automatic Identification of Verbal Multiword Expressions</a></strong><br><a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/s/silvio-cordeiro/>Silvio Ricardo Cordeiro</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/m/maja-buljan/>Maja Buljan</a>
|
<a href=/people/m/marie-candito/>Marie Candito</a>
|
<a href=/people/p/polona-gantar/>Polona Gantar</a>
|
<a href=/people/v/voula-giouli/>Voula Giouli</a>
|
<a href=/people/t/tunga-gungor/>Tunga GÃ¼ngÃ¶r</a>
|
<a href=/people/a/abdelati-hawwari/>Abdelati Hawwari</a>
|
<a href=/people/u/uxoa-inurrieta/>Uxoa IÃ±urrieta</a>
|
<a href=/people/j/jolanta-kovalevskaite/>Jolanta KovalevskaitÄ</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/t/timm-lichte/>Timm Lichte</a>
|
<a href=/people/c/chaya-liebeskind/>Chaya Liebeskind</a>
|
<a href=/people/j/johanna-monti/>Johanna Monti</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra EscartÃ­n</a>
|
<a href=/people/b/behrang-qasemizadeh/>Behrang QasemiZadeh</a>
|
<a href=/people/r/renata-ramisch/>Renata Ramisch</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a>
|
<a href=/people/a/ashwini-vaidya/>Ashwini Vaidya</a>
|
<a href=/people/a/abigail-walsh/>Abigail Walsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4925><div class="card-body p-3 small">This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multiword expressions. We present the annotation methodology, focusing on changes from last year&#8217;s shared task. Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings. Corpora were created for 20 languages, which are also briefly discussed. We report organizational principles behind the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a> and the <a href=https://en.wikipedia.org/wiki/Performance_metric>evaluation metrics</a> employed for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>. The 17 participating <a href=https://en.wikipedia.org/wiki/System>systems</a>, their methods and obtained results are also presented and analysed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4926.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4926 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4926 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4926" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4926/>CRF-Seq and CRF-DepTree at PARSEME Shared Task 2018 : Detecting Verbal MWEs using Sequential and Dependency-Based Approaches<span class=acl-fixed-case>CRF</span>-Seq and <span class=acl-fixed-case>CRF</span>-<span class=acl-fixed-case>D</span>ep<span class=acl-fixed-case>T</span>ree at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Detecting Verbal <span class=acl-fixed-case>MWE</span>s using Sequential and Dependency-Based Approaches</a></strong><br><a href=/people/e/erwan-moreau/>Erwan Moreau</a>
|
<a href=/people/a/ashjan-alsulaimani/>Ashjan Alsulaimani</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4926><div class="card-body p-3 small">This paper describes two systems for detecting Verbal Multiword Expressions (VMWEs) which both competed in the closed track at the PARSEME VMWE Shared Task 2018. CRF-DepTree-categs implements an approach based on the dependency tree, intended to exploit the syntactic and semantic relations between tokens ; CRF-Seq-nocategs implements a robust sequential method which requires only lemmas and morphosyntactic tags. Both <a href=https://en.wikipedia.org/wiki/System>systems</a> ranked in the top half of the ranking, the latter ranking second for token-based evaluation. The code for both <a href=https://en.wikipedia.org/wiki/System>systems</a> is published under the GNU General Public License version 3.0 and is available at.<url>http://github.com/erwanm/adapt-vmwe18</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4927.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4927 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4927 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4927" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4927/>Deep-BGT at PARSEME Shared Task 2018 : Bidirectional LSTM-CRF Model for Verbal Multiword Expression Identification<span class=acl-fixed-case>BGT</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Bidirectional <span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model for Verbal Multiword Expression Identification</a></strong><br><a href=/people/g/gozde-berk/>GÃ¶zde Berk</a>
|
<a href=/people/b/berna-erden/>Berna Erden</a>
|
<a href=/people/t/tunga-gungor/>Tunga GÃ¼ngÃ¶r</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4927><div class="card-body p-3 small">This paper describes the Deep-BGT system that participated to the PARSEME shared task 2018 on automatic identification of verbal multiword expressions (VMWEs). Our system is language-independent and uses the bidirectional Long Short-Term Memory model with a Conditional Random Field layer on top (bidirectional LSTM-CRF). To the best of our knowledge, this paper is the first one that employs the bidirectional LSTM-CRF model for VMWE identification. Furthermore, the gappy 1-level tagging scheme is used for discontiguity and overlaps. Our system was evaluated on 10 languages in the open track and it was ranked the second in terms of the general ranking metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4929.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4929 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4929 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4929/>Mumpitz at PARSEME Shared Task 2018 : A Bidirectional LSTM for the Identification of Verbal Multiword Expressions<span class=acl-fixed-case>M</span>umpitz at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: A Bidirectional <span class=acl-fixed-case>LSTM</span> for the Identification of Verbal Multiword Expressions</a></strong><br><a href=/people/r/rafael-ehren/>Rafael Ehren</a>
|
<a href=/people/t/timm-lichte/>Timm Lichte</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4929><div class="card-body p-3 small">In this paper, we describe Mumpitz, the system we submitted to the PARSEME Shared task on automatic identification of verbal multiword expressions (VMWEs). Mumpitz consists of a Bidirectional Recurrent Neural Network (BRNN) with Long Short-Term Memory (LSTM) units and a <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> that leverages the dependency information provided in the PARSEME corpus data to differentiate VMWEs in a sentence. We submitted results for seven languages in the closed track of the task and for one language in the open track. For the open track we used the same <a href=https://en.wikipedia.org/wiki/System>system</a>, but with pretrained instead of randomly initialized word embeddings to improve the <a href=https://en.wikipedia.org/wiki/System>system</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4931 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4931/>TRAVERSAL at PARSEME Shared Task 2018 : Identification of Verbal Multiword Expressions Using a Discriminative Tree-Structured Model<span class=acl-fixed-case>TRAVERSAL</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Identification of Verbal Multiword Expressions Using a Discriminative Tree-Structured Model</a></strong><br><a href=/people/j/jakub-waszczuk/>Jakub Waszczuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4931><div class="card-body p-3 small">This paper describes a <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to the closed track of the PARSEME shared task (edition 1.1) on automatic identification of verbal multiword expressions (VMWEs). The system represents VMWE identification as a labeling task where one of two labels (MWE or not-MWE) must be predicted for each node in the dependency tree based on local context, including adjacent nodes and their labels. The <a href=https://en.wikipedia.org/wiki/System>system</a> relies on multiclass logistic regression to determine the globally optimal labeling of a tree. The system ranked 1st in the general cross-lingual ranking of the closed track systems, according to both official evaluation measures : MWE-based F1 and token-based F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4932/>VarIDE at PARSEME Shared Task 2018 : Are Variants Really as Alike as Two Peas in a Pod?<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>IDE</span> at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Are Variants Really as Alike as Two Peas in a Pod?</a></strong><br><a href=/people/c/caroline-pasquer/>Caroline Pasquer</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/j/jean-yves-antoine/>Jean-Yves Antoine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4932><div class="card-body p-3 small">We describe the VarIDE system (standing for Variant IDEntification) which participated in the edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions (VMWEs). Our system focuses on the task of VMWE variant identification by using morphosyntactic information in the training data to predict if candidates extracted from the test corpus could be idiomatic, thanks to a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes classifier</a>. We report results for 19 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4933" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4933/>Veyn at PARSEME Shared Task 2018 : Recurrent Neural Networks for VMWE Identification<span class=acl-fixed-case>V</span>eyn at <span class=acl-fixed-case>PARSEME</span> Shared Task 2018: Recurrent Neural Networks for <span class=acl-fixed-case>VMWE</span> Identification</a></strong><br><a href=/people/n/nicolas-zampieri/>Nicolas Zampieri</a>
|
<a href=/people/m/manon-scholivet/>Manon Scholivet</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4933><div class="card-body p-3 small">This paper describes the Veyn system, submitted to the closed track of the PARSEME Shared Task 2018 on automatic identification of verbal multiword expressions (VMWEs). Veyn is based on a sequence tagger using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We represent VMWEs using a variant of the begin-inside-outside encoding scheme combined with the VMWE category tag. In addition to the system description, we present development experiments to determine the best <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging scheme</a>. Veyn is freely available, covers 19 languages, and was ranked ninth (MWE-based) and eight (Token-based) among 13 submissions, considering macro-averaged F1 across languages.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>