<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Abusive Language Online (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Abusive Language Online (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-51>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li></ul></div></div><div id=w18-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-51/>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5100/>Proceedings of the 2nd Workshop on Abusive Language Online (<span class=acl-fixed-case>ALW</span>2)</a></strong><br><a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/j/jacqueline-wernimont/>Jacqueline Wernimont</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5102/>Hate Speech Dataset from a White Supremacy Forum</a></strong><br><a href=/people/o/ona-de-gibert/>Ona de Gibert</a>
|
<a href=/people/n/naiara-perez/>Naiara Perez</a>
|
<a href=/people/a/aitor-garcia-pablos/>Aitor García-Pablos</a>
|
<a href=/people/m/montse-cuadros/>Montse Cuadros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5102><div class="card-body p-3 small">Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, colour, <a href=https://en.wikipedia.org/wiki/Ethnic_group>ethnicity</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Sexual_orientation>sexual orientation</a>, <a href=https://en.wikipedia.org/wiki/Nationality>nationality</a>, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, or other characteristic. Due to the massive rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated web content</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, the amount of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> is also steadily increasing. Over the past years, interest in online hate speech detection and, particularly, the automation of this task has continuously grown, along with the societal impact of the phenomenon. This paper describes a <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech dataset</a> composed of thousands of sentences manually labelled as containing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or not. The sentences have been extracted from <a href=https://en.wikipedia.org/wiki/Stormfront_(website)>Stormfront</a>, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and several baseline experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5104/>Predictive Embeddings for Hate Speech Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rohan-kshirsagar/>Rohan Kshirsagar</a>
|
<a href=/people/t/tyrus-cukuvac/>Tyrus Cukuvac</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a>
|
<a href=/people/s/susan-mcgregor/>Susan McGregor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5104><div class="card-body p-3 small">We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in particular. Using pre-trained word embeddings and max / mean pooling from simple, fully-connected transformations of these embeddings, we are able to predict the occurrence of hate speech on three commonly used publicly available datasets. Our models match or outperform state of the art F1 performance on all three datasets using significantly fewer parameters and minimal feature preprocessing compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5105/>Challenges for Toxic Comment Classification : An In-Depth Error Analysis</a></strong><br><a href=/people/b/betty-van-aken/>Betty van Aken</a>
|
<a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5105><div class="card-body p-3 small">Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task&#8217;s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> that outperforms all individual models. Further, we validate our findings on a second <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The results of the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5106/>Aggression Detection on Social Media Text Using Deep Neural Networks</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/a/aman-varshney/>Aman Varshney</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5106><div class="card-body p-3 small">In the past few years, bully and aggressive posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have grown significantly, causing serious consequences for victims / users of all demographics. Majority of the work in this field has been done for English only. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based classification system</a> for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments of Hindi-English Code-Mixed text to detect the aggressive behaviour of / towards users. Our work focuses on text from users majorly in the Indian Subcontinent. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we used for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is provided by <a href=https://en.wikipedia.org/wiki/TRAC-1>TRAC-1</a> in their shared task. Our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> assigns each <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook post / comment</a> to one of the three predefined categories : Overtly Aggressive, Covertly Aggressive and Non-Aggressive. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2 %.<b>TRAC-1</b>in their shared task. Our classification model assigns each Facebook post/comment to one of the three predefined categories: &#8220;Overtly Aggressive&#8221;, &#8220;Covertly Aggressive&#8221; and &#8220;Non-Aggressive&#8221;. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5107/>Creating a WhatsApp Dataset to Study Pre-teen Cyberbullying<span class=acl-fixed-case>W</span>hats<span class=acl-fixed-case>A</span>pp Dataset to Study Pre-teen Cyberbullying</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/f/filippo-oncini/>Filippo Oncini</a>
|
<a href=/people/e/enrico-piras/>Enrico Piras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5107><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/WhatsApp>WhatsApp</a> is used by teenagers as one major channel of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, such interactions remain invisible due to the <a href=https://en.wikipedia.org/wiki/Privacy_policy>app privacy policies</a> that do not allow ex-post data collection. Indeed, most of the information on these phenomena rely on surveys regarding <a href=https://en.wikipedia.org/wiki/Self-report_study>self-reported data</a>. In order to overcome this limitation, we describe in this paper the activities that led to the creation of a WhatsApp dataset to study <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> among Italian students aged 12-13. We present not only the collected chats with annotations about user role and type of offense, but also the living lab created in a collaboration between researchers and schools to monitor and analyse <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>. Finally, we discuss some open issues, dealing with ethical, operational and epistemic aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5109/>Aggressive language in an online hacking forum</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/s/sergio-pastrana/>Sergio Pastrana</a>
|
<a href=/people/a/alice-hutchings/>Alice Hutchings</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5109><div class="card-body p-3 small">We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classifier</a> for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5110/>The Effects of User Features on Twitter Hate Speech Detection<span class=acl-fixed-case>T</span>witter Hate Speech Detection</a></strong><br><a href=/people/e/elise-fehn-unsvag/>Elise Fehn Unsvåg</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5110><div class="card-body p-3 small">The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. While the incorporation of <a href=https://en.wikipedia.org/wiki/Software_feature>user features</a> resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5112/>Determining Code Words in Euphemistic Hate Speech Using Word Embedding Networks</a></strong><br><a href=/people/r/rijul-magu/>Rijul Magu</a>
|
<a href=/people/j/jiebo-luo/>Jiebo Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5112><div class="card-body p-3 small">While analysis of online explicit abusive language detection has lately seen an ever-increasing focus, implicit abuse detection remains a largely unexplored space. We carry out a study on a subcategory of implicit hate : euphemistic hate speech. We propose a method to assist in identifying unknown euphemisms (or code words) given a set of hateful tweets containing a known code word. Our approach leverages <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Network_theory>network analysis</a> (through centrality measures and community detection) in a manner that can be generalized to identify euphemisms across contexts- not just <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5117/>Cross-Domain Detection of Abusive Language Online</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5117><div class="card-body p-3 small">We investigate to what extent the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained to detect general abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, finding that the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in-domain training, especially when used to augment a smaller dataset with a larger one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5118/>Did you offend me? Classification of Offensive Tweets in Hinglish Language<span class=acl-fixed-case>H</span>inglish Language</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/m/meghna-ayyar/>Meghna Ayyar</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5118><div class="card-body p-3 small">The use of code-switched languages (e.g., <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, which is derived by the blending of <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> with the English language) is getting much popular on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> due to their ease of communication in <a href=https://en.wikipedia.org/wiki/First_language>native languages</a>. However, spelling variations and absence of <a href=https://en.wikipedia.org/wiki/Grammar>grammar rules</a> introduce <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.<i>e.g.</i>, Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5119/>Decipherment for Adversarial Offensive Language Detection</a></strong><br><a href=/people/z/zhelun-wu/>Zhelun Wu</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5119><div class="card-body p-3 small">Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We provide experimental results on three different datasets and show that <a href=https://en.wikipedia.org/wiki/Decipherment>decipherment</a> is an effective tool for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5120/>The Linguistic Ideologies of Deep Abusive Language Classification</a></strong><br><a href=/people/m/michael-castelle/>Michael Castelle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5120><div class="card-body p-3 small">This paper brings together theories from <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_anthropology>linguistic anthropology</a> to critically evaluate the so-called language ideologies the set of beliefs and ways of speaking about languagein the practices of abusive language classification in modern machine learning-based NLP. This argument is made at both a conceptual and empirical level, as we review approaches to <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language</a> from different fields, and use two neural network methods to analyze three datasets developed for <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language classification tasks</a> (drawn from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, and StackOverflow). By evaluating and comparing these results, we argue for the importance of incorporating theories of <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> and <a href=https://en.wikipedia.org/wiki/Metapragmatics>metapragmatics</a> into both the design of classification tasks as well as in ML architectures.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>