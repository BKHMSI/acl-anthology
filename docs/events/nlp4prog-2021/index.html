<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>First Workshop on Natural Language Processing for Programming (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>First Workshop on Natural Language Processing for Programming (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021nlp4prog-1>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=2021nlp4prog-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.nlp4prog-1/>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.0/>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></strong><br><a href=/people/r/royi-lachmy/>Royi Lachmy</a>
|
<a href=/people/z/ziyu-yao/>Ziyu Yao</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/m/milos-gligoric/>Milos Gligoric</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/r/ray-mooney/>Ray Mooney</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.2/>ConTest : A Unit Test Completion Benchmark featuring Context<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>T</span>est: A Unit Test Completion Benchmark featuring Context</a></strong><br><a href=/people/j/johannes-villmow/>Johannes Villmow</a>
|
<a href=/people/j/jonas-depoix/>Jonas Depoix</a>
|
<a href=/people/a/adrian-ulges/>Adrian Ulges</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--2><div class="card-body p-3 small">We introduce CONTEST, a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for NLP-based unit test completion, the task of predicting a test&#8217;s assert statements given its setup and focal method, i.e. the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also features context code called by either. We found <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. Overall, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve a BLEU score of 38.2, while only generating unparsable code in 1.92 % of cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.3/>CommitBERT : Commit Message Generation Using Pre-Trained Programming Language Model<span class=acl-fixed-case>C</span>ommit<span class=acl-fixed-case>BERT</span>: Commit Message Generation Using Pre-Trained Programming Language Model</a></strong><br><a href=/people/t/tae-hwan-jung/>Tae Hwan Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--3><div class="card-body p-3 small">Commit message is a document that summarizes source code changes in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that automatically writes the commit message. To this end, we release 345 K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, <a href=https://en.wikipedia.org/wiki/Go_(programming_language)>Go</a>, <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java</a>, <a href=https://en.wikipedia.org/wiki/JavaScript>JavaScript</a>, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4. Also, we propose the following two training methods to improve the result of generating the commit message : (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.4/>Time-Efficient Code Completion Model for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R Programming Language</a><span class=acl-fixed-case>R</span> Programming Language</a></strong><br><a href=/people/a/artem-popov/>Artem Popov</a>
|
<a href=/people/d/dmitrii-orekhov/>Dmitrii Orekhov</a>
|
<a href=/people/d/denis-litvinov/>Denis Litvinov</a>
|
<a href=/people/n/nikolay-korolev/>Nikolay Korolev</a>
|
<a href=/people/g/gleb-morgachev/>Gleb Morgachev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--4><div class="card-body p-3 small">In this paper we present a deep learning code completion model for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R language</a>. We introduce several <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a> to utilize <a href=https://en.wikipedia.org/wiki/Language_model>language modeling based architecture</a> in the <a href=https://en.wikipedia.org/wiki/Autocomplete>code completion task</a>. With these techniques, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> requires low resources, but still achieves high quality. We also present an evaluation dataset for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R language completion task</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains multiple autocompletion usage contexts that provides robust validation results. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.7/>Shellcode_IA32 : A Dataset for Automatic Shellcode Generation<span class=acl-fixed-case>S</span>hellcode_<span class=acl-fixed-case>IA</span>32: A Dataset for Automatic Shellcode Generation</a></strong><br><a href=/people/p/pietro-liguori/>Pietro Liguori</a>
|
<a href=/people/e/erfan-al-hossami/>Erfan Al-Hossami</a>
|
<a href=/people/d/domenico-cotroneo/>Domenico Cotroneo</a>
|
<a href=/people/r/roberto-natella/>Roberto Natella</a>
|
<a href=/people/b/bojan-cukic/>Bojan Cukic</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--7><div class="card-body p-3 small">We take the first step to address the task of <a href=https://en.wikipedia.org/wiki/Shellcode>automatically generating shellcodes</a>, i.e., small pieces of code used as a payload in the exploitation of a <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>software vulnerability</a>, starting from <a href=https://en.wikipedia.org/wiki/Comment_(computer_programming)>natural language comments</a>. We assemble and release a novel dataset (Shellcode_IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.8/>Reading StackOverflow Encourages Cheating : Adding Question Text Improves Extractive Code Generation<span class=acl-fixed-case>S</span>tack<span class=acl-fixed-case>O</span>verflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation</a></strong><br><a href=/people/g/gabriel-orlanski/>Gabriel Orlanski</a>
|
<a href=/people/a/alex-gittens/>Alex Gittens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--8><div class="card-body p-3 small">Answering a programming question with only its title is difficult as salient contextual information is left out. To address this, we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over 40,000 StackOverflow question texts to be used in conjunction with the corresponding intents from the CoNaLa dataset (Yin et al., 2018). Using both the <a href=https://en.wikipedia.org/wiki/Intention>intent</a> and the question body, we use <a href=https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit>BART</a> to establish a baseline BLEU score of 34.35 for this new task. We then find further improvements of 2.8 % by combining the mined CoNaLa data with the labeled data to achieve a 35.32 BLEU score. We then evaluate the prior state-of-the-art CoNaLa models with this additional <a href=https://en.wikipedia.org/wiki/Data>data</a>. We find that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> of using the body and mined data beats that of the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a 71.96 % BLEU score. Finally, we perform ablations that prove that BART is an unsupervised multimodal learner and examine its extractive behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.9/>Text-to-SQL in the Wild : A Naturally-Occurring Dataset Based on Stack Exchange Data<span class=acl-fixed-case>SQL</span> in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data</a></strong><br><a href=/people/m/moshe-hazoom/>Moshe Hazoom</a>
|
<a href=/people/v/vibhor-malik/>Vibhor Malik</a>
|
<a href=/people/b/ben-bogin/>Ben Bogin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--9><div class="card-body p-3 small">Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a>. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release <a href=https://en.wikipedia.org/wiki/Stack_Exchange>SEDE</a>, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 12,023 pairs of utterances and <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a> collected from real usage on the <a href=https://en.wikipedia.org/wiki/Stack_Exchange>Stack Exchange website</a>. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.10/>Bag-of-Words Baselines for Semantic Code Search</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--10><div class="card-body p-3 small">The task of semantic code search is to retrieve <a href=https://en.wikipedia.org/wiki/Snippet_(programming)>code snippets</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>source code corpus</a> based on an information need expressed in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that traditional bag-of-words IR methods are poorly suited for semantic code search : our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> and <a href=https://en.wikipedia.org/wiki/RM3>RM3</a>, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>