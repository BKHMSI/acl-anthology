<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Joint Conference on Natural Language Processing (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Joint Conference on Natural Language Processing (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#i17-1>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a>
<span class="badge badge-info align-middle ml-1">81&nbsp;papers</span></li><li><a class=align-middle href=#i17-2>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a>
<span class="badge badge-info align-middle ml-1">57&nbsp;papers</span></li><li><a class=align-middle href=#i17-3>Proceedings of the IJCNLP 2017, System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#i17-4>Proceedings of the IJCNLP 2017, Shared Tasks</a>
<span class="badge badge-info align-middle ml-1">29&nbsp;papers</span></li><li><a class=align-middle href=#i17-5>Proceedings of the IJCNLP 2017, Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=i17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/I17-1/>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1000/>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></strong><br><a href=/people/g/greg-kondrak/>Greg Kondrak</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1001/>Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks</a></strong><br><a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1001><div class="card-body p-3 small">While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on two tasks : part-of-speech and semantic tagging. We then measure the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> in NMT models. For instance, we find that higher layers are better at learning <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> while lower layers tend to be better for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. We also observe little effect of the target language on source-side representations, especially in higher quality models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1002/>Context-Aware Smoothing for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1002><div class="card-body p-3 small">In Neural Machine Translation (NMT), each word is represented as a low-dimension, real-value vector for encoding its syntax and semantic information. This means that even if the word is in a different sentence context, it is represented as the fixed vector to learn source representation. Moreover, a large number of Out-Of-Vocabulary (OOV) words, which have different syntax and semantic information, are represented as the same vector representation of unk. To alleviate this problem, we propose a novel context-aware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. Empirical results on NIST Chinese-to-English translation task show that the proposed <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1004/>What does Attention in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Pay Attention to?</a></strong><br><a href=/people/h/hamidreza-ghader/>Hamidreza Ghader</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1004><div class="card-body p-3 small">Attention in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is considered to be an alignment model as well. However, there is no work that specifically studies <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and provides analysis of what is being learned by <a href=https://en.wikipedia.org/wiki/Attention>attention models</a>. Thus, the question still remains that how <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is similar or different from the traditional alignment. In this paper, we provide detailed analysis of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and compare it to traditional alignment. We answer the question of whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is only capable of modelling translational equivalent or it captures more information. We show that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is different from alignment in some cases and is capturing useful information other than alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1007/>Neural Probabilistic Model for Non-projective MST Parsing<span class=acl-fixed-case>MST</span> Parsing</a></strong><br><a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1007><div class="card-body p-3 small">In this paper, we propose a probabilistic parsing model that defines a proper <a href=https://en.wikipedia.org/wiki/Conditional_probability_distribution>conditional probability distribution</a> over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTMCNNs, which automatically benefits from both word- and character-level representations, by using a combination of bidirectional LSTMs and CNNs. On top of the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. By exploiting Kirchhoff&#8217;s Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straightforward end-to-end model training procedure via <a href=https://en.wikipedia.org/wiki/Backpropagation>back-propagation</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 17 different datasets, across 14 different languages. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on nine datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1009/>MIPA : Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting<span class=acl-fixed-case>MIPA</span>: Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1009><div class="card-body p-3 small">We present a pointwise mutual information (PMI)-based approach to formalize paraphrasability and propose a variant of PMI, called MIPA, for the paraphrase acquisition. Our paraphrase acquisition method first acquires lexical paraphrase pairs by bilingual pivoting and then reranks them by PMI and distributional similarity. The complementary nature of information from bilingual corpora and from monolingual corpora makes the proposed method robust. Experimental results show that the proposed method substantially outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as MRR, MAP, coverage, and Spearman&#8217;s correlation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1010/>Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments</a></strong><br><a href=/people/q/quynh-ngoc-thi-do/>Quynh Ngoc Thi Do</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1010><div class="card-body p-3 small">Implicit semantic role labeling (iSRL) is the task of predicting the semantic roles of a predicate that do not appear as explicit arguments, but rather regard common sense knowledge or are mentioned earlier in the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. We introduce an approach to iSRL based on a predictive recurrent neural semantic frame model (PRNSFM) that uses a large unannotated corpus to learn the probability of a sequence of semantic arguments given a predicate. We leverage the sequence probabilities predicted by the PRNSFM to estimate selectional preferences for predicates and their arguments. On the NomBank iSRL test set, our approach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1012/>Enabling Transitivity for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Inference</a> on <a href=https://en.wikipedia.org/wiki/Varieties_of_Chinese>Chinese Verbs</a> Using Probabilistic Soft Logic<span class=acl-fixed-case>C</span>hinese Verbs Using Probabilistic Soft Logic</a></strong><br><a href=/people/w/wei-chung-wang/>Wei-Chung Wang</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1012><div class="card-body p-3 small">To learn more knowledge, enabling transitivity is a vital step for lexical inference. However, most of the lexical inference models with good performance are for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> or <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a>, which can not be directly applied to the <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on events or states. In this paper, we construct the largest Chinese verb lexical inference dataset containing 18,029 verb pairs, where for each pair one of four inference relations are annotated. We further build a probabilistic soft logic (PSL) model to infer verb lexicons using the <a href=https://en.wikipedia.org/wiki/Logic_language>logic language</a>. With PSL, we easily enable transitivity in two layers, the observed layer and the feature layer, which are included in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We further discuss the effect of <a href=https://en.wikipedia.org/wiki/Transient_(oscillation)>transitives</a> within and between these <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>layers</a>. Results show the performance of the proposed <a href=https://en.wikipedia.org/wiki/Partial_differential_equation>PSL model</a> can be improved at least 3.5 % (relative) when the transitivity is enabled. Furthermore, experiments show that enabling transitivity in the observed layer benefits the most.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1013/>An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1013><div class="card-body p-3 small">In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling pe directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.<tex-math>mt</tex-math> (raw MT output) and <tex-math>src</tex-math> (source language input) in a single neural architecture, modeling <tex-math>\\{mt, src\\} \\rightarrow pe</tex-math> directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1014/>Imagination Improves Multimodal Translation</a></strong><br><a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/a/akos-kadar/>Ákos Kádár</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1014><div class="card-body p-3 small">We decompose multimodal translation into two sub-tasks : learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance compared to the state of the art on the Multi30 K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1015/>Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder</a></strong><br><a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1015><div class="card-body p-3 small">End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> learned during the training of NMT systems, and more importantly, which parts of the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> are responsible for learning each of these phenomenon. In this paper we i) analyze how much <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> an NMT decoder learns, and ii) investigate whether injecting target morphology in the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> helps it to produce better translations. To this end we present three methods : i) <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a>, ii) joint-data learning, and iii) <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our results show that explicit <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> helps the <a href=https://en.wikipedia.org/wiki/Machine_learning>decoder</a> learn target language morphology and improves the translation quality by 0.20.6 <a href=https://en.wikipedia.org/wiki/Linguistic_description>BLEU points</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1016/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> through Phrase-based Forced Decoding</a></strong><br><a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichro Sumita</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1016><div class="card-body p-3 small">Compared to traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1017/>Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/c/chunqi-wang/>Chunqi Wang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1017><div class="card-body p-3 small">Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS). Recently, many character-based neural models have been applied to <a href=https://en.wikipedia.org/wiki/Computational_fluid_dynamics>CWS</a>. While they obtain good performance, they have two obvious weaknesses. The first is that they heavily rely on manually designed bigram feature, i.e. they are not good at capturing n-gram features automatically. The second is that <a href=https://en.wikipedia.org/wiki/They>they</a> make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich n-gram features without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. For the second one, we propose an effective approach to integrate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two benchmark datasets : PKU and MSR. Without any <a href=https://en.wikipedia.org/wiki/Software_feature>feature engineering</a>, the model obtains competitive performance 95.7 % on PKU and 97.3 % on MSR. Armed with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> 96.5 % on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PKU</a> and 98.0 % on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>MSR</a>, without using any external labeled resource.<tex-math>n</tex-math>-gram features automatically. The second is that they make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich <tex-math>n</tex-math>-gram features without any feature engineering. For the second one, we propose an effective approach to integrate the proposed model with word embeddings. We evaluate the model on two benchmark datasets: PKU and MSR. Without any feature engineering, the model obtains competitive performance &#8212; 95.7% on PKU and 97.3% on MSR. Armed with word embeddings, the model achieves state-of-the-art performance on both datasets &#8212; 96.5% on PKU and 98.0% on MSR, without using any external labeled resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1018.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1018/>Character-based Joint Segmentation and POS Tagging for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> using Bidirectional RNN-CRF<span class=acl-fixed-case>POS</span> Tagging for <span class=acl-fixed-case>C</span>hinese using Bidirectional <span class=acl-fixed-case>RNN</span>-<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1018><div class="card-body p-3 small">We present a character-based model for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is accurate and robust across <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in different sizes, genres and <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a>. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1019/>Addressing Domain Adaptation for Chinese Word Segmentation with Global Recurrent Structure<span class=acl-fixed-case>C</span>hinese Word Segmentation with Global Recurrent Structure</a></strong><br><a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1019><div class="card-body p-3 small">Boundary features are widely used in traditional Chinese Word Segmentation (CWS) methods as they can utilize unlabeled data to help improve the Out-of-Vocabulary (OOV) word recognition performance. Although various neural network methods for CWS have achieved performance competitive with state-of-the-art systems, these methods, constrained by the domain and size of the training corpus, do not work well in domain adaptation. In this paper, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed for modeling <a href=https://en.wikipedia.org/wiki/Boundary_value_problem>boundary features</a> dynamically. Experiments show that the proposed structure can effectively boost the performance of Chinese Word Segmentation, especially OOV-Recall, which brings benefits to <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1020.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1020/>Information Bottleneck Inspired Method For Chat Text Segmentation</a></strong><br><a href=/people/s/s-vishal/>S Vishal</a>
|
<a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1020><div class="card-body p-3 small">We present a novel technique for segmenting chat conversations using the information bottleneck method (Tishby et al., 2000), augmented with sequential continuity constraints. Furthermore, we utilize critical non-textual clues such as time between two consecutive posts and people mentions within the posts. To ascertain the effectiveness of the proposed method, we have collected data from public Slack conversations and Fresco, a proprietary platform deployed inside our organization. Experiments demonstrate that the proposed method yields an absolute (relative) improvement of as high as 3.23 % (11.25 %). To facilitate future research, we are releasing manual annotations for segmentation on public Slack conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1021/>Distributional Modeling on a Diet : One-shot Word Learning from Text Only</a></strong><br><a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1021><div class="card-body p-3 small">We test whether <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional models</a> can do one-shot learning of definitional properties from text only. Using <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian models</a>, we find that first learning overarching structure in the known data, regularities in textual contexts and in <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>properties</a>, helps <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a>, and that individual context items can be highly informative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1022/>A Computational Study on Word Meanings and Their Distributed Representations via Polymodal Embedding</a></strong><br><a href=/people/j/joohee-park/>Joohee Park</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1022><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a> has become a popular approach to capturing a <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meaning</a>. Besides its success and practical value, however, questions arise about the relationships between a true word meaning and its <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a>. In this paper, we examine such a relationship via polymodal embedding approach inspired by the theory that humans tend to use diverse sources in developing a word meaning. The result suggests that the existing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> lack in capturing certain aspects of <a href=https://en.wikipedia.org/wiki/Semantics>word meanings</a> which can be significantly improved by the polymodal approach. Also, we show distinct characteristics of different types of words (e.g. concreteness) via <a href=https://en.wikipedia.org/wiki/Computational_neuroscience>computational studies</a>. Finally, we show our proposed embedding method outperforms the baselines in the word similarity measure tasks and the hypernym prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1023/>Geographical Evaluation of Word Embeddings</a></strong><br><a href=/people/m/michal-konkol/>Michal Konkol</a>
|
<a href=/people/t/tomas-brychcin/>Tomáš Brychcín</a>
|
<a href=/people/m/michal-nykl/>Michal Nykl</a>
|
<a href=/people/t/tomas-hercig/>Tomáš Hercig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1023><div class="card-body p-3 small">Word embeddings are commonly compared either with human-annotated word similarities or through improvements in natural language processing tasks. We propose a novel principle which compares the information from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with reality. We implement this <a href=https://en.wikipedia.org/wiki/Principle>principle</a> by comparing the information in the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with geographical positions of cities. Our evaluation linearly transforms the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> to optimally fit the real positions of cities and measures the deviation between the position given by word embeddings and the real position. A set of well-known <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with state-of-the-art results were evaluated. We also introduce a <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> that helps with error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1025/>Unsupervised Segmentation of Phoneme Sequences based on Pitman-Yor Semi-Markov Model using Phoneme Length Context<span class=acl-fixed-case>P</span>itman-<span class=acl-fixed-case>Y</span>or Semi-<span class=acl-fixed-case>M</span>arkov Model using Phoneme Length Context</a></strong><br><a href=/people/r/ryu-takeda/>Ryu Takeda</a>
|
<a href=/people/k/kazunori-komatani/>Kazunori Komatani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1025><div class="card-body p-3 small">Unsupervised segmentation of phoneme sequences is an essential process to obtain unknown words during <a href=https://en.wikipedia.org/wiki/Dialogue>spoken dialogues</a>. In this segmentation, an input <a href=https://en.wikipedia.org/wiki/Phoneme>phoneme sequence</a> without delimiters is converted into segmented sub-sequences corresponding to words. The Pitman-Yor semi-Markov model (PYSMM) is promising for this problem, but its performance degrades when it is applied to phoneme-level word segmentation. This is because of insufficient cues for the segmentation, e.g., <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> are improperly treated as single entries and their different contexts are also confused. We propose a phoneme-length context model for PYSMM to give a helpful cue at the <a href=https://en.wikipedia.org/wiki/Phoneme>phoneme-level</a> and to predict succeeding segments more accurately. Our experiments showed that the peak performance with our <a href=https://en.wikipedia.org/wiki/Context_model>context model</a> outperformed those without such a <a href=https://en.wikipedia.org/wiki/Context_model>context model</a> by 0.045 at most in terms of <a href=https://en.wikipedia.org/wiki/F-number>F-measures</a> of estimated segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1026/>A Sensitivity Analysis of (and Practitioners’ Guide to) <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Sentence Classification</a></strong><br><a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1026><div class="card-body p-3 small">Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014 ; Kalchbrenner et al., 2014 ; Johnson and Zhang, 2014 ; Zhang et al., 2016). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization parameters</a>, and so on. It is currently unknown how sensitive <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance is to changes in these configurations for the task of sentence classification. We thus conduct a <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>sensitivity analysis</a> of one-layer CNNs to explore the effect of architecture components on model performance ; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1028/>Turning Distributional Thesauri into Word Vectors for Synonym Extraction and Expansion</a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1028><div class="card-body p-3 small">In this article, we propose to investigate a new <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> and distributed representation adaptation. We have applied and evaluated it for <a href=https://en.wikipedia.org/wiki/English_nouns>English nouns</a> at a large scale about its ability to retrieve <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a>. In this context, we have also illustrated the interest of the developed method for three different tasks : the improvement of already existing <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the fusion of heterogeneous representations and the expansion of synsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1030.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1030/>Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1030><div class="card-body p-3 small">Current research in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> has been hampered by two central problems : (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> that different <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>approaches</a> can model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1031/>Domain-Adaptable Hybrid Generation of RDF Entity Descriptions<span class=acl-fixed-case>RDF</span> Entity Descriptions</a></strong><br><a href=/people/o/or-biran/>Or Biran</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1031><div class="card-body p-3 small">RDF ontologies provide <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> on entities in many domains and continue to grow in size and diversity. While they can be useful as a starting point for generating descriptions of entities, they often miss important information about an entity that can not be captured as simple relations. In addition, generic approaches to generation from <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a> can not capture the unique style and content of specific domains. We describe a framework for hybrid generation of entity descriptions, which combines generation from <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF data</a> with text extracted from a corpus, and extracts unique aspects of the domain from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to create domain-specific generation systems. We show that each component of our approach significantly increases the satisfaction of readers with the text across multiple applications and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1032/>ES-LDA : Entity Summarization using Knowledge-based Topic Modeling<span class=acl-fixed-case>ES</span>-<span class=acl-fixed-case>LDA</span>: Entity Summarization using Knowledge-based Topic Modeling</a></strong><br><a href=/people/s/seyedamin-pouriyeh/>Seyedamin Pouriyeh</a>
|
<a href=/people/m/mehdi-allahyari/>Mehdi Allahyari</a>
|
<a href=/people/k/krzysztof-kochut/>Krzysztof Kochut</a>
|
<a href=/people/g/gong-cheng/>Gong Cheng</a>
|
<a href=/people/h/hamid-reza-arabnia/>Hamid Reza Arabnia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1032><div class="card-body p-3 small">With the advent of the Internet, the amount of Semantic Web documents that describe real-world entities and their inter-links as a set of statements have grown considerably. These descriptions are usually lengthy, which makes the utilization of the underlying entities a difficult task. Entity summarization, which aims to create summaries for real-world entities, has gained increasing attention in recent years. In this paper, we propose a probabilistic topic model, ES-LDA, that combines prior knowledge with statistical learning techniques within a single framework to create more reliable and representative summaries for entities. We demonstrate the effectiveness of our approach by conducting extensive experiments and show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art techniques and enhances the quality of the entity summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1033/>Procedural Text Generation from an Execution Video</a></strong><br><a href=/people/a/atsushi-ushiku/>Atsushi Ushiku</a>
|
<a href=/people/h/hayato-hashimoto/>Hayato Hashimoto</a>
|
<a href=/people/a/atsushi-hashimoto/>Atsushi Hashimoto</a>
|
<a href=/people/s/shinsuke-mori/>Shinsuke Mori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1033><div class="card-body p-3 small">In recent years, there has been a surge of interest in automatically describing images or videos in a <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. These descriptions are useful for image / video search, etc. In this paper, we focus on procedure execution videos, in which a human makes or repairs something and propose a method for generating procedural texts from them. Since video / text pairs available are limited in size, the direct application of end-to-end deep learning is not feasible. Thus we propose to train Faster R-CNN network for <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object recognition</a> and LSTM for text generation and combine them at run time. We took pairs of recipe and cooking video, generated a <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a> from a video, and compared it with the original <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a>. The experimental results showed that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can produce a <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a> as accurate as the state-of-the-art scene descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1034/>Text Sentiment Analysis based on Fusion of Structural Information and Serialization Information</a></strong><br><a href=/people/l/ling-gan/>Ling Gan</a>
|
<a href=/people/h/houyu-gong/>Houyu Gong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1034><div class="card-body p-3 small">Tree-structured Long Short-Term Memory (Tree-LSTM) has been proved to be an effective method in the sentiment analysis task. It extracts structural information on text, and uses Long Short-Term Memory (LSTM) cell to prevent gradient vanish. However, though combining the LSTM cell, it is still a kind of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that extracts the structural information and almost not extracts serialization information. In this paper, we propose three new models in order to combine those two kinds of <a href=https://en.wikipedia.org/wiki/Information>information</a> : the structural information generated by the Constituency Tree-LSTM and the serialization information generated by Long-Short Term Memory neural network. Our experiments show that combining those two kinds of information can give contributes to the performance of the sentiment analysis task compared with the single Constituency Tree-LSTM model and the LSTM model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1035/>Length, <a href=https://en.wikipedia.org/wiki/Interchangeability>Interchangeability</a>, and External Knowledge : Observations from Predicting Argument Convincingness</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/r/robin-bhattacharya/>Robin Bhattacharya</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1035><div class="card-body p-3 small">In this work, we provide insight into three key aspects related to predicting argument convincingness. First, we explicitly display the power that text length possesses for predicting convincingness in an unsupervised setting. Second, we show that a bag-of-words embedding model posts state-of-the-art on a dataset of arguments annotated for convincingness, outperforming an SVM with numerous hand-crafted features as well as recurrent neural network models that attempt to capture semantic composition. Finally, we assess the feasibility of integrating external knowledge when predicting convincingness, as arguments are often more convincing when they contain abundant information and facts. We finish by analyzing the correlations between the various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> we propose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1036/>Exploiting Document Level Information to Improve Event Detection via Recurrent Neural Networks</a></strong><br><a href=/people/s/shaoyang-duan/>Shaoyang Duan</a>
|
<a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/w/wenli-zhao/>Wenli Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1036><div class="card-body p-3 small">This paper tackles the task of event detection, which involves identifying and categorizing events. The previous work mainly exist two problems : (1) the traditional feature-based methods apply cross-sentence information, yet need taking a large amount of human effort to design complicated feature sets and inference rules ; (2) the representation-based methods though overcome the problem of manually extracting features, while just depend on local sentence representation. Considering local sentence context is insufficient to resolve ambiguities in identifying particular event types, therefore, we propose a novel document level Recurrent Neural Networks (DLRNN) model, which can automatically extract cross-sentence clues to improve sentence level event detection without designing complex reasoning rules. Experiment results show that our approach outperforms other state-of-the-art methods on ACE 2005 dataset without external knowledge base.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1037/>Embracing Non-Traditional Linguistic Resources for Low-resource Language Name Tagging</a></strong><br><a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/halidanmu-abudukelimu/>Halidanmu Abudukelimu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1037><div class="card-body p-3 small">Current supervised name tagging approaches are inadequate for most low-resource languages due to the lack of annotated data and actionable linguistic knowledge. All supervised learning methods (including deep neural networks (DNN)) are sensitive to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and thus they are not quite portable without massive clean annotations. We found that the <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of DNN-based name taggers drop rapidly (20%-30 %) when we replace clean manual annotations with noisy annotations in the training data. We propose a new solution to incorporate many non-traditional language universal resources that are readily available but rarely explored in the Natural Language Processing (NLP) community, such as the World Atlas of Linguistic Structure, CIA names, PanLex and survival guides. We acquire and encode various types of non-traditional linguistic resources into a DNN name tagger. Experiments on three low-resource languages show that feeding linguistic knowledge can make DNN significantly more robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, achieving 8%-22 % absolute <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> gains on <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>name tagging</a> without using any human annotation</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1038/>NMT or SMT : Case Study of a Narrow-domain English-Latvian Post-editing Project<span class=acl-fixed-case>NMT</span> or <span class=acl-fixed-case>SMT</span>: Case Study of a Narrow-domain <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>L</span>atvian Post-editing Project</a></strong><br><a href=/people/i/inguna-skadina/>Inguna Skadiņa</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1038><div class="card-body p-3 small">The recent technological shift in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from statistical machine translation (SMT) to neural machine translation (NMT) raises the question of the strengths and weaknesses of NMT. In this paper, we present an analysis of NMT and SMT systems&#8217; outputs from narrow domain English-Latvian MT systems that were trained on a rather small amount of data. We analyze <a href=https://en.wikipedia.org/wiki/Post-editing>post-edits</a> produced by <a href=https://en.wikipedia.org/wiki/Translation>professional translators</a> and manually annotated errors in these outputs. Analysis of post-edits allowed us to conclude that both approaches are comparably successful, allowing for an increase in translators&#8217; productivity, with the NMT system showing slightly worse results. Through the analysis of annotated errors, we found that NMT translations are more fluent than SMT translations. However, <a href=https://en.wikipedia.org/wiki/Error>errors</a> related to <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, especially, mistranslation and omission errors, occur more often in NMT outputs. The word form errors, that characterize the morphological richness of Latvian, are frequent for both systems, but slightly fewer in NMT outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1039/>Towards <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Partially Aligned Corpora</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1039><div class="card-body p-3 small">While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards <a href=https://en.wikipedia.org/wiki/Translation>translation</a> with partially aligned sentence pairs which are derived from the phrase pairs and <a href=https://en.wikipedia.org/wiki/Text_corpus>monolingual corpora</a>. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1040/>Identifying Usage Expression Sentences in Consumer Product Reviews</a></strong><br><a href=/people/s/shibamouli-lahiri/>Shibamouli Lahiri</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1040><div class="card-body p-3 small">In this paper we introduce the problem of identifying usage expression sentences in a consumer product review. We create a human-annotated gold standard dataset of 565 reviews spanning five distinct product categories. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of more than 3,000 annotated sentences. We further introduce a classification system to label sentences according to whether or not they describe some usage. The system combines lexical, syntactic, and semantic features in a product-agnostic fashion to yield good <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> performance. We show the effectiveness of our approach using importance ranking of features, error analysis, and cross-product classification experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1041/>Between Reading Time and Syntactic / Semantic Categories</a></strong><br><a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/s/sachi-kato/>Sachi Kato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1041><div class="card-body p-3 small">This article presents a contrastive analysis between reading time and syntactic / semantic categories in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. We overlaid the reading time annotation of BCCWJ-EyeTrack and a syntactic / semantic category information annotation on the &#8216;Balanced Corpus of Contemporary Written Japanese&#8217;. Statistical analysis based on a <a href=https://en.wikipedia.org/wiki/Mixed_linear_model>mixed linear model</a> showed that verbal phrases tend to have shorter reading times than <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, <a href=https://en.wikipedia.org/wiki/Adverbial_phrase>adverbial phrases</a>, or nominal phrases. The results suggest that the preceding phrases associated with the presenting phrases promote the reading process to shorten the gazing time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1044/>Local Monotonic Attention Mechanism for End-to-End Speech And Language Processing</a></strong><br><a href=/people/a/andros-tjandra/>Andros Tjandra</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1044><div class="card-body p-3 small">Recently, encoder-decoder neural networks have shown impressive performance on many sequence-related tasks. The architecture commonly uses an <a href=https://en.wikipedia.org/wiki/Attentional_control>attentional mechanism</a> which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> between the source and the target sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces <a href=https://en.wikipedia.org/wiki/Sequence_alignment>misalignment</a> on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in several tasks, such as automatic speech recognition (ASR), grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results on ASR, G2P and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between two languages with similar sentence structures, demonstrate that the proposed encoder-decoder model with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1046.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1046/>Diachrony-aware Induction of Binary Latent Representations from Typological Features</a></strong><br><a href=/people/y/yugo-murawaki/>Yugo Murawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1046><div class="card-body p-3 small">Although features of linguistic typology are a promising alternative to lexical evidence for tracing evolutionary history of languages, a large number of missing values in the dataset pose serious difficulties for <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical modeling</a>. In this paper, we combine two existing approaches to the problem : (1) the <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>synchronic approach</a> that focuses on interdependencies between features and (2) the <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>diachronic approach</a> that exploits phylogenetically- and/or spatially-related languages. Specifically, we propose a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian model</a> that (1) represents each language as a sequence of binary latent parameters encoding inter-feature dependencies and (2) relates a language&#8217;s parameters to those of its phylogenetic and spatial neighbors. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> recovers missing values more accurately than others and that <a href=https://en.wikipedia.org/wiki/Induced_representations>induced representations</a> retain phylogenetic and spatial signals observed for surface features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1047/>Image-Grounded Conversations : Multimodal Context for Natural Question and Response Generation</a></strong><br><a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/l/lucy-vanderwende/>Lucy Vanderwende</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1047><div class="card-body p-3 small">The popularity of <a href=https://en.wikipedia.org/wiki/Image_sharing>image sharing</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and the engagement it creates between users reflect the important role that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> plays in <a href=https://en.wikipedia.org/wiki/Conversation>everyday conversations</a>. We present a novel task, Image Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialog research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1048/>A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1048><div class="card-body p-3 small">This study addresses the problem of identifying the meaning of unknown words or entities in a <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. In addition, we construct a new task and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Anonymized Language Modeling for evaluating the ability to capture <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meanings</a> while reading. Experiments conducted using our novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that the proposed variant of RNN language model outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1049/>Using Explicit Discourse Connectives in <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1049><div class="card-body p-3 small">Implicit discourse relation recognition is an extremely challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> due to the lack of indicative connectives. Various neural network architectures have been proposed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> recently, but most of them suffer from the shortage of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. In this paper, we address this problem by procuring additional training data from parallel corpora : When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>explicitation</i>). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1050/>Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse Relation Classification</a></strong><br><a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/j/jingfeng-yang/>Jingfeng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1050><div class="card-body p-3 small">Identifying implicit discourse relations between text spans is a challenging task because it requires understanding the meaning of the text. To tackle this task, recent studies have tried several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> but few of them exploited the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In this work, we explore the idea of incorporating syntactic parse tree into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Specifically, we employ the Tree-LSTM model and Tree-GRU model, which is based on the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structure</a>, to encode the arguments in a relation. And we further leverage the constituent tags to control the semantic composition process in these tree-structured neural networks. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves state-of-the-art performance on PDTB corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1051/>Cross-Lingual Sentiment Analysis Without (Good) Translation</a></strong><br><a href=/people/m/mohamed-abdalla/>Mohamed Abdalla</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1051><div class="card-body p-3 small">Current approaches to cross-lingual sentiment analysis try to leverage the wealth of labeled English data using bilingual lexicons, bilingual vector space embeddings, or machine translation systems. Here we show that it is possible to use a single <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a>, with as few as 2000 word pairs, to capture fine-grained sentiment relationships between words in a cross-lingual setting. We apply these cross-lingual sentiment models to a diverse set of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> to demonstrate their functionality in a non-English context. By effectively leveraging English sentiment knowledge without the need for accurate <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, we can analyze and extract features from other languages with scarce data at a very low cost, thus making sentiment and related analyses for many languages inexpensive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1052/>Implicit Syntactic Features for Target-dependent Sentiment Analysis</a></strong><br><a href=/people/y/yuze-gao/>Yuze Gao</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1052><div class="card-body p-3 small">Targeted sentiment analysis investigates the sentiment polarities on given target mentions from input texts. Different from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level sentiment</a>, it offers more fine-grained knowledge on each entity mention. While early work leveraged syntactic information, recent research has used neural representation learning to induce features automatically, thereby avoiding error propagation of syntactic parsers, which are particularly severe on social media texts. We study a method to leverage syntactic information without explicitly building the parser outputs, by training an encoder-decoder structure parser model on standard syntactic treebanks, and then leveraging its hidden encoder layers when analysing tweets. Such hidden vectors do not contain explicit syntactic outputs, yet encode rich syntactic features. We use them to augment the inputs to a baseline state-of-the-art targeted sentiment classifier, observing significant improvements on various <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. We obtain the best accuracies on all test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1053/>Graph Based Sentiment Aggregation using ConceptNet Ontology<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>N</span>et Ontology</a></strong><br><a href=/people/s/srikanth-tamilselvam/>Srikanth Tamilselvam</a>
|
<a href=/people/s/seema-nagar/>Seema Nagar</a>
|
<a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1053><div class="card-body p-3 small">The sentiment aggregation problem accounts for analyzing the sentiment of a user towards various aspects / features of a product, and meaningfully assimilating the pragmatic significance of these features / aspects from an opinionated text. The current paper addresses the sentiment aggregation problem, by assigning weights to each aspect appearing in the <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a>, that are proportionate to the strategic importance of the aspect in the pragmatic domain. The novelty of this paper is in computing the pragmatic significance (weight) of each aspect, using graph centrality measures (applied on domain specific ontology-graphs extracted from ConceptNet), and deeply ingraining these weights while aggregating the sentiments from opinionated text. We experiment over multiple real-life product review data. Our <a href=https://en.wikipedia.org/wiki/System>system</a> consistently outperforms the state of the art-by as much as a F-score of 20.39 % in one case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1054/>Sentence Modeling with Deep Neural Architecture using Lexicon and Character Attention Mechanism for Sentiment Classification</a></strong><br><a href=/people/h/huy-thanh-nguyen/>Huy Thanh Nguyen</a>
|
<a href=/people/m/minh-le-nguyen/>Minh Le Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1054><div class="card-body p-3 small">Tweet-level sentiment classification in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social networking</a> has many challenges : exploiting <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Semantics>semantic</a>, sentiment, and context in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. To address these problems, we propose a novel approach to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> that uses lexicon features for building lexicon embeddings (LexW2Vs) and generates character attention vectors (CharAVs) by using a Deep Convolutional Neural Network (DeepCNN). Our approach integrates LexW2Vs and CharAVs with continuous word embeddings (ContinuousW2Vs) and dependency-based word embeddings (DependencyW2Vs) simultaneously in order to increase information for each word into a Bidirectional Contextual Gated Recurrent Neural Network (Bi-CGRNN). We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two Twitter sentiment classification datasets. Experimental results show that our model can improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentence-level sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social networking</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1055/>Combining Lightly-Supervised Text Classification Models for Accurate <a href=https://en.wikipedia.org/wiki/Contextual_advertising>Contextual Advertising</a></a></strong><br><a href=/people/y/yiping-jin/>Yiping Jin</a>
|
<a href=/people/d/dittaya-wanvarie/>Dittaya Wanvarie</a>
|
<a href=/people/p/phu-le/>Phu Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1055><div class="card-body p-3 small">In this paper we propose a lightly-supervised framework to rapidly build <a href=https://en.wikipedia.org/wiki/Text_classification>text classifiers</a> for <a href=https://en.wikipedia.org/wiki/Contextual_advertising>contextual advertising</a>. Traditionally text classification techniques require labeled training documents for each predefined class. In the scenario of <a href=https://en.wikipedia.org/wiki/Contextual_advertising>contextual advertising</a>, advertisers often want to target to a specific class of webpages most relevant to their product or service, which may not be covered by a pre-trained classifier. Moreover, the advertisers are interested in whether a webpage is relevant or irrelevant. It is time-consuming to solicit the advertisers for reliable training signals for the negative class. Therefore, it is more suitable to model the problem as a one-class classification problem, in contrast to traditional classification problems where disjoint classes are defined a priori. We first apply two state-of-the-art lightly-supervised classification models, generalized expectation (GE) criteria (Druck et al., 2008) and multinomial naive Bayes (MNB) with priors (Settles, 2011) to one-class classification where the user only needs to provide a small list of labeled words for the target class. To combine the strengths of the two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we fuse them together by using MNB to automatically enrich the constraints for GE training. We also explore ensemble method to combine classifiers. On a corpus of webpages from real-time bidding requests, the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves the highest average F1 of 0.69 and closes more than half of the gap between previous state-of-the-art lightly-supervised models to a fully-supervised MaxEnt model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1056/>Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1056><div class="card-body p-3 small">Despite successful applications across a broad range of NLP tasks, conditional random fields (CRFs), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a>. In this work, we propose an extension to CRFs by integrating <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a>, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> show substantial improvements over strong CRF and LSTM baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1057/>Named Entity Recognition with Stack Residual LSTM and Trainable Bias Decoding<span class=acl-fixed-case>LSTM</span> and Trainable Bias Decoding</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Tran</a>
|
<a href=/people/a/andrew-mackinlay/>Andrew MacKinlay</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1057><div class="card-body p-3 small">Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. The second innovation is a bias decoding mechanism that allows the trained <a href=https://en.wikipedia.org/wiki/System>system</a> to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train / development / test split of the CoNLL 2003 Shared Task NER dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1058.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1058/>Neuramanteau : A Neural Network Ensemble Model for Lexical Blends<span class=acl-fixed-case>N</span>euramanteau: A Neural Network Ensemble Model for Lexical Blends</a></strong><br><a href=/people/k/kollol-das/>Kollol Das</a>
|
<a href=/people/s/shaona-ghosh/>Shaona Ghosh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1058><div class="card-body p-3 small">The problem of blend formation in <a href=https://en.wikipedia.org/wiki/Generative_linguistics>generative linguistics</a> is interesting in the context of <a href=https://en.wikipedia.org/wiki/Neologism>neologism</a>, their quick adoption in modern life and the creative generative process guiding their formation. Blend quality depends on multitude of factors with high degrees of uncertainty. In this work, we investigate if the modern <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a> can sufficiently capture and recognize the creative blend composition process. We propose recurrent neural network sequence-to-sequence models, that are evaluated on multiple blend datasets available in the literature. We propose an ensemble neural and hybrid model that outperforms most of the baselines and <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic models</a> upon evaluation on test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1059.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1059" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1059/>Leveraging <a href=https://en.wikipedia.org/wiki/Discourse>Discourse Information</a> Effectively for Authorship Attribution</a></strong><br><a href=/people/e/elisa-ferracane/>Elisa Ferracane</a>
|
<a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1059><div class="card-body p-3 small">We explore techniques to maximize the effectiveness of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> in the task of <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a significant margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1060/>Lightly-Supervised Modeling of Argument Persuasiveness</a></strong><br><a href=/people/i/isaac-persing/>Isaac Persing</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1060><div class="card-body p-3 small">We propose the first lightly-supervised approach to scoring an argument&#8217;s persuasiveness. Key to our approach is the novel hypothesis that lightly-supervised persuasiveness scoring is possible by explicitly modeling the major errors that negatively impact <a href=https://en.wikipedia.org/wiki/Persuasion>persuasiveness</a>. In an evaluation on a new annotated corpus of online debate arguments, our approach rivals its fully-supervised counterparts in performance by four scoring metrics when using only 10 % of the available training instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1061/>Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models</a></strong><br><a href=/people/y/yi-luan/>Yi Luan</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1061><div class="card-body p-3 small">Building a persona-based conversation agent is challenging owing to the lack of large amounts of speaker-specific conversation data for model training. This paper addresses the problem by proposing a multi-task learning approach to training neural conversation models that leverages both conversation data across speakers and other types of <a href=https://en.wikipedia.org/wiki/Data>data</a> pertaining to the speaker and speaker roles to be modeled. Experiments show that our approach leads to significant improvements over baseline model quality, generating responses that capture more precisely speakers&#8217; traits and <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>speaking styles</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1062.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1062/>Chat Disentanglement : Identifying Semantic Reply Relationships with Random Forests and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1062><div class="card-body p-3 small">Thread disentanglement is a precursor to any high-level analysis of multiparticipant chats. Existing research approaches the problem by calculating the likelihood of two messages belonging in the same thread. Our approach leverages a newly annotated dataset to identify reply relationships. Furthermore, we explore the usage of an RNN, along with large quantities of unlabeled data, to learn semantic relationships between messages. Our proposed <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>, which utilizes a reply classifier and an RNN to generate a set of disentangled threads, is novel and performs well against previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1063.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1063/>Towards Bootstrapping a Polarity Shifter Lexicon using <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a></a></strong><br><a href=/people/m/marc-schulder/>Marc Schulder</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1063><div class="card-body p-3 small">We present a major step towards the creation of the first high-coverage lexicon of polarity shifters. In this work, we bootstrap a lexicon of verbs by exploiting various <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a>. Polarity shifters, such as abandon, are similar to negations (e.g. not) in that they move the polarity of a phrase towards its inverse, as in abandon all hope. While there exist lists of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a>, creating comprehensive lists of polarity shifters is far more challenging due to their sheer number. On a sample of manually annotated verbs we examine a variety of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> for this task. Then we build a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> to increase <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a>. We show that this approach drastically reduces the annotation effort while ensuring a high-precision lexicon. We also show that our acquired knowledge of verbal polarity shifters improves phrase-level sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1064/>Cascading Multiway Attentions for Document-level Sentiment Classification</a></strong><br><a href=/people/d/dehong-ma/>Dehong Ma</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/x/xiaodong-zhang/>Xiaodong Zhang</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1064><div class="card-body p-3 small">Document-level sentiment classification aims to assign the user reviews a sentiment polarity. Previous methods either just utilized the document content without consideration of user and product information, or did not comprehensively consider what roles the three kinds of information play in text modeling. In this paper, to reasonably use all the information, we present the idea that <a href=https://en.wikipedia.org/wiki/User_(computing)>user</a>, product and their combination can all influence the generation of attentions to words and sentences, when judging the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of a document. With this idea, we propose a cascading multiway attention (CMA) model, where multiple ways of using user and product information are cascaded to influence the generation of attentions on the word and sentence layers. Then, sentences and documents are well modeled by multiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1066/>Leveraging Auxiliary Tasks for Document-Level Cross-Domain Sentiment Classification</a></strong><br><a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1066><div class="card-body p-3 small">In this paper, we study <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> with a state-of-the-art hierarchical neural network for document-level sentiment classification. We first design a new <a href=https://en.wikipedia.org/wiki/Task_(computing)>auxiliary task</a> based on sentiment scores of domain-independent words. We then propose two neural network architectures to respectively induce document embeddings and <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> that work well for different domains. When these document and sentence embeddings are used for sentiment classification, we find that with both pseudo and external sentiment lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1067/>Measuring Semantic Relations between Human Activities</a></strong><br><a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1067><div class="card-body p-3 small">The things people do in their daily lives can provide valuable insights into their <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality</a>, values, and interests. Unstructured text data on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> are rich in behavioral content, and automated systems can be deployed to learn about human activity on a broad scale if these systems are able to reason about the content of interest. In order to aid in the evaluation of such <a href=https://en.wikipedia.org/wiki/System>systems</a>, we introduce a new phrase-level semantic textual similarity dataset comprised of human activity phrases, providing a testbed for automated systems that analyze relationships between phrasal descriptions of people&#8217;s actions. Our set of 1,000 pairs of activities is annotated by human judges across four relational dimensions including <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a>, <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a>, motivational alignment, and perceived actor congruence. We evaluate a set of strong baselines for the task of generating scores that correlate highly with human ratings, and we introduce several new approaches to the phrase-level similarity task in the domain of human activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1068/>Learning Transferable Representation for Bilingual Relation Extraction via <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/z/zhuolin-jiang/>Zhuolin Jiang</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/r/ralph-weischedel/>Ralph Weischedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1068><div class="card-body p-3 small">Typically, relation extraction models are trained to extract instances of a <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>relation ontology</a> using only training data from a single language. However, the concepts represented by the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>relation ontology</a> (e.g. ResidesIn, EmployeeOf) are language independent. The numbers of annotated examples available for a given <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> vary between languages. For example, there are far fewer annotated examples in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> than <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Furthermore, using only language-specific training data results in the need to manually annotate equivalently large amounts of training for each new language a system encounters. We propose a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> to learn transferable, discriminative bilingual representation. Experiments on the ACE 2005 multilingual training corpus demonstrate that the joint training process results in significant improvement in relation classification performance over the monolingual counterparts. The learnt representation is discriminative and transferable between languages. When using 10 % (25 K English words, or 30 K Chinese characters) of the training data, our approach results in doubling F1 compared to a monolingual baseline. We achieve comparable performance to the monolingual system trained with 250 K English words (or 300 K Chinese characters) With 50 % of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1071/>Joint Learning of Dialog Act Segmentation and Recognition in Spoken Dialog Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/t/tianyu-zhao/>Tianyu Zhao</a>
|
<a href=/people/t/tatsuya-kawahara/>Tatsuya Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1071><div class="card-body p-3 small">Dialog act segmentation and recognition are basic natural language understanding tasks in <a href=https://en.wikipedia.org/wiki/Spoken_dialog_systems>spoken dialog systems</a>. This paper investigates a unified architecture for these two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, which aims to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance on both of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. Compared with past joint models, the proposed architecture can (1) incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> in dialog act recognition, and (2) integrate <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> for tasks of different levels as a whole, i.e. dialog act segmentation on the word level and dialog act recognition on the <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segment level</a>. Experimental results show that the joint training system outperforms the simple cascading system and the joint coding system on both dialog act segmentation and recognition tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1072/>Predicting Users’ Negative Feedbacks in Multi-Turn Human-Computer Dialogues</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/j/jianan-wang/>Jianan Wang</a>
|
<a href=/people/y/yuanchao-liu/>Yuanchao Liu</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a>
|
<a href=/people/z/zhuoran-wang/>Zhuoran Wang</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1072><div class="card-body p-3 small">User experience is essential for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer dialogue systems</a>. However, it is impractical to ask users to provide explicit feedbacks when the agents&#8217; responses displease them. Therefore, in this paper, we explore to predict users&#8217; imminent dissatisfactions caused by <a href=https://en.wikipedia.org/wiki/Intelligent_agent>intelligent agents</a> by analysing the existing utterances in the dialogue sessions. To our knowledge, this is the first work focusing on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Several possible factors that trigger <a href=https://en.wikipedia.org/wiki/Emotion>negative emotions</a> are modelled. A relation sequence model (RSM) is proposed to encode the sequence of appropriateness of current response with respect to the earlier utterances. The experimental results show that the proposed structure is effective in modelling emotional risk (possibility of negative feedback) than existing conversation modelling approaches. Besides, strategies of obtaining distance supervision data for pre-training are also discussed in this work. Balanced sampling with respect to the last response in the distance supervision data are shown to be reliable for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1073/>Finding Dominant User Utterances And System Responses in Conversations</a></strong><br><a href=/people/d/dhiraj-madan/>Dhiraj Madan</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1073><div class="card-body p-3 small">There are several dialog frameworks which allow manual specification of intents and rule based dialog flow. The rule based framework provides good control to dialog designers at the expense of being more time consuming and laborious. The job of a dialog designer can be reduced if we could identify pairs of user intents and corresponding responses automatically from prior conversations between users and agents. In this paper we propose an approach to find these frequent user utterances (which serve as examples for intents) and corresponding agent responses. We propose a novel SimCluster algorithm that extends standard <a href=https://en.wikipedia.org/wiki/K-means_clustering>K-means algorithm</a> to simultaneously cluster user utterances and agent utterances by taking their adjacency information into account. The method also aligns these <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> to provide pairs of <a href=https://en.wikipedia.org/wiki/Intention>intents</a> and response groups. We compare our results with those produced by using simple Kmeans clustering on a real dataset and observe upto 10 % absolute improvement in <a href=https://en.wikipedia.org/wiki/F-number>F1-scores</a>. Through our experiments on synthetic dataset, we show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> gains more advantage over K-means algorithm when the data has large variance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1074/>End-to-End Task-Completion Neural Dialogue Systems</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1074><div class="card-body p-3 small">One of the major drawbacks of modularized task-completion dialogue systems is that each <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> is trained individually, which presents several challenges. For example, downstream modules are affected by earlier <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, and the performance of the entire <a href=https://en.wikipedia.org/wiki/System>system</a> is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle <a href=https://en.wikipedia.org/wiki/Noise>noises</a> caused by other components of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1077/>Domain Adaptation from User-level Facebook Models to County-level Twitter Predictions<span class=acl-fixed-case>F</span>acebook Models to County-level <span class=acl-fixed-case>T</span>witter Predictions</a></strong><br><a href=/people/d/daniel-rieman/>Daniel Rieman</a>
|
<a href=/people/k/kokil-jaidka/>Kokil Jaidka</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1077><div class="card-body p-3 small">Several studies have demonstrated how language models of user attributes, such as <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, can be built by using the Facebook language of social media users in conjunction with their responses to psychology questionnaires. It is challenging to apply these models to make general predictions about attributes of communities, such as personality distributions across US counties, because it requires 1. the potentially inavailability of the original training data because of privacy and ethical regulations, 2. adapting Facebook language models to Twitter language without retraining the model, and 3. adapting from users to county-level collections of tweets. We propose a two-step algorithm, Target Side Domain Adaptation (TSDA) for such domain adaptation when no labeled Twitter / county data is available. TSDA corrects for the different word distributions between <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and for the varying word distributions across counties by adjusting target side word frequencies ; no changes to the trained model are made. In the case of predicting the <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five county-level personality traits</a>, TSDA outperforms a state-of-the-art domain adaptation method, gives county-level predictions that have fewer extreme outliers, higher year-to-year stability, and higher correlation with county-level outcomes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1078/>Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach</a></strong><br><a href=/people/l/lei-gao/>Lei Gao</a>
|
<a href=/people/a/alexis-kuppersmith/>Alexis Kuppersmith</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1078><div class="card-body p-3 small">In the wake of a polarizing election, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is laden with hateful content. To address various limitations of supervised hate speech classification methods including corpus bias and huge cost of annotation, we propose a weakly supervised two-path bootstrapping approach for an online hate speech detection model leveraging large-scale unlabeled data. This system significantly outperforms hate speech detection systems that are trained in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised manner</a> using manually annotated data. Applying this model on a large quantity of tweets collected before, after, and on election day reveals motivations and patterns of inflammatory language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1081/>Concept-Map-Based Multi-Document Summarization using Concept Coreference Resolution and Global Importance Optimization</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1081><div class="card-body p-3 small">Concept-map-based multi-document summarization is a variant of traditional <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> that produces structured summaries in the form of <a href=https://en.wikipedia.org/wiki/Concept_map>concept maps</a>. In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that addresses several issues in previous methods. It learns to identify and merge coreferent concepts to reduce redundancy, determines their importance with a strong <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> and finds an optimal summary concept map via <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>integer linear programming</a>. It is also computationally more efficient than previous methods, allowing us to summarize larger document sets. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two datasets, finding that it outperforms several approaches from previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1082/>Abstractive Multi-document Summarization by Partial Tree Extraction, Recombination and Linearization</a></strong><br><a href=/people/l/litton-j-kurisinkel/>Litton J Kurisinkel</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1082><div class="card-body p-3 small">Existing work for abstractive multidocument summarization utilise existing <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structures</a> directly extracted from input documents to generate summary sentences. These methods can suffer from lack of consistence and coherence in merging phrases. We introduce a novel approach for abstractive multidocument summarization through partial dependency tree extraction, <a href=https://en.wikipedia.org/wiki/Genetic_recombination>recombination</a> and <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a>. The method entrusts the <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> to generate its own topically coherent sequential structures from scratch for effective communication. Results on TAC 2011, DUC-2004 and 2005 show that our system gives competitive results compared with state of the art abstractive summarization approaches in the literature. We also achieve competitive results in linguistic quality assessed by <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluators</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1083/>Event Argument Identification on Dependency Graphs with Bidirectional LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/a/alex-judea/>Alex Judea</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1083><div class="card-body p-3 small">In this paper we investigate the performance of event argument identification. We show that the performance is tied to syntactic complexity. Based on this finding, we propose a novel and effective <a href=https://en.wikipedia.org/wiki/System>system</a> for event argument identification. Recurrent Neural Networks learn to produce meaningful representations of long and short dependency paths. Convolutional Neural Networks learn to decompose the lexical context of argument candidates. They are combined into a simple system which outperforms a feature-based, state-of-the-art event argument identifier without any manual feature engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1084/>Selective Decoding for Cross-lingual Open Information Extraction</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1084><div class="card-body p-3 small">Cross-lingual open information extraction is the task of distilling facts from the source language into representations in the target language. We propose a novel encoder-decoder model for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. It employs a novel selective decoding mechanism, which explicitly models the sequence labeling process as well as the sequence generation process on the decoder side. Compared to a standard encoder-decoder model, selective decoding significantly increases the performance on a Chinese-English cross-lingual open IE dataset by 3.87-4.49 BLEU and 1.91-5.92 <a href=https://en.wikipedia.org/wiki/F-number>F1</a>. We also extend our approach to low-resource scenarios, and gain promising improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1085/>Event Ordering with a Generalized Model for Sieve Prediction Ranking</a></strong><br><a href=/people/b/bill-mcdowell/>Bill McDowell</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/a/alexander-ororbia-ii/>Alexander Ororbia II</a>
|
<a href=/people/d/david-reitter/>David Reitter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1085><div class="card-body p-3 small">This paper improves on several aspects of a sieve-based event ordering architecture, CAEVO (Chambers et al., 2014), which creates globally consistent temporal relations between events and time expressions. First, we examine the usage of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and semantic role features. With the incorporation of these new features, we demonstrate a 5 % relative F1 gain over our replicated version of CAEVO. Second, we reformulate the architecture&#8217;s sieve-based inference algorithm as a prediction reranking method that approximately optimizes a scoring function computed using classifier precisions. Within this prediction reranking framework, we propose an alternative scoring function, showing an 8.8 % relative gain over the original CAEVO. We further include an in-depth analysis of one of the main datasets that is used to evaluate temporal classifiers, and we show how despite using the densest corpus, there is still a danger of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. While this paper focuses on temporal ordering, its results are applicable to other areas that use sieve-based architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1086/>Open Relation Extraction and Grounding</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1086><div class="card-body p-3 small">Previous open Relation Extraction (open RE) approaches mainly rely on linguistic patterns and constraints to extract important relational triples from large-scale corpora. However, they lack of abilities to cover diverse relation expressions or measure the relative importance of candidate triples within a sentence. It is also challenging to name the relation type of a relational triple merely based on context words, which could limit the usefulness of open RE in downstream applications. We propose a novel importance-based open RE approach by exploiting the global structure of a dependency tree to extract salient triples. We design an unsupervised relation type naming method by grounding relational triples to a large-scale Knowledge Base (KB) schema, leveraging KB triples and weighted context words associated with relational triples. Experiments on the English Slot Filling 2013 dataset demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> achieves 8.1 % higher <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> over state-of-the-art open RE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1087/>Extraction of Gene-Environment Interaction from the Biomedical Literature</a></strong><br><a href=/people/j/jinseon-you/>Jinseon You</a>
|
<a href=/people/j/jin-woo-chung/>Jin-Woo Chung</a>
|
<a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/j/jong-c-park/>Jong C. Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1087><div class="card-body p-3 small">Genetic information in the literature has been extensively looked into for the purpose of discovering the etiology of a disease. As the gene-disease relation is sensitive to external factors, their identification is important to study a disease. Environmental influences, which are usually called Gene-Environment interaction (GxE), have been considered as important factors and have extensively been researched in <a href=https://en.wikipedia.org/wiki/Biology>biology</a>. Nevertheless, there is still a lack of systems for automatic GxE extraction from the biomedical literature due to new challenges : (1) there are no preprocessing tools and corpora for GxE, (2) expressions of GxE are often quite implicit, and (3) document-level comprehension is usually required. We propose to overcome these challenges with neural network models and show that a modified sequence-to-sequence model with a static RNN decoder produces a good performance in GxE recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1088/>Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation<span class=acl-fixed-case>MOOC</span>s via Embedding-Based Graph Propagation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/x/xiaochen-wang/>Xiaochen Wang</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1088><div class="card-body p-3 small">Massive Open Online Courses (MOOCs), offering a new way to study online, are revolutionizing education. One challenging issue in <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a> is how to design effective and fine-grained course concepts such that students with different backgrounds can grasp the essence of the course. In this paper, we conduct a systematic investigation of the problem of course concept extraction for <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a>. We propose to learn latent representations for <a href=https://en.wikipedia.org/wiki/Feasible_region>candidate concepts</a> via an embedding-based method. Moreover, we develop a graph-based propagation algorithm to rank the candidate concepts based on the learned <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a>. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using different <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> from XuetangX and <a href=https://en.wikipedia.org/wiki/Coursera>Coursera</a>. Experimental results show that our method significantly outperforms all the alternative methods (+0.013-0.318 in terms of R-precision ; p0.01, t-test).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1089/>Identity Deception Detection</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/q/quincy-davenport/>Quincy Davenport</a>
|
<a href=/people/a/anna-mengdan-dai/>Anna Mengdan Dai</a>
|
<a href=/people/m/mohamed-abouelenien/>Mohamed Abouelenien</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1089><div class="card-body p-3 small">This paper addresses the task of detecting identity deception in <a href=https://en.wikipedia.org/wiki/Language>language</a>. Using a novel identity deception dataset, consisting of real and portrayed identities from 600 individuals, we show that we can build accurate identity detectors targeting both age and gender, with accuracies of up to 88. We also perform an analysis of the <a href=https://en.wikipedia.org/wiki/Pattern>linguistic patterns</a> used in <a href=https://en.wikipedia.org/wiki/Identity_deception>identity deception</a>, which lead to interesting insights into identity portrayers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1091.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1091/>Dataset for a Neural Natural Language Interface for Databases (NNLIDB)<span class=acl-fixed-case>NNLIDB</span>)</a></strong><br><a href=/people/f/florin-brad/>Florin Brad</a>
|
<a href=/people/r/radu-cristian-alexandru-iacob/>Radu Cristian Alexandru Iacob</a>
|
<a href=/people/i/ionel-alexandru-hosu/>Ionel Alexandru Hosu</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1091><div class="card-body p-3 small">Progress in natural language interfaces to databases (NLIDB) has been slow mainly due to linguistic issues (such as language ambiguity) and domain portability. Moreover, the lack of a large corpus to be used as a standard benchmark has made data-driven approaches difficult to develop and compare. In this paper, we revisit the problem of NLIDBs and recast it as a sequence translation problem. To this end, we introduce a large dataset extracted from the Stack Exchange Data Explorer website, which can be used for training neural natural language interfaces for databases. We also report encouraging baseline results on a smaller manually annotated test corpus, obtained using an attention-based sequence-to-sequence neural network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1093/>Demographic Word Embeddings for Racism Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/g/gael-dias/>Gaël Dias</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1093><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> grant users freedom of speech by allowing them to freely express their thoughts, beliefs, and opinions. Although this represents incredible and unique communication opportunities, it also presents important challenges. Online racism is such an example. In this study, we present a supervised learning strategy to detect racist language on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> that incorporate demographic (Age, Gender, and Location) information. Our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> achieves reasonable <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> over a gold standard dataset (F1=76.3 %) and significantly improves over the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance of demographic-agnostic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1095.Software.txt data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1095.Datasets.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1095" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1095/>Semantic Document Distance Measures and Unsupervised Document Revision Detection</a></strong><br><a href=/people/x/xiaofeng-zhu/>Xiaofeng Zhu</a>
|
<a href=/people/d/diego-klabjan/>Diego Klabjan</a>
|
<a href=/people/p/patrick-bless/>Patrick Bless</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1095><div class="card-body p-3 small">In this paper, we model the document revision detection problem as a minimum cost branching problem that relies on computing document distances. Furthermore, we propose two new document distance measures, word vector-based Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED). Our revision detection system is designed for a large scale corpus and implemented in <a href=https://en.wikipedia.org/wiki/Apache_Spark>Apache Spark</a>. We demonstrate that our system can more precisely detect revisions than state-of-the-art methods by utilizing the Wikipedia revision dumps and simulated data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1096/>An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1096><div class="card-body p-3 small">Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state-of-the-art RC model, we empirically investigate the performance of single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The <a href=https://en.wikipedia.org/wiki/RC_model>RC model</a> is an end-to-end neural network with iterative attention, and uses <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to dynamically control the number of turns. We find that multiple-turn reasoning outperforms single-turn reasoning for all question and answer types ; further, we observe that enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy. % across all question types, and is particularly beneficial to questions with lengthy, descriptive answers. We achieve results competitive to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1097/>Automated Historical Fact-Checking by Passage Retrieval, Word Statistics, and Virtual Question-Answering</a></strong><br><a href=/people/m/mio-kobayashi/>Mio Kobayashi</a>
|
<a href=/people/a/ai-ishii/>Ai Ishii</a>
|
<a href=/people/c/chikara-hoshino/>Chikara Hoshino</a>
|
<a href=/people/h/hiroshi-miyashita/>Hiroshi Miyashita</a>
|
<a href=/people/t/takuya-matsuzaki/>Takuya Matsuzaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1097><div class="card-body p-3 small">This paper presents a hybrid approach to the verification of statements about historical facts. The test data was collected from the world history examinations in a standardized achievement test for high school students. The <a href=https://en.wikipedia.org/wiki/Data>data</a> includes various kinds of false statements that were carefully written so as to deceive the students while they can be disproven on the basis of the teaching materials. Our system predicts the truth or falsehood of a statement based on text search, word cooccurrence statistics, factoid-style question answering, and temporal relation recognition. These <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> contribute to the judgement complementarily and achieved the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1098/>Integrating Subject, Type, and Property Identification for Simple <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> over Knowledge Base</a></strong><br><a href=/people/w/wei-chuan-hsiao/>Wei-Chuan Hsiao</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1098><div class="card-body p-3 small">This paper presents an approach to identify subject, type and property from knowledge base (KB) for answering simple questions. We propose new <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> to rank entity candidates in KB. Besides, we split a relation in <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> into type and property. Each of <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>them</a> is modeled by a bi-directional LSTM. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance on the SimpleQuestions dataset. The hard questions in the experiments are also analyzed in detail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1099.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1099/>DailyDialog : A Manually Labelled Multi-turn Dialogue Dataset<span class=acl-fixed-case>D</span>aily<span class=acl-fixed-case>D</span>ialog: A Manually Labelled Multi-turn Dialogue Dataset</a></strong><br><a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/z/ziqiang-cao/>Ziqiang Cao</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1099><div class="card-body p-3 small">We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The <a href=https://en.wikipedia.org/wiki/Language>language</a> is human-written and less noisy. The dialogues in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> reflect our daily communication way and cover various topics about our daily life. We also manually label the developed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with <a href=https://en.wikipedia.org/wiki/Intentionality>communication intention</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion information</a>. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available on<b>DailyDialog</b>, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The dataset is available on <url>http://yanran.li/dailydialog</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1100/>Inference is Everything : Recasting Semantic Resources into a Unified Evaluation Framework</a></strong><br><a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/p/pushpendre-rastogi/>Pushpendre Rastogi</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1100><div class="card-body p-3 small">We propose to unify a variety of existing semantic classification tasks, such as <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>, and <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, under the heading of Recognizing Textual Entailment (RTE). We present a general <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to automatically generate one or more sentential hypotheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> enables us to probe a statistical RTE model&#8217;s performance on different aspects of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1102/>Multilingual Hierarchical Attention Networks for Document Classification</a></strong><br><a href=/people/n/nikolaos-pappas/>Nikolaos Pappas</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1102><div class="card-body p-3 small">Hierarchical attention networks have recently achieved remarkable performance for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> in a given language. However, when multilingual document collections are considered, training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of <a href=https://en.wikipedia.org/wiki/Language_transfer>cross-language transfer</a>.</div></div></div><hr><div id=i17-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/I17-2/>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2000/>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></strong><br><a href=/people/g/greg-kondrak/>Greg Kondrak</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2001/>CKY-based Convolutional Attention for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a><span class=acl-fixed-case>CKY</span>-based Convolutional Attention for Neural Machine Translation</a></strong><br><a href=/people/t/taiki-watanabe/>Taiki Watanabe</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/t/takashi-ninomiya/>Takashi Ninomiya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2001><div class="card-body p-3 small">This paper proposes a new attention mechanism for neural machine translation (NMT) based on convolutional neural networks (CNNs), which is inspired by the <a href=https://en.wikipedia.org/wiki/CKY_algorithm>CKY algorithm</a>. The proposed <a href=https://en.wikipedia.org/wiki/Attention>attention</a> represents every possible combination of source words (e.g., phrases and structures) through CNNs, which imitates the CKY table in the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. NMT, incorporating the proposed <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, decodes a target sentence on the basis of the <a href=https://en.wikipedia.org/wiki/Attention>attention scores</a> of the hidden states of CNNs. The proposed <a href=https://en.wikipedia.org/wiki/Attention>attention</a> enables NMT to capture <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> from underlying structures of a source sentence without <a href=https://en.wikipedia.org/wiki/Sentence_parsing>sentence parsing</a>. The evaluations on the Asian Scientific Paper Excerpt Corpus (ASPEC) English-Japanese translation task show that the proposed <a href=https://en.wikipedia.org/wiki/Attention>attention</a> gains 0.66 points in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2002/>Supervised Attention for Sequence-to-Sequence Constituency Parsing</a></strong><br><a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2002><div class="card-body p-3 small">The sequence-to-sequence (Seq2Seq) model has been successfully applied to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. Recently, MT performances were improved by incorporating supervised attention into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2003/>Transferring Semantic Roles Using Translation and Syntactic Information</a></strong><br><a href=/people/m/maryam-aminian/>Maryam Aminian</a>
|
<a href=/people/m/mohammad-sadegh-rasooli/>Mohammad Sadegh Rasooli</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2003><div class="card-body p-3 small">Our paper addresses the problem of annotation projection for semantic role labeling for resource-poor languages using supervised annotations from a resource-rich language through parallel data. We propose a transfer method that employs information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. Our experiments yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2005/>Analyzing Well-Formedness of Syllables in <a href=https://en.wikipedia.org/wiki/Japanese_Sign_Language>Japanese Sign Language</a><span class=acl-fixed-case>J</span>apanese <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/s/satoshi-yawata/>Satoshi Yawata</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a>
|
<a href=/people/d/daisuke-hara/>Daisuke Hara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2005><div class="card-body p-3 small">This paper tackles a problem of analyzing the well-formedness of syllables in <a href=https://en.wikipedia.org/wiki/Japanese_Sign_Language>Japanese Sign Language (JSL)</a>. We formulate the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a classification problem that classifies syllables into well-formed or ill-formed. We build a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> that contains hand-coded syllables and their <a href=https://en.wikipedia.org/wiki/Well-formedness>well-formedness</a>. We define a fine-grained feature set based on the hand-coded syllables and train a logistic regression classifier on labeled syllables, expecting to find the discriminative features from the trained <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. We also perform pseudo active learning to investigate the applicability of <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> in analyzing syllables. In the experiments, the best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> with our combinatorial features achieved the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 87.0 %. The pseudo active learning is also shown to be effective showing that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> could reduce about 84 % of training instances to achieve the accuracy of 82.0 % when compared to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> without <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2006.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2006/>Towards Lower Bounds on Number of Dimensions for Word Embeddings</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2006><div class="card-body p-3 small">Word embeddings are a relatively new addition to the modern NLP researcher&#8217;s toolkit. However, unlike other <a href=https://en.wikipedia.org/wiki/Tool>tools</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are used in a black box manner. There are very few studies regarding various <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameters</a>. One such <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameter</a> is the dimension of word embeddings. They are rather decided based on a rule of thumb : in the range 50 to 300. In this paper, we show that the <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>dimension</a> should instead be chosen based on <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus statistics</a>. More specifically, we show that the number of pairwise equidistant words of the corpus vocabulary (as defined by some distance / similarity metric) gives a lower bound on the the number of dimensions, and going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-2008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-2008/>Input-to-Output Gate to Improve RNN Language Models<span class=acl-fixed-case>RNN</span> Language Models</a></strong><br><a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2008><div class="card-body p-3 small">This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2009.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-2009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-2009/>Counterfactual Language Model Adaptation for Suggesting Phrases</a></strong><br><a href=/people/k/kenneth-arnold/>Kenneth Arnold</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2009><div class="card-body p-3 small">Mobile devices use <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to suggest words and phrases for use in <a href=https://en.wikipedia.org/wiki/Computer_terminal>text entry</a>. Traditional <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are based on contextual word frequency in a static corpus of text. However, certain types of phrases, when offered to writers as suggestions, may be systematically chosen more often than their frequency would predict. In this paper, we propose the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of generating suggestions that writers accept, a related but distinct <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to making accurate predictions. Although this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is fundamentally interactive, we propose a counterfactual setting that permits offline training and evaluation. We find that even a simple <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can capture text characteristics that improve <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2012.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2012/>Learning Kernels over Strings using <a href=https://en.wikipedia.org/wiki/Gaussian_function>Gaussian Processes</a><span class=acl-fixed-case>G</span>aussian Processes</a></strong><br><a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2012><div class="card-body p-3 small">Non-contiguous word sequences are widely known to be important in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>modelling natural language</a>. However they not explicitly encoded in <a href=https://en.wikipedia.org/wiki/Universal_Coded_Character_Set>common text representations</a>. In this work we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a> using <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a>, capable of flexibly representing non-contiguous sequences. Specifically, we derive a vectorised version of the string kernel algorithm and their <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>, allowing efficient <a href=https://en.wikipedia.org/wiki/Hyperparameter_optimization>hyperparameter optimisation</a> as part of a Gaussian Process framework. Experiments on synthetic data and text regression for emotion analysis show the promise of this technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2013/>Substring Frequency Features for Segmentation of Japanese Katakana Words with Unlabeled Corpora<span class=acl-fixed-case>J</span>apanese Katakana Words with Unlabeled Corpora</a></strong><br><a href=/people/y/yoshinari-fujinuma/>Yoshinari Fujinuma</a>
|
<a href=/people/a/alvin-grissom-ii/>Alvin Grissom II</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2013><div class="card-body p-3 small">Word segmentation is crucial in natural language processing tasks for unsegmented languages. In <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, many out-of-vocabulary words appear in the <a href=https://en.wikipedia.org/wiki/Katakana>phonetic syllabary katakana</a>, making segmentation more difficult due to the lack of clues found in mixed script settings. In this paper, we propose a straightforward approach based on a variant of tf-idf and apply it to the problem of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. Even though our method uses only an unlabeled corpus, experimental results show that it achieves performance comparable to existing methods that use manually labeled corpora. Furthermore, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> improves performance of simple <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation models</a> trained on a manually labeled corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2014/>MONPA : Multi-objective Named-entity and Part-of-speech Annotator for Chinese using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network</a><span class=acl-fixed-case>MONPA</span>: Multi-objective Named-entity and Part-of-speech Annotator for <span class=acl-fixed-case>C</span>hinese using Recurrent Neural Network</a></strong><br><a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/y/yi-jie-huang/>Yi-Jie Huang</a>
|
<a href=/people/s/shu-hao-yeh/>Shu-Hao Yeh</a>
|
<a href=/people/c/chun-hung-chen/>Chun-Hung Chen</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2014><div class="card-body p-3 small">Part-of-speech (POS) tagging and named entity recognition (NER) are crucial steps in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In addition, the difficulty of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> places additional burden on those who intend to deal with languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and pipelined systems often suffer from <a href=https://en.wikipedia.org/wiki/Propagation_of_error>error propagation</a>. This work proposes an end-to-end model using character-based recurrent neural network (RNN) to jointly accomplish segmentation, POS tagging and NER of a Chinese sentence. Experiments on previous word segmentation and NER datasets show that a single model with the proposed architecture is comparable to those trained specifically for each task, and outperforms freely-available softwares. Moreover, we provide a web-based interface for the public to easily access this resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2015/>Recall is the Proper Evaluation Metric for <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2015><div class="card-body p-3 small">We extensively analyse the correlations and drawbacks of conventionally employed evaluation metrics for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Unlike in standard <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> favours under-splitting systems and therefore can be misleading in <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Overall, based on both theoretical and experimental analysis, we propose that precision should be excluded from the standard evaluation metrics and that the evaluation score obtained by using only <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> is sufficient and better correlated with the performance of word segmentation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2016/>Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2016><div class="card-body p-3 small">Low-resource named entity recognition is still an open problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world&#8217;s languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entities</a> for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2017/>Segment-Level Neural Conditional Random Fields for Named Entity Recognition</a></strong><br><a href=/people/m/motoki-sato/>Motoki Sato</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2017><div class="card-body p-3 small">We present Segment-level Neural CRF, which combines <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with a linear chain CRF for segment-level sequence modeling tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> and syntactic chunking. Our segment-level CRF can consider higher-order label dependencies compared with conventional word-level CRF. Since it is difficult to consider all possible variable length segments, our method uses segment lattice constructed from the word-level tagging model to reduce the search space. Performing experiments on NER and chunking, we demonstrate that our method outperforms conventional word-level CRF with <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2018/>Integrating Vision and Language Datasets to Measure Word Concreteness</a></strong><br><a href=/people/g/gitit-kehat/>Gitit Kehat</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2018><div class="card-body p-3 small">We present and take advantage of the inherent visualizability properties of words in visual corpora (the textual components of vision-language datasets) to compute concreteness scores for <a href=https://en.wikipedia.org/wiki/Word>words</a>. Our simple method does not require hand-annotated concreteness score lists for training, and yields state-of-the-art results when evaluated against concreteness scores lists and previously derived scores, as well as when used for metaphor detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2019/>Semantic Features Based on Word Alignments for Estimating Quality of Text Simplification</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2019><div class="card-body p-3 small">This paper examines the usefulness of <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> based on <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> for estimating the quality of text simplification. Specifically, we introduce seven types of alignment-based features computed on the basis of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and paraphrase lexicons. Through an empirical experiment using the QATS dataset, we confirm that we can achieve the state-of-the-art performance only with these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2020/>Injecting Word Embeddings with Another Language’s Resource : An Application of Bilingual Embeddings</a></strong><br><a href=/people/p/prakhar-pandey/>Prakhar Pandey</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2020><div class="card-body p-3 small">Word embeddings learned from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> can be improved by injecting knowledge from external resources, while at the same time also specializing them for similarity or relatedness. These knowledge resources (like WordNet, Paraphrase Database) may not exist for all languages. In this work we introduce a method to inject <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> of a language with knowledge resource of another language by leveraging bilingual embeddings. First we improve word embeddings of <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> using resources of English and test them on variety of word similarity tasks. Then we demonstrate the utility of our method by creating improved embeddings for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu and Telugu languages</a> using Hindi WordNet, beating the previously established baseline for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2021/>Improving Black-box Speech Recognition using Semantic Parsing</a></strong><br><a href=/people/r/rodolfo-corona/>Rodolfo Corona</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2021><div class="card-body p-3 small">Speech is a natural channel for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a> in robotics and consumer applications. Natural language understanding pipelines that start with <a href=https://en.wikipedia.org/wiki/Manner_of_articulation>speech</a> can have trouble recovering from <a href=https://en.wikipedia.org/wiki/Manner_of_articulation>speech recognition errors</a>. Black-box automatic speech recognition (ASR) systems, built for general purpose use, are unable to take advantage of in-domain language models that could otherwise ameliorate these errors. In this work, we present a method for re-ranking black-box ASR hypotheses using an in-domain language model and <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> trained for a particular task. Our re-ranking method significantly improves both transcription accuracy and semantic understanding over a state-of-the-art ASR&#8217;s vanilla output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2022/>Revisiting the Design Issues of Local Models for Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/y/yuichiroh-matsubayashi/>Yuichiroh Matsubayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2022><div class="card-body p-3 small">The research trend in Japanese predicate-argument structure (PAS) analysis is shifting from pointwise prediction models with local features to global models designed to search for globally optimal solutions. However, the existing <a href=https://en.wikipedia.org/wiki/General_circulation_model>global models</a> tend to employ only relatively simple local features ; therefore, the overall performance gains are rather limited. The importance of designing a local model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, outperforming the state-of-the-art global models in F1 on a common benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2023/>Natural Language Informs the Interpretation of Iconic Gestures : A Computational Approach</a></strong><br><a href=/people/t/ting-han/>Ting Han</a>
|
<a href=/people/j/julian-hough/>Julian Hough</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2023><div class="card-body p-3 small">When giving descriptions, speakers often signify object shape or size with <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gestures</a>. Such so-called &#8216;iconic&#8217; gestures represent their meaning through their relevance to referents in the verbal content, rather than having a conventional form. The gesture form on its own is often ambiguous, and the aspect of the referent that it highlights is constrained by what the language makes salient. We show how the verbal content guides gesture interpretation through a computational model that frames the task as a multi-label classification task that maps multimodal utterances to semantic categories, using annotated human-human data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2027.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2027.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2027/>Can <a href=https://en.wikipedia.org/wiki/Discourse_relation>Discourse Relations</a> be Identified Incrementally?</a></strong><br><a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2027><div class="card-body p-3 small">Humans process language word by word and construct partial linguistic structures on the fly before the end of the sentence is perceived. Inspired by this <a href=https://en.wikipedia.org/wiki/Cognition>cognitive ability</a>, incremental algorithms for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a> have been proposed and demonstrated promising performance. For discourse relation (DR) parsing, however, it is not yet clear to what extent humans can recognize DRs incrementally, because the latent &#8216;nodes&#8217; of discourse structure can span clauses and sentences. To answer this question, this work investigates incrementality in discourse processing based on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> annotated with DR signals. We find that DRs are dominantly signaled at the boundary between the two constituent discourse units. The findings complement existing <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistic theories</a> on expectation in discourse processing and provide direction for incremental discourse parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-2028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-2028/>Speaker Role Contextual Modeling for <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a> and Dialogue Policy Learning</a></strong><br><a href=/people/t/ta-chung-chi/>Ta-Chung Chi</a>
|
<a href=/people/p/po-chun-chen/>Po-Chun Chen</a>
|
<a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2028><div class="card-body p-3 small">Language understanding (LU) and dialogue policy learning are two essential components in conversational systems. Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits. This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues. The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and dialogue policy learning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2029/>Diversifying Neural Conversation Model with Maximal Marginal Relevance</a></strong><br><a href=/people/y/yiping-song/>Yiping Song</a>
|
<a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2029><div class="card-body p-3 small">Neural conversation systems, typically using sequence-to-sequence (seq2seq) models, are showing promising progress recently. However, traditional seq2seq suffer from a severe weakness : during beam search decoding, they tend to rank universal replies at the top of the candidate list, resulting in the lack of diversity among candidate replies. Maximum Marginal Relevance (MMR) is a <a href=https://en.wikipedia.org/wiki/Ranking_(statistics)>ranking algorithm</a> that has been widely used for subset selection. In this paper, we propose the MMR-BS decoding method, which incorporates MMR into the beam search (BS) process of seq2seq. The MMR-BS method improves the diversity of generated replies without sacrificing their high relevance with the user-issued query. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the best performance among other comparison methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2030/>Dialog for Language to Code</a></strong><br><a href=/people/s/shobhit-chaurasia/>Shobhit Chaurasia</a>
|
<a href=/people/r/raymond-mooney/>Raymond J. Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2030><div class="card-body p-3 small">Generating computer code from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> has been a long-standing problem. Prior work in this <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>domain</a> has restricted itself to <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>generating code</a> in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in <a href=https://en.wikipedia.org/wiki/Automatic_programming>code generation</a>, we focus on synthesizing <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditional statements</a> in the form of IFTTT recipes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2031/>Using Analytic Scoring Rubrics in the Automatic Assessment of College-Level Summary Writing Tasks in L2<span class=acl-fixed-case>L</span>2</a></strong><br><a href=/people/t/tamara-sladoljev-agejev/>Tamara Sladoljev-Agejev</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2031><div class="card-body p-3 small">Assessing summaries is a demanding, yet useful task which provides valuable information on <a href=https://en.wikipedia.org/wiki/Linguistic_competence>language competence</a>, especially for <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learners</a>. We consider automated scoring of college-level summary writing task in English as a second language (EL2). We adopt the Reading-for-Understanding (RU) cognitive framework, extended with the Reading-to-Write (RW) element, and use analytic scoring with six rubrics covering content and writing quality. We show that <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression models</a> with reference-based and linguistic features considerably outperform the baselines across all the rubrics. Moreover, we find interesting correlations between summary features and analytic rubrics, revealing the links between the RU and RW constructs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2032/>A Statistical Framework for Product Description Generation</a></strong><br><a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/y/yutai-hou/>Yutai Hou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2032><div class="card-body p-3 small">We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute-description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute-specified generation) separately. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2033/>Automatic Text Summarization Using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> with Embedding Features</a></strong><br><a href=/people/g/gyoung-ho-lee/>Gyoung Ho Lee</a>
|
<a href=/people/k/kong-joo-lee/>Kong Joo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2033><div class="card-body p-3 small">An automatic text summarization system can automatically generate a short and brief summary that contains a main concept of an original document. In this work, we explore the advantages of simple embedding features in Reinforcement leaning approach to automatic text summarization tasks. In addition, we propose a novel <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning network</a> for estimating Q-values used in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement learning</a>. We evaluate our model by using ROUGE scores with DUC 2001, 2002, Wikipedia, ACL-ARC data. Evaluation results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is competitive with the previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2034.Datasets.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2034/>SSAS : <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Similarity</a> for Abstractive Summarization<span class=acl-fixed-case>SSAS</span>: Semantic Similarity for Abstractive Summarization</a></strong><br><a href=/people/r/raghuram-vadapalli/>Raghuram Vadapalli</a>
|
<a href=/people/l/litton-j-kurisinkel/>Litton J Kurisinkel</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2034><div class="card-body p-3 small">Ideally a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> evaluating an abstract system summary should represent the extent to which the system-generated summary approximates the semantic inference conceived by the reader using a human-written reference summary. Most of the previous approaches relied upon word or syntactic sub-sequence overlap to evaluate system-generated summaries. Such <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can not evaluate the summary at semantic inference level. Through this work we introduce the metric of Semantic Similarity for Abstractive Summarization (SSAS), which leverages natural language inference and paraphrasing techniques to frame a novel approach to evaluate system summaries at semantic inference level. SSAS is based upon a weighted composition of quantities representing the level of agreement, <a href=https://en.wikipedia.org/wiki/Contradiction>contradiction</a>, <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independence</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>, and optionally ROUGE score between a system-generated and a human-written summary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2035/>Taking into account Inter-sentence Similarity for Update Summarization</a></strong><br><a href=/people/m/maali-mnasri/>Maâli Mnasri</a>
|
<a href=/people/g/gael-de-chalendar/>Gaël de Chalendar</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2035><div class="card-body p-3 small">Following Gillick and Favre (2009), a lot of work about extractive summarization has modeled this task by associating two contrary constraints : one aims at maximizing the coverage of the summary with respect to its information content while the other represents its size limit. In this context, the notion of <a href=https://en.wikipedia.org/wiki/Redundancy_(linguistics)>redundancy</a> is only implicitly taken into account. In this article, we extend the framework defined by Gillick and Favre (2009) by examining how and to what extent integrating semantic sentence similarity into an update summarization system can improve its results. We show more precisely the impact of this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> through evaluations performed on DUC 2007 and TAC 2008 and 2009 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2037/>Dual Constrained Question Embeddings with Relational Knowledge Bases for Simple Question Answering</a></strong><br><a href=/people/k/kaustubh-kulkarni/>Kaustubh Kulkarni</a>
|
<a href=/people/r/riku-togashi/>Riku Togashi</a>
|
<a href=/people/h/hideyuki-maeda/>Hideyuki Maeda</a>
|
<a href=/people/s/sumio-fujita/>Sumio Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2037><div class="card-body p-3 small">Embedding based approaches are shown to be effective for solving simple Question Answering (QA) problems in recent works. The major drawback of current approaches is that they look only at the similarity (constraint) between a question and a head, relation pair. Due to the absence of tail (answer) in the questions, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often require paraphrase datasets to obtain adequate <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. In this paper, we propose a dual constraint model which exploits the embeddings obtained by Trans * family of algorithms to solve the simple QA problem without using any additional resources such as paraphrase datasets. The results obtained prove that the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learned using <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>dual constraints</a> are better than those with <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>single constraint models</a> having similar architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2038 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2038/>Efficiency-aware Answering of Compositional Questions using Answer Type Prediction</a></strong><br><a href=/people/d/david-ziegler/>David Ziegler</a>
|
<a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2038><div class="card-body p-3 small">This paper investigates the problem of answering compositional factoid questions over knowledge bases (KB) under efficiency constraints. The method, called TIPI, (i) decomposes compositional questions, (ii) predicts answer types for individual sub-questions, (iii) reasons over the compatibility of joint types, and finally, (iv) formulates compositional SPARQL queries respecting type constraints. TIPI&#8217;s answer type predictor is trained using distant supervision, and exploits lexical, syntactic and embedding-based features to compute context- and hierarchy-aware candidate answer types for an input question. Experiments on a recent <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> show that TIPI results in state-of-the-art performance under the real-world assumption that only a single SPARQL query can be executed over the <a href=https://en.wikipedia.org/wiki/Kilobyte>KB</a>, and substantial reduction in the number of queries in the more general case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2039/>High Recall Open IE for Relation Discovery<span class=acl-fixed-case>IE</span> for Relation Discovery</a></strong><br><a href=/people/h/hady-elsahar/>Hady Elsahar</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/f/frederique-laforest/>Frederique Laforest</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2039><div class="card-body p-3 small">Relation Discovery discovers predicates (relation types) from a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> relying on the co-occurrence of two named entities in the same sentence. This is a very narrowing constraint : it represents only a small fraction of all <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation mentions</a> in practice. In this paper we propose a high recall approach for <a href=https://en.wikipedia.org/wiki/Open_IE>Open IE</a>, which enables covering up to 16 times more sentences in a large corpus. Comparison against OpenIE systems shows that our proposed approach achieves 28 % improvement over the highest recall OpenIE system and 6 % improvement in precision than the same system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2040/>Using Context Events in Neural Network Models for Event Temporal Status Identification</a></strong><br><a href=/people/z/zeyu-dai/>Zeyu Dai</a>
|
<a href=/people/w/wenlin-yao/>Wenlin Yao</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2040><div class="card-body p-3 small">Focusing on the task of identifying event temporal status, we find that events directly or indirectly governing the target event in a dependency tree are most important contexts. Therefore, we extract dependency chains containing context events and use them as input in neural network models, which consistently outperform previous models using local context words as input. Visualization verifies that the dependency chain representation can effectively capture the context events which are closely related to the target event and play key roles in predicting event temporal status.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2041/>Identifying Protein-protein Interactions in Biomedical Literature using Recurrent Neural Networks with Long Short-Term Memory</a></strong><br><a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/n/nai-wen-chang/>Nai-Wen Chang</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2041><div class="card-body p-3 small">In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network model</a> for identifying protein-protein interactions in <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. Experiments on two largest public benchmark datasets, AIMed and BioInfer, demonstrate that our approach significantly surpasses state-of-the-art methods with relative improvements of 10 % and 18 %, respectively. Cross-corpus evaluation also demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> remains robust despite using different training data. These results suggest that RNN can effectively capture semantic relationships among proteins as well as generalizes over different corpora, without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2042/>Identifying Empathetic Messages in <a href=https://en.wikipedia.org/wiki/Online_health_communities>Online Health Communities</a></a></strong><br><a href=/people/h/hamed-khanpour/>Hamed Khanpour</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a>
|
<a href=/people/p/prakhar-biyani/>Prakhar Biyani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2042><div class="card-body p-3 small">Empathy captures one&#8217;s ability to correlate with and understand others&#8217; emotional states and experiences. Messages with empathetic content are considered as one of the main advantages for joining online health communities due to their potential to improve people&#8217;s moods. Unfortunately, to this date, no computational studies exist that automatically identify empathetic messages in online health communities. We propose a combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, and show that the proposed model outperforms each individual model (CNN and LSTM) as well as several baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2043/>Fake News Detection Through Multi-Perspective Speaker Profiles</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/m/minglei-li/>Minglei Li</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2043><div class="card-body p-3 small">Automatic fake news detection is an important, yet very challenging topic. Traditional <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> using lexical features have only very limited success. This paper proposes a novel method to incorporate <a href=https://en.wikipedia.org/wiki/Public_speaking>speaker profiles</a> into an attention based LSTM model for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a>. Speaker profiles contribute to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in two ways. One is to include <a href=https://en.wikipedia.org/wiki/Them>them</a> in the attention model. The other includes <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(M)>them</a> as additional input data. By adding speaker profiles such as <a href=https://en.wikipedia.org/wiki/Political_party>party affiliation</a>, speaker title, location and <a href=https://en.wikipedia.org/wiki/Credit_history>credit history</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a> by 14.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using a benchmark fake news detection dataset. This proves that speaker profiles provide valuable information to validate the credibility of news articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2044/>Improving Neural Text Normalization with Data Augmentation at Character- and Morphological Levels</a></strong><br><a href=/people/i/itsumi-saito/>Itsumi Saito</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/k/kugatsu-sadamitsu/>Kugatsu Sadamitsu</a>
|
<a href=/people/s/satoshi-kobashikawa/>Satoshi Kobashikawa</a>
|
<a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2044><div class="card-body p-3 small">In this study, we investigated the effectiveness of augmented data for encoder-decoder-based neural normalization models. Attention based encoder-decoder models are greatly effective in generating many natural languages. % such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or machine summarization. In general, we have to prepare for a large amount of training data to train an encoder-decoder model. Unlike <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, there are few training data for text-normalization tasks. In this paper, we propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for generating augmented data. The experimental results with Japanese dialect normalization indicate that our methods are effective for an encoder-decoder model and achieve higher BLEU score than that of <a href=https://en.wikipedia.org/wiki/Baseline_(typography)>baselines</a>. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2045/>Using <a href=https://en.wikipedia.org/wiki/Social_network>Social Networks</a> to Improve Language Variety Identification with Neural Networks</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/s/shotaro-misawa/>Shotaro Misawa</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2045><div class="card-body p-3 small">We propose a hierarchical neural network model for language variety identification that integrates information from a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. Recently, language variety identification has enjoyed heightened popularity as an advanced task of <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> uses additional texts from a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a> to improve language variety identification from two perspectives. First, they are used to introduce the effects of <a href=https://en.wikipedia.org/wiki/Homophily>homophily</a>. Secondly, they are used as expanded training data for shared layers of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. By introducing information from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improved its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 1.67-5.56. Compared to state-of-the-art baselines, these improved performances are better in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and comparable in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Furthermore, we analyzed the cases of <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> when the model showed weak performances, and found that the effect of <a href=https://en.wikipedia.org/wiki/Homophily>homophily</a> is likely to be weak due to sparsity and noises compared to languages with the strong performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2046/>Boosting Neural Machine Translation</a></strong><br><a href=/people/d/dakun-zhang/>Dakun Zhang</a>
|
<a href=/people/j/jungi-kim/>Jungi Kim</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2046><div class="card-body p-3 small">Training efficiency is one of the main problems for Neural Machine Translation (NMT). Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high <a href=https://en.wikipedia.org/wiki/Computational_cost>computation cost</a>, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on data boosting and <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrap</a> with no modifications to the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. It imitates the learning process of humans, which typically spend more time when learning difficult concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to 1.63 BLEU while saving 20 % of training time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2048/>Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT<span class=acl-fixed-case>SMT</span></a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/m/maulik-shah/>Maulik Shah</a>
|
<a href=/people/p/pradyot-prakash/>Pradyot Prakash</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2048><div class="card-body p-3 small">We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2049/>Key-value Attention Mechanism for Neural Machine Translation</a></strong><br><a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2049><div class="card-body p-3 small">In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two types of <a href=https://en.wikipedia.org/wiki/Computer_data_storage>memory</a> known as the key and the value. The <a href=https://en.wikipedia.org/wiki/Key_(cryptography)>key</a> is used for calculating the <a href=https://en.wikipedia.org/wiki/Attention>attention distribution</a>, and the <a href=https://en.wikipedia.org/wiki/Value_(computer_science)>value</a> is used for encoding the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context representation</a>. Experiments on three different tasks indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms an NMT model with a conventional <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Furthermore, we perform experiments with a conventional NMT framework, in which a part of the initial value of a weight matrix is set to zero so that the matrix is as the same initial-state as the key-value attention mechanism. As a result, we obtain comparable results with the key-value attention mechanism without changing the <a href=https://en.wikipedia.org/wiki/Neural_circuit>network structure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2054/>Building Large Chinese Corpus for <a href=https://en.wikipedia.org/wiki/Spoken_dialogue>Spoken Dialogue</a> Research in Specific Domains<span class=acl-fixed-case>C</span>hinese Corpus for Spoken Dialogue Research in Specific Domains</a></strong><br><a href=/people/c/changliang-li/>Changliang Li</a>
|
<a href=/people/x/xiuying-wang/>Xiuying Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2054><div class="card-body p-3 small">Corpus is a valuable resource for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and data-driven natural language processing systems, especially for spoken dialogue research in specific domains. However, there is little non-English corpora, particular for ones in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Spoken by the nation with the largest population in the world, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> become increasingly prevalent and popular among millions of people worldwide. In this paper, we build a large-scale and high-quality Chinese corpus, called CSDC (Chinese Spoken Dialogue Corpus). It contains five domains and more than 140 thousand dialogues in all. Each sentence in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is annotated with slot information additionally compared to other <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>. To our best knowledge, this is the largest Chinese spoken dialogue corpus, as well as the first one with slot information. With this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we proposed a method and did a well-designed experiment. The indicative result is reported at last.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2055/>Identifying Speakers and Listeners of Quoted Speech in Literary Works</a></strong><br><a href=/people/c/chak-yan-yeung/>Chak Yan Yeung</a>
|
<a href=/people/j/john-s-y-lee/>John Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2055><div class="card-body p-3 small">We present the first study that evaluates both speaker and listener identification for direct speech in literary texts. Our approach consists of two steps : identification of speakers and listeners near the quotes, and dialogue chain segmentation. Evaluation results show that this approach outperforms a rule-based approach that is state-of-the-art on a corpus of literary texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2059/>CVBed : Structuring CVs usingWord Embeddings<span class=acl-fixed-case>CVB</span>ed: Structuring <span class=acl-fixed-case>CV</span>s using<span class=acl-fixed-case>W</span>ord Embeddings</a></strong><br><a href=/people/s/shweta-garg/>Shweta Garg</a>
|
<a href=/people/s/sudhanshu-s-singh/>Sudhanshu S Singh</a>
|
<a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2059><div class="card-body p-3 small">Automatic analysis of curriculum vitae (CVs) of applicants is of tremendous importance in recruitment scenarios. The semi-structuredness of <a href=https://en.wikipedia.org/wiki/Curriculum_vitae>CVs</a>, however, makes CV processing a challenging task. We propose a solution towards transforming CVs to follow a unified structure, thereby, paving ways for smoother CV analysis. The problem of <a href=https://en.wikipedia.org/wiki/Restructuring>restructuring</a> is posed as a section relabeling problem, where each section of a given <a href=https://en.wikipedia.org/wiki/Document_type_definition>CV</a> gets reassigned to a predefined label. Our relabeling method relies on semantic relatedness computed between section header, content and labels, based on phrase-embeddings learned from a large pool of <a href=https://en.wikipedia.org/wiki/Curriculum_vitae>CVs</a>. We follow different <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to measure <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>. Our best <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristic</a> achieves an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 93.17 % on a test dataset with <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard labels</a> obtained using manual annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2060.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2060.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2060/>Leveraging Diverse Lexical Chains to Construct Essays for Chinese College Entrance Examination<span class=acl-fixed-case>C</span>hinese College Entrance Examination</a></strong><br><a href=/people/l/liunian-li/>Liunian Li</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jin-ge-yao/>Jin-ge Yao</a>
|
<a href=/people/s/siming-yan/>Siming Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2060><div class="card-body p-3 small">In this work we study the challenging task of automatically constructing essays for Chinese college entrance examination where the topic is specified in advance. We explore a sentence extraction framework based on diversified lexical chains to capture <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> and richness. Experimental analysis shows the effectiveness of our approach and reveals the importance of information richness in essay writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2061/>Draw and Tell : Multimodal Descriptions Outperform Verbal- or Sketch-Only Descriptions in an Image Retrieval Task</a></strong><br><a href=/people/t/ting-han/>Ting Han</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2061><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Language>language</a> conveys meaning largely symbolically, actual communication acts typically contain iconic elements as well : People gesture while they speak, or may even draw sketches while explaining something. Image retrieval prima facie seems like a task that could profit from combined symbolic and iconic reference, but it is typically set up to work either from <a href=https://en.wikipedia.org/wiki/Language>language</a> only, or via (iconic) sketches with no verbal contribution. Using a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model of grounded language semantics</a> and a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model of sketch-to-image mapping</a>, we show that adding even very reduced iconic information to a verbal image description improves <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. Verbal descriptions paired with fully detailed sketches still perform better than these <a href=https://en.wikipedia.org/wiki/Sketch_comedy>sketches</a> alone. We see these results as supporting the assumption that <a href=https://en.wikipedia.org/wiki/Natural_user_interface>natural user interfaces</a> should respond to <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal input</a>, where possible, rather than just <a href=https://en.wikipedia.org/wiki/Natural_language_processing>language</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2062/>Grammatical Error Correction with Neural Reinforcement Learning</a></strong><br><a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2062><div class="card-body p-3 small">We propose a neural encoder-decoder model with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (NRL)</a> for grammatical error correction (GEC). Unlike conventional <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> directly optimizes towards an objective that considers a sentence-level, task-specific evaluation metric, avoiding the exposure bias issue in <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>MLE</a>. We demonstrate that NRL outperforms <a href=https://en.wikipedia.org/wiki/Machine_learning>MLE</a> both in human and automated evaluation metrics, achieving the state-of-the-art on a fluency-oriented GEC corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2063/>Coreference Resolution on Math Problem Text in Japanese<span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/t/takuya-matsuzaki/>Takuya Matsuzaki</a>
|
<a href=/people/s/satoshi-sato/>Satoshi Sato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2063><div class="card-body p-3 small">This paper describes a coreference resolution system for math problem text. Case frame dictionaries and a math taxonomy are utilized for supplying <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> deals with various <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric phenomena</a> beyond well-studied entity coreferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2064/>Utilizing Visual Forms of Japanese Characters for Neural Review Classification<span class=acl-fixed-case>J</span>apanese Characters for Neural Review Classification</a></strong><br><a href=/people/y/yota-toyama/>Yota Toyama</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2064><div class="card-body p-3 small">We propose a novel method that exploits visual information of ideograms and logograms in analyzing Japanese review documents. Our method first converts font images of Japanese characters into <a href=https://en.wikipedia.org/wiki/Character_encoding>character embeddings</a> using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. It then constructs document embeddings from the character embeddings based on Hierarchical Attention Networks, which represent the documents based on attention mechanisms from a character level to a sentence level. The document embeddings are finally used to predict the labels of documents. Our method provides a way to exploit visual features of characters in languages with <a href=https://en.wikipedia.org/wiki/Ideogram>ideograms</a> and <a href=https://en.wikipedia.org/wiki/Logogram>logograms</a>. In the experiments, our method achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparable to a character embedding-based model while our method has much fewer parameters since it does not need to keep embeddings of thousands of characters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2065/>A Multi-task Learning Approach to Adapting Bilingual Word Embeddings for Cross-lingual Named Entity Recognition</a></strong><br><a href=/people/d/dingquan-wang/>Dingquan Wang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2065><div class="card-body p-3 small">We show how to adapt bilingual word embeddings (BWE&#8217;s) to bootstrap a cross-lingual name-entity recognition (NER) system in a language with no labeled data. We assume a setting where we are given a comparable corpus with NER labels for the source language only ; our goal is to build a <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a> for the target language. The proposed multi-task model jointly trains bilingual word embeddings while optimizing a NER objective. This creates <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> that are both shared between languages and fine-tuned for the NER task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2068/>CWIG3G2-Complex Word Identification Task across Three Text Genres and Two User Groups<span class=acl-fixed-case>CWIG</span>3<span class=acl-fixed-case>G</span>2 - Complex Word Identification Task across Three Text Genres and Two User Groups</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2068><div class="card-body p-3 small">Complex word identification (CWI) is an important task in <a href=https://en.wikipedia.org/wiki/Accessibility>text accessibility</a>. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI dataset (CWIG3G2) covering three text genres News, WikiNews, and Wikipedia) annotated by both native and non-native English speakers. Unlike previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, we cover <a href=https://en.wikipedia.org/wiki/Phrase>single words</a>, as well as <a href=https://en.wikipedia.org/wiki/Phrase>complex phrases</a>, and present them for judgment in a paragraph context. We present the first study on cross-genre and cross-group CWI, showing measurable influences in native language and genre types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2069.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-2069/>Generating Stylistically Consistent Dialog Responses with Transfer Learning</a></strong><br><a href=/people/r/reina-akama/>Reina Akama</a>
|
<a href=/people/k/kazuaki-inada/>Kazuaki Inada</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2069><div class="card-body p-3 small">We propose a novel, data-driven, and stylistically consistent dialog response generation system. To create a user-friendly system, it is crucial to make generated responses not only appropriate but also stylistically consistent. For leaning both the properties effectively, our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> has two training stages inspired by <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. First, we train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate appropriate responses, and then we ensure that the responses have a specific style. Experimental results demonstrate that the proposed method produces stylistically consistent responses while maintaining the appropriateness of the responses learned in a general domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2071/>Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging</a></strong><br><a href=/people/y/yllias-chali/>Yllias Chali</a>
|
<a href=/people/m/moin-tanvee/>Moin Tanvee</a>
|
<a href=/people/m/mir-tafseer-nayeem/>Mir Tafseer Nayeem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2071><div class="card-body p-3 small">We propose a submodular function-based summarization system which integrates three important measures namely importance, coverage, and non-redundancy to detect the important sentences for the summary. We design monotone and submodular functions which allow us to apply an efficient and scalable <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a> to obtain informative and well-covered summaries. In addition, we integrate two abstraction-based methods namely sentence compression and merging for generating an abstractive sentence set. We design our summarization models for both generic and query-focused summarization. Experimental results on DUC-2004 and DUC-2007 datasets show that our generic and query-focused summarizers have outperformed the state-of-the-art summarization systems in terms of ROUGE-1 and ROUGE-2 recall and <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2072/>Domain Adaptation for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> with Domain Adversarial Neural Network</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2072><div class="card-body p-3 small">Relations are expressed in many domains such as <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>, <a href=https://en.wikipedia.org/wiki/Blog>weblogs</a> and phone conversations. Trained on a source domain, a relation extractor&#8217;s performance degrades when applied to target domains other than the source. A common yet labor-intensive method for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> is to construct a target-domain-specific labeled dataset for adapting the extractor. In response, we present an unsupervised domain adaptation method which only requires labels from the source domain. Our method is a joint model consisting of a CNN-based relation classifier and a domain-adversarial classifier. The two <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> are optimized jointly to learn a domain-independent representation for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on the target domain. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on all three test domains of ACE 2005.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2073 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2073/>Lexical Simplification with the Deep Structured Similarity Model</a></strong><br><a href=/people/l/lis-pereira/>Lis Pereira</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/john-s-y-lee/>John Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2073><div class="card-body p-3 small">We explore the application of a Deep Structured Similarity Model (DSSM) to <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>. Our results show that the DSSM can effectively capture fine-grained features to perform <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> when ranking substitution candidates, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2074/>Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2074><div class="card-body p-3 small">This paper explores the idea of robot editors, automated proofreaders that enable journalists to improve the quality of their articles. We propose a novel neural model of multi-task learning that both generates proofread sentences and predicts the editing operations required to rewrite the source sentences and create the proofread ones. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained using logs of the revisions made professional editors revising draft newspaper articles written by journalists. Experiments demonstrate the effectiveness of our multi-task learning approach and the potential value of using revision logs for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2076 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-2076.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-2076" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-2076/>Deriving Consensus for Multi-Parallel Corpora : an English Bible Study<span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>B</span>ible Study</a></strong><br><a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2076><div class="card-body p-3 small">What can you do with multiple noisy versions of the same text? We present a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> which generates a single <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus</a> between multi-parallel corpora. By maximizing a function of linguistic features between word pairs, we jointly learn a single corpus-wide multiway alignment : a <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus</a> between 27 versions of the <a href=https://en.wikipedia.org/wiki/Bible_translations_into_English>English Bible</a>. We additionally produce English paraphrases, word-level distributions of tags, and consensus dependency parses. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is language independent and applicable to any multi-parallel corpora. Given the Bible&#8217;s unique role as alignable bitext for over 800 of the world&#8217;s languages, this consensus alignment and resulting resources offer value for multilingual annotation projection, and also shed potential insights into the Bible itself.</div></div></div><hr><div id=i17-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/I17-3/>Proceedings of the IJCNLP 2017, System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3000/>Proceedings of the <span class=acl-fixed-case>IJCNLP</span> 2017, System Demonstrations</a></strong><br><a href=/people/s/seong-bae-park/>Seong-Bae Park</a>
|
<a href=/people/t/thepchai-supnithi/>Thepchai Supnithi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3003/>WiseReporter : A Korean Report Generation System<span class=acl-fixed-case>W</span>ise<span class=acl-fixed-case>R</span>eporter: A <span class=acl-fixed-case>K</span>orean Report Generation System</a></strong><br><a href=/people/y/yunseok-noh/>Yunseok Noh</a>
|
<a href=/people/s/su-jeong-choi/>Su Jeong Choi</a>
|
<a href=/people/s/seong-bae-park/>Seong-Bae Park</a>
|
<a href=/people/s/se-young-park/>Se-Young Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3003><div class="card-body p-3 small">We demonstrate a report generation system called WiseReporter. The WiseReporter generates a text report of a specific topic which is usually given as a keyword by verbalizing knowledge base facts involving the topic. This demonstration does not demonstate only the report itself, but also the processes how the sentences for the report are generated. We are planning to enhance WiseReporter in the future by adding <a href=https://en.wikipedia.org/wiki/Data_analysis>data analysis</a> based on deep learning architecture and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3004/>Encyclolink : A Cross-Encyclopedia, Cross-language Article-Linking System and Web-based Search Interface<span class=acl-fixed-case>E</span>ncyclolink: A Cross-Encyclopedia,Cross-language Article-Linking System and Web-based Search Interface</a></strong><br><a href=/people/y/yu-chun-wang/>Yu-Chun Wang</a>
|
<a href=/people/k/ka-ming-wong/>Ka Ming Wong</a>
|
<a href=/people/c/chun-kai-wu/>Chun-Kai Wu</a>
|
<a href=/people/c/chao-lin-pan/>Chao-Lin Pan</a>
|
<a href=/people/r/richard-tzong-han-tsai/>Richard Tzong-Han Tsai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3004><div class="card-body p-3 small">Cross-language article linking (CLAL) is the task of finding corresponding article pairs across encyclopedias of different languages. In this paper, we present Encyclolink, a web-based CLAL search interface designed to help users find equivalent encyclopedia articles in <a href=https://en.wikipedia.org/wiki/Baidu_Baike>Baidu Baike</a> for a given English Wikipedia article title query. Encyclolink is powered by our cross-encyclopedia entity embedding CLAL system (0.8 MRR). The browser-based Interface provides users with a clear and easily readable preview of the contents of retrieved articles for comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3005/>A Telecom-Domain Online Customer Service Assistant Based on <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> with Word Embedding and Intent Classification</a></strong><br><a href=/people/j/jui-yang-wang/>Jui-Yang Wang</a>
|
<a href=/people/m/min-feng-kuo/>Min-Feng Kuo</a>
|
<a href=/people/j/jen-chieh-han/>Jen-Chieh Han</a>
|
<a href=/people/c/chao-chuang-shih/>Chao-Chuang Shih</a>
|
<a href=/people/c/chun-hsun-chen/>Chun-Hsun Chen</a>
|
<a href=/people/p/po-ching-lee/>Po-Ching Lee</a>
|
<a href=/people/r/richard-tzong-han-tsai/>Richard Tzong-Han Tsai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3005><div class="card-body p-3 small">In the paper, we propose an information retrieval based (IR-based) Question Answering (QA) system to assist online customer service staffs respond users in the telecom domain. When user asks a question, the <a href=https://en.wikipedia.org/wiki/System>system</a> retrieves a set of relevant answers and ranks them. Moreover, our system uses a novel reranker to enhance the ranking result of <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. It employs the word2vec model to represent the sentences as vectors. It also uses a sub-category feature, predicted by the <a href=https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm>k-nearest neighbor algorithm</a>. Finally, the <a href=https://en.wikipedia.org/wiki/System>system</a> returns the top five candidate answers, making online staffs find answers much more efficiently.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3006/>TOTEMSS : Topic-based, Temporal Sentiment Summarisation for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a><span class=acl-fixed-case>TOTEMSS</span>: Topic-based, Temporal Sentiment Summarisation for <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/a/adam-tsakalidis/>Adam Tsakalidis</a>
|
<a href=/people/s/spiros-georgakopoulos-kolaitis/>Spiros Georgakopoulos Kolaitis</a>
|
<a href=/people/s/symeon-papadopoulos/>Symeon Papadopoulos</a>
|
<a href=/people/l/lazaros-apostolidis/>Lazaros Apostolidis</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/r/rob-procter/>Rob Procter</a>
|
<a href=/people/y/yiannis-kompatsiaris/>Yiannis Kompatsiaris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3006><div class="card-body p-3 small">We present a system for time sensitive, topic based summarisation of the sentiment around target entities and topics in collections of tweets. We describe the main elements of the <a href=https://en.wikipedia.org/wiki/System>system</a> and illustrate its functionality with two examples of sentiment analysis of topics related to the 2017 UK general election.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3007/>MUSST : A Multilingual Syntactic Simplification Tool<span class=acl-fixed-case>MUSST</span>: A Multilingual Syntactic Simplification Tool</a></strong><br><a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/a/alessio-palmero-aprosio/>Alessio Palmero Aprosio</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/t/tamara-martin-wanton/>Tamara Martín Wanton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3007><div class="card-body p-3 small">We describe MUSST, a multilingual syntactic simplification tool. The tool supports <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplifications</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and can be easily extended to other languages. Our implementation includes a set of general-purpose simplification rules, as well as a sentence selection module (to select sentences to be simplified) and a confidence model (to select only promising simplifications). The tool was implemented in the context of the European project SIMPATICO on text simplification for Public Administration (PA) texts. Our evaluation on sentences in the PA domain shows that we obtain correct simplifications for 76 % of the simplified cases in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 71 % of the cases in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. For <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, the results are lower (38 %) but the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> is still under development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3008/>XMU Neural Machine Translation Online Service<span class=acl-fixed-case>XMU</span> Neural Machine Translation Online Service</a></strong><br><a href=/people/b/boli-wang/>Boli Wang</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/j/jinming-hu/>Jinming Hu</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3008><div class="card-body p-3 small">We demonstrate a neural machine translation web service. Our NMT service provides web-based translation interfaces for a variety of language pairs. We describe the architecture of NMT runtime pipeline and the training details of NMT models. We also show several applications of our online translation interfaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3009/>Semantics-Enhanced Task-Oriented Dialogue Translation : A Case Study on Hotel Booking</a></strong><br><a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/j/jinhua-du/>Jinhua Du</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3009><div class="card-body p-3 small">We showcase TODAY, a semantics-enhanced task-oriented dialogue translation system, whose novelties are : (i) task-oriented named entity (NE) definition and a hybrid strategy for NE recognition and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> ; and (ii) a novel grounded semantic method for dialogue understanding and task-order management. TODAY is a case-study demo which can efficiently and accurately assist customers and agents in different languages to reach an agreement in a dialogue for the hotel booking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3010/>NNVLP : A Neural Network-Based Vietnamese Language Processing Toolkit<span class=acl-fixed-case>NNVLP</span>: A Neural Network-Based <span class=acl-fixed-case>V</span>ietnamese Language Processing Toolkit</a></strong><br><a href=/people/t/thai-hoang-pham/>Thai-Hoang Pham</a>
|
<a href=/people/x/xuan-khoai-pham/>Xuan-Khoai Pham</a>
|
<a href=/people/t/tuan-anh-nguyen/>Tuan-Anh Nguyen</a>
|
<a href=/people/p/phuong-le-hong/>Phuong Le-Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3010><div class="card-body p-3 small">This paper demonstrates neural network-based toolkit namely NNVLP for essential Vietnamese language processing tasks including part-of-speech (POS) tagging, chunking, Named Entity Recognition (NER). Our toolkit is a combination of bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which outperforms previously published toolkits on these three tasks. We provide both of API and web demo for this <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-3011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-3011/>ClassifierGuesser : A Context-based Classifier Prediction System for Chinese Language Learners<span class=acl-fixed-case>C</span>lassifier<span class=acl-fixed-case>G</span>uesser: A Context-based Classifier Prediction System for <span class=acl-fixed-case>C</span>hinese Language Learners</a></strong><br><a href=/people/n/nicole-peinelt/>Nicole Peinelt</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/s/shu-kai-hsieh/>Shu-Kai Hsieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3011><div class="card-body p-3 small">Classifiers are <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> that are used to express quantities in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and are especially difficult for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a>. In contrast to previous studies, we argue that the choice of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> is highly contextual and train context-aware machine learning models based on a novel publicly available dataset, outperforming previous baselines. We further present use cases for our <a href=https://en.wikipedia.org/wiki/Database>database</a> and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> in an interactive demo system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3012/>Automatic Difficulty Assessment for Chinese Texts<span class=acl-fixed-case>C</span>hinese Texts</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/m/meichun-liu/>Meichun Liu</a>
|
<a href=/people/c/chun-yin-lam/>Chun Yin Lam</a>
|
<a href=/people/t/tak-on-lau/>Tak On Lau</a>
|
<a href=/people/b/bing-li/>Bing Li</a>
|
<a href=/people/k/keying-li/>Keying Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3012><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Web_application>web-based interface</a> that automatically assesses reading difficulty of <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese texts</a>. The system performs <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and dependency parsing on the input text, and then determines the difficulty levels of the vocabulary items and grammatical constructions in the text. Furthermore, the system highlights the words and phrases that must be simplified or re-written in order to conform to the user-specified target difficulty level. Evaluation results show that the <a href=https://en.wikipedia.org/wiki/System>system</a> accurately identifies the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary level</a> of 89.9 % of the words, and detects <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> points at 0.79 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and 0.83 <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3013/>Verb Replacer : An English Verb Error Correction System<span class=acl-fixed-case>E</span>nglish Verb Error Correction System</a></strong><br><a href=/people/y/yu-hsuan-wu/>Yu-Hsuan Wu</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3013><div class="card-body p-3 small">According to the analysis of Cambridge Learner Corpus, using a wrong verb is the most common type of <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a>. This paper describes Verb Replacer, a system for detecting and correcting potential verb errors in a given sentence. In our approach, alternative verbs are considered to replace the verb based on an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error-annotated corpus</a> and verb-object collocations. The method involves applying <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> on channel models, parsing the sentence, identifying the verbs, retrieving a small set of alternative verbs, and evaluating each alternative. Our method combines and improves channel and language models, resulting in high <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of detecting and correcting verb misuse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3014/>Learning Synchronous Grammar Patterns for Assisted Writing for Second Language Learners</a></strong><br><a href=/people/c/chi-en-wu/>Chi-En Wu</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/j/jim-chang/>Jim Chang</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3014><div class="card-body p-3 small">In this paper, we present a method for extracting Synchronous Grammar Patterns (SGPs) from a given parallel corpus in order to assisted second language learners in writing. A grammar pattern consists of a head word (verb, noun, or adjective) and its syntactic environment. A synchronous grammar pattern describes a <a href=https://en.wikipedia.org/wiki/Grammar>grammar pattern</a> in the target language (e.g., <a href=https://en.wikipedia.org/wiki/English_language>English</a>) and its counterpart in an other language (e.g., <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>), serving the purpose of native language support. Our method involves identifying the grammar patterns in the target language, aligning these <a href=https://en.wikipedia.org/wiki/Pattern_recognition>patterns</a> with the target language patterns, and finally filtering valid SGPs. The extracted SGPs with examples are then used to develop a prototype writing assistant system, called WriteAhead / bilingual. Evaluation on a set of randomly selected SGPs shows that our <a href=https://en.wikipedia.org/wiki/System>system</a> provides satisfactory writing suggestions for English as a Second Language (ESL) learners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3015/>Guess What : A <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering Game</a> via On-demand Knowledge Validation</a></strong><br><a href=/people/y/yu-sheng-li/>Yu-Sheng Li</a>
|
<a href=/people/c/chien-hui-tseng/>Chien-Hui Tseng</a>
|
<a href=/people/c/chian-yun-huang/>Chian-Yun Huang</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3015><div class="card-body p-3 small">In this paper, we propose an idea of ondemand knowledge validation and fulfill the idea through an interactive Question-Answering (QA) game system, which is named Guess What. An object (e.g. dog) is first randomly chosen by the <a href=https://en.wikipedia.org/wiki/System>system</a>, and then a user can repeatedly ask the system questions in natural language to guess what the object is. The <a href=https://en.wikipedia.org/wiki/System>system</a> would respond with yes / no along with a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence score</a>. Some useful hints can also be given if needed. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> provides a pioneering example of on-demand knowledge validation in dialog environment to address such needs in <a href=https://en.wikipedia.org/wiki/Intelligent_agent>AI agents / chatbots</a>. Moreover, the released log data that the system gathered can be used to identify the most critical concepts / attributes of an existing knowledge base, which reflects human&#8217;s cognition about the world.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3016/>STCP : Simplified-Traditional Chinese Conversion and Proofreading<span class=acl-fixed-case>STCP</span>: Simplified-Traditional <span class=acl-fixed-case>C</span>hinese Conversion and Proofreading</a></strong><br><a href=/people/j/jiarui-xu/>Jiarui Xu</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/c/chen-tse-tsai/>Chen-Tse Tsai</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3016><div class="card-body p-3 small">This paper aims to provide an effective tool for conversion between <a href=https://en.wikipedia.org/wiki/Simplified_Chinese_characters>Simplified Chinese</a> and Traditional Chinese. We present STCP, a customizable system comprising statistical conversion model, and proofreading web interface. Experiments show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves comparable character-level conversion performance with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-art systems</a>. In addition, our proofreading interface can effectively support <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnostics</a> and data annotation. STCP is available at<url>http://lagos.lti.cs.cmu.edu:8002/</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-3017/>Deep Neural Network based system for solving Arithmetic Word problems</a></strong><br><a href=/people/p/purvanshi-mehta/>Purvanshi Mehta</a>
|
<a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/v/vinayak-athavale/>Vinayak Athavale</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-3017><div class="card-body p-3 small">This paper presents DILTON a <a href=https://en.wikipedia.org/wiki/System>system</a> which solves simple arithmetic word problems. DILTON uses a Deep Neural based model to solve math word problems. DILTON divides the question into two parts-worldstate and query. The worldstate and the query are processed separately in two different <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>networks</a> and finally, the <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>networks</a> are merged to predict the final operation. We report the first deep learning approach for the prediction of operation between two numbers. DILTON learns to predict operations with 88.81 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of primary school questions</a>.</div></div></div><hr><div id=i17-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/I17-4/>Proceedings of the IJCNLP 2017, Shared Tasks</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4000/>Proceedings of the <span class=acl-fixed-case>IJCNLP</span> 2017, Shared Tasks</a></strong><br><a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4001/>IJCNLP-2017 Task 1 : Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>IJCNLP</span>-2017 Task 1: <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/g/gaoqi-rao/>Gaoqi Rao</a>
|
<a href=/people/b/baolin-zhang/>Baolin Zhang</a>
|
<a href=/people/e/endong-xun/>Endong Xun</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4001><div class="card-body p-3 small">This paper presents the IJCNLP 2017 shared task for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese grammatical error diagnosis (CGED)</a> which seeks to identify grammatical error types and their range of occurrence within sentences written by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as foreign language. We describe the task definition, data preparation, <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a>, and evaluation results. Of the 13 teams registered for this shared task, 5 teams developed the <a href=https://en.wikipedia.org/wiki/System>system</a> and submitted a total of 13 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for <a href=https://en.wikipedia.org/wiki/Educational_technology>educational applications</a>, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4002/>IJCNLP-2017 Task 2 : Dimensional Sentiment Analysis for Chinese Phrases<span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Dimensional Sentiment Analysis for <span class=acl-fixed-case>C</span>hinese Phrases</a></strong><br><a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4002><div class="card-body p-3 small">This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and <a href=https://en.wikipedia.org/wiki/Arousal>arousal</a> represents the degree of excitement and calm. Of the 19 teams registered for this shared task for two-dimensional sentiment analysis, 13 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing. All data sets with gold standards and scoring script are made publicly available to researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4003/>IJCNLP-2017 Task 3 : Review Opinion Diversification (RevOpiD-2017)<span class=acl-fixed-case>IJCNLP</span>-2017 Task 3: Review Opinion Diversification (<span class=acl-fixed-case>R</span>ev<span class=acl-fixed-case>O</span>pi<span class=acl-fixed-case>D</span>-2017)</a></strong><br><a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a>
|
<a href=/people/a/avijit-thawani/>Avijit Thawani</a>
|
<a href=/people/m/mayank-panchal/>Mayank Panchal</a>
|
<a href=/people/a/anubhav-gupta/>Anubhav Gupta</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4003><div class="card-body p-3 small">Unlike Entity Disambiguation in <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search results</a>, Opinion Disambiguation is a relatively unexplored topic. RevOpiD shared task at IJCNLP-2107 aimed to attract attention towards this research problem. In this paper, we summarize the first run of this task and introduce a new dataset that we have annotated for the purpose of evaluating <a href=https://en.wikipedia.org/wiki/Opinion_mining>Opinion Mining</a>, Summarization and Disambiguation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4004/>IJCNLP-2017 Task 4 : Customer Feedback Analysis<span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Customer Feedback Analysis</a></strong><br><a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a>
|
<a href=/people/y/yasufumi-moriya/>Yasufumi Moriya</a>
|
<a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/d/declan-groves/>Declan Groves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4004><div class="card-body p-3 small">This document introduces the IJCNLP 2017 Shared Task on Customer Feedback Analysis. In this shared task we have prepared corpora of customer feedback in four languages, i.e. English, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. They were annotated in a common meanings categorization, which was improved from an ADAPT-Microsoft pivot study on customer feedback. Twenty teams participated in the shared task and twelve of them have submitted prediction results. The results show that performance of prediction meanings of <a href=https://en.wikipedia.org/wiki/Customer_service>customer feedback</a> is reasonable well in four languages. Nine system description papers are archived in the shared tasks proceeding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4005/>IJCNLP-2017 Task 5 : Multi-choice Question Answering in Examinations<span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: Multi-choice Question Answering in Examinations</a></strong><br><a href=/people/s/shangmin-guo/>Shangmin Guo</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/z/zhuoyu-wei/>Zhuoyu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4005><div class="card-body p-3 small">The IJCNLP-2017 Multi-choice Question Answering(MCQA) task aims at exploring the performance of current Question Answering(QA) techniques via the realworld complex questions collected from Chinese Senior High School Entrance Examination papers and CK12 website1. The questions are all 4-way multi-choice questions writing in Chinese and English respectively that cover a wide range of subjects, e.g. Biology, <a href=https://en.wikipedia.org/wiki/History>History</a>, <a href=https://en.wikipedia.org/wiki/List_of_life_sciences>Life Science</a> and etc. And, all questions are restrained within the <a href=https://en.wikipedia.org/wiki/Elementary_school_(United_States)>elementary and middle school level</a>. During the whole procedure of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, 7 teams submitted 323 runs in total. This paper describes the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4006/>Alibaba at IJCNLP-2017 Task 1 : Embedding Grammatical Features into LSTMs for Chinese Grammatical Error Diagnosis Task<span class=acl-fixed-case>A</span>libaba at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 1: Embedding Grammatical Features into <span class=acl-fixed-case>LSTM</span>s for <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Task</a></strong><br><a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/j/jun-tao/>Jun Tao</a>
|
<a href=/people/g/guangwei-xu/>Guangwei Xu</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4006><div class="card-body p-3 small">This paper introduces Alibaba NLP team system on IJCNLP 2017 shared task No. 1 Chinese Grammatical Error Diagnosis (CGED). The task is to diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W). We treat the task as a sequence tagging problem and design some handcraft features to solve it. Our system is mainly based on the LSTM-CRF model and 3 <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble strategies</a> are applied to improve the performance. At the identification level and the position level our <a href=https://en.wikipedia.org/wiki/System>system</a> gets the highest F1 scores. At the position level, which is the most difficult level, we perform best on all metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4007/>THU_NGN at IJCNLP-2017 Task 2 : Dimensional Sentiment Analysis for Chinese Phrases with Deep LSTM<span class=acl-fixed-case>THU</span>_<span class=acl-fixed-case>NGN</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Dimensional Sentiment Analysis for <span class=acl-fixed-case>C</span>hinese Phrases with Deep <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4007><div class="card-body p-3 small">Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valence-arousal ratings for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese affective words and phrases</a> automatically. In this task, we propose an approach using a densely connected LSTM network and word features to identify dimensional sentiment on valence and arousal for words and phrases jointly. We use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as major feature and choose part of speech (POS) and word clusters as additional features to train the dense LSTM network. The evaluation results of our submissions (1st and 2nd in average performance) validate the effectiveness of our system to predict valence and arousal dimensions for Chinese words and phrases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4008/>IIIT-H at IJCNLP-2017 Task 3 : A Bidirectional-LSTM Approach for Review Opinion Diversification<span class=acl-fixed-case>IIIT</span>-<span class=acl-fixed-case>H</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 3: A Bidirectional-<span class=acl-fixed-case>LSTM</span> Approach for Review Opinion Diversification</a></strong><br><a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/p/prathyusha-danda/>Prathyusha Danda</a>
|
<a href=/people/s/silpa-kanneganti/>Silpa Kanneganti</a>
|
<a href=/people/s/soujanya-lanka/>Soujanya Lanka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4008><div class="card-body p-3 small">The Review Opinion Diversification (Revopid-2017) shared task focuses on selecting top-k reviews from a set of reviews for a particular product based on a specific criteria. In this paper, we describe our approaches and results for modeling the ranking of reviews based on their usefulness score, this being the first of the three subtasks under this shared task. Instead of posing this as a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression problem</a>, we modeled this as a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> where we want to identify whether a review is useful or not. We employed a bi-directional LSTM to represent each review and is used with a softmax layer to predict the usefulness score. We chose the review with highest usefulness score, then find its cosine similarity score with rest of the reviews. This is done in order to ensure diversity in the selection of top-k reviews. On the top-5 list prediction, we finished 3rd while in top-10 list one, we are placed 2nd in the shared task. We have discussed the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the results in detail in the paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4009/>Bingo at IJCNLP-2017 Task 4 : Augmenting Data using <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Cross-linguistic Customer Feedback Classification<span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Augmenting Data using Machine Translation for Cross-linguistic Customer Feedback Classification</a></strong><br><a href=/people/h/heba-elfardy/>Heba Elfardy</a>
|
<a href=/people/m/manisha-srivastava/>Manisha Srivastava</a>
|
<a href=/people/w/wei-xiao/>Wei Xiao</a>
|
<a href=/people/j/jared-kramer/>Jared Kramer</a>
|
<a href=/people/t/tarun-agarwal/>Tarun Agarwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4009><div class="card-body p-3 small">The ability to automatically and accurately process <a href=https://en.wikipedia.org/wiki/Customer_service>customer feedback</a> is a necessity in the private sector. Unfortunately, <a href=https://en.wikipedia.org/wiki/Customer_feedback>customer feedback</a> can be one of the most difficult types of data to work with due to the sheer volume and variety of services, products, languages, and cultures that comprise the customer experience. In order to address this issue, our team built a suite of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on a four-language, multi-label corpus released as part of the shared task on Customer Feedback Analysis at IJCNLP 2017. In addition to standard text preprocessing, we translated each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> into each other language to increase the size of the training datasets. Additionally, we also used <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in our feature engineering step. Ultimately, we trained classifiers using <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a>, <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forest</a>, and Long Short-Term Memory (LSTM) Recurrent Neural Networks. Overall, we achieved a Macro-Average F-score between 48.7 % and 56.0 % for the four languages and ranked 3/12 for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 3/7 for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, 1/8 for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and 2/7 for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4010/>ADAPT Centre Cone Team at IJCNLP-2017 Task 5 : A Similarity-Based Logistic Regression Approach to Multi-choice Question Answering in an Examinations Shared Task<span class=acl-fixed-case>ADAPT</span> Centre Cone Team at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: A Similarity-Based Logistic Regression Approach to Multi-choice Question Answering in an Examinations Shared Task</a></strong><br><a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4010><div class="card-body p-3 small">We describe the work of a team from the ADAPT Centre in Ireland in addressing automatic answer selection for the Multi-choice Question Answering in Examinations shared task. The system is based on a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> over the string similarities between question, answer, and additional text. We obtain the highest grade out of six <a href=https://en.wikipedia.org/wiki/System>systems</a> : 48.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on a validation set (vs. a baseline of 29.45 %) and 45.6 % on a test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4012/>CVTE at IJCNLP-2017 Task 1 : Character Checking System for Chinese Grammatical Error Diagnosis Task<span class=acl-fixed-case>CVTE</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 1: Character Checking System for <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Task</a></strong><br><a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/s/suixue-wang/>Suixue Wang</a>
|
<a href=/people/g/guanyu-jiang/>Guanyu Jiang</a>
|
<a href=/people/t/tianyuan-you/>Tianyuan You</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4012><div class="card-body p-3 small">Grammatical error diagnosis is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. This paper introduces CVTE Character Checking System in the NLP-TEA-4 shared task for CGED 2017, we use Bi-LSTM to generate the probability of every character, then take two kinds of strategies to decide whether a character is correct or not. This system is probably more suitable to deal with the error type of bad word selection, which is one of four types of errors, and the rest are words re-dundancy, words missing and words disorder. Finally the second <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> achieves better F1 score than the first one at all of detection level, identification level, position level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4015/>CIAL at IJCNLP-2017 Task 2 : An Ensemble Valence-Arousal Analysis System for Chinese Words and Phrases<span class=acl-fixed-case>CIAL</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: An Ensemble Valence-Arousal Analysis System for <span class=acl-fixed-case>C</span>hinese Words and Phrases</a></strong><br><a href=/people/z/zheng-wen-lin/>Zheng-Wen Lin</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/c/chen-ann-wang/>Chen-Ann Wang</a>
|
<a href=/people/y/yu-lun-hsieh/>Yu-Lun Hsieh</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4015><div class="card-body p-3 small">Sentiment lexicon is very helpful in dimensional sentiment applications. Because of countless <a href=https://en.wikipedia.org/wiki/List_of_Chinese_words_of_English_origin>Chinese words</a>, developing a method to predict unseen <a href=https://en.wikipedia.org/wiki/List_of_Chinese_words_of_English_origin>Chinese words</a> is required. The proposed method can handle both words and phrases by using an ADVWeight List for <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a>, which in turn improves our performance at phrase level. The evaluation results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> is effective in dimensional sentiment analysis for Chinese phrases. The Mean Absolute Error (MAE) and Pearson&#8217;s Correlation Coefficient (PCC) for <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>Valence</a> are 0.723 and 0.835, respectively, and those for <a href=https://en.wikipedia.org/wiki/Arousal>Arousal</a> are 0.914 and 0.756, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4016/>Alibaba at IJCNLP-2017 Task 2 : A Boosted Deep System for Dimensional Sentiment Analysis of Chinese Phrases<span class=acl-fixed-case>A</span>libaba at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: A Boosted Deep System for Dimensional Sentiment Analysis of <span class=acl-fixed-case>C</span>hinese Phrases</a></strong><br><a href=/people/x/xin-zhou/>Xin Zhou</a>
|
<a href=/people/j/jian-wang/>Jian Wang</a>
|
<a href=/people/x/xu-xie/>Xu Xie</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4016><div class="card-body p-3 small">This paper introduces Team Alibaba&#8217;s systems participating IJCNLP 2017 shared task No. 2 Dimensional Sentiment Analysis for Chinese Phrases (DSAP). The systems mainly utilize a multi-layer neural networks, with multiple features input such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, part-of-speech-tagging (POST), word clustering, prefix type, character embedding, cross sentiment input, and AdaBoost method for model training. For word level task our best run achieved MAE 0.545 (ranked 2nd), PCC 0.892 (ranked 2nd) in valence prediction and MAE 0.857 (ranked 1st), PCC 0.678 (ranked 2nd) in arousal prediction. For average performance of word and phrase task we achieved <a href=https://en.wikipedia.org/wiki/Mean_squared_error>MAE</a> 0.5355 (ranked 3rd), <a href=https://en.wikipedia.org/wiki/Proportionality_(mathematics)>PCC</a> 0.8965 (ranked 3rd) in valence prediction and <a href=https://en.wikipedia.org/wiki/Mean_squared_error>MAE</a> 0.661 (ranked 3rd), <a href=https://en.wikipedia.org/wiki/Proportionality_(mathematics)>PCC</a> 0.766 (ranked 2nd) in arousal prediction. In the final our submitted system achieved 2nd in mean rank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4017/>NLPSA at IJCNLP-2017 Task 2 : Imagine Scenario : Leveraging Supportive Images for Dimensional Sentiment Analysis<span class=acl-fixed-case>NLPSA</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Imagine Scenario: Leveraging Supportive Images for Dimensional Sentiment Analysis</a></strong><br><a href=/people/s/szu-min-chen/>Szu-Min Chen</a>
|
<a href=/people/z/zi-yuan-chen/>Zi-Yuan Chen</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4017><div class="card-body p-3 small">Categorical sentiment classification has drawn much attention in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, while less work has been conducted for dimensional sentiment analysis (DSA). Recent works for DSA utilize either <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base features</a>, or <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual language resources</a>. In this paper, we propose our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for IJCNLP 2017 Dimensional Sentiment Analysis for Chinese Phrases shared task. Our model incorporates <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as well as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>image features</a>, attempting to simulate human&#8217;s imaging behavior toward <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Though the performance is not comparable to others in the end, we conduct several experiments with possible reasons discussed, and analyze the drawbacks of our model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4018/>NCYU at IJCNLP-2017 Task 2 : Dimensional Sentiment Analysis for Chinese Phrases using Vector Representations<span class=acl-fixed-case>NCYU</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Dimensional Sentiment Analysis for <span class=acl-fixed-case>C</span>hinese Phrases using Vector Representations</a></strong><br><a href=/people/j/jui-feng-yeh/>Jui-Feng Yeh</a>
|
<a href=/people/j/jian-cheng-tsai/>Jian-Cheng Tsai</a>
|
<a href=/people/b/bo-wei-wu/>Bo-Wei Wu</a>
|
<a href=/people/t/tai-you-kuang/>Tai-You Kuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4018><div class="card-body p-3 small">This paper presents two vector representations proposed by National Chiayi University (NCYU) about phrased-based sentiment detection which was used to compete in dimensional sentiment analysis for Chinese phrases (DSACP) at IJCNLP 2017. The vector-based sentiment phraselike unit analysis models are proposed in this article. E-HowNet-based clustering is used to obtain the values of <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>valence</a> and arousal for sentiment words first. An out-of-vocabulary function is also defined in this article to measure the dimensional emotion values for unknown words. For predicting the corresponding values of sentiment phrase-like unit, a vectorbased approach is proposed here. According to the experimental results, we can find the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> is efficacious.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4019/>MainiwayAI at IJCNLP-2017 Task 2 : Ensembles of Deep Architectures for Valence-Arousal Prediction<span class=acl-fixed-case>M</span>ainiway<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Ensembles of Deep Architectures for Valence-Arousal Prediction</a></strong><br><a href=/people/y/yassine-benajiba/>Yassine Benajiba</a>
|
<a href=/people/j/jin-sun/>Jin Sun</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a>
|
<a href=/people/z/zhiliang-weng/>Zhiliang Weng</a>
|
<a href=/people/o/or-biran/>Or Biran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4019><div class="card-body p-3 small">This paper introduces Mainiway AI Labs submitted system for the IJCNLP 2017 shared task on Dimensional Sentiment Analysis of Chinese Phrases (DSAP), and related experiments. Our approach consists of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> with various architectures, and our best <a href=https://en.wikipedia.org/wiki/System>system</a> is a voted ensemble of networks. We achieve a <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>Mean Absolute Error</a> of 0.64 in valence prediction and 0.68 in arousal prediction on the test set, both placing us as the 5th ranked team in the competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4021 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4021/>NTOUA at IJCNLP-2017 Task 2 : Predicting Sentiment Scores of Chinese Words and Phrases<span class=acl-fixed-case>NTOUA</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 2: Predicting Sentiment Scores of <span class=acl-fixed-case>C</span>hinese Words and Phrases</a></strong><br><a href=/people/c/chuan-jie-lin/>Chuan-Jie Lin</a>
|
<a href=/people/h/hao-tsung-chang/>Hao-Tsung Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4021><div class="card-body p-3 small">This paper describes the approaches of sentimental score prediction in the NTOU DSA system participating in DSAP this year. The <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> to predict scores for words are adapted from our <a href=https://en.wikipedia.org/wiki/System>system</a> last year. The approach to predict scores for phrases is keyword-based machine learning method. The performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> is good in predicting scores of phrases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4023 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4023/>JUNLP at IJCNLP-2017 Task 3 : A Rank Prediction Model for Review Opinion Diversification<span class=acl-fixed-case>JUNLP</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 3: A Rank Prediction Model for Review Opinion Diversification</a></strong><br><a href=/people/m/monalisa-dey/>Monalisa Dey</a>
|
<a href=/people/a/anupam-mondal/>Anupam Mondal</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4023><div class="card-body p-3 small">IJCNLP-17 Review Opinion Diversification (RevOpiD-2017) task has been designed for ranking the top-k reviews of a product from a set of reviews, which assists in identifying a summarized output to express the opinion of the entire review set. The task is divided into three independent subtasks as subtask-A, subtask-B, and subtask-C. Each of these three subtasks selects the top-k reviews based on helpfulness, representativeness, and exhaustiveness of the opinions expressed in the review set individually. In order to develop the modules and predict the rank of reviews for all three subtasks, we have employed two well-known supervised classifiers namely, Nave Bayes and <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a> on the top of several extracted features such as the number of nouns, number of verbs, and number of sentiment words etc from the provided datasets. Finally, the organizers have helped to validate the predicted outputs for all three subtasks by using their evaluation metrics. The <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> provide the scores of list size 5 as (0.80 (mth)) for subtask-A, (0.86 (cos), 0.87 (cos d), 0.71 (cpr), 4.98 (a-dcg), and 556.94 (wt)) for subtask B, and (10.94 (unwt) and 0.67 (recall)) for subtask C individually.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4024/>All-In-1 at IJCNLP-2017 Task 4 : Short Text Classification with One Model for All Languages<span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Short Text Classification with One Model for All Languages</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4024><div class="card-body p-3 small">We present All-In-1, a simple <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> for multilingual text classification that does not require any parallel data. It is based on a traditional Support Vector Machine classifier exploiting multilingual word embeddings and character n-grams. Our model is simple, easily extendable yet very effective, overall ranking 1st (out of 12 teams) in the IJCNLP 2017 shared task on customer feedback analysis in four languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4025 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4025/>SentiNLP at IJCNLP-2017 Task 4 : Customer Feedback Analysis Using a Bi-LSTM-CNN Model<span class=acl-fixed-case>S</span>enti<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Customer Feedback Analysis Using a <span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CNN</span> Model</a></strong><br><a href=/people/s/shuying-lin/>Shuying Lin</a>
|
<a href=/people/h/huosheng-xie/>Huosheng Xie</a>
|
<a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/k/k-robert-lai/>K. Robert Lai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4025><div class="card-body p-3 small">The analysis of customer feedback is useful to provide good customer service. There are a lot of online customer feedback are produced. Manual classification is impractical because the high volume of data. Therefore, the automatic classification of the customer feedback is of importance for the analysis system to identify meanings or intentions that the customer express. The aim of shared Task 4 of IJCNLP 2017 is to classify the <a href=https://en.wikipedia.org/wiki/Customer_service>customer feedback</a> into six tags categorization. In this paper, we present a system that uses <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to express the feature of the sentence in the corpus and the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> as the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to complete the shared task. And then the <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> is used to get final predictive result. The proposed method get ranked first among twelve teams in terms of micro-averaged F1 and second for accura-cy metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4027 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4027/>ADAPT at IJCNLP-2017 Task 4 : A Multinomial Naive Bayes Classification Approach for Customer Feedback Analysis task<span class=acl-fixed-case>ADAPT</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: A Multinomial Naive <span class=acl-fixed-case>B</span>ayes Classification Approach for Customer Feedback Analysis task</a></strong><br><a href=/people/p/pintu-lohar/>Pintu Lohar</a>
|
<a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/h/haithem-afli/>Haithem Afli</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4027><div class="card-body p-3 small">In this age of the digital economy, promoting organisations attempt their best to engage the customers in the feedback provisioning process. With the assistance of customer insights, an organisation can develop a better product and provide a better service to its customer. In this paper, we analyse the real world samples of customer feedback from Microsoft Office customers in four languages, i.e., <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and conclude a five-plus-one-classes categorisation (comment, request, bug, complaint, meaningless and undetermined) for meaning classification. The task is to % access multilingual corpora annotated by the proposed meaning categorization scheme and develop a system to determine what class(es) the customer feedback sentences should be annotated as in four languages. We propose following approaches to accomplish this task : (i) a multinomial naive bayes (MNB) approach for multi-label classification, (ii) MNB with one-vs-rest classifier approach, and (iii) the combination of the multilabel classification-based and the sentiment classification-based approach. Our best system produces <a href=https://en.wikipedia.org/wiki/F-score>F-scores</a> of 0.67, 0.83, 0.72 and 0.7 for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, respectively. The results are competitive to the best ones for all languages and secure 3rd and 5th position for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, respectively, among all submitted systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4028 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4028/>OhioState at IJCNLP-2017 Task 4 : Exploring Neural Architectures for Multilingual Customer Feedback Analysis<span class=acl-fixed-case>O</span>hio<span class=acl-fixed-case>S</span>tate at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Exploring Neural Architectures for Multilingual Customer Feedback Analysis</a></strong><br><a href=/people/d/dushyanta-dhyani/>Dushyanta Dhyani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4028><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>systems</a> for IJCNLP 2017 Shared Task on Customer Feedback Analysis. We experimented with simple neural architectures that gave competitive performance on certain <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. This includes shallow CNN and Bi-Directional LSTM architectures with Facebook&#8217;s Fasttext as a baseline model. Our best performing model was in the Top 5 systems using the Exact-Accuracy and Micro-Average-F1 metrics for the Spanish (85.28 % for both) and French (70 % and 73.17 % respectively) task, and outperformed all the other models on comment (87.28 %) and meaningless (51.85 %) tags using Micro Average F1 by Tags metric for the French task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4030 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4030/>NITMZ-JU at IJCNLP-2017 Task 4 : Customer Feedback Analysis<span class=acl-fixed-case>NITMZ</span>-<span class=acl-fixed-case>JU</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Customer Feedback Analysis</a></strong><br><a href=/people/s/somnath-banerjee/>Somnath Banerjee</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/r/riyanka-manna/>Riyanka Manna</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/a/alexander-gelbukh/>Alexander Gelbukh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4030><div class="card-body p-3 small">In this paper, we describe a deep learning framework for analyzing the customer feedback as part of our participation in the shared task on Customer Feedback Analysis at the 8th International Joint Conference on Natural Language Processing (IJCNLP 2017). A Convolutional Neural Network (CNN) based deep neural network model was employed for the customer feedback task. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> was evaluated on two <a href=https://en.wikipedia.org/wiki/Language>languages</a>, namely, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4031 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4031/>IITP at IJCNLP-2017 Task 4 : Auto Analysis of Customer Feedback using CNN and GRU Network<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 4: Auto Analysis of Customer Feedback using <span class=acl-fixed-case>CNN</span> and <span class=acl-fixed-case>GRU</span> Network</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4031><div class="card-body p-3 small">Analyzing customer feedback is the best way to channelize the data into new <a href=https://en.wikipedia.org/wiki/Marketing_strategy>marketing strategies</a> that benefit entrepreneurs as well as customers. Therefore an automated system which can analyze the <a href=https://en.wikipedia.org/wiki/Consumer_behaviour>customer behavior</a> is in great demand. Users may write feedbacks in any language, and hence mining appropriate information often becomes intractable. Especially in a traditional feature-based supervised model, it is difficult to build a generic system as one has to understand the concerned language for finding the relevant <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. In order to overcome this, we propose deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) based approaches that do not require handcrafting of features. We evaluate these techniques for analyzing customer feedback sentences on four languages, namely <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Our empirical analysis shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the second rank in <a href=https://en.wikipedia.org/wiki/French_language>French</a>, with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 71.75 % and third ranks for all the other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4032/>YNUDLG at IJCNLP-2017 Task 5 : A CNN-LSTM Model with Attention for Multi-choice Question Answering in Examinations<span class=acl-fixed-case>YNUDLG</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: A <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model with Attention for Multi-choice Question Answering in Examinations</a></strong><br><a href=/people/m/min-wang/>Min Wang</a>
|
<a href=/people/q/qingxun-liu/>Qingxun Liu</a>
|
<a href=/people/p/peng-ding/>Peng Ding</a>
|
<a href=/people/y/yongbin-li/>Yongbin Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4032><div class="card-body p-3 small">In this paper, we perform convolutional neural networks (CNN) to learn the joint representations of question-answer pairs first, then use the joint representations as the inputs of the long short-term memory (LSTM) with attention to learn the answer sequence of a question for labeling the matching quality of each answer. We also incorporating external knowledge by training Word2Vec on Flashcards data, thus we get more compact embedding. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves better or comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>. The proposed approach achieves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.39, 0.42 in English valid set, test set, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4033 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4033/>ALS at IJCNLP-2017 Task 5 : Answer Localization System for Multi-Choice Question Answering in Exams<span class=acl-fixed-case>ALS</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: Answer Localization System for Multi-Choice Question Answering in Exams</a></strong><br><a href=/people/c/changliang-li/>Changliang Li</a>
|
<a href=/people/c/cunliang-kong/>Cunliang Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4033><div class="card-body p-3 small">Multi-choice question answering in exams is a typical QA task. To accomplish this task, we present an answer localization method to locate answers shown in web pages, considering structural information and semantic information both. Using this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> as basis, we analyze sentences and paragraphs appeared on web pages to get predictions. With this answer localization system, we get effective results on both <a href=https://en.wikipedia.org/wiki/Data_validation>validation dataset</a> and <a href=https://en.wikipedia.org/wiki/Test_data>test dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4035/>YNU-HPCC at IJCNLP-2017 Task 5 : Multi-choice Question Answering in Exams Using an Attention-based LSTM Model<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: Multi-choice Question Answering in Exams Using an Attention-based <span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/h/hang-yuan/>Hang Yuan</a>
|
<a href=/people/y/you-zhang/>You Zhang</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4035><div class="card-body p-3 small">A shared task is a typical question answering task that aims to test how accurately the participants can answer the questions in exams. Typically, for each question, there are four candidate answers, and only one of the answers is correct. The existing methods for such a task usually implement a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> or <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>long short-term memory (LSTM)</a>. However, both RNN and LSTM are biased models in which the words in the tail of a sentence are more dominant than the words in the header. In this paper, we propose the use of an attention-based LSTM (AT-LSTM) model for these tasks. By adding an attention mechanism to the standard LSTM, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can more easily capture long contextual information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-4036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-4036 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-4036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-4036/>JU NITM at IJCNLP-2017 Task 5 : A Classification Approach for Answer Selection in Multi-choice Question Answering System<span class=acl-fixed-case>JU</span> <span class=acl-fixed-case>NITM</span> at <span class=acl-fixed-case>IJCNLP</span>-2017 Task 5: A Classification Approach for Answer Selection in Multi-choice Question Answering System</a></strong><br><a href=/people/s/sandip-sarkar/>Sandip Sarkar</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-4036><div class="card-body p-3 small">This paper describes the participation of the JU NITM team in IJCNLP-2017 Task 5 : Multi-choice Question Answering in Examinations. The main aim of this shared task is to choose the correct option for each multi-choice question. Our proposed model includes <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>vector representations</a> as feature and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. At first we represent question and answer in <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> and after that find the cosine similarity between those two vectors. Finally we apply <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification approach</a> to find the correct answer. Our system was only developed for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> obtained an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 40.07 % for test dataset and 40.06 % for valid dataset.</div></div></div><hr><div id=i17-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/I17-5/>Proceedings of the IJCNLP 2017, Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5000/>Proceedings of the <span class=acl-fixed-case>IJCNLP</span> 2017, Tutorial Abstracts</a></strong><br><a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5001/>Deep Learning in Lexical Analysis and Parsing</a></strong><br><a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5001><div class="card-body p-3 small">Neural networks, also with a fancy name <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, just right can overcome the above feature engineering problem. In theory, they can use non-linear activation functions and multiple layers to automatically find useful <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. The novel <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network structures</a>, such as convolutional or recurrent, help to reduce the difficulty further. These deep learning models have been successfully used for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical analysis</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. In this tutorial, we will give a review of each line of work, by contrasting them with traditional <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>, and organizing them in consistent orders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5002/>Multilingual Vector Representations of Words, Sentences, and Documents</a></strong><br><a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5002><div class="card-body p-3 small">Neural vector representations are now ubiquitous in all subfields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>. While methods such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe are well-known, this tutorial focuses on multilingual and cross-lingual vector representations, of words, but also of sentences and documents as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5003/>Open-Domain Neural Dialogue Systems</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5003><div class="card-body p-3 small">In the past decade, spoken dialogue systems have been the most prominent component in today&#8217;s personal assistants. A lot of devices have incorporated dialogue system modules, which allow users to speak naturally in order to finish tasks more efficiently. The traditional conversational systems have rather complex and/or modular pipelines. The advance of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> has recently risen the applications of neural models to dialogue modeling. Nevertheless, applying deep learning technologies for building robust and scalable dialogue systems is still a challenging task and an open research area as it requires deeper understanding of the classic pipelines as well as detailed knowledge on the benchmark of the models of the prior work and the recent state-of-the-art work. Therefore, this tutorial is designed to focus on an overview of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> development while describing most recent research for building task-oriented and chit-chat dialogue systems, and summarizing the challenges. We target the audience of students and practitioners who have some deep learning background, who want to get more familiar with conversational dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5004/>Neural Machine Translation : Basics, Practical Aspects and Recent Trends</a></strong><br><a href=/people/f/fabien-cromieres/>Fabien Cromieres</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5004><div class="card-body p-3 small">Machine Translation (MT) is a sub-field of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> which has experienced a number of paradigm shifts since its inception. Up until 2014, Phrase Based Statistical Machine Translation (PBSMT) approaches used to be the state of the art. In late 2014, Neural Machine Translation (NMT) was introduced and was proven to outperform all PBSMT approaches by a significant margin. Since then, the NMT approaches have undergone several transformations which have pushed the state of the art even further. This tutorial is primarily aimed at researchers who are either interested in or are fairly new to the world of NMT and want to obtain a deep understanding of NMT fundamentals. Because it will also cover the latest developments in <a href=https://en.wikipedia.org/wiki/Non-verbal_communication>NMT</a>, it should also be useful to attendees with some experience in <a href=https://en.wikipedia.org/wiki/Non-verbal_communication>NMT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5005/>The Ultimate Presentation Makeup Tutorial : How to Polish your Posters, Slides and Presentations Skills<span class=acl-fixed-case>P</span>olish your Posters, Slides and Presentations Skills</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5005><div class="card-body p-3 small">There is no question that our research community have, and still has been producing an insurmountable amount of interesting strategies, models and tools to a wide array of problems and challenges in diverse areas of knowledge. But for as long as interesting work has existed, we&#8217;ve been plagued by a great unsolved mystery : how come there is so much interesting work being published in conferences, but not as many interesting and engaging posters and presentations being featured in them? In this tutorial, we present practical step-by-step makeup solutions for poster, slides and oral presentations in order to help researchers who feel like they are not able to convey the importance of their research to the community in conferences.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>