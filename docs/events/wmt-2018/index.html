<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Statistical Machine Translation (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Statistical Machine Translation (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-63>Proceedings of the Third Conference on Machine Translation: Research Papers</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#w18-64>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a>
<span class="badge badge-info align-middle ml-1">51&nbsp;papers</span></li></ul></div></div><div id=w18-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-63/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6300/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6301/>Scaling Neural Machine Translation</a></strong><br><a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6301><div class="card-body p-3 small">Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT&#8217;14 English-German translation, we match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of Vaswani et al. (2017) in under 5 hours when training on 8 <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and we obtain a new state of the art of 29.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT&#8217;14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6302/>Character-level Chinese-English Translation through ASCII Encoding<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Translation through <span class=acl-fixed-case>ASCII</span> Encoding</a></strong><br><a href=/people/n/nikola-i-nikolov/>Nikola I. Nikolov</a>
|
<a href=/people/y/yuhuang-hu/>Yuhuang Hu</a>
|
<a href=/people/m/mi-xue-tan/>Mi Xue Tan</a>
|
<a href=/people/r/richard-h-r-hahnloser/>Richard H.R. Hahnloser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6302><div class="card-body p-3 small">Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They mainly do well for <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language pairs</a>, where the languages share the same <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a>. However, for <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the gap between the two different <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> poses a major challenge because of a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, by breaking down <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> into linguistic units similar to that of <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>. We use the Wubi encoding scheme, which preserves the original shape and semantic information of the characters, while also being reversible. We show promising results from training Wubi-based models on the character- and subword-level with recurrent as well as convolutional models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6304/>An Analysis of Attention Mechanisms : The Case of <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> in Neural Machine Translation</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6304><div class="card-body p-3 small">Recent work has shown that the encoder-decoder attention mechanisms in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> are different from the <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> pay more attention to <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context tokens</a> when translating <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>. We explore the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution patterns</a> when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is not the main mechanism used by NMT models to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for WSD. The experimental results suggest that NMT models learn to encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to align source and target tokens and the last few layers learn to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the related but unaligned context tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6305/>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>roatian Human and Machine Translation</a></strong><br><a href=/people/m/margita-sostaric/>Margita Šoštarić</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6305><div class="card-body p-3 small">We present an analysis of a number of <a href=https://en.wikipedia.org/wiki/Coreference>coreference phenomena</a> in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> produced by <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6306/>Coreference and Coherence in Neural Machine Translation : A Study Using Oracle Experiments</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6306><div class="card-body p-3 small">Cross-sentence context can provide valuable information in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence</a>. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, of up to 7.02 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and 1.89 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6307/>A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6307><div class="card-body p-3 small">The translation of pronouns presents a special challenge to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to this day, since <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> often requires context outside the current sentence. Recent work on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is needed to assess how well <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> translate inter-sentential phenomena such as <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6309/>A neural interlingua for multilingual machine translation</a></strong><br><a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a>
|
<a href=/people/s/shaonan-zhang/>Shaonan Zhang</a>
|
<a href=/people/j/jason-sun/>Jason Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6309><div class="card-body p-3 small">We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6310/>Improving Neural Language Models with Weight Norm Initialization and Regularization</a></strong><br><a href=/people/c/christian-herold/>Christian Herold</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6310><div class="card-body p-3 small">Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrices</a> in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a <a href=https://en.wikipedia.org/wiki/Grid_search>grid search</a>. With this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank (PTB)</a> and from 61.45 to 60.13 on <a href=https://en.wikipedia.org/wiki/WikiText>WikiText-2 (WT2)</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6311/>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6311><div class="card-body p-3 small">Recent works in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> extracted from <a href=https://en.wikipedia.org/wiki/Europarl>Europarl v7</a> and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6313/>Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6313><div class="card-body p-3 small">To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, and each embedding space) and consider each component&#8217;s contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6314/>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection</a></strong><br><a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a>
|
<a href=/people/t/tetsuji-nakagawa/>Tetsuji Nakagawa</a>
|
<a href=/people/c/ciprian-chelba/>Ciprian Chelba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6314><div class="card-body p-3 small">Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6315/>Using Monolingual Data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : a Systematic Study</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6315><div class="card-body p-3 small">Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-6319.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-6319/>A Call for Clarity in Reporting BLEU Scores<span class=acl-fixed-case>BLEU</span> Scores</a></strong><br><a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6319><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to the <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> are often not reported or are hard to find, and consequently, BLEU scores between papers can not be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6320/>Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for gisting</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6320><div class="card-body p-3 small">A popular application of machine translation (MT) is gisting : MT is consumed as is to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses reading comprehension questionnaires (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, gap-filling (GF), a form of cloze testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are : (a) both <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a> and GF clearly identify MT to be useful ; (b) global RCQ and GF rankings for the MT systems are mostly in agreement ; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a>, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of <a href=https://en.wikipedia.org/wiki/Glucosamine>GF</a> as a cheaper alternative to <a href=https://en.wikipedia.org/wiki/Carboxylic_acid>RCQ</a>.<i>gisting</i>: MT is consumed <i>as is</i> to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses <i>reading comprehension questionnaires</i> (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, <i>gap-filling</i> (GF), a form of <i>cloze</i> testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6321/>Simple Fusion : Return of the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a></a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6321><div class="card-body p-3 small">Neural Machine Translation (NMT) typically leverages <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training : We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual probability</a> of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and <a href=https://en.wikipedia.org/wiki/Cold_fusion>cold fusion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6324/>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6324><div class="card-body p-3 small">We work on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation system</a> that addresses these challenges using eight <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language families</a> as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in qualitative evaluation where our translations are akin to human translations in a preliminary study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6325/>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6325><div class="card-body p-3 small">Transfer learning has been proven as an effective technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> under low-resource conditions. Existing methods require a common target language, <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a>, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a parent model for a high-resource language pair and then continue the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> on a low-resource pair only by replacing the training corpus. This <a href=https://en.wikipedia.org/wiki/Child_model>child model</a> performs significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained for low-resource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.</div></div></div><hr><div id=w18-64><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-64.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-64/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6400/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6403/>Findings of the WMT 2018 Biomedical Translation Shared Task : Evaluation on Medline test sets<span class=acl-fixed-case>WMT</span> 2018 Biomedical Translation Shared Task: Evaluation on <span class=acl-fixed-case>M</span>edline test sets</a></strong><br><a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/c/cristian-grozea/>Cristian Grozea</a>
|
<a href=/people/a/amy-siu/>Amy Siu</a>
|
<a href=/people/m/madeleine-kittner/>Madeleine Kittner</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6403><div class="card-body p-3 small">Machine translation enables the <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> and Spanish). We describe the development of the various <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test sets</a>, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year&#8217;s BLEU scores were somewhat higher that last year&#8217;s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6404/>An Empirical Study of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for the Shared Task of WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/y/yiming-wang/>Yiming Wang</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a>
|
<a href=/people/s/shiqi-li/>Shiqi Li</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6404><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT18 shared news translation task. We participated in the English-to-Chinese direction and get the best BLEU (43.8) scores among all the participants. The submitted <a href=https://en.wikipedia.org/wiki/System>system</a> focus on data clearing and techniques to build a competitive <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Unlike other participants, the submitted system are mainly relied on the data filtering to obtain the best BLEU score. We do data filtering not only for provided sentences but also for the back translated sentences. The techniques we apply for data filtering include filtering by rules, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and translation models. We also conduct several experiments to validate the effectiveness of training techniques. According to our experiments, the Annealing Adam optimizing function and ensemble decoding are the most effective techniques for the model training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6406/>The TALP-UPC Machine Translation Systems for WMT18 News Shared Translation Task<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18 News Shared Translation Task</a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6406><div class="card-body p-3 small">In this article we describe the TALP-UPC research group participation in the WMT18 news shared translation task for Finnish-English and Estonian-English within the multi-lingual subtrack. All of our primary submissions implement an attention-based Neural Machine Translation architecture. Given that <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> and Estonian belong to the same language family and are similar, we use as training data the combination of the datasets of both language pairs to paliate the data scarceness of each individual pair. We also report the translation quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>systems</a> trained on individual language pair data to serve as baseline and comparison reference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6411/>The AFRL WMT18 Systems : Ensembling, Continuation and Combination<span class=acl-fixed-case>AFRL</span> <span class=acl-fixed-case>WMT</span>18 Systems: Ensembling, Continuation and Combination</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/k/katherine-young/>Katherine Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6411><div class="card-body p-3 small">This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT18 evaluation campaign. This year, we examined the developments and additions to popular neural machine translation toolkits and measure improvements in performance on the RussianEnglish language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6412/>The University of Edinburgh’s Submissions to the WMT18 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>18 News Translation Task</a></strong><br><a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6412><div class="card-body p-3 small">The University of Edinburgh made submissions to all 14 language pairs in the news translation task, with strong performances in most pairs. We introduce new RNN-variant, mixed RNN / Transformer ensembles, data selection and weighting, and extensions to back-translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6416/>CUNI Submissions in WMT18<span class=acl-fixed-case>CUNI</span> Submissions in <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/r/roman-sudarikov/>Roman Sudarikov</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6416><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/Estonian_language>English-Estonian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>English-Czech</a>. Our main focus was the low-resource language pair of Estonian and English for which we utilized Finnish parallel data in a simple method. We first train a parent model for the high-resource language pair followed by <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> on the related low-resource language pair. This approach brings a substantial performance boost over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> trained only on Estonian-English parallel data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Transformer architecture. For the English to Czech translation, we have evaluated our last year models of hybrid phrase-based approach and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> mainly for comparison purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6418/>JUCBNMT at WMT2018 News Translation Task : Character Based Neural Machine Translation of Finnish to English<span class=acl-fixed-case>JUCBNMT</span> at <span class=acl-fixed-case>WMT</span>2018 News Translation Task: Character Based Neural Machine Translation of <span class=acl-fixed-case>F</span>innish to <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/sainik-mahata/>Sainik Kumar Mahata</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6418><div class="card-body p-3 small">In the current work, we present a description of the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to WMT 2018 News Translation Shared task. The <a href=https://en.wikipedia.org/wiki/System>system</a> was created to translate news text from <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> used a Character Based Neural Machine Translation model to accomplish the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The current paper documents the preprocessing steps, the description of the submitted <a href=https://en.wikipedia.org/wiki/System>system</a> and the results produced using the same. Our <a href=https://en.wikipedia.org/wiki/System>system</a> garnered a BLEU score of 12.9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6420/>PROMT Systems for WMT 2018 Shared Translation Task<span class=acl-fixed-case>PROMT</span> Systems for <span class=acl-fixed-case>WMT</span> 2018 Shared Translation Task</a></strong><br><a href=/people/a/alexander-molchanov/>Alexander Molchanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6420><div class="card-body p-3 small">This paper describes the PROMT submissions for the WMT 2018 Shared News Translation Task. This year we participated only in the English-Russian language pair. We built two primary neural networks-based systems : 1) a pure Marian-based neural system and 2) a hybrid system which incorporates OpenNMT-based neural post-editing component into our RBMT engine. We also submitted pure rule-based translation (RBMT) for <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrast</a>. We show competitive results with both primary submissions which significantly outperform the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>RBMT baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6422/>The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018<span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6422><div class="card-body p-3 small">We present our experiments in the scope of the news translation task in WMT 2018, in directions : EnglishGerman. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with a deeper architecture. By using techniques to limit the memory consumption, we were able to train <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> that are 4 times larger on one <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by 0.5 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6424/>CUNI Transformer Neural MT System for WMT18<span class=acl-fixed-case>CUNI</span> Transformer Neural <span class=acl-fixed-case>MT</span> System for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6424><div class="card-body p-3 small">We describe our NMT system submitted to the WMT2018 shared task in news translation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering of the synthetic data</a>. We pre-process the input sentences using <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p 0.05) better than all other English-Czech and Czech-English systems in WMT2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6426" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6426/>The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/p/parnia-bahar/>Parnia Bahar</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/a/arne-nix/>Arne Nix</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6426><div class="card-body p-3 small">This paper describes the statistical machine translation systems developed at RWTH Aachen University for the GermanEnglish, EnglishTurkish and ChineseEnglish translation tasks of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use ensembles of neural machine translation systems based on the Transformer architecture. Our main focus is on the GermanEnglish task where we to all automatic scored first with respect metrics provided by the organizers. We identify data selection, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, <a href=https://en.wikipedia.org/wiki/Batch_processing>batch size</a> and model dimension as important hyperparameters. In total we improve by 6.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over our last year&#8217;s submission and by 4.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over the winning system of the 2017 GermanEnglish task. In EnglishTurkish task, we show 3.6 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvement over the last year&#8217;s winning system. We further report results on the <a href=https://en.wikipedia.org/wiki/English_language>ChineseEnglish task</a> where we improve 2.2 % BLEU on average over our baseline systems but stay behind the 2018 winning systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6427/>The University of Cambridge’s Machine Translation Systems for WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>C</span>ambridge’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6427><div class="card-body p-3 small">The University of Cambridge submission to the WMT18 news translation task focuses on the combination of diverse models of translation. We compare recurrent, convolutional, and self-attention-based neural models on German-English, English-German, and Chinese-English. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> combines all neural models together with a phrase-based SMT system in an MBR-based scheme. We report small but consistent gains on top of strong Transformer ensembles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6428/>The LMU Munich Unsupervised Machine Translation Systems<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation Systems</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6428><div class="card-body p-3 small">We describe LMU Munich&#8217;s unsupervised machine translation systems for EnglishGerman translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6429/>Tencent Neural Machine Translation Systems for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/li-gong/>Li Gong</a>
|
<a href=/people/w/wenhuan-zhu/>Wenhuan Zhu</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/c/chao-bian/>Chao Bian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6429><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task on EnglishChinese language pair. Our systems are based on attentional sequence-to-sequence models with some form of <a href=https://en.wikipedia.org/wiki/Recursion>recursion</a> and self-attention. Some data augmentation methods are also introduced to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. The best <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> result is obtained with ensemble and reranking techniques. Our ChineseEnglish system achieved the highest cased BLEU score among all 16 submitted systems, and our EnglishChinese system ranked the third out of 18 submitted systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6431/>The University of Maryland’s Chinese-English Neural Machine Translation Systems at WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6431><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submission to the WMT 2018 ChineseEnglish news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>, we improve over the Transformer baseline by +1.4 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for ChineseEnglish and +3.97 BLEU for EnglishChinese on newstest2017. Our best systems reach <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> of 24.4 for <a href=https://en.wikipedia.org/wiki/Chinese_English>ChineseEnglish</a> and 39.0 for <a href=https://en.wikipedia.org/wiki/Chinese_English>EnglishChinese</a> on newstest2018.<i>newstest2017</i>. Our best systems reach BLEU scores of 24.4 for Chinese&#8594;English and 39.0 for English&#8594;Chinese on <i>newstest2018</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6432/>EvalD Reference-Less Discourse Evaluation for WMT18<span class=acl-fixed-case>E</span>val<span class=acl-fixed-case>D</span> Reference-Less Discourse Evaluation for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jiri-mirovsky/>Jiří Mírovský</a>
|
<a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6432><div class="card-body p-3 small">We present the results of automatic evaluation of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> in machine translation (MT) outputs using the EVALD tool. EVALD was originally designed and trained to assess the quality of <a href=https://en.wikipedia.org/wiki/Writing>human writing</a>, for <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.<i>human</i> writing, for native speakers and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6436/>Fine-grained evaluation of German-English Machine Translation based on a Test Suite<span class=acl-fixed-case>G</span>erman-<span class=acl-fixed-case>E</span>nglish Machine Translation based on a Test Suite</a></strong><br><a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/e/eleftherios-avramidis/>Eleftherios Avramidis</a>
|
<a href=/people/a/aljoscha-burchardt/>Aljoscha Burchardt</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6436><div class="card-body p-3 small">We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The <a href=https://en.wikipedia.org/wiki/Test_suite>test suite</a> has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>verb tenses</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_mood>moods</a>. The MT outputs are evaluated in a semi-automatic way through <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare <a href=https://en.wikipedia.org/wiki/System>systems</a> based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> and we identify grammatical phenomena where the overall performance of MT is relatively low.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6437/>The Word Sense Disambiguation Test Suite at WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6437><div class="card-body p-3 small">We present a task to measure an MT system&#8217;s capability to translate ambiguous words with their correct sense according to the given context. The task is based on the GermanEnglish Word Sense Disambiguation (WSD) test set ContraWSD (Rios Gonzales et al., 2017), but it has been filtered to reduce noise, and the evaluation has been adapted to assess MT output directly rather than scoring existing translations. We evaluate all GermanEnglish submissions to the WMT&#8217;18 shared translation task, plus a number of submissions from previous years, and find that performance on the task has markedly improved compared to the 2016 WMT submissions (81%93 % accuracy on the WSD task). We also find that the unsupervised submissions to the task have a low WSD capability, and predominantly translate ambiguous source words with the same sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6440 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6440/>The AFRL-Ohio State WMT18 Multimodal System : Combining Visual with Traditional<span class=acl-fixed-case>AFRL</span>-<span class=acl-fixed-case>O</span>hio <span class=acl-fixed-case>S</span>tate <span class=acl-fixed-case>WMT</span>18 Multimodal System: Combining Visual with Traditional</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/j/joshua-sandvick/>Joshua Sandvick</a>
|
<a href=/people/m/michael-hutt/>Michael Hutt</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/john-duselis/>John Duselis</a>
|
<a href=/people/j/james-davis/>James Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6440><div class="card-body p-3 small">AFRL-Ohio State extends its usage of visual domain-driven machine translation for use as a peer with traditional <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6441/>CUNI System for the WMT18 Multimodal Translation Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6441><div class="card-body p-3 small">We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. We evaluate two methods of incorporating the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> in the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> : first, we include the image representation as another input to the network ; second, we train the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> to predict the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a> and self-attentive textual baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6442/>Sheffield Submissions for WMT18 Multimodal Translation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Shared Task</a></strong><br><a href=/people/c/chiraag-lala/>Chiraag Lala</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6442><div class="card-body p-3 small">This paper describes the University of Sheffield&#8217;s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking approaches. More specifically, n-best translation candidates from this system are re-ranked using novel multimodal cross-lingual word sense disambiguation models. For task 1b, we explore three approaches : (i) re-ranking based on cross-lingual word sense disambiguation (as for task 1), (ii) re-ranking based on consensus of NMT n-best lists from German-Czech, French-Czech and English-Czech systems, and (iii) data augmentation by generating English source data through <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from French to English and from German to English followed by hypothesis selection using a multimodal-reranker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6444/>Translation of Biomedical Documents with Focus on Spanish-English<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/mirela-stefania-duma/>Mirela-Stefania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6444><div class="card-body p-3 small">For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> performed best on <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese-English</a>, where a BLEU score of 41.84 placed <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> third out of seven runs submitted by three institutions. In this paper, we describe our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and results with a special focus on <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish-English</a> where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for selecting domain-specific data for training models which obtain good results using only 10 % of the general domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6445/>Ensemble of Translators with Automatic Selection of the Best Translation the submission of FOKUS to the WMT 18 biomedical translation task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 18 biomedical translation task –</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6445><div class="card-body p-3 small">This paper describes the system of Fraunhofer FOKUS for the WMT 2018 biomedical translation task. Our approach, described here, was to automatically select the most promising <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> from a set of candidates produced with NMT (Transformer) models. We selected the highest fidelity translation of each sentence by using a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, <a href=https://en.wikipedia.org/wiki/Stemming>stemming</a> and a set of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. Our method is simple, can use any machine translators, and requires no further training in addition to that already employed to build the NMT models. The downside is that the score did not increase over the best in <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a>, but was quite close to it (difference about 0.5 BLEU).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6446/>LMU Munich’s Neural Machine Translation Systems at WMT 2018<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich’s Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6446><div class="card-body p-3 small">We present the LMU Munich machine translation systems for the EnglishGerman language pair. We have built neural machine translation systems for both translation directions (EnglishGerman and GermanEnglish) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year&#8217;s strong biomedical translation engine for EnglishGerman (Huck et al., 2017a). Considerable progress has been made in the latter <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which we report on in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6447/>Hunter NMT System for WMT18 Biomedical Translation Task : Transfer Learning in Neural Machine Translation<span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WMT</span>18 Biomedical Translation Task: Transfer Learning in Neural Machine Translation</a></strong><br><a href=/people/a/abdul-khan/>Abdul Khan</a>
|
<a href=/people/s/subhadarshi-panda/>Subhadarshi Panda</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a>
|
<a href=/people/l/lampros-flokas/>Lampros Flokas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6447><div class="card-body p-3 small">This paper describes the submission of Hunter Neural Machine Translation (NMT) to the WMT&#8217;18 Biomedical translation task from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a>. The discrepancy between training and test data distribution brings a challenge to translate text in new domains. Beyond the previous work of combining in-domain with out-of-domain models, we found accuracy and efficiency gain in combining different in-domain models. We conduct extensive experiments on NMT with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Furthermore, we applied <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble learning</a> on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.<i>transfer learning</i>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the baseline. Furthermore, we applied ensemble learning on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the baseline. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6448 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6448/>UFRGS Participation on the WMT Biomedical Translation Shared Task<span class=acl-fixed-case>UFRGS</span> Participation on the <span class=acl-fixed-case>WMT</span> Biomedical Translation Shared Task</a></strong><br><a href=/people/f/felipe-soares/>Felipe Soares</a>
|
<a href=/people/k/karin-becker/>Karin Becker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6448><div class="card-body p-3 small">This paper describes the machine translation systems developed by the Universidade Federal do Rio Grande do Sul (UFRGS) team for the biomedical translation shared task. Our systems are based on <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, using the Moses and OpenNMT toolkits, respectively. We participated in four translation directions for the English / Spanish and English / Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved the best <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> according to the official shared task evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6449/>Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task<span class=acl-fixed-case>R</span>omance Languages for the Biomedical <span class=acl-fixed-case>WMT</span> 2018 task</a></strong><br><a href=/people/b/brian-tubay/>Brian Tubay</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6449><div class="card-body p-3 small">The Transformer architecture has become the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which relies on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention-based mechanisms</a>, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using <a href=https://en.wikipedia.org/wiki/List_of_programming_languages_by_type>multi-source languages</a> from the same family allows improvements of over 6 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6455/>ITER : Improving Translation Edit Rate through Optimizable Edit Costs<span class=acl-fixed-case>ITER</span>: Improving Translation Edit Rate through Optimizable Edit Costs</a></strong><br><a href=/people/j/joybrata-panja/>Joybrata Panja</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6455><div class="card-body p-3 small">The paper presents our participation in the WMT 2018 Metrics Shared Task. We propose an improved version of Translation Edit / Error Rate (TER). In addition to including the basic edit operations in TER, namely-insertion, deletion, substitution and shift, our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> also allows stem matching, optimizable edit costs and better <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> so as to correlate better with human judgement scores. The proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> shows much higher correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> than TER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6456 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6456" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6456/>RUSE : Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation<span class=acl-fixed-case>RUSE</span>: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6456><div class="card-body p-3 small">We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that can not be captured by local features based on character or word N-grams. Although training <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using small-scale translation datasets with manual evaluation is difficult, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use a multi-layer perceptron regressor based on three types of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6459 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6459/>Neural Machine Translation for English-Tamil<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/h/himanshu-choudhary/>Himanshu Choudhary</a>
|
<a href=/people/a/aditya-kumar-pathak/>Aditya Kumar Pathak</a>
|
<a href=/people/r/rajiv-ratan-saha/>Rajiv Ratan Saha</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6459><div class="card-body p-3 small">A huge amount of valuable resources is available on the web in English, which are often translated into local languages to facilitate knowledge sharing among local people who are not much familiar with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. However, translating such content manually is very tedious, costly, and time-consuming process. To this end, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is an efficient approach to translate text without any human involvement. Neural machine translation (NMT) is one of the most recent and effective <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation technique</a> amongst all existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. In this paper, we apply NMT for English-Tamil language pair. We propose a novel neural machine translation technique using word-embedding along with Byte-Pair-Encoding (BPE) to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for languages which do not have much translations available online. We use the BLEU score for evaluating the <a href=https://en.wikipedia.org/wiki/System>system</a> performance. Experimental results confirm that our proposed MIDAS translator (8.33 BLEU score) outperforms <a href=https://en.wikipedia.org/wiki/Google_Translate>Google translator</a> (3.75 BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6460 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6460/>The Benefit of Pseudo-Reference Translations in Quality Estimation of MT Output<span class=acl-fixed-case>MT</span> Output</a></strong><br><a href=/people/m/melania-duma/>Melania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6460><div class="card-body p-3 small">In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before. Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair. They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6461 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6461/>Supervised and Unsupervised Minimalist Quality Estimators : Vicomtech’s Participation in the WMT 2018 Quality Estimation Task<span class=acl-fixed-case>WMT</span> 2018 Quality Estimation Task</a></strong><br><a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/e/eva-martinez-garcia/>Eva Martínez Garcia</a>
|
<a href=/people/a/andoni-azpeitia/>Andoni Azpeitia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6461><div class="card-body p-3 small">We describe Vicomtech&#8217;s participation in the WMT 2018 shared task on quality estimation, for which we submitted minimalist quality estimators. The core of our approach is based on two simple features : <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical translation overlaps</a> and language model cross-entropy scores. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are exploited in two system variants : uMQE is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a>, where the final quality score is obtained by averaging individual feature scores ; sMQE is a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised variant</a>, where the final score is estimated by a Support Vector Regressor trained on the available annotated datasets. The main goal of our minimalist approach to quality estimation is to provide reliable estimators that require minimal deployment effort, few resources, and, in the case of uMQE, do not depend on costly data annotation or <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Our approach was applied to all language pairs in sentence quality estimation, obtaining competitive results across the board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6463 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6463/>Sheffield Submissions for the WMT18 Quality Estimation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for the <span class=acl-fixed-case>WMT</span>18 Quality Estimation Shared Task</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6463><div class="card-body p-3 small">In this paper we present the University of Sheffield submissions for the WMT18 Quality Estimation shared task. We discuss our submissions to all four sub-tasks, where ours is the only team to participate in all language pairs and variations (37 combinations). Our <a href=https://en.wikipedia.org/wiki/System>systems</a> show competitive results and outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> in nearly all cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6464 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6464/>UAlacant machine translation quality estimation at WMT 2018 : a simple approach using phrase tables and feed-forward neural networks<span class=acl-fixed-case>UA</span>lacant machine translation quality estimation at <span class=acl-fixed-case>WMT</span> 2018: a simple approach using phrase tables and feed-forward neural networks</a></strong><br><a href=/people/f/felipe-sanchez-martinez/>Felipe Sánchez-Martínez</a>
|
<a href=/people/m/miquel-espla-gomis/>Miquel Esplà-Gomis</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6464><div class="card-body p-3 small">We describe the Universitat d&#8217;Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.<i>OK</i> or <i>BAD</i>, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6465 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6465/>Alibaba Submission for WMT18 Quality Estimation Task<span class=acl-fixed-case>A</span>libaba Submission for <span class=acl-fixed-case>WMT</span>18 Quality Estimation Task</a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/k/kai-fan/>Kai Fan</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/f/fengming-zhou/>Fengming Zhou</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6465><div class="card-body p-3 small">The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The <a href=https://en.wikipedia.org/wiki/System>system</a> has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6466 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6466/>Quality Estimation with Force-Decoded Attention and Cross-lingual Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6466><div class="card-body p-3 small">This paper describes the submissions of the team from the University of Tartu for the sentence-level Quality Estimation shared task of WMT18. The proposed models use features based on attention weights of a neural machine translation system and cross-lingual phrase embeddings as input features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Two of the proposed models require only a neural machine translation system with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> with no additional resources. Results show that combining <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and baseline features leads to significant improvements over the baseline features alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6467 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6467/>MS-UEdin Submission to the WMT2018 APE Shared Task : Dual-Source Transformer for Automatic Post-Editing<span class=acl-fixed-case>MS</span>-<span class=acl-fixed-case>UE</span>din Submission to the <span class=acl-fixed-case>WMT</span>2018 <span class=acl-fixed-case>APE</span> Shared Task: Dual-Source Transformer for Automatic Post-Editing</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6467><div class="card-body p-3 small">This paper describes the Microsoft and University of Edinburgh submission to the Automatic Post-editing shared task at WMT2018. Based on training data and systems from the WMT2017 shared task, we re-implement our own models from the last shared task and introduce improvements based on extensive parameter sharing. Next we experiment with our implementation of dual-source transformer models and data selection for the IT domain. Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6468 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6468/>A Transformer-Based Multi-Source Automatic Post-Editing System</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6468><div class="card-body p-3 small">This paper presents our EnglishGerman Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture : two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt) for generating the post-edited sentence. We compare this multi-source architecture (i.e, <a href=https://en.wikipedia.org/wiki/Source_code>src</a>, mt pe) to a monolingual transformer (i.e., mt pe) model and an ensemble combining the multi-source src, mt pe and single-source mt pe models. For both the PBSMT and the NMT task, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> yields the best results, followed by the multi-source model and last the single-source approach. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a>, achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6469 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6469/>DFKI-MLT System Description for the WMT18 Automatic Post-editing Task<span class=acl-fixed-case>DFKI</span>-<span class=acl-fixed-case>MLT</span> System Description for the <span class=acl-fixed-case>WMT</span>18 Automatic Post-editing Task</a></strong><br><a href=/people/d/daria-pylypenko/>Daria Pylypenko</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6469><div class="card-body p-3 small">This paper presents the Automatic Post-editing (APE) systems submitted by the DFKI-MLT group to the WMT&#8217;18 APE shared task. Three monolingual neural sequence-to-sequence APE systems were trained using target-language data only : one using an attentional recurrent neural network architecture and two using the attention-only (transformer) architecture. The <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.<i>transformer</i>) architecture. The training data was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6472 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6472/>The Speechmatics Parallel Corpus Filtering System for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-ash/>Tom Ash</a>
|
<a href=/people/r/remi-francis/>Remi Francis</a>
|
<a href=/people/w/will-williams/>Will Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6472><div class="card-body p-3 small">Our entry to the parallel corpus filtering task uses a two-step strategy. The first step uses a series of pragmatic hard &#8216;rules&#8217; to remove the worst example sentences. This first step reduces the effective corpus size down from the initial 1 billion to 160 million tokens. The second step uses four different <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a> weighted to produce a score that is then used for further filtering down to 100 or 10 million tokens. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> produces competitive results without requiring excessive fine tuning to the exact task or language pair. The first step in isolation provides a very fast filter that gives most of the gains of the final <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6477 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6477/>An Unsupervised System for Parallel Corpus Filtering</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6477><div class="card-body p-3 small">In this paper we describe LMU Munich&#8217;s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.<i>WMT 2018 Parallel Corpus Filtering</i> shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6478 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6478/>Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6478><div class="card-body p-3 small">In this work we introduce dual conditional cross-entropy filtering for noisy parallel data. For each sentence pair of the noisy parallel corpus we compute cross-entropy scores according to two inverse translation models trained on clean data. We penalize divergent cross-entropies and weigh the penalty by the cross-entropy average of both models. Sorting or thresholding according to these scores results in better subsets of parallel data. We achieve higher BLEU scores with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on parallel data filtered only from Paracrawl than with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean WMT data. We further evaluate our method in the context of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6480 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6480/>Measuring sentence parallelism using Mahalanobis distances : The NRC unsupervised submissions to the WMT18 Parallel Corpus Filtering shared task<span class=acl-fixed-case>NRC</span> unsupervised submissions to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6480><div class="card-body p-3 small">The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a clean corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the only sample of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel text</a> in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one tasktranslating the European Medicines Agency corpus (Tiedemann, 2009)scored among the best systems even in the 10M-word conditions.<i>only</i> sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task&#8212;translating the European Medicines Agency corpus (Tiedemann, 2009)&#8212;scored among the best systems even in the 10M-word conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6481 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6481/>Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric : The NRC supervised submissions to the Parallel Corpus Filtering task<span class=acl-fixed-case>NRC</span> supervised submissions to the Parallel Corpus Filtering task</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6481><div class="card-body p-3 small">We present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSia novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-million-word evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing systemNRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6482 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6482/>Alibaba Submission to the WMT18 Parallel Corpus Filtering Task<span class=acl-fixed-case>A</span>libaba Submission to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering Task</a></strong><br><a href=/people/j/jun-lu/>Jun Lu</a>
|
<a href=/people/x/xiaoyu-lv/>Xiaoyu Lv</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6482><div class="card-body p-3 small">This paper describes the Alibaba Machine Translation Group submissions to the WMT 2018 Shared Task on Parallel Corpus Filtering. While evaluating the quality of the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, the three characteristics of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> are investigated, i.e. 1) the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual / translation quality</a>, 2) the <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual quality</a> and 3) the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus diversity</a>. Both rule-based and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6483 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6483/>UTFPR at WMT 2018 : Minimalistic Supervised Corpora Filtering for Machine Translation<span class=acl-fixed-case>UTFPR</span> at <span class=acl-fixed-case>WMT</span> 2018: Minimalistic Supervised Corpora Filtering for Machine Translation</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6483><div class="card-body p-3 small">We present the UTFPR systems at the WMT 2018 parallel corpus filtering task. Our supervised approach discerns between good and bad translations by training classic binary classification models over an artificially produced binary classification dataset derived from a high-quality translation set, and a minimalistic set of 6 semantic distance features that rely only on easy-to-gather resources. We rank translations by their probability for the good label. Our results show that <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> pairs best with our approach, yielding more consistent results throughout the different settings evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6484 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6484/>The ILSP / ARC submission to the WMT 2018 Parallel Corpus Filtering Shared Task<span class=acl-fixed-case>ILSP</span>/<span class=acl-fixed-case>ARC</span> submission to the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Shared Task</a></strong><br><a href=/people/v/vassilis-papavassiliou/>Vassilis Papavassiliou</a>
|
<a href=/people/s/sokratis-sofianopoulos/>Sokratis Sofianopoulos</a>
|
<a href=/people/p/prokopis-prokopidis/>Prokopis Prokopidis</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6484><div class="card-body p-3 small">This paper describes the submission of the Institute for Language and Speech Processing / Athena Research and Innovation Center (ILSP / ARC) for the WMT 2018 Parallel Corpus Filtering shared task. We explore several properties of sentences and sentence pairs that our system explored in the context of the task with the purpose of clustering sentence pairs according to their appropriateness in training MT systems. We also discuss alternative methods for ranking the sentence pairs of the most appropriate clusters with the aim of generating the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (of 10 and 100 million words as required in the task) that were evaluated. By summarizing the results of several experiments that were carried out by the organizers during the evaluation phase, our submission achieved an average BLEU score of 26.41, even though it does not make use of any language-specific resources like <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual lexica</a>, monolingual corpora, or MT output, while the average score of the best participant system was 27.91.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6485 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6485/>SYSTRAN Participation to the WMT2018 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>SYSTRAN</span> Participation to the <span class=acl-fixed-case>WMT</span>2018 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6485><div class="card-body p-3 small">This paper describes the participation of <a href=https://en.wikipedia.org/wiki/SYSTRAN>SYSTRAN</a> to the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018). We participate for the first time using a neural sentence similarity classifier which aims at predicting the relatedness of sentence pairs in a multilingual context. The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6487 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6487/>The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Filtering System for the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Task</a></strong><br><a href=/people/n/nick-rossenbach/>Nick Rossenbach</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/a/aman-gokrani/>Aman Gokrani</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6487><div class="card-body p-3 small">This paper describes the submission of RWTH Aachen University for the DeEn parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence pairs</a> are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural language models</a> and <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a> based on the transformer architecture. A <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on 10 M randomly sampled tokens reaches a performance of 9.2 % BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8 % BLEU.<i>EMNLP 2018 Third Conference on Machine Translation</i> (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6488 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6488" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6488/>Prompsit’s submission to WMT 2018 Parallel Corpus Filtering shared task<span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a>
|
<a href=/people/m/marta-banon/>Marta Bañón</a>
|
<a href=/people/s/sergio-ortiz-rojas/>Sergio Ortiz-Rojas</a>
|
<a href=/people/g/gema-ramirez-sanchez/>Gema Ramírez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6488><div class="card-body p-3 small">This paper describes Prompsit Language Engineering&#8217;s submissions to the WMT 2018 parallel corpus filtering shared task. Our four submissions were based on an automatic classifier for identifying pairs of sentences that are mutual translations. A set of hand-crafted hard rules for discarding sentences with evident flaws were applied before the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. We explored different strategies for achieving a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> with diverse vocabulary and fluent sentences : language model scoring, an active-learning-inspired data selection algorithm and n-gram saturation. Our submissions were very competitive in comparison with other participants on the 100 million word training corpus.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>