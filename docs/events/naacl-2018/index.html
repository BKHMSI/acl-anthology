<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>North American Chapter of the Association for Computational Linguistics (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>North American Chapter of the Association for Computational Linguistics (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#n18-1>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a>
<span class="badge badge-info align-middle ml-1">122&nbsp;papers</span></li><li><a class=align-middle href=#n18-2>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a>
<span class="badge badge-info align-middle ml-1">76&nbsp;papers</span></li><li><a class=align-middle href=#n18-3>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#n18-4>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#n18-5>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#n18-6>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-05>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">31&nbsp;papers</span></li><li><a class=align-middle href=#w18-06>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-07>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-08>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-09>Proceedings of the Workshop on Figurative Language Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-10>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-11>Proceedings of the Second Workshop on Computational Modeling of Peopleâ€™s Opinions, Personality, and Emotions in Social Media</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-12>Proceedings of the Second Workshop on Subword/Character LEvel Models</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-13>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-14>Proceedings of the First International Workshop on Spatial Language Understanding</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-15>Proceedings of the First Workshop on Storytelling</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w18-16>Proceedings of the Second Workshop on Stylistic Variation</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-17>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li></ul></div></div><div id=n18-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-1/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></strong><br><a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276388781 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1001/>Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition</a></strong><br><a href=/people/z/zhenghui-wang/>Zhenghui Wang</a>
|
<a href=/people/y/yanru-qu/>Yanru Qu</a>
|
<a href=/people/l/liheng-chen/>Liheng Chen</a>
|
<a href=/people/j/jian-shen/>Jian Shen</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/s/shaodian-zhang/>Shaodian Zhang</a>
|
<a href=/people/y/yimei-gao/>Yimei Gao</a>
|
<a href=/people/g/gen-gu/>Gen Gu</a>
|
<a href=/people/k/ken-chen/>Ken Chen</a>
|
<a href=/people/y/yong-yu/>Yong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1001><div class="card-body p-3 small">We study the problem of named entity recognition (NER) from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical records</a>, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components : (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276389935 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1002/>Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/d/denilson-barbosa/>Denilson Barbosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1002><div class="card-body p-3 small">The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a> to entity mentions in text. Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence. Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features. Instead, we propose an end-to-end solution with a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that uses a variant of cross-entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones. Also, previous work solve FETC a <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> followed by ad-hoc post-processing. In contrast, our solution is more elegant : we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and consistently outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on established <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276386708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1004/>A Deep Generative Model of Vowel Formant Typology</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1004><div class="card-body p-3 small">What makes some types of <a href=https://en.wikipedia.org/wiki/Language>languages</a> more probable than others? For instance, we know that almost all <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a> contain the <a href=https://en.wikipedia.org/wiki/Vowel>vowel phoneme</a> /i/ ; why should that be? The field of <a href=https://en.wikipedia.org/wiki/Linguistic_typology>linguistic typology</a> seeks to answer these questions and, thereby, divine the mechanisms that underlie <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. In our work, we tackle the problem of vowel system typology, i.e., we propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative probability model</a> of which vowels a language contains. In contrast to previous work, we work directly with the acoustic informationthe first two formant valuesrather than modeling discrete sets of symbols from the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>international phonetic alphabet</a>. We develop a novel generative probability model and report results on over 200 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276387788 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1005/>Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/j/jesus-manuel-mager-hois/>Jesus Manuel Mager Hois</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza-Ruiz</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich SchÃ¼tze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1005><div class="card-body p-3 small">Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings. We then propose two novel multi-task training approachesone with, one without need for external unlabeled resources, and two corresponding data augmentation methods, improving over the neural baseline for all languages. Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75 %. We provide our morphological segmentation datasets for <a href=https://en.wikipedia.org/wiki/Mexicanero_language>Mexicanero</a>, <a href=https://en.wikipedia.org/wiki/Nahuatl>Nahuatl</a>, Wixarika and Yorem Nokki for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276415442 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1006/>Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation</a></strong><br><a href=/people/p/peyman-passban/>Peyman Passban</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1006><div class="card-body p-3 small">Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, an additional morphology table is plugged into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Each time the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> samples from a target vocabulary, the table sends auxiliary signals from the most relevant <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> in order to enrich the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>&#8217;s current state and constrain it to provide better predictions. We evaluated our model to translate <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as three MRLs and observed significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276439724 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1007/>Parsing Speech : a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information</a></strong><br><a href=/people/t/trang-tran/>Trang Tran</a>
|
<a href=/people/s/shubham-toshniwal/>Shubham Toshniwal</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1007><div class="card-body p-3 small">In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276441141 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1008/>Tied Multitask Learning for Neural Speech Translation</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1008><div class="card-body p-3 small">We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> where the second task decoder receives information from the decoder of the first <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, since higher-level intermediate representations should provide useful information. Second, we apply <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> that encourages transitivity and <a href=https://en.wikipedia.org/wiki/Inverse_problem>invertibility</a>. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. It also leads to better performance when using <a href=https://en.wikipedia.org/wiki/Attentional_control>attention information</a> for word discovery over unsegmented input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1010/>Attentive Interaction Model : Modeling Changes in View in Argumentation</a></strong><br><a href=/people/y/yohan-jo/>Yohan Jo</a>
|
<a href=/people/s/shivani-poddar/>Shivani Poddar</a>
|
<a href=/people/b/byungsoo-jeon/>Byungsoo Jeon</a>
|
<a href=/people/q/qinlan-shen/>Qinlan Shen</a>
|
<a href=/people/c/carolyn-rose/>Carolyn RosÃ©</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1010><div class="card-body p-3 small">We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder&#8217;s (OH&#8217;s) reasoning and a challenger&#8217;s argument, with the goal of predicting if the argument successfully changes the OH&#8217;s view. The model has two components : (1) vulnerable region detection, an attention model that identifies parts of the OH&#8217;s reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH&#8217;s reasoning and that of the challenger&#8217;s argument. Based on evaluation on discussions from the Change My View forum on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, the two components work together to predict an OH&#8217;s change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1011/>Automatic Focus Annotation : Bringing Formal Pragmatics Alive in Analyzing the Information Structure of Authentic Data</a></strong><br><a href=/people/r/ramon-ziai/>Ramon Ziai</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1011><div class="card-body p-3 small">Analyzing language in context, both from a theoretical and from a computational perspective, is receiving increased interest. Complementing the research in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> on discourse and information structure, in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> identifying discourse concepts was also shown to improve the performance of certain applications, for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building on the research that established detailed annotation guidelines for manual annotation of information structural concepts for written (Dipper et al., 2007 ; Ziai and Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents the first approach automating the analysis of focus in authentic written data. Our classification approach combines a range of lexical, syntactic, and semantic features to achieve an accuracy of 78.1 % for identifying focus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1012.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1012/>Dear Sir or Madam, May I Introduce the GYAFC Dataset : Corpus, Benchmarks and Metrics for Formality Style Transfer<span class=acl-fixed-case>I</span> Introduce the <span class=acl-fixed-case>GYAFC</span> Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1012><div class="card-body p-3 small">Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and evaluation datasets</a>, as well as benchmarks and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. In this work, we create the largest <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1013/>Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph</a></strong><br><a href=/people/z/zeyu-dai/>Zeyu Dai</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1013><div class="card-body p-3 small">We argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph, or independently from all discourse relations and the overall paragraph-level discourse structure. With the goal of improving implicit discourse relation classification, we introduce a paragraph-level neural networks that model inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predict a sequence of discourse relations in a paragraph. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous state-of-the-art systems on the benchmark corpus of PDTB.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1014/>A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation</a></strong><br><a href=/people/j/juraj-juraska/>Juraj Juraska</a>
|
<a href=/people/p/panagiotis-karagiannis/>Panagiotis Karagiannis</a>
|
<a href=/people/k/kevin-bowden/>Kevin Bowden</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1014><div class="card-body p-3 small">Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Using a range of automatic metrics, as well as <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human evaluators</a>, we show that our approach achieves better results than state-of-the-art models on the same <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1023.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1023/>Looking Beyond the Surface : A Challenge Set for Reading Comprehension over Multiple Sentences</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1023><div class="card-body p-3 small">We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500 + questions for 1000 + paragraphs across 7 different domains (elementary school science, <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Guide_book>travel guides</a>, <a href=https://en.wikipedia.org/wiki/Narrative>fiction stories</a>, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we found <a href=https://en.wikipedia.org/wiki/Solver>human solvers</a> to achieve an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 88.1 %. We analyze a range of <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baselines</a>, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1024/>Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input</a></strong><br><a href=/people/y/youmna-farag/>Youmna Farag</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1024><div class="card-body p-3 small">We demonstrate that current state-of-the-art approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences, and propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1025/>QuickEdit : Editing Text & Translations by Crossing Words Out<span class=acl-fixed-case>Q</span>uick<span class=acl-fixed-case>E</span>dit: Editing Text & Translations by Crossing Words Out</a></strong><br><a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1025><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for computer-assisted text editing. It applies to translation post-editing and to <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a>. Our proposal relies on very simple interactions : a human editor modifies a sentence by marking tokens they would like the system to change. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> which takes as input a sentence along with change markers. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> through a <a href=https://en.wikipedia.org/wiki/User_study>user study</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1026/>Tempo-Lexical Context Driven Word Embedding for Cross-Session Search Task Extraction</a></strong><br><a href=/people/p/procheta-sen/>Procheta Sen</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1026><div class="card-body p-3 small">Task extraction is the process of identifying search intents over a set of queries potentially spanning multiple search sessions. Most existing research on task extraction has focused on identifying tasks within a single session, where the notion of a session is defined by a fixed length time window. By contrast, in this work we seek to identify <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that span across multiple sessions. To identify tasks, we conduct a <a href=https://en.wikipedia.org/wiki/Global_analysis>global analysis</a> of a query log in its entirety without restricting analysis to individual temporal windows. To capture inherent task semantics, we represent queries as vectors in an <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract space</a>. We learn the embedding of query words in this space by leveraging the temporal and lexical contexts of queries. Embedded query vectors are then clustered into tasks. Experiments demonstrate that task extraction effectiveness is improved significantly with our proposed method of query vector embedding in comparison to existing approaches that make use of documents retrieved from a collection to estimate semantic similarities between queries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1028.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276395060 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1028/>Variable Typing : Assigning Meaning to Variables in Mathematical Text</a></strong><br><a href=/people/y/yiannos-stathopoulos/>Yiannos Stathopoulos</a>
|
<a href=/people/s/simon-baker/>Simon Baker</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1028><div class="card-body p-3 small">Information about the meaning of mathematical variables in text is useful in NLP / IR tasks such as symbol disambiguation, <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and mathematical information retrieval (MIR). We introduce variable typing, the task of assigning one mathematical type (multi-word technical terms referring to mathematical concepts) to each variable in a sentence of mathematical text. As part of this work, we also introduce a new annotated data set composed of 33,524 data points extracted from scientific documents published on arXiv. Our intrinsic evaluation demonstrates that our <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> is sufficient to successfully train and evaluate current <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> from three different model architectures. The best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is evaluated on an extrinsic task : MIR, by producing a typed formula index. Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing.<i>variable typing</i>, the task of assigning one\n <i>mathematical type</i> (multi-word technical terms referring\n to mathematical concepts) to each variable in a sentence of\n mathematical text. As part of this work, we also introduce a new\n annotated data set composed of 33,524 data points extracted from\n scientific documents published on arXiv. Our intrinsic\n evaluation demonstrates that our data set is sufficient to\n successfully train and evaluate current classifiers from three\n different model architectures. The best performing model is\n evaluated on an extrinsic task: MIR, by producing a <i>typed formula index</i>. Our results show that the best performing MIR\n models make use of our typed index, compared to a formula index\n only containing raw symbols, thereby demonstrating the\n usefulness of variable typing.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276416920 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1029/>Learning beyond Datasets : Knowledge Graph Augmented Neural Networks for Natural Language Processing</a></strong><br><a href=/people/a/annervaz-k-m/>Annervaz K M</a>
|
<a href=/people/s/somnath-basu-roy-chowdhury/>Somnath Basu Roy Chowdhury</a>
|
<a href=/people/a/ambedkar-dukkipati/>Ambedkar Dukkipati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1029><div class="card-body p-3 small">Machine Learning has been the quintessential solution for many AI problems, but <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> are heavily dependent on specific training data. Some <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> can be incorporated with prior knowledge using a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian setup</a>, but these <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> that can extract relevant prior support facts from <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> depending on the task using attention mechanism. We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276448004 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1032/>Universal Neural Machine Translation for Extremely Low Resource Languages</a></strong><br><a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a>
|
<a href=/people/j/jacob-devlin/>Jacob Devlin</a>
|
<a href=/people/v/victor-o-k-li/>Victor O.K. Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1032><div class="card-body p-3 small">In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data. Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language. The lexical part is shared through a Universal Lexical Representation to support multi-lingual word-level sharing. The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages. This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multi-lingual training and back-translation. Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276397933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1034" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1034/>Deep Dirichlet Multinomial Regression<span class=acl-fixed-case>D</span>irichlet Multinomial Regression</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1034><div class="card-body p-3 small">Dirichlet Multinomial Regression (DMR) and other supervised topic models can incorporate arbitrary document-level features to inform topic priors. However, their ability to model corpora are limited by the representation and selection of these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> a choice the topic modeler must make. Instead, we seek <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can learn the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature representations</a> upon which to condition topic selection. We present deep Dirichlet Multinomial Regression (dDMR), a generative topic model that simultaneously learns document feature representations and topics. We evaluate dDMR on three datasets : New York Times articles with fine-grained tags, Amazon product reviews with product images, and Reddit posts with subreddit identity. dDMR learns representations that outperform DMR and LDA according to heldout perplexity and are more effective at downstream predictive tasks as the number of topics grows. Additionally, human subjects judge dDMR topics as being more representative of associated document features. Finally, we find that supervision leads to faster convergence as compared to an LDA baseline and that dDMR&#8217;s model fit is less sensitive to training parameters than DMR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276422518 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1035/>Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/n/nicholas-beauchamp/>Nicholas Beauchamp</a>
|
<a href=/people/s/sarah-shugars/>Sarah Shugars</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1035><div class="card-body p-3 small">Millions of conversations are generated every day on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. With limited attention, it is challenging for users to select which discussions they would like to participate in. Here we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for microblog conversation recommendation. While much prior work has focused on post-level recommendation, we exploit both the conversational context, and user content and behavior preferences. We propose a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> that jointly captures : (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics. Experimental results on two Twitter datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms methods that only model content without considering <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1036.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276425357 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1036/>Before Name-Calling : Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1036><div class="card-body p-3 small">Arguing without committing a fallacy is one of the main requirements of an ideal debate. But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument. As existing research lacks solid empirical investigation of the typology of <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominem arguments</a> as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominem</a> using explainable neural network architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276453229 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1037/>Scene Graph Parsing as Dependency Parsing</a></strong><br><a href=/people/y/yu-siang-wang/>Yu-Siang Wang</a>
|
<a href=/people/c/chenxi-liu/>Chenxi Liu</a>
|
<a href=/people/x/xiaohui-zeng/>Xiaohui Zeng</a>
|
<a href=/people/a/alan-yuille/>Alan Yuille</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1037><div class="card-body p-3 small">In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions. In particular, we consider the scene graph representation that considers objects together with their attributes and relations : this <a href=https://en.wikipedia.org/wiki/Representation_theory>representation</a> has been proved useful across a variety of vision and language applications. We begin by introducing an alternative but equivalent edge-centric view of scene graphs that connect to dependency parses. Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-to-end training. The scene graphs generated by our learned neural dependency parser achieve an <a href=https://en.wikipedia.org/wiki/F-score>F-score similarity</a> of 49.67 % to <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>ground truth graphs</a> on our evaluation set, surpassing best previous approaches by 5 %. We further demonstrate the effectiveness of our learned <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on image retrieval applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631187 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1039/>Comparatives, Quantifiers, Proportions : a Multi-Task Model for the Learning of Quantities from Vision</a></strong><br><a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1039><div class="card-body p-3 small">The present work investigates whether different quantification mechanisms (set comparison, vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target / non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1040.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276455887 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1040/>Being Negative but Constructively : Lessons Learnt from Creating Better Visual Question Answering Datasets</a></strong><br><a href=/people/w/wei-lun-chao/>Wei-Lun Chao</a>
|
<a href=/people/h/hexiang-hu/>Hexiang Hu</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1040><div class="card-body p-3 small">Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> should strive to achieve. In this paper, we study a crucial component of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> : how can we design good <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>? We focus on the design of <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice based datasets</a> where the learner has to select the right answer from a set of candidate ones including the target (i.e., the correct one) and the decoys (i.e., the incorrect ones). Through careful analysis of the results attained by state-of-the-art <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> learn from the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In particular, the resulting learner can ignore the <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a>, the question, or both while still doing well on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via.<url>http://www.teds.usc.edu/website_vqa/</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1041/>Abstract Meaning Representation for <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>Paraphrase Detection</a><span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Paraphrase Detection</a></strong><br><a href=/people/f/fuad-issa/>Fuad Issa</a>
|
<a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/x/xiaohui-yan/>Xiaohui Yan</a>
|
<a href=/people/y/yi-chang/>Yi Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1041><div class="card-body p-3 small">Abstract Meaning Representation (AMR) parsing aims at abstracting away from the syntactic realization of a sentence, and denote only its meaning in a canonical form. As such, it is ideal for <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, a problem in which one is required to specify whether two sentences have the same meaning. We show that nave use of AMR in <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> is not necessarily useful, and turn to describe a technique based on latent semantic analysis in combination with AMR parsing that significantly advances state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> for the Microsoft Research Paraphrase Corpus. Our best results in the transductive setting are 86.6 % for <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 90.0 % for <a href=https://en.wikipedia.org/wiki/F-number>F_1 measure</a>.<tex-math>_1</tex-math> measure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1043/>Can Network Embedding of Distributional Thesaurus Be Combined with Word Vectors for Better Representation?</a></strong><br><a href=/people/a/abhik-jana/>Abhik Jana</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1043><div class="card-body p-3 small">Distributed representations of words learned from text have proved to be successful in various natural language processing tasks in recent times. While some methods represent words as vectors computed from text using predictive model (Word2vec) or dense count based model (GloVe), others attempt to represent these in a distributional thesaurus network structure where the neighborhood of a word is a set of words having adequate context overlap. Being motivated by recent surge of research in network embedding techniques (DeepWalk, LINE, node2vec etc.), we turn a distributional thesaurus network into dense word vectors and investigate the usefulness of distributional thesaurus embedding in improving overall word representation. This is the first attempt where we show that combining the proposed word representation obtained by distributional thesaurus embedding with the state-of-the-art word representations helps in improving the performance by a significant margin when evaluated against NLP tasks like word similarity and relatedness, synonym detection, analogy detection. Additionally, we show that even without using any handcrafted lexical resources we can come up with <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> having comparable performance in the word similarity and relatedness tasks compared to the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> where a lexical resource has been used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1044.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1044/>Deep Neural Models of Semantic Shift</a></strong><br><a href=/people/a/alex-rosenfeld/>Alex Rosenfeld</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1044><div class="card-body p-3 small">Diachronic distributional models track changes in word use over time. In this paper, we propose a deep neural network diachronic distributional model. Instead of modeling lexical change via a <a href=https://en.wikipedia.org/wiki/Time_series>time series</a> as is done in previous work, we represent <a href=https://en.wikipedia.org/wiki/Time>time</a> as a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous variable</a> and model a word&#8217;s usage as a function of time. Additionally, we have also created a novel synthetic task which measures a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to capture the semantic trajectory. This evaluation quantitatively measures how well a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> captures the semantic trajectory of a word over time. Finally, we explore how well the <a href=https://en.wikipedia.org/wiki/Derivative>derivatives</a> of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used to measure the speed of lexical change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1045.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1045/>Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection</a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/z/ziyun-wang/>Ziyun Wang</a>
|
<a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1045><div class="card-body p-3 small">Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, relation extraction, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Supervised learning from labeled hypernym sources, such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, limits the coverage of these models, which can be addressed by learning <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from unlabeled text. Existing <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are awareevaluating on 11 datasets using multiple existing as well as newly proposed scoring functionswe find that our method provides up to double the precision of previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1046/>Mining Possessions : Existence, Type and Temporal Anchors</a></strong><br><a href=/people/d/dhivya-chinnappa/>Dhivya Chinnappa</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1046><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and experiments to mine <a href=https://en.wikipedia.org/wiki/Possession_(linguistics)>possession relations</a> from text. Specifically, we target alienable and control possessions, and assign temporal anchors indicating when the possession holds between possessor and possessee. We present new <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for this task, and experimental results using both traditional <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Results show that the three subtasks (predicting possession existence, possession type and temporal anchors) can be automated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1047/>Neural Tensor Networks with Diagonal Slice Matrices</a></strong><br><a href=/people/t/takahiro-ishihara/>Takahiro Ishihara</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/h/hitoshi-manabe/>Hitoshi Manabe</a>
|
<a href=/people/m/masashi-shimbo/>Masashi Shimbo</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1047><div class="card-body p-3 small">Although neural tensor networks (NTNs) have been successful in many NLP tasks, they require a large number of parameters to be estimated, which often leads to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and a long training time. We address these issues by applying <a href=https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>eigendecomposition</a> to each slice matrix of a <a href=https://en.wikipedia.org/wiki/Tensor>tensor</a> to reduce its number of paramters. First, we evaluate our proposed NTN models on knowledge graph completion. Second, we extend the models to recursive NTNs (RNTNs) and evaluate them on logical reasoning tasks. These experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn better and faster than the original (R)NTNs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1048/>Post-Specialisation : Retrofitting Vectors of Words Unseen in Lexical Resources</a></strong><br><a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a>
|
<a href=/people/g/goran-glavas/>Goran GlavaÅ¡</a>
|
<a href=/people/n/nikola-mrksic/>Nikola MrkÅ¡iÄ‡</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1048><div class="card-body p-3 small">Word vector specialisation (also known as retrofitting) is a portable, light-weight approach to fine-tuning arbitrary distributional word vector spaces by injecting external knowledge from rich lexical resources such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. By design, these post-processing methods only update the vectors of words occurring in external lexicons, leaving the representations of all unseen words intact. In this paper, we show that constraint-driven vector space specialisation can be extended to unseen words. We propose a novel post-specialisation method that : a) preserves the useful linguistic knowledge for seen words ; while b) propagating this external signal to unseen words in order to improve their vector representations as well. Our post-specialisation approach explicits a non-linear specialisation function in the form of a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> by learning to predict specialised vectors from their original distributional counterparts. The learned <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a> is then used to specialise vectors of unseen words. This approach, applicable to any post-processing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks : dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1052/>Relevant Emotion Ranking from Text Constrained with Emotion Relationships</a></strong><br><a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/y/yang-yang/>Yang Yang</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1052><div class="card-body p-3 small">Text might contain or invoke multiple emotions with varying intensities. As such, <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>, to predict multiple <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> associated with a given text, can be cast into a multi-label classification problem. We would like to go one step further so that a ranked list of relevant emotions are generated where top ranked emotions are more intensely associated with text compared to lower ranked emotions, whereas the rankings of irrelevant emotions are not important. A novel framework of relevant emotion ranking is proposed to tackle the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. In the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, the <a href=https://en.wikipedia.org/wiki/Loss_function>objective loss function</a> is designed elaborately so that both emotion prediction and <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a> of only relevant emotions can be achieved. Moreover, we observe that some emotions co-occur more often while other emotions rarely co-exist. Such information is incorporated into the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>. Experimental results on two real-world corpora show that the proposed framework can effectively deal with <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> and performs remarkably better than the state-of-the-art <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection approaches</a> and multi-label learning methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1053/>Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality and Multi-Linguality</a></strong><br><a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/p/palaash-sawant/>Palaash Sawant</a>
|
<a href=/people/s/sukanta-sen/>Sukanta Sen</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1053><div class="card-body p-3 small">Efficient word representations play an important role in solving various problems related to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, <a href=https://en.wikipedia.org/wiki/Data_mining>data mining</a>, <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a> etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the hand-crafted features for the <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. We show the efficacy of the proposed <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> against <a href=https://en.wikipedia.org/wiki/Scientific_method>state-of-the-art methods</a> in two experimental setups i.e. multi-lingual and cross-lingual.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1054.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1054" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1054/>SRL4ORL : Improving Opinion Role Labeling Using Multi-Task Learning with Semantic Role Labeling<span class=acl-fixed-case>SRL</span>4<span class=acl-fixed-case>ORL</span>: Improving Opinion Role Labeling Using Multi-Task Learning with Semantic Role Labeling</a></strong><br><a href=/people/a/ana-marasovic/>Ana MarasoviÄ‡</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1054><div class="card-body p-3 small">For over a decade, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> has been used to extract opinion-holder-target structures from text to answer the question Who expressed what kind of sentiment towards what?. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>labeled training data</a> and address this issue using different multi-task learning (MTL) techniques with a related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276371501 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1057/>Noising and Denoising Natural Language : Diverse Backtranslation for Grammar Correction</a></strong><br><a href=/people/z/ziang-xie/>Ziang Xie</a>
|
<a href=/people/g/guillaume-genthial/>Guillaume Genthial</a>
|
<a href=/people/s/stanley-xie/>Stanley Xie</a>
|
<a href=/people/a/andrew-y-ng/>Andrew Ng</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1057><div class="card-body p-3 small">Translation-based methods for grammar correction that directly map noisy, ungrammatical text to their clean counterparts are able to correct a broad range of errors ; however, such techniques are bottlenecked by the need for a large parallel corpus of noisy and clean sentence pairs. In this paper, we consider synthesizing <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> by noising a clean monolingual corpus. While most previous approaches introduce perturbations using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> computed from local context windows, we instead develop error generation processes using a neural sequence transduction model trained to translate clean examples to their noisy counterparts. Given a corpus of clean examples, we propose beam search noising procedures to synthesize additional noisy examples that human evaluators were nearly unable to discriminate from nonsynthesized examples. Surprisingly, when trained on additional <a href=https://en.wikipedia.org/wiki/Data>data</a> synthesized using our best-performing noising scheme, our model approaches the same performance as when trained on additional nonsynthesized data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1058/>Self-Training for Jointly Learning to Ask and Answer Questions</a></strong><br><a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1058><div class="card-body p-3 small">Building curious machines that can answer as well as ask questions is an important challenge for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>. The two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and question generation are usually tackled separately in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP literature</a>. At the same time, both require significant amounts of supervised data which is hard to obtain in many domains. To alleviate these issues, we propose a self-training method for jointly learning to ask as well as answer questions, leveraging unlabeled text along with labeled question answer pairs for <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We evaluate our approach on four benchmark datasets : SQUAD, MS MARCO, WikiQA and TrecQA, and show significant improvements over a number of established baselines on both question answering and question generation tasks. We also achieved new state-of-the-art results on two competitive answer sentence selection tasks : WikiQA and TrecQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276431486 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1060/>A Meaning-Based Statistical English Math Word Problem Solver<span class=acl-fixed-case>E</span>nglish Math Word Problem Solver</a></strong><br><a href=/people/c/chao-chun-liang/>Chao-Chun Liang</a>
|
<a href=/people/y/yu-shiang-wong/>Yu-Shiang Wong</a>
|
<a href=/people/y/yi-chung-lin/>Yi-Chung Lin</a>
|
<a href=/people/k/keh-yih-su/>Keh-Yih Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1060><div class="card-body p-3 small">We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper. It first analyzes the text, transforms both body and question parts into their corresponding <a href=https://en.wikipedia.org/wiki/Logical_form>logic forms</a>, and then performs <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on them. The associated context of each quantity is represented with proposed <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>role-tags</a> (e.g., <a href=https://en.wikipedia.org/wiki/Subjunctive_mood>nsubj</a>, <a href=https://en.wikipedia.org/wiki/Verb>verb</a>, etc.), which provides the flexibility for annotating an extracted <a href=https://en.wikipedia.org/wiki/Quantity>math quantity</a> with its associated <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> (i.e., the <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>physical meaning</a> of this quantity). Statistical models are proposed to select the <a href=https://en.wikipedia.org/wiki/Operator_(mathematics)>operator</a> and operands. A noisy dataset is designed to assess if a <a href=https://en.wikipedia.org/wiki/Solver>solver</a> solves MWPs mainly via understanding or mechanical pattern matching. Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1062/>Querying Word Embeddings for Similarity and Relatedness</a></strong><br><a href=/people/f/fatemeh-torabi-asr/>Fatemeh Torabi Asr</a>
|
<a href=/people/r/robert-zinkov/>Robert Zinkov</a>
|
<a href=/people/m/michael-jones/>Michael Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1062><div class="card-body p-3 small">Word embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data. Skipgram generates a set of word and context embeddings, the latter typically discarded after training. We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev & McRae, 2016). Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282318359 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1063/>Semantic Structural Evaluation for Text Simplification</a></strong><br><a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1063><div class="card-body p-3 small">Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA&#8217;s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1067/>Neural Models of Factuality</a></strong><br><a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1067><div class="card-body p-3 small">We present two neural models for event factuality prediction, which yield significant performance gains over previous models on three event factuality datasets : FactBank, UW, and MEANTIME. We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date. We report <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> results on this extended factuality dataset as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1069" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1069/>Acquisition of Phrase Correspondences Using Natural Deduction Proofs</a></strong><br><a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/p/pascual-martinez-gomez/>Pascual MartÃ­nez-GÃ³mez</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1069><div class="card-body p-3 small">How to identify, extract, and use phrasal knowledge is a crucial problem for the task of Recognizing Textual Entailment (RTE). To solve this problem, we propose a method for detecting paraphrases via natural deduction proofs of semantic relations between sentence pairs. Our solution relies on a graph reformulation of partial variable unifications and an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that induces subgraph alignments between meaning representations. Experiments show that our method can automatically detect various <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> that are absent from existing paraphrase databases. In addition, the detection of paraphrases using proof information improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of RTE tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1070/>Automatic Stance Detection Using End-to-End Memory Networks</a></strong><br><a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/l/lluis-marquez/>LluÃ­s MÃ rquez</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1070><div class="card-body p-3 small">We present an effective end-to-end memory network model that jointly (i) predicts whether a given document can be considered as relevant evidence for a given claim, and (ii) extracts snippets of evidence that can be used to reason about the factuality of the target claim. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines the advantages of convolutional and recurrent neural networks as part of a memory network. We further introduce a <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrix</a> at the inference level of the memory network in order to extract snippets of evidence for input claims more accurately. Our experiments on a public benchmark dataset, FakeNewsChallenge, demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1073/>Efficient <a href=https://en.wikipedia.org/wiki/Sequence_learning>Sequence Learning</a> with Group Recurrent Networks</a></strong><br><a href=/people/f/fei-gao/>Fei Gao</a>
|
<a href=/people/l/lijun-wu/>Lijun Wu</a>
|
<a href=/people/l/li-zhao/>Li Zhao</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1073><div class="card-body p-3 small">Recurrent neural networks have achieved state-of-the-art results in many artificial intelligence tasks, such as <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and so on. One of the key factors to these successes is big models. However, training such <a href=https://en.wikipedia.org/wiki/Big_data>big models</a> usually takes days or even weeks of time even if using tens of <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU cards</a>. In this paper, we propose an efficient architecture to improve the efficiency of such RNN model training, which adopts the <a href=https://en.wikipedia.org/wiki/Group_action_(mathematics)>group strategy</a> for recurrent layers, while exploiting the representation rearrangement strategy between layers as well as time steps. To demonstrate the advantages of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we conduct experiments on several datasets and tasks. The results show that our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves comparable or better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparing with <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, with a much smaller number of parameters and at a much lower <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1075" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1075/>Global Relation Embedding for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin GÃ¼r</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1075><div class="card-body p-3 small">We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> can be improved from 83.9 % to 89.3 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1077.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1077/>Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1077><div class="card-body p-3 small">Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We argue that this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> would gain from the availability of a <a href=https://en.wikipedia.org/wiki/Resource>resource</a> that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource a probabilistic knowledge base acquired in the news domain by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (19872007). We show that existing temporal extraction systems can be improved via this <a href=https://en.wikipedia.org/wiki/Resource>resource</a>. As a byproduct, we also show that interesting statistics can be retrieved from this <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resource</a>, which can potentially benefit other time-aware tasks. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> and <a href=https://en.wikipedia.org/wiki/Resource>resource</a> are both publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1078/>Multimodal Named Entity Recognition for Short Social Media Posts</a></strong><br><a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1078><div class="card-body p-3 small">We introduce a new task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> or Snapchat captions, which comprise short text with accompanying images. These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word / character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.<i>modality-attention</i>\n module which learns to attenuate irrelevant modalities while amplifying\n the most informative ones to extract contexts from, adaptive to each\n sample and token. The proposed MNER model with modality attention\n significantly outperforms the state-of-the-art text-only NER models by\n successfully leveraging provided visual contexts, opening up potential\n applications of MNER on myriads of social media platforms.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1079/>Nested Named Entity Recognition Revisited</a></strong><br><a href=/people/a/arzoo-katiyar/>Arzoo Katiyar</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1079><div class="card-body p-3 small">We propose a novel recurrent neural network-based approach to simultaneously handle nested named entity recognition and nested entity mention detection. The model learns a <a href=https://en.wikipedia.org/wiki/Hypergraph>hypergraph representation</a> for nested entities using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. In evaluations on three standard <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>, we show that our approach significantly outperforms existing state-of-the-art methods, which are feature-based. The approach is also efficient : <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> operates linearly in the number of tokens and the number of possible output labels at any token. Finally, we present an extension of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that jointly learns the head of each entity mention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1081/>Supervised Open Information Extraction</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/j/julian-michael/>Julian Michael</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1081><div class="card-body p-3 small">We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of <a href=https://en.wikipedia.org/wiki/Open_IE>Open IE</a> as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> outperforms the existing state-of-the-art Open IE systems on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1084" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1084/>Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses<span class=acl-fixed-case>M</span>onte <span class=acl-fixed-case>C</span>arlo Syntax Marginals for Exploring and Using Dependency Parses</a></strong><br><a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/b/brendan-oconnor/>Brendan Oâ€™Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1084><div class="card-body p-3 small">Dependency parsing research, which has made significant gains in recent years, typically focuses on improving the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of single-tree predictions. However, <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> is inherent to <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>natural language syntax</a>, and communicating such <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> is important for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> and better-informed downstream applications. In this work, we propose a transition sampling algorithm to sample from the full joint distribution of parse trees defined by a transition-based parsing model, and demonstrate the use of the samples in probabilistic dependency analysis. First, we define the new task of dependency path prediction, inferring syntactic substructures over part of a sentence, and provide the first analysis of performance on this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Second, we demonstrate the usefulness of our Monte Carlo syntax marginal method for parser error analysis and <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a>. Finally, we use this method to propagate parse uncertainty to two downstream information extraction applications : identifying persons killed by police and semantic role assignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1085.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1085/>Neural Particle Smoothing for Sampling from Conditional Sequence Models</a></strong><br><a href=/people/c/chu-cheng-lin/>Chu-Cheng Lin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1085><div class="card-body p-3 small">We introduce neural particle smoothing, a <a href=https://en.wikipedia.org/wiki/Sequential_Monte_Carlo_method>sequential Monte Carlo method</a> for sampling annotations of an input string from a given <a href=https://en.wikipedia.org/wiki/Probability_model>probability model</a>. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this <a href=https://en.wikipedia.org/wiki/Innovation>innovation</a> can improve the quality of the sample. To motivate our formal choices, we explain how neural transduction models and our <a href=https://en.wikipedia.org/wiki/Sampler_(musical_instrument)>sampler</a> can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1086/>Neural Syntactic Generative Models with Exact Marginalization</a></strong><br><a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1086><div class="card-body p-3 small">We present neural syntactic generative models with exact marginalization that support both <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a> and <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. Exact marginalization is made tractable through <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> over <a href=https://en.wikipedia.org/wiki/Shift-reduce>shift-reduce parsing</a> and minimal RNN-based feature sets. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1087/>Noise-Robust Morphological Disambiguation for Dialectal Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1087><div class="card-body p-3 small">User-generated text tends to be noisy with many lexical and orthographic inconsistencies, making natural language processing (NLP) tasks more challenging. The challenging nature of noisy text processing is exacerbated for dialectal content, where in addition to spelling and lexical differences, dialectal text is characterized with morpho-syntactic and phonetic variations. These issues increase sparsity in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> and reduce <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We present a neural morphological tagging and disambiguation model for <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic</a>, with various extensions to handle noisy and inconsistent content. Our models achieve about 5 % <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>relative error reduction</a> (1.1 % absolute improvement) for full morphological analysis, and around 22 % <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>relative error reduction</a> (1.8 % absolute improvement) for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, over a state-of-the-art baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1088" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1088/>Parsing Tweets into Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/y/yi-zhu/>Yi Zhu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1088><div class="card-body p-3 small">We study the problem of analyzing <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> with universal dependencies (UD). We extend the UD guidelines to cover special constructions in tweets that affect <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (Tweebank v2) that is four times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>, we build a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> to parse raw tweets into UD. To overcome the annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves an improvement of 2.2 in <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>LAS</a> over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1089" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1089/>Robust Multilingual Part-of-Speech Tagging via Adversarial Training</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1089><div class="card-body p-3 small">Adversarial training (AT) is a powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization method</a> for <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, aiming to achieve <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to input perturbations. Yet, the specific effects of the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> obtained from AT are still unclear in the context of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a> performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669962 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1092/>Deep Generative Model for Joint Alignment and Word Representation</a></strong><br><a href=/people/m/miguel-rios/>Miguel Rios</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/k/khalil-simaan/>Khalil Simaâ€™an</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1092><div class="card-body p-3 small">This work exploits translation data as a source of semantically relevant learning signal for models of word representation. In particular, we exploit equivalence through translation as a form of distributional context and jointly learn how to embed and align with a deep generative model. Our EmbedAlign model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments. Besides, it embeds <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>words</a> as posterior probability densities, rather than <a href=https://en.wikipedia.org/wiki/Point_estimation>point estimates</a>, which allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model&#8217;s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>, and text similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282323369 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1094/>Exploring the Role of Prior Beliefs for Argument Persuasion</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1094><div class="card-body p-3 small">Public debate forums provide a common platform for exchanging opinions on a topic of interest. While recent studies in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> have provided empirical evidence that the language of the debaters and their patterns of interaction play a key role in changing the mind of a reader, research in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> has shown that prior beliefs can affect our interpretation of an argument and could therefore constitute a competing alternative explanation for resistance to changing one&#8217;s stance. To study the actual effect of language use vs. prior beliefs on <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, we provide a new dataset and propose a controlled setting that takes into consideration two reader-level factors : <a href=https://en.wikipedia.org/wiki/Ideology>political and religious ideology</a>. We find that prior beliefs affected by these reader-level factors play a more important role than language use effects and argue that it is important to account for them in NLP studies of persuasion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282327794 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1096/>Author Commitment and Social Power : Automatic Belief Tagging to Infer the Social Context of Interactions</a></strong><br><a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/p/premkumar-ganeshkumar/>Premkumar Ganeshkumar</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1096><div class="card-body p-3 small">Understanding how <a href=https://en.wikipedia.org/wiki/Power_(social_and_political)>social power structures</a> affect the way we interact with one another is of great interest to social scientists who want to answer fundamental questions about human behavior, as well as to computer scientists who want to build automatic methods to infer the social contexts of interactions. In this paper, we employ advancements in extra-propositional semantics extraction within <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to study how author commitment reflects the social context of an interactions. Specifically, we investigate whether the level of <a href=https://en.wikipedia.org/wiki/Commitment>commitment</a> expressed by individuals in an <a href=https://en.wikipedia.org/wiki/Organizational_behavior>organizational interaction</a> reflects the <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical power structures</a> they are part of. We find that <a href=https://en.wikipedia.org/wiki/Hierarchy>subordinates</a> use significantly more instances of non-commitment than superiors. More importantly, we also find that subordinates attribute propositions to other agents more often than superiors do an aspect that has not been studied before. Finally, we show that enriching lexical features with commitment labels captures important distinctions in <a href=https://en.wikipedia.org/wiki/Social_relation>social meanings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1097/>Comparing Automatic and Human Evaluation of Local Explanations for Text Classification</a></strong><br><a href=/people/d/dong-nguyen/>Dong Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1097><div class="card-body p-3 small">Text classification models are becoming increasingly complex and opaque, however for many applications it is essential that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are interpretable. Recently, a variety of <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> have been proposed for generating local explanations. While robust evaluations are needed to drive further progress, so far it is unclear which evaluation approaches are suitable. This paper is a first step towards more robust evaluations of local explanations. We evaluate a variety of local explanation approaches using automatic measures based on word deletion. Furthermore, we show that an evaluation using a <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing experiment</a> correlates moderately with these automatic measures and that a variety of other factors also impact the human judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669869 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1098/>Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/s/subburam-rajaram/>Subburam Rajaram</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich SchÃ¼tze</a>
|
<a href=/people/b/bernt-andrassy/>Bernt Andrassy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1098><div class="card-body p-3 small">Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> (named as SPAN) to quantify the capability of <a href=https://en.wikipedia.org/wiki/Dynamic_topic_model>dynamic topic model</a> to capture word evolution in topics over time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1099/>Lessons from the Bible on Modern Topics : Low-Resource Multilingual Topic Model Evaluation<span class=acl-fixed-case>B</span>ible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation</a></strong><br><a href=/people/s/shudong-hao/>Shudong Hao</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1099><div class="card-body p-3 small">Multilingual topic models enable <a href=https://en.wikipedia.org/wiki/Document_analysis>document analysis</a> across languages through coherent multilingual summaries of the data. However, there is no standard and effective <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> in low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1100/>Explainable Prediction of Medical Codes from Clinical Text</a></strong><br><a href=/people/j/james-mullenbach/>James Mullenbach</a>
|
<a href=/people/s/sarah-wiegreffe/>Sarah Wiegreffe</a>
|
<a href=/people/j/jon-duke/>Jon Duke</a>
|
<a href=/people/j/jimeng-sun/>Jimeng Sun</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1100><div class="card-body p-3 small">Clinical notes are <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text documents</a> that are created by clinicians for each patient encounter. They are typically accompanied by <a href=https://en.wikipedia.org/wiki/Medical_code>medical codes</a>, which describe the diagnosis and treatment. Annotating these <a href=https://en.wikipedia.org/wiki/Code>codes</a> is labor intensive and error prone ; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts <a href=https://en.wikipedia.org/wiki/Medical_classification>medical codes</a> from clinical text. Our method aggregates information across the document using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282330248 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1101/>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</a></strong><br><a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/n/nikita-nangia/>Nikita Nangia</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1101><div class="card-body p-3 small">This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this <a href=https://en.wikipedia.org/wiki/Resource>resource</a> is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, despite the two showing similar levels of inter-annotator agreement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1102.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282332520 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1102/>Filling Missing Paths : Modeling Co-occurrences of Word Pairs and Dependency Paths for Recognizing Lexical Semantic Relations</a></strong><br><a href=/people/k/koki-washio/>Koki Washio</a>
|
<a href=/people/t/tsuneaki-kato/>Tsuneaki Kato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1102><div class="card-body p-3 small">Recognizing lexical semantic relations between word pairs is an important task for many applications of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. One of the mainstream approaches to this task is to exploit the lexico-syntactic paths connecting two target words, which reflect the <a href=https://en.wikipedia.org/wiki/Semantics>semantic relations</a> of word pairs. However, this method requires that the considered words co-occur in a sentence. This requirement is hardly satisfied because of <a href=https://en.wikipedia.org/wiki/Zipf&#8217;s_law>Zipf&#8217;s law</a>, which states that most content words occur very rarely. In this paper, we propose novel methods with a neural model of P(path|w1,w2) to solve this problem. Our proposed model of P (path|w1, w2) can be learned in an unsupervised manner and can generalize the co-occurrences of word pairs and dependency paths. This model can be used to augment the path data of word pairs that do not co-occur in the corpus, and extract features capturing relational information from word pairs. Our experimental results demonstrate that our methods improve on previous neural approaches based on dependency paths and successfully solve the focused problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282333968 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1103/>Specialising <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Word Vectors</a> for Lexical Entailment</a></strong><br><a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a>
|
<a href=/people/n/nikola-mrksic/>Nikola MrkÅ¡iÄ‡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1103><div class="card-body p-3 small">We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation. By injecting external linguistic constraints (e.g., WordNet links) into the initial <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, the LE specialisation procedure brings true <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy-hypernymy pairs</a> closer together in the transformed <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a>. The proposed asymmetric distance measure adjusts the norms of word vectors to reflect the actual WordNet-style hierarchy of concepts. Simultaneously, a joint objective enforces <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> using the symmetric cosine distance, yielding a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/282336638 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1104/>Cross-Lingual Abstract Meaning Representation Parsing<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation Parsing</a></strong><br><a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1104><div class="card-body p-3 small">Abstract Meaning Representation (AMR) research has mostly focused on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We show that it is possible to use AMR annotations for <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a semantic representation for sentences written in other languages. We exploit an AMR parser for English and parallel corpora to learn AMR parsers for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Qualitative analysis show that the new <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> overcome structural differences between the languages. We further propose a method to evaluate the <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> that does not require gold standard data in the target languages. This <a href=https://en.wikipedia.org/wiki/Methodology>method</a> highly correlates with the gold standard evaluation, obtaining a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation coefficient</a> of 0.95.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898201 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1107/>End-to-End Graph-Based TAG Parsing with <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a><span class=acl-fixed-case>TAG</span> Parsing with Neural Networks</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/r/robert-frank/>Robert Frank</a>
|
<a href=/people/p/pauli-xu/>Pauli Xu</a>
|
<a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1107><div class="card-body p-3 small">We present a graph-based Tree Adjoining Grammar (TAG) parser that uses BiLSTMs, highway connections, and character-level CNNs. Our best end-to-end parser, which jointly performs supertagging, <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>, and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, outperforms the previously reported best results by more than 2.2 LAS and UAS points. The graph-based parsing architecture allows for global inference and rich feature representations for TAG parsing, alleviating the fundamental trade-off between transition-based and graph-based parsing systems. We also demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery. This provides further support for the claim that TAG is a viable <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> for problems that require rich structural analysis of sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1108/>Colorless Green Recurrent Networks Dream Hierarchically</a></strong><br><a href=/people/k/kristina-gulordava/>Kristina Gulordava</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1108><div class="card-body p-3 small">Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs can not rely on semantic or lexical cues (The colorless green ideas I ate with the chair sleep furiously), and, for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>long-distance agreement</a>, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1110/>Early Text Classification Using Multi-Resolution Concept Representations</a></strong><br><a href=/people/a/adrian-pastor-lopez-monroy/>Adrian Pastor LÃ³pez-Monroy</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio A. GonzÃ¡lez</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/h/hugo-jair-escalante/>Hugo Jair Escalante</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1110><div class="card-body p-3 small">The intensive use of <a href=https://en.wikipedia.org/wiki/Electronic_communication>e-communications</a> in everyday life has given rise to new threats and risks. When the vulnerable asset is the user, detecting these potential attacks before they cause serious damages is extremely important. This paper proposes a novel document representation to improve the early detection of risks in social media sources. The goal is to effectively identify the potential risk using as few text as possible and with as much anticipation as possible. Accordingly, we devise a Multi-Resolution Representation (MulR), which allows us to generate multiple views of the analyzed text. These views capture different semantic meanings for words and documents at different levels of detail, which is very useful in early scenarios to model the variable amounts of evidence. Intuitively, the representation captures better the content of short documents (very early stages) in low resolutions, whereas large documents (medium / large stages) are better modeled with higher resolutions. We evaluate the proposed ideas in two different tasks where <a href=https://en.wikipedia.org/wiki/Anticipation>anticipation</a> is critical : sexual predator detection and <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression detection</a>. The experimental evaluation for these early <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> revealed that the proposed approach outperforms previous <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> by a considerable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1112/>Pivot Based Language Modeling for Improved Neural Domain Adaptation</a></strong><br><a href=/people/y/yftah-ziser/>Yftah Ziser</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1112><div class="card-body p-3 small">Representation learning with pivot-based methods and with Neural Networks (NNs) have lead to significant progress in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. However, most previous work that follows these approaches does not explicitly exploit the structure of the input text, and its output is most often a single <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vector</a> for the entire text. In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner. Particularly, our model processes the information in the text with a sequential NN (LSTM) and its output consists of a <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vector</a> for every input word. Unlike most previous representation learning models in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, PBLM can naturally feed structure aware text classifiers such as LSTM and CNN. We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1113/>Reinforced Co-Training</a></strong><br><a href=/people/j/jiawei-wu/>Jiawei Wu</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1113><div class="card-body p-3 small">Co-training is a popular semi-supervised learning framework to utilize a large amount of unlabeled data in addition to a small labeled set. Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training. However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> between the unlabeled and the labeled subsets, and fails to explore the data space. In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on. More specifically, our approach uses <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> to learn a data selection policy with a small labeled dataset, and then exploits this <a href=https://en.wikipedia.org/wiki/Policy>policy</a> to train the co-training classifiers automatically. Experimental results on clickbait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1114/>Tensor Product Generation Networks for Deep NLP Modeling<span class=acl-fixed-case>NLP</span> Modeling</a></strong><br><a href=/people/q/qiuyuan-huang/>Qiuyuan Huang</a>
|
<a href=/people/p/paul-smolensky/>Paul Smolensky</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/l/li-deng/>Li Deng</a>
|
<a href=/people/d/dapeng-wu/>Dapeng Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1114><div class="card-body p-3 small">We present a new approach to the design of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, based on the general technique of <a href=https://en.wikipedia.org/wiki/Tensor_product_representation>Tensor Product Representations (TPRs)</a> for encoding and processing symbol structures in distributed neural networks. A <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> the Tensor Product Generation Network (TPGN) is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of <a href=https://en.wikipedia.org/wiki/Internal_representation>internal representations</a> and <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a>, which prove to contain considerable <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical content</a>. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1116/>Combining Character and Word Information in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using a Multi-Level Attention</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1116><div class="card-body p-3 small">Natural language sentences, being hierarchical, can be represented at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, like <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Subword>subwords</a>, or <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a> is better for a particular translation task. In this paper, we improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information ; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1117/>Dense Information Flow for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yanyao-shen/>Yanyao Shen</a>
|
<a href=/people/x/xu-tan/>Xu Tan</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1117><div class="card-body p-3 small">Recently, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework. From the optimization perspective, residual connections are adopted to improve learning performance for both <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> in most of these deep architectures, and advanced attention connections are applied as well. Inspired by the success of the DenseNet model in computer vision problems, in this paper, we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve <a href=https://en.wikipedia.org/wiki/Attentional_control>attention quality</a>. Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1119/>Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation</a></strong><br><a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/d/david-vilar/>David Vilar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1119><div class="card-body p-3 small">The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability : lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>. We present a <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for lexically constrained decoding with a <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of O(1) in the number of constraints. We demonstrate the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>&#8217;s remarkable ability to properly place these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>, and use it to explore the shaky relationship between <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and BLEU scores. Our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is available as part of <a href=https://en.wikipedia.org/wiki/Sockeye>Sockeye</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1120/>Guiding Neural Machine Translation with Retrieved Translation Pieces</a></strong><br><a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichro Sumita</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1120><div class="card-body p-3 small">One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call translation pieces. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Speed>speed</a>, and simplicity of implementation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1121/>Handling <a href=https://en.wikipedia.org/wiki/Homograph>Homographs</a> in Neural Machine Translation</a></strong><br><a href=/people/f/frederick-liu/>Frederick Liu</a>
|
<a href=/people/h/han-lu/>Han Lu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1121><div class="card-body p-3 small">Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct <a href=https://en.wikipedia.org/wiki/Translation>translation</a> based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of translating <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1122.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1122/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Conditional Sequence Generative Adversarial Nets</a></strong><br><a href=/people/z/zhen-yang/>Zhen Yang</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1122><div class="card-body p-3 small">This paper proposes an approach for applying <a href=https://en.wikipedia.org/wiki/Global_area_network>GANs</a> to <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>. The <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a> aims to generate sentences which are hard to be discriminated from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>human-translated sentences</a> (i.e., the golden target sentences) ; And the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> makes efforts to discriminate the machine-generated sentences from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>human-translated ones</a>. The two sub models play a mini-max game and achieve the <a href=https://en.wikipedia.org/wiki/Win-win_game>win-win situation</a> when they reach a <a href=https://en.wikipedia.org/wiki/Nash_equilibrium>Nash Equilibrium</a>. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1123/>Neural Machine Translation for Bilingually Scarce Scenarios : a Deep Multi-Task Learning Approach</a></strong><br><a href=/people/p/poorya-zaremoodi/>Poorya Zaremoodi</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1123><div class="card-body p-3 small">Neural machine translation requires large amount of parallel training text to learn a reasonable quality <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation model</a>. This is particularly inconvenient for language pairs for which enough <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel text</a> is not available. In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-task learning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>. This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext to learn from. We empirically analyze and show the effectiveness of our multitask learning approach on three translation tasks : <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-French</a>, <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-Farsi</a>, and <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-Vietnamese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1124.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1124" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1124/>Self-Attentive Residual Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/l/lesly-miculicich-werlen/>Lesly Miculicich Werlen</a>
|
<a href=/people/n/nikolaos-pappas/>Nikolaos Pappas</a>
|
<a href=/people/d/dhananjay-ram/>Dhananjay Ram</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1124><div class="card-body p-3 small">Neural sequence-to-sequence networks with attention have achieved remarkable performance for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a <a href=https://en.wikipedia.org/wiki/Recency_bias>recency bias</a> and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over previous words contributes directly to the prediction of the next word. The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs. The analysis of the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> learned by the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> confirms that it emphasizes a wider context, and that it captures syntactic-like structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1126/>Context Sensitive Neural Lemmatization with Lematus<span class=acl-fixed-case>L</span>ematus</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1126><div class="card-body p-3 small">The main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> actually helps in these cases. We introduce Lematus, a <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> based on a standard encoder-decoder architecture, which incorporates character-level sentence context. We evaluate its lemmatization accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. In both settings, we show that including <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> significantly improves results against a context-free version of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Context helps more for ambiguous words than for unseen words, though the latter has a greater effect on overall performance differences between languages. We also compare to three previous context-sensitive lemmatization systems, which all use pre-extracted edit trees as well as hand-selected features and/or additional sources of information such as tagged training data. Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the fulldata setting, and performs on par in the lowerresource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1127/>Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media</a></strong><br><a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/a/adrian-pastor-lopez-monroy/>Adrian Pastor LÃ³pez-Monroy</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio GonzÃ¡lez</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1127><div class="card-body p-3 small">Recognizing named entities in a document is a key task in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Although current state-of-the-art approaches to this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> reach a high performance on clean text (e.g. newswire genres), those <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> dramatically degrade when they are moved to noisy environments such as social media domains. We present two systems that address the challenges of processing social media data using character-level phonetics and <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and Part-of-Speech tags as features. The first model is a multitask end-to-end Bidirectional Long Short-Term Memory (BLSTM)-Conditional Random Field (CRF) network whose output layer contains two CRF classifiers. The second model uses a multitask BLSTM network as <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> that transfers the learning to a CRF classifier for the final prediction. Our systems outperform the current F1 scores of the state of the art on the Workshop on Noisy User-generated Text 2017 dataset by 2.45 % and 3.69 %, establishing a more suitable approach for social media environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/277349441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1131/>A Neural Layered Model for Nested Named Entity Recognition</a></strong><br><a href=/people/m/meizhi-ju/>Meizhi Ju</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1131><div class="card-body p-3 small">Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic model</a> outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7 % and 72.2 % on GENIA and ACE2005 datasets, respectively, in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1133.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671743 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1133/>KBGAN : <a href=https://en.wikipedia.org/wiki/Adversarial_learning>Adversarial Learning</a> for Knowledge Graph Embeddings<span class=acl-fixed-case>KBGAN</span>: Adversarial Learning for Knowledge Graph Embeddings</a></strong><br><a href=/people/l/liwei-cai/>Liwei Cai</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1133><div class="card-body p-3 small">We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> typically only contain positive facts, sampling useful negative training examples is a nontrivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets : FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1136.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672916 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1136/>Identifying Semantic Divergences in <a href=https://en.wikipedia.org/wiki/Parallel_text>Parallel Text</a> without Annotations</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1136><div class="card-body p-3 small">Recognizing that even correct translations are not always semantically equivalent, we automatically detect meaning divergences in parallel sentence pairs with a deep neural model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation. We show that our <a href=https://en.wikipedia.org/wiki/Semantic_model>semantic model</a> detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/277348853 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1137/>Bootstrapping Generators from Noisy Data</a></strong><br><a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1137><div class="card-body p-3 small">A core step in statistical data-to-text generation concerns learning correspondences between <a href=https://en.wikipedia.org/wiki/Data_structure>structured data representations</a> (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generators</a> from <a href=https://en.wikipedia.org/wiki/Data_set>large scale datasets</a> where the data (e.g., <a href=https://en.wikipedia.org/wiki/Dbpedia>DBPedia facts</a>) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/277348640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1138/>SHAPED : Shared-Private Encoder-Decoder for Text Style Adaptation<span class=acl-fixed-case>SHAPED</span>: Shared-Private Encoder-Decoder for Text Style Adaptation</a></strong><br><a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/n/nan-ding/>Nan Ding</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1138><div class="card-body p-3 small">Supervised training of abstractive language generation models results in learning conditional probabilities over language sequences based on the supervised training signal. When the training signal contains a variety of <a href=https://en.wikipedia.org/wiki/Writing_style>writing styles</a>, such models may end up learning an &#8216;average&#8217; style that is directly influenced by the training data make-up and can not be controlled by the needs of an application. We describe a family of model architectures capable of capturing both generic language characteristics via shared model parameters, as well as particular style characteristics via private model parameters. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are able to generate <a href=https://en.wikipedia.org/wiki/Language>language</a> according to a specific learned style, while still taking advantage of their power to model generic language phenomena. Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-the-fly style adaptation based on the textual input alone. Experimentally, we find that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> consistently outperform <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that encapsulate single-style or average-style language generation capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1139/>Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization</a></strong><br><a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/s/shreyas-shetty/>Shreyas Shetty</a>
|
<a href=/people/p/parag-jain/>Parag Jain</a>
|
<a href=/people/a/anirban-laha/>Anirban Laha</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1139><div class="card-body p-3 small">In this work, we focus on the task of generating natural language descriptions from a structured table of facts containing fields (such as nationality, occupation, etc) and values (such as <a href=https://en.wikipedia.org/wiki/Indian_people>Indian</a>, <a href=https://en.wikipedia.org/wiki/Actor>actor</a>, director, etc). One simple choice is to treat the table as a sequence of fields and values and then use a standard seq2seq model for this task. However, such a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is too generic and does not exploit task specific characteristics. For example, while generating descriptions from a table, a human would attend to information at two levels : (i) the fields (macro level) and (ii) the values within the field (micro level). Further, a human would continue attending to a field for a few timesteps till all the information from that field has been rendered and then never return back to this field (because there is nothing left to say about it). To capture this behavior we use (i) a fused bifocal attention mechanism which exploits and combines this micro and macro level information and (ii) a gated orthogonalization mechanism which tries to ensure that a field is remembered for a few time steps and then forgotten. We experiment with a recently released dataset which contains fact tables about people and their corresponding one line biographical descriptions in English. In addition, we also introduce two similar <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Our experiments show that the proposed model gives 21 % relative improvement over a recently proposed state of the art method and 10 % relative improvement over basic seq2seq models.<url>https://github.com/PrekshaNema25/StructuredData_To_Descriptions</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1140/>CliCR : a Dataset of Clinical Case Reports for Machine Reading Comprehension<span class=acl-fixed-case>C</span>li<span class=acl-fixed-case>CR</span>: a Dataset of Clinical Case Reports for Machine Reading Comprehension</a></strong><br><a href=/people/s/simon-suster/>Simon Å uster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1140><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine comprehension</a> in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20 % F1) between the best human and machine readers. We analyze the skills required for successful answering and show how <a href=https://en.wikipedia.org/wiki/Reading>reader</a> performance varies depending on the applicable skills. We find that inferences using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1141/>Learning to Collaborate for Question Answering and Asking</a></strong><br><a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/z/zhao-yan/>Zhao Yan</a>
|
<a href=/people/z/zhirui-zhang/>Zhirui Zhang</a>
|
<a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/y/yuanhua-lv/>Yuanhua Lv</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1141><div class="card-body p-3 small">Question answering (QA) and question generation (QG) are closely related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that could improve each other ; however, the connection of these two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> is not well explored in literature. In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG. We present a training algorithm that generalizes both Generative Adversarial Network (GAN) and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario. The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, and improving the QA model with QG through adding artificially generated training instances. We conduct experiments on both document based and knowledge based question answering tasks. We have two main findings. Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient. Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the QA model. Learning when to regard generated questions as positive instances could bring performance boost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1143/>Supervised and Unsupervised Transfer Learning for Question Answering</a></strong><br><a href=/people/y/yu-an-chung/>Yu-An Chung</a>
|
<a href=/people/h/hung-yi-lee/>Hung-Yi Lee</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1143><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> has been shown to be successful for tasks like <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object and speech recognition</a>, its applicability to <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA)</a> has yet to be well-studied. In this paper, we conduct extensive experiments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is significantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al., 2016). In particular, one of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieves the state-of-the-art on all target datasets ; for the <a href=https://en.wikipedia.org/wiki/Test_of_English_as_a_Foreign_Language>TOEFL listening comprehension test</a>, it outperforms the previous best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by 7 %. Finally, we show that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is helpful even in unsupervised scenarios when correct answers for target QA dataset examples are not available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1146/>Deconfounded Lexicon Induction for Interpretable Social Science</a></strong><br><a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/k/kelly-shen/>Kelly Shen</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/s/stefan-wagner/>Stefan Wagner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1146><div class="card-body p-3 small">NLP algorithms are increasingly used in <a href=https://en.wikipedia.org/wiki/Computational_social_science>computational social science</a> to take <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic observations</a> and predict outcomes like <a href=https://en.wikipedia.org/wiki/Preference>human preferences</a> or actions. Making these <a href=https://en.wikipedia.org/wiki/Social_model>social models</a> transparent and interpretable often requires identifying <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in the input that predict outcomes while also controlling for potential <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a>. We formalize this need as a new task : inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithms</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The first uses a bifurcated architecture to separate the explanatory power of the text and <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a>. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> from learned weights and <a href=https://en.wikipedia.org/wiki/Attentional_control>attentional scores</a>. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with narrative persuasion ; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and log odds.<i>narrative persuasion</i>; more\n predictive and less confound-related than those of standard\n feature weighting and lexicon induction techniques like\n regression and log odds.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1149.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1149/>A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications<span class=acl-fixed-case>P</span>eer<span class=acl-fixed-case>R</span>ead): Collection, Insights and <span class=acl-fixed-case>NLP</span> Applications</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/m/madeleine-van-zuylen/>Madeleine van Zuylen</a>
|
<a href=/people/s/sebastian-kohlmeier/>Sebastian Kohlmeier</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1149><div class="card-body p-3 small">Peer reviewing is a central component in the <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publishing process</a>. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 14.7 K paper drafts and the corresponding accept / reject decisions in top-tier venues including ACL, <a href=https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology>NIPS</a> and ICLR. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> also includes 10.7 K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the <a href=https://en.wikipedia.org/wiki/Peer_review>peer reviews</a>. We also propose two novel NLP tasks based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and provide simple baseline models. In the first <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we show that simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can predict whether a paper is accepted with up to 21 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as &#8216;originality&#8217; and &#8216;impact&#8217;.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1150/>Deep Communicating Agents for Abstractive Summarization</a></strong><br><a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1150><div class="card-body p-3 small">We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>, trained end-to-end using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> or multiple non-communicating encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1152 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1152/>Estimating Summary Quality with Pairwise Preferences</a></strong><br><a href=/people/m/markus-zopf/>Markus Zopf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1152><div class="card-body p-3 small">Automatic evaluation systems in the field of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automatic summarization</a> have been relying on the availability of gold standard summaries for over ten years. Gold standard summaries are expensive to obtain and often require the availability of domain experts to achieve high quality. In this paper, we propose an alternative evaluation approach based on pairwise preferences of sentences. In comparison to gold standard summaries, they are simpler and cheaper to obtain. In our experiments, we show that humans are able to provide useful feedback in the form of pairwise preferences. The new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> performs better than the three most popular versions of ROUGE with less expensive human input. We also show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can reuse already available evaluation data and achieve even better results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1158/>Ranking Sentences for Extractive Summarization with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1158><div class="card-body p-3 small">Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a> which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671902 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1163/>Detecting Egregious Conversations between Customers and Virtual Agents</a></strong><br><a href=/people/t/tommy-sandbank/>Tommy Sandbank</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/d/david-konopnicki/>David Konopnicki</a>
|
<a href=/people/j/john-richards/>John Richards</a>
|
<a href=/people/d/david-piorkowski/>David Piorkowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1163><div class="card-body p-3 small">Virtual agents are becoming a prominent channel of interaction in <a href=https://en.wikipedia.org/wiki/Customer_service>customer service</a>. Not all <a href=https://en.wikipedia.org/wiki/Customer_relationship_management>customer interactions</a> are smooth, however, and some can become almost comically bad. In such instances, a <a href=https://en.wikipedia.org/wiki/Agent-based_model>human agent</a> might need to step in and salvage the conversation. Detecting bad conversations is important since disappointing customer service may threaten <a href=https://en.wikipedia.org/wiki/Loyalty_business_model>customer loyalty</a> and impact revenue. In this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and user-agent interaction. Using logs of two commercial systems, we show that using these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> improves the detection F1-score by around 20 % over using textual features alone. In addition, we show that those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are common across two quite different domains and, arguably, universal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1164/>Learning to Disentangle Interleaved Conversational Threads with a Siamese Hierarchical Network and Similarity Ranking<span class=acl-fixed-case>S</span>iamese Hierarchical Network and Similarity Ranking</a></strong><br><a href=/people/j/jyun-yu-jiang/>Jyun-Yu Jiang</a>
|
<a href=/people/f/francine-chen/>Francine Chen</a>
|
<a href=/people/y/yan-ying-chen/>Yan-Ying Chen</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1164><div class="card-body p-3 small">An enormous amount of <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a> occurs online every day, such as on chat platforms where multiple conversations may take place concurrently. Interleaved conversations lead to difficulties in not only following discussions but also retrieving relevant information from simultaneous messages. Conversation disentanglement aims to separate <a href=https://en.wikipedia.org/wiki/Interpersonal_communication>intermingled messages</a> into <a href=https://en.wikipedia.org/wiki/Conversation>detached conversations</a>. In this paper, we propose to leverage <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> for conversation disentanglement. A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages. With the estimated similarity scores, our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for conversation identification by similarity ranking (CISIR) then derives conversations based on high-confidence message pairs and pairwise redundancy. Experiments were conducted with four publicly available <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of conversations from <a href=https://en.wikipedia.org/wiki/Internet_Relay_Chat>Reddit and IRC channels</a>. The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673049 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1165/>Variational Knowledge Graph Reasoning</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1165><div class="card-body p-3 small">Inferring missing links in knowledge graphs (KG) has attracted a lot of attention from the research community. In this paper, we tackle a practical query answering task involving predicting the relation of a given entity pair. We frame this prediction problem as an inference problem in a probabilistic graphical model and aim at resolving it from a variational inference perspective. In order to model the relation between the query entity pair, we assume that there exists an underlying <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> (paths connecting two nodes) in the KG, which carries the equivalent semantics of their relations. However, due to the intractability of connections in large KGs, we propose to use variation inference to maximize the evidence lower bound. More specifically, our framework (Diva) is composed of three <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, i.e. a posterior approximator, a prior (path finder), and a likelihood (path reasoner). By using <a href=https://en.wikipedia.org/wiki/Variational_inference>variational inference</a>, we are able to incorporate them closely into a unified architecture and jointly optimize them to perform KG reasoning. With active interactions among these sub-modules, Diva is better at handling <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and coping with more complex reasoning scenarios. In order to evaluate our method, we conduct the experiment of the link prediction task on multiple datasets and achieve state-of-the-art performances on both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673836 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1168/>Interpretable Charge Predictions for Criminal Cases : Learning to Generate Court Views from Fact Descriptions</a></strong><br><a href=/people/h/hai-ye/>Hai Ye</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/z/zhunchen-luo/>Zhunchen Luo</a>
|
<a href=/people/w/wenhan-chao/>Wenhan Chao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1168><div class="card-body p-3 small">In this paper, we propose to study the problem of court view generation from the fact description in a <a href=https://en.wikipedia.org/wiki/Criminal_law>criminal case</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> aims to improve the interpretability of charge prediction systems and help automatic legal document generation. We formulate this task as a text-to-text natural language generation (NLG) problem. Sequence-to-sequence model has achieved cutting-edge performances in many NLG tasks. However, due to the non-distinctions of fact descriptions, it is hard for Seq2Seq model to generate charge-discriminative court views. In this work, we explore charge labels to tackle this issue. We propose a label-conditioned Seq2Seq model with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for this problem, to decode court views conditioned on encoded charge labels. Experimental results show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1169.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673818 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1169/>Delete, Retrieve, Generate : a Simple Approach to Sentiment and Style Transfer</a></strong><br><a href=/people/j/juncen-li/>Juncen Li</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1169><div class="card-body p-3 small">We consider the task of text attribute transfer : transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its <a href=https://en.wikipedia.org/wiki/Content_(media)>attribute-independent content</a> (e.g., screen is just the right size to screen is too small). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> from attribute-independent content in an unsupervised way. Previous work using <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial methods</a> has struggled to produce high-quality outputs. In this paper, we propose simpler <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> motivated by the observation that <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>text attributes</a> are often marked by distinctive phrases (e.g., too small). Our strongest method extracts content words by deleting phrases associated with the sentence&#8217;s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22 % more inputs than the best previous system, averaged over three attribute transfer datasets : altering sentiment of reviews on <a href=https://en.wikipedia.org/wiki/Yelp>Yelp</a>, altering sentiment of reviews on <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>, and altering image captions to be more romantic or humorous.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673796 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1170/>Adversarial Example Generation with Syntactically Controlled Paraphrase Networks</a></strong><br><a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1170><div class="card-body p-3 small">We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic form</a> (e.g., a <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituency parse</a>), SCPNs are trained to produce a <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a> of the sentence with the desired <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntax</a>. We show it is possible to create <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for this task by first doing backtranslation at a very large scale, and then using a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) fool pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671439 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1173/>Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1173><div class="card-body p-3 small">Predicting the emotional value of lexical items is a well-known problem in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. While research has focused on polarity for quite a long time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations). In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data. We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem. In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions. We investigate whether <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> turns out to outperform each one of them. Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671591 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1174/>Human Needs Categorization of Affective Events Using Labeled and Unlabeled Data</a></strong><br><a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1174><div class="card-body p-3 small">We often talk about events that impact us positively or negatively. For example I got a job is good news, but I lost my job is bad news. When we discuss an event, we not only understand its <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective polarity</a> but also the reason why the event is beneficial or detrimental. For example, getting or losing a job has affective polarity primarily because it impacts us financially. Our work aims to categorize affective events based upon human need categories that often explain people&#8217;s motivations and desires : PHYSIOLOGICAL, HEALTH, LEISURE, SOCIAL, FINANCIAL, COGNITION, and FREEDOM. We create classification models based on event expressions as well as <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that use contexts surrounding event mentions. We also design a co-training model that learns from unlabeled data by simultaneously training event expression and event context classifiers in an iterative learning process. Our results show that <a href=https://en.wikipedia.org/wiki/Co-training>co-training</a> performs well, producing substantially better results than the individual <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672970 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1176/>Linguistic Cues to Deception and Perceived Deception in Interview Dialogues</a></strong><br><a href=/people/s/sarah-ita-levitan/>Sarah Ita Levitan</a>
|
<a href=/people/a/angel-maredia/>Angel Maredia</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1176><div class="card-body p-3 small">We explore deception detection in interview dialogues. We analyze a set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> in both truthful and deceptive responses to interview questions. We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers. Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language. This analysis motivated our selection of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> experiments aimed at classifying globally deceptive speech. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance is 72.74 % F1-Score (about 17 % better than human performance), which is achieved using a combination of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> and <a href=https://en.wikipedia.org/wiki/Phenotypic_trait>individual traits</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672896 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1177/>Unified Pragmatic Models for Generating and Following Instructions</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1177><div class="card-body p-3 small">We show that explicit pragmatic inference aids in correctly generating and following <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a> for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to tasks with <a href=https://en.wikipedia.org/wiki/Sequential_analysis>sequential structure</a>. Evaluation of <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a> and <a href=https://en.wikipedia.org/wiki/Language_interpretation>interpretation</a> shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672945 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1178/>Hierarchical Structured Model for Fine-to-Coarse Manifesto Text Analysis</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1178><div class="card-body p-3 small">Election manifestos document the intentions, motives, and views of political parties. They are often used for analysing a party&#8217;s fine-grained position on a particular issue, as well as for coarse-grained positioning of a party on the leftright spectrum. In this paper we propose a two-stage model for automatically performing both levels of analysis over <a href=https://en.wikipedia.org/wiki/Manifesto>manifestos</a>. In the first step we employ a hierarchical multi-task structured deep model to predict fine- and coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic. We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1179.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673944 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1179/>Behavior Analysis of NLI Models : Uncovering the Influence of Three Factors on Robustness<span class=acl-fixed-case>NLI</span> Models: Uncovering the Influence of Three Factors on Robustness</a></strong><br><a href=/people/i/ivan-sanchez/>Ivan Sanchez</a>
|
<a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1179><div class="card-body p-3 small">Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores. Here, we go beyond this single evaluation metric to examine <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to semantically-valid alterations to the input data. We identify three factors-insensitivity, polarity and unseen pairs-and compare their impact on three SNLI models under a variety of conditions. Our results demonstrate a number of strengths and weaknesses in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; ability to generalise to new in-domain instances. In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>statistical correlations</a> between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> or found in the dataset used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1180/>Assessing Language Proficiency from Eye Movements in Reading</a></strong><br><a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/b/boris-katz/>Boris Katz</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1180><div class="card-body p-3 small">We present a novel approach for determining learners&#8217; second language proficiency which utilizes behavioral traces of eye movements during reading. Our approach provides stand-alone eyetracking based English proficiency scores which reflect the extent to which the learner&#8217;s gaze patterns in reading are similar to those of native English speakers. We show that our <a href=https://en.wikipedia.org/wiki/Test_score>scores</a> correlate strongly with standardized English proficiency tests. We also demonstrate that gaze information can be used to accurately predict the outcomes of such tests. Our approach yields the strongest performance when the test taker is presented with a suite of sentences for which we have eyetracking data from other readers. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> remains effective even using <a href=https://en.wikipedia.org/wiki/Eyetracking>eyetracking</a> with sentences for which eye movement data have not been previously collected. By deriving <a href=https://en.wikipedia.org/wiki/Language_proficiency>proficiency</a> as an automatic byproduct of <a href=https://en.wikipedia.org/wiki/Eye_movement>eye movements</a> during ordinary reading, our approach offers a potentially valuable new tool for second language proficiency assessment. More broadly, our results open the door to future methods for inferring reader characteristics from the behavioral traces of reading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1184/>Unsupervised Induction of Linguistic Categories with Records of Reading, Speaking, and Writing</a></strong><br><a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/a/ana-valeria-gonzalez-garduno/>Ana Valeria GonzÃ¡lez-GarduÃ±o</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a>
|
<a href=/people/a/anders-sogaard/>Anders SÃ¸gaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1184><div class="card-body p-3 small">When learning POS taggers and syntactic chunkers for low-resource languages, different resources may be available, and often all we have is a small tag dictionary, motivating type-constrained unsupervised induction. Even small dictionaries can improve the performance of unsupervised induction algorithms. This paper shows that performance can be further improved by including data that is readily available or can be easily obtained for most <a href=https://en.wikipedia.org/wiki/Language>languages</a>, i.e., <a href=https://en.wikipedia.org/wiki/Eye_tracking>eye-tracking</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech</a>, or <a href=https://en.wikipedia.org/wiki/Keystroke_logging>keystroke logs</a> (or any combination thereof). We project information from all these data sources into shared spaces, in which the union of words is represented. For English unsupervised POS induction, the additional information, which is not required at test time, leads to an average error reduction on Ontonotes domains of 1.5 % over systems augmented with state-of-the-art <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. On <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 5.4 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> over a word embeddings baseline. We also achieve significant improvements for syntactic chunk induction. Our analysis shows that improvements are even bigger when the available tag dictionaries are smaller.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1185/>Challenging Reading Comprehension on Daily Conversation : Passage Completion on Multiparty Dialog</a></strong><br><a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/t/tomasz-jurczyk/>Tomasz Jurczyk</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1185><div class="card-body p-3 small">This paper presents a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and a robust deep learning architecture for a task in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, passage completion, on multiparty dialog. Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog. Since there is no dataset that challenges the task of passage completion in this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, we create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by selecting transcripts from a <a href=https://en.wikipedia.org/wiki/Television_show>TV show</a> that comprise 1,681 dialogs, generating passages for each dialog through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, and annotating mentions of characters in both the dialog and the passages. Given this dataset, we build a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> that integrates rich feature extraction from <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> into sequence modeling in <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, optimized by utterance and dialog level attentions. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on this task in a different genre using bidirectional LSTM, showing a 13.0+% improvement for longer dialogs. Our analysis shows the effectiveness of the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and suggests a direction to machine comprehension on multiparty dialog.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1186/>Dialog Generation Using Multi-Turn Reasoning Neural Networks</a></strong><br><a href=/people/x/xianchao-wu/>Xianchao Wu</a>
|
<a href=/people/a/ander-martinez/>Ander MartÃ­nez</a>
|
<a href=/people/m/momo-klyen/>Momo Klyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1186><div class="card-body p-3 small">In this paper, we propose a generalizable dialog generation approach that adapts multi-turn reasoning, one recent advancement in the field of document comprehension, to generate responses (answers) by taking current conversation session context as a document and current query as a question. The major idea is to represent a conversation session into memories upon which attention-based memory reading mechanism can be performed multiple times, so that (1) user&#8217;s query is properly extended by contextual clues and (2) optimal responses are step-by-step generated. Considering that the speakers of one conversation are not limited to be one, we separate the single memory used for document comprehension into different groups for speaker-specific topic and opinion embedding. Namely, we utilize the queries&#8217; memory, the responses&#8217; memory, and their unified memory, following the time sequence of the conversation session. Experiments on Japanese 10-sentence (5-round) conversation modeling show impressive results on how multi-turn reasoning can produce more diverse and acceptable responses than state-of-the-art single-turn and non-reasoning baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1187/>Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/g/gokhan-tur/>Gokhan TÃ¼r</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-TÃ¼r</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/l/larry-heck/>Larry Heck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1187><div class="card-body p-3 small">In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions. Popular methods for learning task-oriented dialogues include applying <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with <a href=https://en.wikipedia.org/wiki/User-generated_content>user feedback</a> on supervised pre-training models. Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to-end with the proposed learning method. Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with user feedback after the imitation learning stage further improves the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>&#8217;s capability in successfully completing a task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1188.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1188.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1188/>LSDSCC : a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics<span class=acl-fixed-case>LSDSCC</span>: a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics</a></strong><br><a href=/people/z/zhen-xu/>Zhen Xu</a>
|
<a href=/people/n/nan-jiang/>Nan Jiang</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a>
|
<a href=/people/w/wenge-rong/>Wenge Rong</a>
|
<a href=/people/b/bowen-wu/>Bowen Wu</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a>
|
<a href=/people/z/zhuoran-wang/>Zhuoran Wang</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1188><div class="card-body p-3 small">It has been proven that automatic conversational agents can be built up using the Endto-End Neural Response Generation (NRG) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation metrics for testing. This paper proposes a Large Scale Domain-Specific Conversational Corpus (LSDSCC) composed of high-quality queryresponse pairs extracted from the domainspecific online forum, with thorough preprocessing and cleansing procedures. Also, a <a href=https://en.wikipedia.org/wiki/Test_set>testing set</a>, including multiple diverse responses annotated for each query, is constructed, and on this basis, the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for measuring the diversity of generated results are further presented. We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The experimental results have shown that our proposed <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can be taken as a new benchmark dataset for the NRG task, and the presented metrics are promising to guide the optimization of NRG models by quantifying the diversity of the generated responses reasonably.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1191/>Mining Evidences for Concept Stock Recommendation</a></strong><br><a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1191><div class="card-body p-3 small">We investigate the task of mining relevant stocks given a topic of concern on <a href=https://en.wikipedia.org/wiki/Emerging_market>emerging capital markets</a>, for which there is lack of <a href=https://en.wikipedia.org/wiki/Structural_analysis>structural understanding</a>. Deep learning is leveraged to mine evidences from large scale textual data, which contain valuable market information. In particular, distributed word similarities trained over large scale raw texts are taken as a basis of relevance measuring, and deep reinforcement learning is leveraged to learn a strategy of topic expansion, given a small amount of manually labeled data from financial analysts. Results on two Chinese stock market datasets show that our method outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> using <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval techniques</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1192/>Binarized LSTM Language Model<span class=acl-fixed-case>LSTM</span> Language Model</a></strong><br><a href=/people/x/xuan-liu/>Xuan Liu</a>
|
<a href=/people/d/di-cao/>Di Cao</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1192><div class="card-body p-3 small">Long short-term memory (LSTM) language model (LM) has been widely investigated for <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition (ASR)</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Although excellent performance is obtained for large vocabulary tasks, tremendous <a href=https://en.wikipedia.org/wiki/Computer_memory>memory consumption</a> prohibits the use of LSTM LM in low-resource devices. The memory consumption mainly comes from the word embedding layer. In this paper, a novel binarized LSTM LM is proposed to address the problem. Words are encoded into binary vectors and other LSTM parameters are further binarized to achieve high <a href=https://en.wikipedia.org/wiki/Data_compression>memory compression</a>. This is the first effort to investigate binary LSTM for large vocabulary LM. Experiments on both English and Chinese LM and ASR tasks showed that can achieve a <a href=https://en.wikipedia.org/wiki/Compression_ratio>compression ratio</a> of 11.3 without any loss of <a href=https://en.wikipedia.org/wiki/Line_level>LM</a> and ASR performances and a <a href=https://en.wikipedia.org/wiki/Compression_ratio>compression ratio</a> of 31.6 with acceptable minor performance degradation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1194/>How Time Matters : Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues</a></strong><br><a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/p/pei-chieh-yuan/>Pei-Chieh Yuan</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1194><div class="card-body p-3 small">Spoken language understanding (SLU) is an essential component in <a href=https://en.wikipedia.org/wiki/Interpersonal_communication>conversational systems</a>. Most SLU components treats each utterance independently, and then the following <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> aggregate the multi-turn information in the separate phases. In order to avoid <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and effectively utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contexts</a>, prior work leveraged <a href=https://en.wikipedia.org/wiki/History>history</a> for contextual SLU. However, most previous <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> only paid attention to the related content in history utterances, ignoring their temporal information. In the dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, in other words, time-aware attention should be in a decaying manner. Therefore, this paper designs and investigates various types of time-decay attention on the sentence-level and speaker-level, and further proposes a flexible universal time-decay attention mechanism. The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed time-decay attention mechanisms significantly improve the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a> for contextual understanding performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1195/>Towards Understanding Text Factors in Oral Reading</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/v/van-rynald-t-liceralde/>Van Rynald T. Liceralde</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1195><div class="card-body p-3 small">Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the <a href=https://en.wikipedia.org/wiki/Reading_rate>reading rate</a>, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1198 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1198/>Object Counts ! Bringing Explicit Detections Back into Image Captioning</a></strong><br><a href=/people/j/josiah-wang/>Josiah Wang</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1198><div class="card-body p-3 small">The use of explicit object detectors as an intermediate step to image captioning which used to constitute an essential stage in early work is often bypassed in the currently dominant end-to-end approaches, where the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> is conditioned directly on a mid-level image embedding. We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well. We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections. Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation. It also reveals that different <a href=https://en.wikipedia.org/wiki/Object_(philosophy)>object categories</a> contribute in different ways towards image captioning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1203.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672864 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1203/>Learning to Map Context-Dependent Sentences to Executable Formal Queries</a></strong><br><a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1203><div class="card-body p-3 small">We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction history, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach combines implicit and explicit modeling of references between utterances. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the ATIS flight planning interactions, and demonstrate the benefits of modeling context and explicit references.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1204/>Neural Text Generation in Stories Using Entity Representations as Context</a></strong><br><a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1204><div class="card-body p-3 small">We introduce an approach to neural text generation that explicitly represents entities mentioned in the text. Entity representations are vectors that are updated as the text proceeds ; they are designed specifically for narrative text like <a href=https://en.wikipedia.org/wiki/Fiction>fiction</a> or <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news stories</a>. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations : mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evaluation on automatically generated text in story contexts ; this study supports our emphasis on <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a> and suggests directions for further research.</div></div></div><hr><div id=n18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-2/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></strong><br><a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2003.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2003/>Gender Bias in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : Evaluation and Debiasing Methods</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2003><div class="card-body p-3 small">In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for co-reference resolution focused on <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>, WinoBias. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2005/>Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Small Labelled Dataset</a></strong><br><a href=/people/p/pavithra-rajendran/>Pavithra Rajendran</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/s/simon-parsons/>Simon Parsons</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2005><div class="card-body p-3 small">Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of reviews have been widely investigated in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> in general, and, in particular, in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. This is a subset of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> that deals with extracting arguments and the relations among them from user-based content. A major problem faced by <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> research is the lack of human-annotated data. In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets. We do this by building on previous work that explores the classification of opinions present in reviews based whether the stance is expressed explicitly or implicitly. In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit / explicit opinion classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2006/>Multi-Task Learning for Argumentation Mining in Low-Resource Settings</a></strong><br><a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/t/tobias-kahse/>Tobias Kahse</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2006><div class="card-body p-3 small">We investigate whether and where <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning (MTL)</a> can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2007/>Neural Models for Reasoning over Multiple Mentions Using Coreference</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/z/zhilin-yang/>Zhilin Yang</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2007><div class="card-body p-3 small">Many problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets Wikihop, LAMBADA and the bAbi AI tasks with large gains when training data is scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2010/>Natural Language Generation by Hierarchical Decoding with Linguistic Patterns</a></strong><br><a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/k/kai-ling-lo/>Kai-Ling Lo</a>
|
<a href=/people/y/yi-ting-yeh/>Yi-Ting Yeh</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2010><div class="card-body p-3 small">Natural language generation (NLG) is a critical component in spoken dialogue systems. Classic NLG can be divided into two phases : (1) sentence planning : deciding on the overall <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>, (2) surface realization : determining specific word forms and flattening the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a> into a string. Many simple NLG models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure ; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion. However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge. This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size. Furthermore, the design of the hierarchical decoding is flexible and easily-extendible in various NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2012/>RankME : Reliable Human Ratings for Natural Language Generation<span class=acl-fixed-case>R</span>ank<span class=acl-fixed-case>ME</span>: Reliable Human Ratings for Natural Language Generation</a></strong><br><a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a>
|
<a href=/people/o/ondrej-dusek/>OndÅ™ej DuÅ¡ek</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2012><div class="card-body p-3 small">Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2013/>Sentence Simplification with Memory-Augmented Neural Networks</a></strong><br><a href=/people/t/tu-vu/>Tu Vu</a>
|
<a href=/people/b/baotian-hu/>Baotian Hu</a>
|
<a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2013><div class="card-body p-3 small">Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have paved the way for novel approaches to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we adapt an <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> with augmented memory capacities called Neural Semantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2014/>A Corpus of Non-Native Written English Annotated for Metaphor<span class=acl-fixed-case>E</span>nglish Annotated for Metaphor</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/c/chee-wee-leong/>Chee Wee (Ben) Leong</a>
|
<a href=/people/m/michael-flor/>Michael Flor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2014><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 240 argumentative essays written by non-native speakers of English annotated for <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is made publicly available. We provide benchmark performance of state-of-the-art systems on this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and explore the relationship between writing proficiency and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor use</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2016.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2016/>An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols</a></strong><br><a href=/people/c/chaitanya-kulkarni/>Chaitanya Kulkarni</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/r/raghu-machiraju/>Raghu Machiraju</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2016><div class="card-body p-3 small">We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2017/>Annotation Artifacts in Natural Language Inference Data</a></strong><br><a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2017><div class="card-body p-3 small">Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a> leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67 % of <a href=https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism>SNLI</a> (Bowman et. al, 2015) and 53 % of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a> are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2018/>Humor Recognition Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/p/peng-yu-chen/>Peng-Yu Chen</a>
|
<a href=/people/v/von-wun-soo/>Von-Wun Soo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2018><div class="card-body p-3 small">Humor is an essential but most fascinating element in personal communication. How to build <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to discover the structures of humor, recognize <a href=https://en.wikipedia.org/wiki/Humour>humor</a> and even generate <a href=https://en.wikipedia.org/wiki/Humour>humor</a> remains a challenge and there have been yet few attempts on it. In this paper, we construct and collect four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with distinct joke types in both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and conduct learning experiments on humor recognition. We implement a Convolutional Neural Network (CNN) with extensive filter size, number and Highway Networks to increase the depth of networks. Results show that our model outperforms in recognition of different types of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> with benchmarks collected in both English and Chinese languages on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> in comparison to previous works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2020.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2020/>Reference-less Measure of Faithfulness for Grammatical Error Correction</a></strong><br><a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2020><div class="card-body p-3 small">We propose USim, a semantic measure for Grammatical Error Correction (that measures the semantic faithfulness of the output to the source, thereby complementing existing reference-less measures (RLMs) for measuring the output&#8217;s grammaticality. USim operates by comparing the semantic symbolic structure of the source and the correction, without relying on manually-curated references. Our experiments establish the validity of USim, by showing that the semantic structures can be consistently applied to ungrammatical text, that valid corrections obtain a high USim similarity score to the source, and that invalid corrections obtain a lower score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2024/>Analogies in Complex Verb Meaning Shifts : the Effect of Affect in Semantic Similarity Models</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian KÃ¶per</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2024><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> to detect and distinguish analogies in meaning shifts between German base and complex verbs. In contrast to <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-based studies</a>, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrates that regular shifts represent the smallest class. Classification experiments relying on a standard similarity model successfully distinguish between four types of shifts, with verb classes boosting the performance, and affective features for abstractness, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> representing the most salient indicators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2025/>Character-Based Neural Networks for Sentence Pair Modeling</a></strong><br><a href=/people/w/wuwei-lan/>Wuwei Lan</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2025><div class="card-body p-3 small">Sentence pair modeling is critical for many NLP tasks, such as paraphrase identification, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic textual similarity</a>, and natural language inference. Most state-of-the-art neural models for these tasks rely on pretrained word embedding and compose sentence-level semantics in varied ways ; however, few works have attempted to verify whether we really need pretrained embeddings in these tasks. In this paper, we study how effective subword-level (character and character n-gram) representations are in sentence pair modeling. Though it is well-known that subword models are effective in tasks with single sentence input, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2027.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2027/>Diachronic Usage Relatedness (DURel): A Framework for the Annotation of Lexical Semantic Change<span class=acl-fixed-case>DUR</span>el): A Framework for the Annotation of Lexical Semantic Change</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/s/stefanie-eckmann/>Stefanie Eckmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2027><div class="card-body p-3 small">We propose a framework that extends synchronic polysemy annotation to diachronic changes in lexical meaning, to counteract the lack of resources for evaluating computational models of lexical semantic change. Our framework exploits an intuitive notion of semantic relatedness, and distinguishes between innovative and reductive meaning changes with high inter-annotator agreement. The resulting <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test set</a> for <a href=https://en.wikipedia.org/wiki/German_language>German</a> comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2029.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2029.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2029" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2029/>Discriminating between Lexico-Semantic Relations with the Specialization Tensor Model</a></strong><br><a href=/people/g/goran-glavas/>Goran GlavaÅ¡</a>
|
<a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2029><div class="card-body p-3 small">We present a simple and effective feed-forward neural architecture for discriminating between lexico-semantic relations (synonymy, antonymy, hypernymy, and meronymy). Our Specialization Tensor Model (STM) simultaneously produces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a bilingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2030/>Evaluating bilingual word embeddings on the long tail</a></strong><br><a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/t/tobias-eder/>Tobias Eder</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2030><div class="card-body p-3 small">Bilingual word embeddings are useful for bilingual lexicon induction, the task of mining translations of given words. Many studies have shown that bilingual word embeddings perform well for bilingual lexicon induction but they focused on frequent words in general domains. For many applications, bilingual lexicon induction of rare and domain-specific words is of critical importance. Therefore, we design a new task to evaluate bilingual word embeddings on rare words in different domains. We show that state-of-the-art approaches fail on this task and present simple new techniques to improve bilingual word embeddings for mining rare words. We release new gold standard datasets and code to stimulate research on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2031/>Frustratingly Easy Meta-Embedding Computing Meta-Embeddings by Averaging Source Word Embeddings</a></strong><br><a href=/people/j/joshua-coates/>Joshua Coates</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2031><div class="card-body p-3 small">Creating accurate meta-embeddings from pre-trained source embeddings has received attention lately. Methods based on global and locally-linear transformation and <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a> have shown to produce accurate meta-embeddings. In this paper, we show that the <a href=https://en.wikipedia.org/wiki/Arithmetic_mean>arithmetic mean</a> of two distinct word embedding sets yields a performant meta-embedding that is comparable or better than more complex meta-embedding learning methods. The result seems counter-intuitive given that <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a> in different source embeddings are not comparable and can not be simply averaged. We give insight into why <a href=https://en.wikipedia.org/wiki/Average>averaging</a> can still produce accurate meta-embedding despite the incomparability of the source vector spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2032/>Introducing Two Vietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness<span class=acl-fixed-case>V</span>ietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2032><div class="card-body p-3 small">We present two novel datasets for the low-resource language Vietnamese to assess models of semantic similarity : ViCon comprises pairs of <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2037/>Similarity Measures for the Detection of Clinical Conditions with Verbal Fluency Tasks</a></strong><br><a href=/people/f/felipe-paula/>Felipe Paula</a>
|
<a href=/people/r/rodrigo-wilkens/>Rodrigo Wilkens</a>
|
<a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2037><div class="card-body p-3 small">Semantic Verbal Fluency tests have been used in the detection of certain <a href=https://en.wikipedia.org/wiki/Disease>clinical conditions</a>, like <a href=https://en.wikipedia.org/wiki/Dementia>Dementia</a>. In particular, given a sequence of semantically related words, a large number of switches from one semantic class to another has been linked to clinical conditions. In this work, we investigate three similarity measures for automatically identifying switches in semantic chains : <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> from a manually constructed resource, and word association strength and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>, both calculated from corpora. This information is used for building classifiers to distinguish healthy controls from clinical cases with early stages of Alzheimer&#8217;s Disease and Mild Cognitive Deficits. The overall results indicate that for clinical conditions the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that use these <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> outperform those that use a gold standard taxonomy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2039/>The Word Analogy Testing Caveat</a></strong><br><a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2039><div class="card-body p-3 small">There are some important problems in the evaluation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> using standard word analogy tests. In particular, in virtue of the assumptions made by systems generating the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, these remain tests over randomness. We show that even supposing there were such word analogy regularities that should be detected in the word embeddings obtained via unsupervised means, standard word analogy test implementation practices provide distorted or contrived results. We raise concerns regarding the use of <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>Principal Component Analysis</a> to 2 or 3 dimensions as a provision of visual evidence for the existence of word analogy relations in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Finally, we propose some solutions to these problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2040/>Transition-Based Chinese AMR Parsing<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2040><div class="card-body p-3 small">This paper presents the first AMR parser built on the Chinese AMR bank. By applying a transition-based AMR parsing framework to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we first investigate how well the transitions first designed for <a href=https://en.wikipedia.org/wiki/English_language>English AMR parsing</a> generalize to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and provide a comparative analysis between the transitions for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We then perform a detailed <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> to identify the major challenges in Chinese AMR parsing that we hope will inform future research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2041/>Knowledge-Enriched Two-Layered Attention Network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/a/abhishek-kumar/>Abhishek Kumar</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2041><div class="card-body p-3 small">We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. The novel two-layered attention network takes advantage of the <a href=https://en.wikipedia.org/wiki/Knowledge_base>external knowledge bases</a> to improve the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. It uses the Knowledge Graph Embedding generated using the <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> surpasses the top system of SemEval 2017 Task 5. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2043/>Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/p/prateek-vij/>Prateek Vij</a>
|
<a href=/people/g/gangeshwar-krishnamurthy/>Gangeshwar Krishnamurthy</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2043><div class="card-body p-3 small">Aspect-based Sentiment Analysis is a fine-grained task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classification</a> for multiple aspects in a sentence. Present neural-based models exploit aspect and its contextual information in the sentence but largely ignore the inter-aspect dependencies. In this paper, we incorporate this pattern by simultaneous classification of all aspects in a sentence along with temporal dependency processing of their corresponding sentence representations using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a>. Results on the benchmark SemEval 2014 dataset suggest the effectiveness of our proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2045/>Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2045><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external memory chains with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2049.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898113 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2049/>Modeling Semantic Plausibility by Injecting World Knowledge</a></strong><br><a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2049><div class="card-body p-3 small">Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested. However both are physically plausible events. This paper introduces the task of semantic plausibility : recognizing plausible but possibly novel events. We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball. Simple models based on <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional representations</a> perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost. Our error analysis shows that our new dataset is a great testbed for semantic plausibility models : more sophisticated <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> and propagation could address many of the remaining errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2050/>A Bi-Model Based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling<span class=acl-fixed-case>RNN</span> Semantic Frame Parsing Model for Intent Detection and Slot Filling</a></strong><br><a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2050><div class="card-body p-3 small">Intent detection and slot filling are two main tasks for building a spoken language understanding(SLU) system. Multiple <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based models</a> have demonstrated good results on these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The most effective algorithms are based on the structures of sequence to sequence models (or encoder-decoder models), and generate the intents and semantic tags either using separate models. Most of the previous studies, however, either treat the <a href=https://en.wikipedia.org/wiki/Intention>intent detection</a> and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and <a href=https://en.wikipedia.org/wiki/Intention>intent</a>. None of the approaches consider the cross-impact between the intent detection task and the slot filling task. In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> achieves state-of-art result on the benchmark <a href=https://en.wikipedia.org/wiki/Automated_airport_weather_station>ATIS data</a>, with about 0.5 % intent accuracy improvement and 0.9 % slot filling improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2052/>A Laypeople Study on Terminology Identification across Domains and Task Definitions</a></strong><br><a href=/people/a/anna-hatty/>Anna HÃ¤tty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2052><div class="card-body p-3 small">This paper introduces a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of term annotation. Given that even experts vary significantly in their understanding of termhood, and that term identification is mostly performed as a binary task, we offer a novel perspective to explore the common, natural understanding of what constitutes a term : Laypeople annotate single-word and multi-word terms, across four domains and across four task definitions. Analyses based on <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> offer insights into differences in term specificity, <a href=https://en.wikipedia.org/wiki/Granularity>term granularity</a> and subtermhood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2053/>A Novel Embedding Model for Knowledge Base Completion Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/t/tu-dinh-nguyen/>Tu Dinh Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/d/dinh-phung/>Dinh Phung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2053><div class="card-body p-3 small">In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a>, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, so that it can capture global relationships and transitional characteristics between entities and relations in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> are operated on the <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> to generate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a>. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a> are then concatenated into a single <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> representing the input triple. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> is multiplied with a <a href=https://en.wikipedia.org/wiki/Weight_vector>weight vector</a> via a <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a> to return a score. This <a href=https://en.wikipedia.org/wiki/Score_(game)>score</a> is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2054/>Cross-language Article Linking Using Cross-Encyclopedia Entity Embedding</a></strong><br><a href=/people/c/chun-kai-wu/>Chun-Kai Wu</a>
|
<a href=/people/r/richard-tzong-han-tsai/>Richard Tzong-Han Tsai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2054><div class="card-body p-3 small">Cross-language article linking (CLAL) is the task of finding corresponding article pairs of different languages across <a href=https://en.wikipedia.org/wiki/Encyclopedia>encyclopedias</a>. This task is a difficult disambiguation problem in which one article must be selected among several candidate articles with similar titles and contents. Existing works focus on engineering text-based or link-based features for this task, which is a time-consuming job, and some of these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are only applicable within the same <a href=https://en.wikipedia.org/wiki/Encyclopedia>encyclopedia</a>. In this paper, we address these problems by proposing cross-encyclopedia entity embedding. Unlike other works, our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> does not rely on known cross-language pairs. We apply our method to <a href=https://en.wikipedia.org/wiki/CLAL>CLAL</a> between <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> and Chinese Baidu Baike. Our <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> improve performance relative to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 29.62 %. Tested 30 times, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an average improvement of 2.76 % over the current best <a href=https://en.wikipedia.org/wiki/System>system</a> (26.86 % over baseline), a statistically significant result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2055/>Identifying the Most Dominant Event in a News Article by Mining Event Coreference Relations</a></strong><br><a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/k/kaushik-raju/>Kaushik Raju</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2055><div class="card-body p-3 small">Identifying the most dominant and central event of a document, which governs and connects other foreground and background events in the document, is useful for many applications, such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, storyline generation and <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a>. We observed that the central event of a document usually has many coreferential event mentions that are scattered throughout the document for enabling a smooth transition of subtopics. Our empirical experiments, using gold event coreference relations, have shown that the central event of a document can be well identified by mining properties of event coreference chains. But the performance drops when switching to system predicted event coreference relations. In addition, we found that the central event can be more accurately identified by further considering the number of sub-events as well as the realis status of an event.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2057/>Keep Your Bearings : Lightly-Supervised Information Extraction with <a href=https://en.wikipedia.org/wiki/Ladder_network>Ladder Networks</a> That Avoids <a href=https://en.wikipedia.org/wiki/Semantic_drift>Semantic Drift</a></a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2057><div class="card-body p-3 small">We propose a novel approach to <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> that uses <a href=https://en.wikipedia.org/wiki/Ladder_network>ladder networks</a> (Rasmus et al., 2015). In particular, we focus on the task of named entity classification, defined as identifying the correct label (e.g., person or organization name) of an entity mention in a given context. Our approach is simple, efficient and has the benefit of being robust to <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, a dominant problem in most <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning systems</a>. We empirically demonstrate the superior performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for named entity classification. We obtain between 62 % and 200 % improvement over the state-of-art baseline on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2058/>Semi-Supervised Event Extraction with Paraphrase Clusters</a></strong><br><a href=/people/j/james-ferguson/>James Ferguson</a>
|
<a href=/people/c/colin-lockard/>Colin Lockard</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2058><div class="card-body p-3 small">Supervised event extraction systems are limited in their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> due to the lack of available training data. We present a method for self-training event extraction systems by bootstrapping additional training data. This is done by taking advantage of the occurrence of multiple mentions of the same event instances across newswire articles from multiple sources. If our system can make a high-confidence extraction of some mentions in such a cluster, it can then acquire diverse training examples by adding the other mentions as well. Our experiments show significant performance improvements on multiple event extractors over ACE 2005 and TAC-KBP 2015 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2059.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2059/>Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text<span class=acl-fixed-case>C</span>hinese Literature Text</a></strong><br><a href=/people/j/ji-wen/>Ji Wen</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2059><div class="card-body p-3 small">Relation classification is an important semantic processing task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose the task of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation classification</a> for Chinese literature text. A new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a> is constructed to facilitate the study in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized dependency tree, which has the benefits of reducing the complexity of the whole <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly improves the <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> by 10.3, and outperforms the state-of-the-art approaches on <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2061/>Syntactically Aware Neural Architectures for Definition Extraction</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2061><div class="card-body p-3 small">Automatically identifying definitional knowledge in text corpora (Definition Extraction or DE) is an important task with direct applications in, among others, Automatic Glossary Generation, Taxonomy Learning, Question Answering and Semantic Search. It is generally cast as a binary classification problem between definitional and non-definitional sentences. In this paper we present a set of neural architectures combining Convolutional and Recurrent Neural Networks, which are further enriched by incorporating linguistic information via syntactic dependencies. Our experimental results in the task of sentence classification, on two benchmarking DE datasets (one generic, one domain-specific), show that these models obtain consistent state of the art results. Furthermore, we demonstrate that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean Wikipedia-like definitions can successfully be applied to more noisy domain-specific corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2064/>Automatically Selecting the Best Dependency Annotation Design with Dynamic Oracles</a></strong><br><a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/o/ophelie-lacroix/>OphÃ©lie Lacroix</a>
|
<a href=/people/f/francois-yvon/>FranÃ§ois Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2064><div class="card-body p-3 small">This work introduces a new strategy to compare the numerous conventions that have been proposed over the years for expressing dependency structures and discover the one for which a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> will achieve the highest <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance. Instead of associating each sentence in the training set with a single gold reference we propose to consider a set of references encoding alternative syntactic representations. Training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> with a dynamic oracle will then automatically select among all alternatives the reference that will be predicted with the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experiments on the UD corpora show the validity of this approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2065/>Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning<span class=acl-fixed-case>CCG</span> Parsing over Multiple Sentences for Improved Logical Reasoning</a></strong><br><a href=/people/m/masashi-yoshikawa/>Masashi Yoshikawa</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2065><div class="card-body p-3 small">In formal logic-based approaches to Recognizing Textual Entailment (RTE), a Combinatory Categorial Grammar (CCG) parser is used to parse input premises and hypotheses to obtain their logical formulas. Here, it is important that the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> processes the sentences consistently ; failing to recognize the similar syntactic structure results in inconsistent predicate argument structures among them, in which case the succeeding theorem proving is doomed to failure. In this work, we present a simple method to extend an existing CCG parser to parse a set of sentences consistently, which is achieved with an inter-sentence modeling with Markov Random Fields (MRF). When combined with existing <a href=https://en.wikipedia.org/wiki/Logic_programming>logic-based systems</a>, our method always shows improvement in the <a href=https://en.wikipedia.org/wiki/Real-time_computing>RTE</a> experiments on English and Japanese languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2066/>Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>FranÃ§ois Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2066><div class="card-body p-3 small">Because the most common transition systems are projective, training a transition-based dependency parser often implies to either ignore or rewrite the non-projective training examples, which has an adverse impact on accuracy. In this work, we propose a simple modification of dynamic oracles, which enables the use of non-projective data when training projective parsers. Evaluation on 73 treebanks shows that our method achieves significant gains (+2 to +7 UAS for the most non-projective languages) and consistently outperforms traditional projectivization and pseudo-projectivization approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2067/>Improving Coverage and Runtime Complexity for Exact Inference in Non-Projective Transition-Based Dependency Parsers</a></strong><br><a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos GÃ³mez-RodrÃ­guez</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2067><div class="card-body p-3 small">We generalize Cohen, Gmez-Rodrguez, and Satta&#8217;s (2011) parser to a family of non-projective transition-based dependency parsers allowing polynomial-time exact inference. This includes novel <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> with better coverage than Cohen et al. (2011), and even a variant that reduces <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> to O(n^6), improving over the known bounds in exact inference for non-projective transition-based parsing. We hope that this piece of theoretical work inspires design of novel <a href=https://en.wikipedia.org/wiki/Transition_system>transition systems</a> with better coverage and better <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time guarantees</a>.<tex-math>O(n^6)</tex-math>, improving over the known bounds in exact inference for non-projective transition-based parsing. We hope that this piece of theoretical work inspires design of novel transition systems with better coverage and better run-time guarantees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2068/>Towards a Variability Measure for Multiword Expressions</a></strong><br><a href=/people/c/caroline-pasquer/>Caroline Pasquer</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/j/jean-yves-antoine/>Jean-Yves Antoine</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2068><div class="card-body p-3 small">One of the most outstanding properties of multiword expressions (MWEs), especially verbal ones (VMWEs), important both in theoretical models and applications, is their idiosyncratic variability. Some MWEs are always continuous, while some others admit certain types of insertions. Components of some MWEs are rarely or never modified, while some others admit either specific or unrestricted modification. This unpredictable variability profile of MWEs hinders modeling and processing them as <a href=https://en.wikipedia.org/wiki/Word_space>words-with-spaces</a> on the one hand, and as regular syntactic structures on the other hand. Since variability of MWEs is a matter of scale rather than a binary property, we propose a 2-dimensional language-independent measure of variability dedicated to verbal MWEs based on syntactic and discontinuity-related clues. We assess its relevance with respect to a linguistic benchmark and its utility for the tasks of VMWE classification and variant identification on a <a href=https://en.wikipedia.org/wiki/French_language>French corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2072" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2072/>Contextual Augmentation : Data Augmentation by Words with Paradigmatic Relations</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2072><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> based on the convolutional or recurrent neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2074/>Self-Attention with Relative Position Representations</a></strong><br><a href=/people/p/peter-shaw/>Peter Shaw</a>
|
<a href=/people/j/jakob-uszkoreit/>Jakob Uszkoreit</a>
|
<a href=/people/a/ashish-vaswani/>Ashish Vaswani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2074><div class="card-body p-3 small">Relying entirely on an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> introduced by Vaswani et al. (2017) achieves state-of-the-art results for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> yields improvements of 1.3 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> and 0.3 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> over <a href=https://en.wikipedia.org/wiki/Bitwise_operation>absolute position representations</a>, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2077/>Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation<span class=acl-fixed-case>H</span>y<span class=acl-fixed-case>TER</span> Machine Translation Evaluation</a></strong><br><a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2077><div class="card-body p-3 small">We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions. The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, HyTER with automatically built paraphrase lattices. We show that although the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for tuning and evaluation of current MT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2078.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2078/>Exploiting <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Graph Convolutional Networks</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2078><div class="card-body p-3 small">Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a>. In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the EnglishGerman language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2079.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2079/>Incremental Decoding and Training Methods for <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>Simultaneous Translation</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2079><div class="card-body p-3 small">We address the problem of <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a> by modifying the Neural MT decoder to operate with dynamically built encoder and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2080/>Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models</a></strong><br><a href=/people/d/david-vilar/>David Vilar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2080><div class="card-body p-3 small">In this paper we explore the use of Learning Hidden Unit Contribution for the task of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> was initially proposed in the context of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> for adapting a general <a href=https://en.wikipedia.org/wiki/System>system</a> to the specific acoustic characteristics of each speaker. Similar in spirit, in a machine translation framework we want to adapt a general system to a specific domain. We show that the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves improvements of up to 2.6 BLEU points over a general system, and up to 6 BLEU points if the initial <a href=https://en.wikipedia.org/wiki/System>system</a> has only been trained on out-of-domain data, a situation which may easily happen in practice. The good performance together with its short training time and small <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> make it a very attractive solution for domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2081/>Neural Machine Translation Decoding with Terminology Constraints</a></strong><br><a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/a/adria-de-gispert/>AdriÃ  de Gispert</a>
|
<a href=/people/g/gonzalo-iglesias/>Gonzalo Iglesias</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2081><div class="card-body p-3 small">Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2082" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2082/>On the Evaluation of Semantic Phenomena in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using Natural Language Inference</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2082><div class="card-body p-3 small">We propose a process for investigating the extent to which <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> appears most suited to supporting inferences at the syntax-semantics interface, as compared to <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a> requiring <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2083/>Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation</a></strong><br><a href=/people/n/nima-pourdamghani/>Nima Pourdamghani</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2083><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for improving <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> using word similarities. This method is based on encouraging common alignment links between semantically similar words. We use <a href=https://en.wikipedia.org/wiki/Word_vector>word vectors</a> trained on <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> to estimate <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a>. Our experiments on translating fifteen languages into English show consistent BLEU score improvements across the languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2084.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2084" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2084/>When and Why Are Pre-Trained Word Embeddings Useful for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>?</a></strong><br><a href=/people/y/ye-qi/>Ye Qi</a>
|
<a href=/people/d/devendra-sachan/>Devendra Sachan</a>
|
<a href=/people/m/matthieu-felix/>Matthieu Felix</a>
|
<a href=/people/s/sarguna-padmanabhan/>Sarguna Padmanabhan</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2084><div class="card-body p-3 small">The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora can not be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for <a href=https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance_spectroscopy>NMT</a> has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases providing gains of up to 20 BLEU points in the most favorable setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2086/>The <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>Computational Complexity</a> of Distinctive Feature Minimization in Phonology</a></strong><br><a href=/people/h/hubie-chen/>Hubie Chen</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2086><div class="card-body p-3 small">We analyze the complexity of the problem of determining whether a set of <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> forms a natural class and, if so, that of finding the minimal feature specification for the <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class</a>. A standard assumption in <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a> is that finding a minimal feature specification is an automatic part of <a href=https://en.wikipedia.org/wiki/Language_acquisition>acquisition</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. We find that the natural class decision problem is tractable (i.e. is in P), while the minimization problem is not ; the decision version of the problem which determines whether a <a href=https://en.wikipedia.org/wiki/Natural_class>natural class</a> can be defined with k features or less is NP-complete. We also show that, empirically, a <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a> for finding minimal feature specifications will sometimes fail, and thus can not be assumed to be the basis for human performance in solving the problem.<tex-math>k</tex-math> features or less is NP-complete. We also show that, empirically, a greedy algorithm for finding minimal feature specifications will sometimes fail, and thus cannot be assumed to be the basis for human performance in solving the problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2089" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2089/>Crowdsourcing Question-Answer Meaning Representations</a></strong><br><a href=/people/j/julian-michael/>Julian Michael</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2089><div class="card-body p-3 small">We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with over 5,000 sentences and 100,000 questions. A qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, NomBank, and QA-SRL) along with many previously under-resourced ones, including implicit arguments and relations. We also report baseline models for question generation and answering, and summarize a recent approach for using QAMR labels to improve an Open IE system. These results suggest the freely available QAMR data and annotation scheme should support significant future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2091/>Robust Machine Comprehension Models via Adversarial Training</a></strong><br><a href=/people/y/yicheng-wang/>Yicheng Wang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2091><div class="card-body p-3 small">It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50 % decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent&#8217;s semantic perturbations (e.g., antonyms), we jointly improve the model&#8217;s semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly more robust, achieving a 36.5 % increase in F1 score under many different types of <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial evaluation</a> while maintaining performance on the regular SQuAD task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2092/>Simple and Effective Semi-Supervised Question Answering</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/d/danish-danish/>Danish Danish</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2092><div class="card-body p-3 small">Recent success of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora. However, large domain specific annotated corpora are limited and expensive to construct. In this work, we envision a <a href=https://en.wikipedia.org/wiki/System>system</a> where the end user specifies a set of base documents and only a few labelled examples. Our system exploits the document structure to create cloze-style questions from these base documents ; pre-trains a powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> on the cloze style questions ; and further fine-tunes the model on the labeled examples. We evaluate our proposed system across three diverse datasets from different domains, and find it to be highly effective with very little labeled data. We attain more than 50 % F1 score on SQuAD and TriviaQA with less than a thousand labelled examples. We are also releasing a set of 3.2 M cloze-style questions for practitioners to use while building <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2094/>Community Member Retrieval on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> Using Textual Information</a></strong><br><a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/s/shobhit-hathi/>Shobhit Hathi</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2094><div class="card-body p-3 small">This paper addresses the problem of community membership detection using only text features in a scenario where a small number of positive labeled examples defines the community. The solution introduces an unsupervised proxy task for learning user embeddings : user re-identification. Experiments with 16 different communities show that the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are more effective for community membership identification than common unsupervised representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2096/>Predicting Foreign Language Usage from English-Only Social Media Posts<span class=acl-fixed-case>E</span>nglish-Only Social Media Posts</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/s/stephen-ranshous/>Stephen Ranshous</a>
|
<a href=/people/l/lawrence-phillips/>Lawrence Phillips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2096><div class="card-body p-3 small">Social media is known for its multi-cultural and multilingual interactions, a natural product of which is <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>. Multilingual speakers mix languages they tweet to address a different audience, express certain feelings, or attract attention. This paper presents a large-scale analysis of 6 million tweets produced by 27 thousand multilingual users speaking 12 other languages besides <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We rely on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to build <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> to infer non-English languages that users speak exclusively from their <a href=https://en.wikipedia.org/wiki/Twitter>English tweets</a>. Unlike native language identification task, we rely on large amounts of informal social media communications rather than <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL essays</a>. We contrast the predictive power of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> trained on lexical, syntactic, and stylistic signals with <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> learned from word, character and byte representations extracted from <a href=https://en.wikipedia.org/wiki/Twitter>English only tweets</a>. We report that <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>, style and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> are the most predictive of non-English languages that users speak on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Neural network models learned from byte representations of user content combined with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> yield the best performance. Finally, by analyzing cross-lingual transfer the influence of non-English languages on various levels of linguistic performance in English, we present novel findings on stylistic and syntactic variations across speakers of 12 languages in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2098/>A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard Table Summarization</a></strong><br><a href=/people/p/parag-jain/>Parag Jain</a>
|
<a href=/people/a/anirban-laha/>Anirban Laha</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a>
|
<a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/s/shreyas-shetty/>Shreyas Shetty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2098><div class="card-body p-3 small">Structured data summarization involves <a href=https://en.wikipedia.org/wiki/Natural-language_generation>generation of natural language summaries</a> from <a href=https://en.wikipedia.org/wiki/Structured_data>structured input data</a>. In this work, we consider summarizing structured data occurring in the form of tables as they are prevalent across a wide variety of domains. We formulate the standard table summarization problem, which deals with tables conforming to a single predefined schema. To this end, we propose a mixed hierarchical attention based encoder-decoder model which is able to leverage the structure in addition to the content of the tables. Our experiments on the publicly available weathergov dataset show around 18 BLEU (around 30 %) improvement over the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2099.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2099/>Effective <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> for a New Type of Summarization Task</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2099><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose targeted summarization as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include. We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect high-quality summaries for our task, a necessary step for the development of automatic systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2100.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2100/>Key2Vec : Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings<span class=acl-fixed-case>K</span>ey2<span class=acl-fixed-case>V</span>ec: Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings</a></strong><br><a href=/people/d/debanjan-mahata/>Debanjan Mahata</a>
|
<a href=/people/j/john-kuriakose/>John Kuriakose</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2100><div class="card-body p-3 small">Keyphrase extraction is a fundamental task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that facilitates mapping of documents to a set of representative phrases. In this paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles. Specifically, we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Evaluations are performed on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> producing state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2101 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2101/>Learning to Generate Wikipedia Summaries for Underserved Languages from Wikidata<span class=acl-fixed-case>W</span>ikipedia Summaries for Underserved Languages from <span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/l/lucie-aimee-kaffee/>Lucie-AimÃ©e Kaffee</a>
|
<a href=/people/h/hady-elsahar/>Hady Elsahar</a>
|
<a href=/people/p/pavlos-vougiouklis/>Pavlos Vougiouklis</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/f/frederique-laforest/>FrÃ©dÃ©rique Laforest</a>
|
<a href=/people/j/jonathon-hare/>Jonathon Hare</a>
|
<a href=/people/e/elena-simperl/>Elena Simperl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2101><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphological rich language with a larger vocabulary than <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and <a href=https://en.wikipedia.org/wiki/Esperanto>Esperanto</a>, a <a href=https://en.wikipedia.org/wiki/Constructed_language>constructed language</a> known for its easy acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2102 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2102/>Multi-Reward Reinforced Summarization with Saliency and Entailment</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2102><div class="card-body p-3 small">Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, <a href=https://en.wikipedia.org/wiki/Logical_consequence>directed logical entailment</a>, and <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>non-redundancy</a>. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions : ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases / words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these <a href=https://en.wikipedia.org/wiki/Reward_system>rewards</a> are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results on CNN / Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2103/>Objective Function Learning to Match Human Judgements for Optimization-Based Summarization</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2103><div class="card-body p-3 small">Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function including human judgments as supervision and automatically generated data as <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. We extract summaries with a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> using as a <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a>. We observe strong and promising performances across datasets in both automatic and manual evaluation.<tex-math>\\theta</tex-math> including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using <tex-math>\\theta</tex-math> as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2105.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2105/>Unsupervised Keyphrase Extraction with Multipartite Graphs</a></strong><br><a href=/people/f/florian-boudin/>Florian Boudin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2105><div class="card-body p-3 small">We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents keyphrase candidates and topics in a single <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> and exploits their mutually reinforcing relationship to improve candidate ranking. We further introduce a novel mechanism to incorporate keyphrase selection preferences into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experiments conducted on three widely used datasets show significant improvements over state-of-the-art <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph-based models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2106/>Where Have I Heard This Story Before? Identifying Narrative Similarity in Movie Remakes<span class=acl-fixed-case>I</span> Heard This Story Before? Identifying Narrative Similarity in Movie Remakes</a></strong><br><a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2106><div class="card-body p-3 small">People can identify correspondences between narratives in everyday life. For example, an analogy with the <a href=https://en.wikipedia.org/wiki/Cinderella>Cinderella story</a> may be made in describing the unexpected success of an underdog in seemingly different stories. We present a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for story understanding : identifying instances of similar narratives from a collection of narrative texts. We present an initial approach for this problem, which finds correspondences between narratives in terms of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot events</a>, and resemblances between characters and their <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>social relationships</a>. Our approach yields an 8 % absolute improvement in performance over a competitive information-retrieval baseline on a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot summaries</a> of 577 movie remakes from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671532 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2107/>Multimodal Emoji Prediction</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/f/francesco-ronzano/>Francesco Ronzano</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2107><div class="card-body p-3 small">Emojis are small images that are commonly included in <a href=https://en.wikipedia.org/wiki/SMS>social media text messages</a>. The combination of visual and textual content in the same message builds up a modern way of communication, that <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal approach</a> that is able to predict <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Instagram>Instagram posts</a>. Instagram posts are composed of <a href=https://en.wikipedia.org/wiki/Photograph>pictures</a> together with texts which sometimes include <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We show that these <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> can be predicted by using the text, but also using the picture. Our main finding is that incorporating the two synergistic modalities, in a combined model, improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and therefore can complement each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672817 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2108/>Higher-Order Coreference Resolution with Coarse-to-Fine Inference</a></strong><br><a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2108><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Differentiable_function>fully-differentiable approximation</a> to higher-order inference for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. Our approach uses the antecedent distribution from a span-ranking architecture as an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to iteratively refine span representations. This enables the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the English OntoNotes benchmark, while being far more computationally efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2109 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673868 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2109/>Non-Projective Dependency Parsing with Non-Local Transitions</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel FernÃ¡ndez-GonzÃ¡lez</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos GÃ³mez-RodrÃ­guez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2109><div class="card-body p-3 small">We present a novel transition system, based on the Covington non-projective parser, introducing non-local transitions that can directly create arcs involving nodes to the left of the current focus positions. This avoids the need for long sequences of No-Arcs transitions to create long-distance arcs, thus alleviating <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. The resulting <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> outperforms the original version and achieves the best accuracy on the Stanford Dependencies conversion of the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> among greedy transition-based parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2110 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2110/>Detecting Linguistic Characteristics of Alzheimerâ€™s Dementia by Interpreting Neural Models<span class=acl-fixed-case>A</span>lzheimerâ€™s Dementia by Interpreting Neural Models</a></strong><br><a href=/people/s/sweta-karlekar/>Sweta Karlekar</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2110><div class="card-body p-3 small">Alzheimer&#8217;s disease (AD) is an irreversible and progressive brain disease that can be stopped or slowed down with <a href=https://en.wikipedia.org/wiki/Therapy>medical treatment</a>. Language changes serve as a sign that a patient&#8217;s cognitive functions have been impacted, potentially leading to early diagnosis. In this work, we use <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP techniques</a> to classify and analyze the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic characteristics</a> of AD patients using the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs, and their combination, to distinguish between language samples from AD and control patients. We achieve a new <a href=https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables>independent benchmark accuracy</a> for the AD classification task. More importantly, we next interpret what these neural models have learned about the linguistic characteristics of AD patients, via analysis based on activation clustering and first-derivative saliency techniques. We then perform novel automatic pattern discovery inside activation clusters, and consolidate AD patients&#8217; distinctive grammar patterns. Additionally, we show that first derivative saliency can not only rediscover previous language patterns of AD patients, but also shed light on the limitations of neural models. Lastly, we also include analysis of gender-separated AD data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2112/>Feudal Reinforcement Learning for <a href=https://en.wikipedia.org/wiki/Dialogue_management>Dialogue Management</a> in Large Domains</a></strong><br><a href=/people/i/inigo-casanueva/>IÃ±igo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>PaweÅ‚ Budzianowski</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/milica-gasic/>Milica GaÅ¡iÄ‡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2112><div class="card-body p-3 small">Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps ; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset. The structural information included in the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state. This, combined with an information sharing mechanism between slots, increases the scalability to <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>large domains</a>. We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2113/>Evaluating Historical Text Normalization Systems : How Well Do They Generalize?</a></strong><br><a href=/people/a/alexander-robertson/>Alexander Robertson</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2113><div class="card-body p-3 small">We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these <a href=https://en.wikipedia.org/wiki/System>systems</a> would actually work in practicei.e., for new datasets or languages ; in comparison to more nave systems ; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a nave baseline system. We show that the neural models generalize well to unseen words in tests on five languages ; nevertheless, they provide no clear benefit over the nave baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, including both intrinsic and extrinsic measures where possible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2115.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2115/>Natural Language to Structured Query Generation via Meta-Learning</a></strong><br><a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/c/chenglong-wang/>Chenglong Wang</a>
|
<a href=/people/r/rishabh-singh/>Rishabh Singh</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2115><div class="card-body p-3 small">In conventional supervised training, a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is trained to fit all the training examples. However, having a monolithic model may not always be the best strategy, as examples could vary widely. In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function. When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%5.4 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a> gains over the non-meta-learning counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2118 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2118/>Slot-Gated Modeling for Joint Slot Filling and Intent Prediction</a></strong><br><a href=/people/c/chih-wen-goo/>Chih-Wen Goo</a>
|
<a href=/people/g/guang-gao/>Guang Gao</a>
|
<a href=/people/y/yun-kai-hsu/>Yun-Kai Hsu</a>
|
<a href=/people/c/chih-li-huo/>Chih-Li Huo</a>
|
<a href=/people/t/tsung-chieh-chen/>Tsung-Chieh Chen</a>
|
<a href=/people/k/keng-wei-hsu/>Keng-Wei Hsu</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2118><div class="card-body p-3 small">Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and <a href=https://en.wikipedia.org/wiki/Intention>intent</a> have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between <a href=https://en.wikipedia.org/wiki/Intention>intent</a> and slot attention vectors in order to obtain better semantic frame results by the <a href=https://en.wikipedia.org/wiki/Global_optimization>global optimization</a>. The experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> significantly improves sentence-level semantic frame accuracy with 4.2 % and 1.9 % relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2119/>An Evaluation of Image-Based Verb Prediction Models against Human Eye-Tracking Data</a></strong><br><a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2119><div class="card-body p-3 small">Recent research in language and vision has developed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for predicting and disambiguating verbs from <a href=https://en.wikipedia.org/wiki/Image>images</a>. Here, we ask whether the predictions made by such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2120 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2120/>Learning to Color from Language</a></strong><br><a href=/people/v/varun-manjunatha/>Varun Manjunatha</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/l/larry-davis/>Larry Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2120><div class="card-body p-3 small">Automatic colorization is the process of adding <a href=https://en.wikipedia.org/wiki/Color>color</a> to <a href=https://en.wikipedia.org/wiki/Grayscale>greyscale images</a>. We condition this <a href=https://en.wikipedia.org/wiki/Process_(computing)>process</a> on <a href=https://en.wikipedia.org/wiki/Language>language</a>, allowing end users to manipulate a colorized image by feeding in different captions. We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version. Furthermore, we demonstrate through crowdsourced experiments that we can dramatically alter <a href=https://en.wikipedia.org/wiki/Colorization>colorizations</a> simply by manipulating descriptive color words in captions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2125 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2125/>Watch, Listen, and Describe : Globally and Locally Aligned Cross-Modal Attentions for Video Captioning</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/y/yuan-fang-wang/>Yuan-Fang Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2125><div class="card-body p-3 small">A major challenge for video captioning is to combine audio and visual cues. Existing multi-modal fusion methods have shown encouraging results in video understanding. However, the temporal structures of multiple modalities at different granularities are rarely explored, and how to selectively fuse the multi-modal representations at different levels of details remains uncharted. In this paper, we propose a novel hierarchically aligned cross-modal attention (HACA) framework to learn and selectively fuse both global and local temporal dynamics of different modalities. Furthermore, for the first time, we validate the superior performance of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep audio features</a> on the video captioning task. Finally, our HACA model significantly outperforms the previous best systems and achieves new state-of-the-art results on the widely used MSR-VTT dataset.</div></div></div><hr><div id=n18-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-3/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)</a></strong><br><a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a>
|
<a href=/people/j/jennifer-chu-carroll/>Jennifer Chu-Carroll</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277630837 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3001/>Scalable Wide and Deep Learning for Computer Assisted Coding</a></strong><br><a href=/people/m/marilisa-amoia/>Marilisa Amoia</a>
|
<a href=/people/f/frank-diehl/>Frank Diehl</a>
|
<a href=/people/j/jesus-gimenez/>Jesus Gimenez</a>
|
<a href=/people/j/joel-pinto/>Joel Pinto</a>
|
<a href=/people/r/raphael-schumann/>Raphael Schumann</a>
|
<a href=/people/f/fabian-stemmer/>Fabian Stemmer</a>
|
<a href=/people/p/paul-vozila/>Paul Vozila</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3001><div class="card-body p-3 small">In recent years the use of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical records</a> has accelerated resulting in large volumes of medical data when a patient visits a healthcare facility. As a first step towards reimbursement healthcare institutions need to associate ICD-10 billing codes to these documents. This is done by trained clinical coders who may use a computer assisted solution for shortlisting of codes. In this work, we present our work to build a machine learning based scalable system for predicting <a href=https://en.wikipedia.org/wiki/ICD-10>ICD-10 codes</a> from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical records</a>. We address data imbalance issues by implementing two <a href=https://en.wikipedia.org/wiki/Systems_architecture>system architectures</a> using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression models</a>. We illustrate the pros and cons of those system designs and show that the best performance can be achieved by leveraging the advantages of both using a system combination approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277630853 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3003/>A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding</a></strong><br><a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/d/dongchan-kim/>Dongchan Kim</a>
|
<a href=/people/j/joo-kyung-kim/>Joo-Kyung Kim</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3003><div class="card-body p-3 small">Intelligent personal digital assistants (IPDAs), a popular real-life application with spoken language understanding capabilities, can cover potentially thousands of overlapping domains for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, and the task of finding the best domain to handle an utterance becomes a challenging problem on a large scale. In this paper, we propose a set of efficient and scalable shortlisting-reranking neural models for effective large-scale domain classification for <a href=https://en.wikipedia.org/wiki/Intelligence_quotient>IPDAs</a>. The shortlisting stage focuses on efficiently trimming all domains down to a list of k-best candidate domains, and the reranking stage performs a list-wise reranking of the initial k-best domains with additional contextual information. We show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> with extensive experiments on 1,500 IPDA domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631102 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3005/>Data Collection for Dialogue System : A Startup Perspective</a></strong><br><a href=/people/y/yiping-kang/>Yiping Kang</a>
|
<a href=/people/y/yunqi-zhang/>Yunqi Zhang</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/l/lingjia-tang/>Lingjia Tang</a>
|
<a href=/people/j/jason-mars/>Jason Mars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3005><div class="card-body p-3 small">Industrial dialogue systems such as Apple Siri and Google Now rely on large scale diverse and robust training data to enable their sophisticated conversation capability. Crowdsourcing provides a scalable and inexpensive way of <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> but collecting high quality data efficiently requires thoughtful orchestration of the crowdsourcing jobs. Prior study of this topic have focused on tasks only in the academia settings with limited scope or only provide intrinsic dataset analysis, lacking indication on how it affects the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance. In this paper, we present a study of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing methods</a> for a user intent classification task in our deployed <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> of 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts and analyze quantitatively the quality of the collected data and the downstream model performance on a test set of real user queries from production logs. Our observation provides insights into designing efficient crowdsourcing jobs and provide recommendations for future dialogue system data collection process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631118 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3006/>Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>On-Line Reinforcement Learning</a></a></strong><br><a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-TÃ¼r</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/g/gokhan-tur/>Gokhan TÃ¼r</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3006><div class="card-body p-3 small">End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are expensive to collect and annotate, since each task involves a separate schema and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>database of entities</a>. Further, the Wizard-of-Oz approach commonly used for dialogue collection does not provide sufficient coverage of salient dialogue flows, which is critical for guaranteeing an acceptable task completion rate in consumer-facing conversational agents. In this paper, we study a recently proposed approach for building an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> for arbitrary tasks by combining dialogue self-play and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to generate fully-annotated dialogues with diverse and natural utterances. We discuss the advantages of this approach for industry applications of conversational agents, wherein an agent can be rapidly bootstrapped to deploy in front of users and further optimized via interactive learning from actual users of the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3008/>Atypical Inputs in Educational Applications</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/a/aoife-cahill/>Aoife Cahill</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a>
|
<a href=/people/b/brian-riordan/>Brian Riordan</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3008><div class="card-body p-3 small">In large-scale educational assessments, the use of automated scoring has recently become quite common. While the majority of student responses can be processed and scored without difficulty, there are a small number of responses that have atypical characteristics that make it difficult for an automated scoring system to assign a correct score. We describe a <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> that detects and processes these kinds of responses at run-time. We present the most frequent kinds of what are called non-scorable responses along with effective filtering models based on various NLP and speech processing technologies. We give an overview of two operational automated scoring systems one for essay scoring and one for speech scoring and describe the filtering models they use. Finally, we present an evaluation and analysis of <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering models</a> used for spoken responses in an <a href=https://en.wikipedia.org/wiki/Language_proficiency>assessment of language proficiency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631374 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3013/>Accelerating NMT Batched Beam Decoding with LMBR Posteriors for Deployment<span class=acl-fixed-case>NMT</span> Batched Beam Decoding with <span class=acl-fixed-case>LMBR</span> Posteriors for Deployment</a></strong><br><a href=/people/g/gonzalo-iglesias/>Gonzalo Iglesias</a>
|
<a href=/people/w/william-tambellini/>William Tambellini</a>
|
<a href=/people/a/adria-de-gispert/>AdriÃ  De Gispert</a>
|
<a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3013><div class="card-body p-3 small">We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and <a href=https://en.wikipedia.org/wiki/Batch_processing>batching</a> on <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631480 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3015/>From dictations to clinical reports using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a></a></strong><br><a href=/people/g/gregory-finley/>Gregory Finley</a>
|
<a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/n/najmeh-sadoughi/>Najmeh Sadoughi</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/a/amanda-robinson/>Amanda Robinson</a>
|
<a href=/people/n/nico-axtmann/>Nico Axtmann</a>
|
<a href=/people/m/michael-brenndoerfer/>Michael Brenndoerfer</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3015><div class="card-body p-3 small">A typical <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> to document clinical encounters entails dictating a summary, running <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and post-processing the resulting text into a formatted letter. Post-processing entails a host of transformations including punctuation restoration, <a href=https://en.wikipedia.org/wiki/Truecasing>truecasing</a>, marking sections and headers, converting dates and numerical expressions, parsing lists, etc. In conventional implementations, most of these <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are accomplished by individual <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>. We introduce a novel holistic approach to <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> that relies on machine callytranslation. We show how this technique outperforms an alternative conventional systemeven learning to correct speech recognition errors during post-processingwhile being much simpler to maintain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3016/>Benchmarks and models for entity-oriented polarity detection</a></strong><br><a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/a/arto-klami/>Arto Klami</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3016><div class="card-body p-3 small">We address the problem of determining entity-oriented polarity in <a href=https://en.wikipedia.org/wiki/Business_journalism>business news</a>. This can be viewed as classifying the polarity of the sentiment expressed toward a given mention of a company in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a>. We present a complete, end-to-end approach to the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over 17,000 manually labeled documents, which is substantially larger than any currently available resources. We propose a benchmark solution based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> for classifying entity-oriented polarity. Although our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is much larger than those currently available, it is small on the scale of datasets commonly used for training robust neural network models. To compensate for this, we use transfer learningpre-train the model on a much larger <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, annotated for a related but different classification task, in order to learn a good representation for business text, and then fine-tune it on the smaller polarity dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669655 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3017/>Selecting Machine-Translated Data for Quick Bootstrapping of a Natural Language Understanding System</a></strong><br><a href=/people/j/judith-gaspers/>Judith Gaspers</a>
|
<a href=/people/p/penny-karanasou/>Penny Karanasou</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3017><div class="card-body p-3 small">This paper investigates the use of Machine Translation (MT) to bootstrap a Natural Language Understanding (NLU) system for a new language for the use case of a large-scale voice-controlled device. The goal is to decrease the cost and time needed to get an annotated corpus for the new language, while still having a large enough coverage of user requests. Different methods of filtering MT data in order to keep utterances that improve NLU performance and language-specific post-processing methods are investigated. These methods are tested in a large-scale NLU task with translating around 10 millions training utterances from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. The results show a large improvement for using MT data over a grammar-based and over an in-house data collection baseline, while reducing the manual effort greatly. Both filtering and post-processing approaches improve results further.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669529 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3018/>Fast and Scalable Expansion of Natural Language Understanding Functionality for <a href=https://en.wikipedia.org/wiki/Intelligent_agent>Intelligent Agents</a></a></strong><br><a href=/people/a/anuj-kumar-goyal/>Anuj Kumar Goyal</a>
|
<a href=/people/a/angeliki-metallinou/>Angeliki Metallinou</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3018><div class="card-body p-3 small">Fast expansion of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language functionality</a> of intelligent virtual agents is critical for achieving engaging and informative interactions. However, developing accurate <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for new <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language domains</a> is a time and data intensive process. We propose efficient deep neural network architectures that maximally re-use available resources through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Our methods are applied for expanding the understanding capabilities of a popular commercial agent and are evaluated on hundreds of new domains, designed by internal or external developers. We demonstrate that our proposed methods significantly increase <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in low resource settings and enable rapid development of accurate <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with less data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3019/>Bag of Experts Architectures for Model Reuse in Conversational Language Understanding</a></strong><br><a href=/people/r/rahul-jha/>Rahul Jha</a>
|
<a href=/people/a/alex-marin/>Alex Marin</a>
|
<a href=/people/s/suvamsh-shivaprasad/>Suvamsh Shivaprasad</a>
|
<a href=/people/i/imed-zitouni/>Imed Zitouni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3019><div class="card-body p-3 small">Slot tagging, the task of detecting entities in input user utterances, is a key component of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a> for <a href=https://en.wikipedia.org/wiki/Personal_digital_assistant>personal digital assistants</a>. Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow. To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models. Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06 % in absolute F1-score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16 % when only 25 % of the training data is used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669567 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3020/>Multi-lingual neural title generation for e-Commerce browse pages</a></strong><br><a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/n/nicola-ueffing/>Nicola Ueffing</a>
|
<a href=/people/g/gregor-leusch/>Gregor Leusch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3020><div class="card-body p-3 small">To provide better access of the inventory to buyers and better <a href=https://en.wikipedia.org/wiki/Search_engine_optimization>search engine optimization</a>, e-Commerce websites are automatically generating millions of browse pages. A browse page consists of a set of slot name / value pairs within a given category, grouping multiple items which share some characteristics. These browse pages require a title describing the content of the page. Since the number of browse pages are huge, manual creation of these titles is infeasible. Previous statistical and neural approaches depend heavily on the availability of large amounts of data in a language. In this research, we apply sequence-to-sequence models to generate <a href=https://en.wikipedia.org/wiki/Title_(publishing)>titles</a> for high-resource as well as low-resource languages by leveraging <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We train these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on multi-lingual data, thereby creating one joint model which can generate titles in various different languages. Performance of the title generation system is evaluated on three different languages ; <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, with a particular focus on low-resourced French language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669493 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-3021/>A Novel Approach to Part Name Discovery in Noisy Text</a></strong><br><a href=/people/n/nobal-bikram-niraula/>Nobal Bikram Niraula</a>
|
<a href=/people/d/daniel-whyatt/>Daniel Whyatt</a>
|
<a href=/people/a/anne-kao/>Anne Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3021><div class="card-body p-3 small">As a specialized example of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, part name extraction is an area that presents unique challenges. Part names are typically multi-word terms longer than two words. There is little consistency in how terms are described in noisy free text, with variations spawned by <a href=https://en.wikipedia.org/wiki/Typographical_error>typos</a>, ad hoc abbreviations, <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a>, and incomplete names. This makes search and analyses of parts in these <a href=https://en.wikipedia.org/wiki/Data>data</a> extremely challenging. In this paper, we present our algorithm, PANDA (Part Name Discovery Analytics), based on a unique method that exploits statistical, linguistic and machine learning techniques to discover part names in noisy text such as that in manufacturing quality documentation, supply chain management records, service communication logs, and maintenance reports. Experiments show that PANDA is scalable and outperforms existing techniques significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-3027/>Document-based Recommender System for Job Postings using Dense Representations</a></strong><br><a href=/people/a/ahmed-elsafty/>Ahmed Elsafty</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-3027><div class="card-body p-3 small">Job boards and professional social networks heavily use <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a> in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows, for example, the application of item-to-item based recommendations. In this work, we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme : (1) we exploit historic user interactions to automatically create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset of similar jobs</a> that enables an offline evaluation. (2) In addition, we conduct an online A / B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>job titles</a> with <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>full-text job descriptions</a>. In particular, this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation, this approach allows us to increase the <a href=https://en.wikipedia.org/wiki/Click-through_rate>click-through rate</a> on <a href=https://en.wikipedia.org/wiki/Job_description>job recommendations</a> for active users by 8.0 %.</div></div></div><hr><div id=n18-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-4/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/s/silvio-cordeiro/>Silvio Ricardo Cordeiro</a>
|
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/u/umashanthi-pavalanathan/>Umashanthi Pavalanathan</a>
|
<a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631295 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-4001/>Alignment, Acceptance, and Rejection of Group Identities in Online Political Discourse</a></strong><br><a href=/people/h/hagyeong-shin/>Hagyeong Shin</a>
|
<a href=/people/g/gabriel-doyle/>Gabriel Doyle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4001><div class="card-body p-3 small">Conversation is a joint social process, with participants cooperating to exchange information. This process is helped along through linguistic alignment : participants&#8217; adoption of each other&#8217;s word use. This alignment is robust, appearing many settings, and is nearly always positive. We create an alignment model for examining alignment in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter conversations</a> across antagonistic groups. This model finds that some word categories, specifically <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> used to establish <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>group identity</a> and <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>common ground</a>, are negatively aligned. This negative alignment is observed despite other categories, which are less related to the <a href=https://en.wikipedia.org/wiki/Group_dynamics>group dynamics</a>, showing the standard positive alignment. This suggests that alignment is strongly biased toward cooperative alignment, but that different linguistic features can show substantially different behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4002/>Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of Estonian Particle Verbs<span class=acl-fixed-case>E</span>stonian Particle Verbs</a></strong><br><a href=/people/e/eleri-aedmaa/>Eleri Aedmaa</a>
|
<a href=/people/m/maximilian-koper/>Maximilian KÃ¶per</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4002><div class="card-body p-3 small">This paper presents two novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and a random-forest classifier to automatically predict <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal vs. non-literal language usage</a> for a highly frequent type of <a href=https://en.wikipedia.org/wiki/Interlingue>multi-word expression</a> in a low-resource language, i.e., <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a>. We demonstrate the value of language-specific indicators induced from theoretical linguistic research, which outperform a high majority baseline when combined with language-independent features of non-literal language (such as abstractness).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4003/>Verb Alternations and Their Impact on Frame Induction</a></strong><br><a href=/people/e/esther-seyffarth/>Esther Seyffarth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4003><div class="card-body p-3 small">Frame induction is the automatic creation of frame-semantic resources similar to <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> or <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, which map lexical units of a language to frame representations of each lexical unit&#8217;s semantics. For verbs, these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> usually include a specification of their argument slots and of the selectional restrictions that apply to each slot. Verbs that participate in diathesis alternations have different syntactic realizations whose <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> are closely related, but not identical. We discuss the influence that such alternations have on frame induction, compare several possible frame structures for verbs in the <a href=https://en.wikipedia.org/wiki/Causative_alternation>causative alternation</a>, and propose a systematic analysis of alternating verbs that encodes their similarities as well as their differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4005/>Towards Qualitative Word Embeddings Evaluation : Measuring Neighbors Variation</a></strong><br><a href=/people/b/benedicte-pierrejean/>BÃ©nÃ©dicte Pierrejean</a>
|
<a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4005><div class="card-body p-3 small">We propose a method to study the variation lying between different word embeddings models trained with different parameters. We explore the variation between <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with only one varying parameter by observing the distributional neighbors variation and show how changing only one parameter can have a massive impact on a given <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>. We show that the <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variation</a> is not affecting all words of the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> equally. Variation is influenced by parameters such as setting a parameter to its minimum or maximum value but it also depends on the corpus intrinsic features such as the frequency of a word. We identify semantic classes of words remaining stable across the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained and specific words having high variation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4006/>A Deeper Look into Dependency-Based Word Embeddings</a></strong><br><a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4006><div class="card-body p-3 small">We investigate the effect of various dependency-based word embeddings on distinguishing between functional and domain similarity, word similarity rankings, and two downstream tasks in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Variations include word embeddings trained using context windows from Stanford and Universal dependencies at several levels of enhancement (ranging from unlabeled, to Enhanced++ dependencies). Results are compared to basic linear contexts and evaluated on several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We found that embeddings trained with Universal and Stanford dependency contexts excel at different tasks, and that enhanced dependencies often improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4007/>Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha Kameswara Sarma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4007><div class="card-body p-3 small">This research proposal describes two <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that are aimed at learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for data sparse and sentiment rich data sets. The goal is to use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> adapted for domain specific data sets in downstream applications such as sentiment classification. The first approach learns word embeddings in a supervised fashion via SWESA (Supervised Word Embeddings for Sentiment Analysis), an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on data sets that are of modest size. SWESA leverages document labels to jointly learn polarity-aware word embeddings and a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to classify unseen documents. In the second approach domain adapted (DA) word embeddings are learned by exploiting the specificity of domain specific data sets and the breadth of generic word embeddings. The new embeddings are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA. Experimental results on binary sentiment classification tasks using both approaches for standard <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> are presented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276463184 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-4010/>End-to-End Learning of Task-Oriented Dialogs</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4010><div class="card-body p-3 small">In this thesis proposal, we address the limitations of conventional pipeline design of task-oriented dialog systems and propose end-to-end learning solutions. We design neural network based dialog system that is able to robustly track dialog state, interface with knowledge bases, and incorporate structured query results into system responses to successfully complete task-oriented dialog. In learning such neural network based dialog systems, we propose hybrid offline training and online interactive learning methods. We introduce a multi-task learning method in pre-training the dialog agent in a supervised manner using task-oriented dialog corpora. The supervised training agent can further be improved via interacting with users and learning online from user demonstration and feedback with imitation and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. In addressing the sample efficiency issue with online policy learning, we further propose a method by combining the learning-from-user and learning-from-simulation approaches to improve the online interactive learning efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4014/>Japanese Predicate Conjugation for Neural Machine Translation<span class=acl-fixed-case>J</span>apanese Predicate Conjugation for Neural Machine Translation</a></strong><br><a href=/people/m/michiki-kurosawa/>Michiki Kurosawa</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4014><div class="card-body p-3 small">Neural machine translation (NMT) has a drawback in that can generate only high-frequency words owing to the computational costs of the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a> in the output layer. In Japanese-English NMT, Japanese predicate conjugation causes an increase in <a href=https://en.wikipedia.org/wiki/Japanese_vocabulary>vocabulary size</a>. For example, one verb can have as many as 19 surface varieties. In this research, we focus on <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>predicate conjugation</a> for compressing the vocabulary size in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. The vocabulary list is filled with the various forms of verbs. We propose methods using predicate conjugation information without discarding <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can generate low-frequency words and deal with unknown words. Two methods were considered to introduce conjugation information : the first considers it as a token (conjugation token) and the second considers it as an embedded vector (conjugation feature). The results using these methods demonstrate that the vocabulary size can be compressed by approximately 86.1 % (Tanaka corpus) and the NMT models can output the words not in the training data set. Furthermore, BLEU scores improved by 0.91 points in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese-to-English translation</a>, and 0.32 points in <a href=https://en.wikipedia.org/wiki/Japanese_language>English-to-Japanese translation</a> with <a href=https://en.wikipedia.org/wiki/ASPEC>ASPEC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4015/>Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4015><div class="card-body p-3 small">Sentence representations can capture a wide range of information that can not be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4016/>Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora</a></strong><br><a href=/people/s/sree-harsha-ramesh/>Sree Harsha Ramesh</a>
|
<a href=/people/k/krishna-prasad-sankaranarayanan/>Krishna Prasad Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4016><div class="card-body p-3 small">Resources for the non-English languages are scarce and this paper addresses this problem in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, by automatically extracting parallel sentence pairs from the multilingual articles available on the Internet. In this paper, we have used an end-to-end Siamese bidirectional recurrent neural network to generate parallel sentences from comparable multilingual articles in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Subsequently, we have showed that using the harvested dataset improved BLEU scores on both NMT and phrase-based SMT systems for the low-resource language pairs : EnglishHindi and EnglishTamil, when compared to training exclusively on the limited bilingual corpora collected for these language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4017/>Training a Ranking Function for Open-Domain Question Answering</a></strong><br><a href=/people/p/phu-mon-htut/>Phu Mon Htut</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4017><div class="card-body p-3 small">In recent years, there have been amazing advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> for <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>. In <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>, the machine reader has to extract the answer from the given ground truth paragraph. Recently, the state-of-the-art machine reading models achieve human level performance in SQuAD which is a reading comprehension-style question answering (QA) task. The success of <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> has inspired researchers to combine <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> with <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> to tackle open-domain QA. However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question. In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question. Additionally, we analyze the relative importance of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and word level relevance matching in open-domain QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4019/>Sensing and Learning Human Annotators Engaged in Narrative Sensemaking<span class=acl-fixed-case>S</span>ensing and Learning Human Annotators Engaged in Narrative Sensemaking</a></strong><br><a href=/people/m/mckenna-tornblad/>McKenna Tornblad</a>
|
<a href=/people/l/luke-lapresi/>Luke Lapresi</a>
|
<a href=/people/c/christopher-homan/>Christopher Homan</a>
|
<a href=/people/r/raymond-ptucha/>Raymond Ptucha</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4019><div class="card-body p-3 small">While labor issues and <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality assurance</a> in <a href=https://en.wikipedia.org/wiki/Crowdwork>crowdwork</a> are increasingly studied, how annotators make sense of texts and how they are personally impacted by doing so are not. We study these questions via a narrative-sorting annotation task, where carefully selected (by sequentiality, topic, emotional content, and length) collections of tweets serve as examples of everyday storytelling. As readers process these narratives, we measure their <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Electrodermal_activity>galvanic skin response</a>, and self-reported reactions. From the perspective of annotator well-being, a reassuring outcome was that the sorting task did not cause a measurable <a href=https://en.wikipedia.org/wiki/Fight-or-flight_response>stress response</a>, however readers reacted to <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. In terms of <a href=https://en.wikipedia.org/wiki/Sensemaking>sensemaking</a>, readers were more confident when sorting sequential, target-topical, and highly emotional tweets. As <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> becomes more common, this research sheds light onto the perceptive capabilities and emotional impact of human readers.</div></div></div><hr><div id=n18-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-5/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Demonstrations</a></strong><br><a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/t/tim-paek/>Tim Paek</a>
|
<a href=/people/m/manasi-patwardhan/>Manasi Patwardhan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5002/>Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures</a></strong><br><a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/m/mengping-li/>Mengping Li</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5002><div class="card-body p-3 small">We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazon&#8217;s Lambda service for feedforward evaluation and <a href=https://en.wikipedia.org/wiki/DynamoDB>DynamoDB</a> for storing word embeddings. Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances. All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention. We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5003/>An automated medical scribe for documenting clinical encounters</a></strong><br><a href=/people/g/gregory-finley/>Gregory Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/a/amanda-robinson/>Amanda Robinson</a>
|
<a href=/people/m/michael-brenndoerfer/>Michael Brenndoerfer</a>
|
<a href=/people/n/najmeh-sadoughi/>Najmeh Sadoughi</a>
|
<a href=/people/j/james-fone/>James Fone</a>
|
<a href=/people/n/nico-axtmann/>Nico Axtmann</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5003><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Medical_scribe>medical scribe</a> is a clinical professional who charts patientphysician encounters in real time, relieving physicians of most of their administrative burden and substantially increasing productivity and job satisfaction. We present a complete implementation of an automated medical scribe. Our system can serve either as a scalable, standardized, and economical alternative to human scribes ; or as an assistive tool for them, providing a first draft of a report along with a convenient means to modify it. This solution is, to our knowledge, the first automated scribe ever presented and relies upon multiple speech and language technologies, including <a href=https://en.wikipedia.org/wiki/Speaker_diarization>speaker diarization</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>medical speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Knowledge_extraction>knowledge extraction</a>, and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5004/>CL Scholar : The ACL Anthology Knowledge Graph Miner<span class=acl-fixed-case>CL</span> Scholar: The <span class=acl-fixed-case>ACL</span> <span class=acl-fixed-case>A</span>nthology Knowledge Graph Miner</a></strong><br><a href=/people/m/mayank-singh/>Mayank Singh</a>
|
<a href=/people/p/pradeep-dogga/>Pradeep Dogga</a>
|
<a href=/people/s/sohan-patro/>Sohan Patro</a>
|
<a href=/people/d/dhiraj-barnwal/>Dhiraj Barnwal</a>
|
<a href=/people/r/ritam-dutt/>Ritam Dutt</a>
|
<a href=/people/r/rajarshi-haldar/>Rajarshi Haldar</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5004><div class="card-body p-3 small">We present CL Scholar, the ACL Anthology knowledge graph miner to facilitate high-quality search and exploration of current research progress in the computational linguistics community. In contrast to previous works, periodically crawling, <a href=https://en.wikipedia.org/wiki/Search_engine_indexing>indexing</a> and processing of new incoming articles is completely automated in the current <a href=https://en.wikipedia.org/wiki/System>system</a>. CL Scholar utilizes both textual and network information for knowledge graph construction. As an additional novel initiative, CL Scholar supports more than 1200 scholarly natural language queries along with standard keyword-based search on constructed knowledge graph. It answers binary, statistical and list based natural language queries. The current <a href=https://en.wikipedia.org/wiki/System>system</a> is deployed at. We also provide REST API support along with bulk download facility. Our code and data are available at.<url>http://cnerg.iitkgp.ac.in/aclakg</url>. We also provide REST API\n support along with bulk download facility. Our code and data are available\n at <url>https://github.com/CLScholar</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5006/>ClaimRank : Detecting Check-Worthy Claims in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and English<span class=acl-fixed-case>C</span>laim<span class=acl-fixed-case>R</span>ank: Detecting Check-Worthy Claims in <span class=acl-fixed-case>A</span>rabic and <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/i/israa-jaradat/>Israa Jaradat</a>
|
<a href=/people/p/pepa-gencheva/>Pepa Gencheva</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto BarrÃ³n-CedeÃ±o</a>
|
<a href=/people/l/lluis-marquez/>LluÃ­s MÃ rquez</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5006><div class="card-body p-3 small">We present ClaimRank, an online system for detecting check-worthy claims. While originally trained on political debates, the <a href=https://en.wikipedia.org/wiki/System>system</a> can work for any kind of text, e.g., <a href=https://en.wikipedia.org/wiki/Interview>interviews</a> or just regular news articles. Its aim is to facilitate manual fact-checking efforts by prioritizing the claims that <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checkers</a> should consider first. ClaimRank supports both <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, it is trained on actual annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus it can mimic the claim selection strategies for each and any of them, as well as for the union of them all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5007/>360 Stance Detection</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/j/john-glover/>John Glover</a>
|
<a href=/people/a/afshin-mehrabani/>Afshin Mehrabani</a>
|
<a href=/people/p/parsa-ghaffari/>Parsa Ghaffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5007><div class="card-body p-3 small">The proliferation of fake news and <a href=https://en.wikipedia.org/wiki/Filter_bubble>filter bubbles</a> makes it increasingly difficult to form an unbiased, balanced opinion towards a topic. To ameliorate this, we propose 360 Stance Detection, a tool that aggregates news with multiple perspectives on a topic. It presents them on a spectrum ranging from support to opposition, enabling the user to base their opinion on multiple pieces of diverse evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5009 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5009/>ELISA-EDL : A Cross-lingual Entity Extraction, Linking and Localization System<span class=acl-fixed-case>ELISA</span>-<span class=acl-fixed-case>EDL</span>: A Cross-lingual Entity Extraction, Linking and Localization System</a></strong><br><a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5009><div class="card-body p-3 small">We demonstrate ELISA-EDL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets, resources and system training and testing APIs publicly available for research purpose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5010/>Entity Resolution and Location Disambiguation in the Ancient Hindu Temples Domain using Web Data<span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>H</span>indu Temples Domain using Web Data</a></strong><br><a href=/people/a/ayush-maheshwari/>Ayush Maheshwari</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/g/ganesh-ramakrishnan/>Ganesh Ramakrishnan</a>
|
<a href=/people/j/j-saketha-nath/>J. Saketha Nath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5010><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for resolving entities and disambiguating locations based on publicly available web data in the domain of <a href=https://en.wikipedia.org/wiki/Hindu_temple>ancient Hindu Temples</a>. Scarce, <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> poses a challenge to Entity Resolution(ER) and snippet ranking. Additionally, because the same set of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> may be associated with multiple locations, Location Disambiguation(LD) is a problem. The mentions and descriptions of temples exist in the order of hundreds of thousands, with such data generated by various users in various forms such as text (Wikipedia pages), videos (YouTube videos), blogs, etc. We demonstrate an integrated approach using a combination of grammar rules for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and unsupervised (clustering) algorithms to resolve entity and locations with high confidence. A demo of our <a href=https://en.wikipedia.org/wiki/System>system</a> is accessible at. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is open source and available on GitHub.<url>tinyurl.com/templedemos</url>. Our system is\n open source and available on GitHub.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5011/>Madly Ambiguous : A Game for Learning about Structural Ambiguity and Why Itâ€™s Hard for Computers</a></strong><br><a href=/people/a/ajda-gokcen/>Ajda Gokcen</a>
|
<a href=/people/e/ethan-hill/>Ethan Hill</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5011><div class="card-body p-3 small">Madly Ambiguous is an open source, online game aimed at teaching audiences of all ages about <a href=https://en.wikipedia.org/wiki/Structural_ambiguity>structural ambiguity</a> and why it&#8217;s hard for computers. After a brief introduction to <a href=https://en.wikipedia.org/wiki/Structural_ambiguity>structural ambiguity</a>, users are challenged to complete a sentence in a way that tricks the computer into guessing an incorrect interpretation. Behind the scenes are two different NLP-based methods for classifying the user&#8217;s input, one representative of classic rule-based approaches to disambiguation and the other representative of recent neural network approaches. Qualitative feedback from the system&#8217;s use in online, classroom, and science museum settings indicates that it is engaging and successful in conveying the intended take home messages. A demo of Madly Ambiguous can be played at.<url>http://madlyambiguous.osu.edu</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-5012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-5012/>VnCoreNLP : A Vietnamese Natural Language Processing Toolkit<span class=acl-fixed-case>V</span>n<span class=acl-fixed-case>C</span>ore<span class=acl-fixed-case>NLP</span>: A <span class=acl-fixed-case>V</span>ietnamese Natural Language Processing Toolkit</a></strong><br><a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5012><div class="card-body p-3 small">We present an easy-to-use and fast toolkit, namely VnCoreNLPa Java NLP annotation pipeline for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at :<url>https://github.com/vncorenlp/VnCoreNLP</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5014/>Generating Continuous Representations of Medical Texts</a></strong><br><a href=/people/g/graham-spinks/>Graham Spinks</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5014><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that generates <a href=https://en.wikipedia.org/wiki/Medical_literature>medical texts</a> while learning an informative, continuous representation with discriminative features. During training the input to the <a href=https://en.wikipedia.org/wiki/System>system</a> is a dataset of captions for <a href=https://en.wikipedia.org/wiki/Medical_ultrasound>medical X-Rays</a>. The acquired continuous representations are of particular interest for use in many machine learning techniques where the discrete and high-dimensional nature of textual input is an obstacle. We use an Adversarially Regularized Autoencoder to create realistic text in both an unconditional and conditional setting. We show that this technique is applicable to medical texts which often contain syntactic and domain-specific shorthands. A quantitative evaluation shows that we achieve a lower model perplexity than a traditional LSTM generator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5017/>RiskFinder : A Sentence-level Risk Detector for Financial Reports<span class=acl-fixed-case>R</span>isk<span class=acl-fixed-case>F</span>inder: A Sentence-level Risk Detector for Financial Reports</a></strong><br><a href=/people/y/yu-wen-liu/>Yu-Wen Liu</a>
|
<a href=/people/l/liang-chih-liu/>Liang-Chih Liu</a>
|
<a href=/people/c/chuan-ju-wang/>Chuan-Ju Wang</a>
|
<a href=/people/m/ming-feng-tsai/>Ming-Feng Tsai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5017><div class="card-body p-3 small">This paper presents a web-based information system, RiskFinder, for facilitating the analyses of soft and hard information in <a href=https://en.wikipedia.org/wiki/Financial_statement>financial reports</a>. In particular, the <a href=https://en.wikipedia.org/wiki/System>system</a> broadens the analyses from the word level to sentence level, which makes the <a href=https://en.wikipedia.org/wiki/System>system</a> useful for practitioner communities and unprecedented among financial academics. The proposed system has four main components : 1) a Form 10-K risk-sentiment dataset, consisting of a set of risk-labeled financial sentences and pre-trained sentence embeddings ; 2) metadata, including basic information on each company that published the Form 10-K financial report as well as several relevant financial measures ; 3) an interface that highlights risk-related sentences in the financial reports based on the latest sentence embedding techniques ; 4) a visualization of financial time-series data for a corresponding company. This paper also conducts some case studies to showcase that the <a href=https://en.wikipedia.org/wiki/System>system</a> can be of great help in capturing valuable insight within large amounts of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual information</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> is now online available at.<url>https://cfda.csie.org/RiskFinder/</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5018/>SMILEE : Symmetric Multi-modal Interactions with Language-gesture Enabled (AI) Embodiment<span class=acl-fixed-case>SMILEE</span>: Symmetric Multi-modal Interactions with Language-gesture Enabled (<span class=acl-fixed-case>AI</span>) Embodiment</a></strong><br><a href=/people/s/sujeong-kim/>Sujeong Kim</a>
|
<a href=/people/d/david-salter/>David Salter</a>
|
<a href=/people/l/luke-deluccia/>Luke DeLuccia</a>
|
<a href=/people/k/kilho-son/>Kilho Son</a>
|
<a href=/people/m/mohamed-r-amer/>Mohamed R. Amer</a>
|
<a href=/people/a/amir-tamrakar/>Amir Tamrakar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5018><div class="card-body p-3 small">We demonstrate an intelligent conversational agent system designed for advancing human-machine collaborative tasks. The <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> is able to interpret a user&#8217;s communicative intent from both their verbal utterances and non-verbal behaviors, such as <a href=https://en.wikipedia.org/wiki/Gesture>gestures</a>. The agent is also itself able to communicate both with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> and <a href=https://en.wikipedia.org/wiki/Gesture_recognition>gestures</a>, through its embodiment as an <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatar</a> thus facilitating natural symmetric multi-modal interactions. We demonstrate two <a href=https://en.wikipedia.org/wiki/Intelligent_agent>intelligent agents</a> with specialized skills in the Blocks World as use-cases of our <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5019/>Decision Conversations Decoded</a></strong><br><a href=/people/l/lea-deleris/>LÃ©a Deleris</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/k/killian-levacher/>Killian Levacher</a>
|
<a href=/people/m/martin-stephenson/>Martin Stephenson</a>
|
<a href=/people/f/francesca-bonin/>Francesca Bonin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5019><div class="card-body p-3 small">We describe the vision and current version of a Natural Language Processing system aimed at group decision making facilitation. Borrowing from the scientific field of <a href=https://en.wikipedia.org/wiki/Decision_analysis>Decision Analysis</a>, its essential role is to identify alternatives and criteria associated with a given decision, to keep track of who proposed them and of the expressed sentiment towards them. Based on this information, the <a href=https://en.wikipedia.org/wiki/System>system</a> can help identify agreement and dissent or recommend an alternative. Overall, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> seeks to help a group reach a decision in a natural yet auditable fashion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-5020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-5020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-5020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-5020/>Sounding Board : A User-Centric and Content-Driven Social Chatbot</a></strong><br><a href=/people/h/hao-fang/>Hao Fang</a>
|
<a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-5020><div class="card-body p-3 small">We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa Prize. The system architecture consists of several components including spoken language processing, <a href=https://en.wikipedia.org/wiki/Dialogue_management>dialogue management</a>, <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a>, and <a href=https://en.wikipedia.org/wiki/Content_management>content management</a>, with emphasis on user-centric and content-driven design. We also share insights gained from large-scale online logs based on 160,000 conversations with real-world users.</div></div></div><hr><div id=n18-6><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N18-6/>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-6000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca Passonneau</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154243 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6003/>Scalable Construction and Reasoning of Massive Knowledge Bases</a></strong><br><a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6003><div class="card-body p-3 small">In today&#8217;s information-based society, there is abundant knowledge out there carried in the form of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language texts</a> (e.g., <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>, scientific publications), which spans across various domains (e.g., corporate documents, <a href=https://en.wikipedia.org/wiki/Advertising>advertisements</a>, <a href=https://en.wikipedia.org/wiki/Act_of_Parliament>legal acts</a>, medical reports), which grows at an astonishing rate. Yet this knowledge is mostly inaccessible to computers and overwhelming for human experts to absorb. How to turn such massive and unstructured text data into structured, actionable knowledge, and furthermore, how to teach machines learn to reason and complete the extracted knowledge is a grand challenge to the research community. Traditional IE systems assume abundant human annotations for training high quality machine learning models, which is impractical when trying to deploy IE systems to a broad range of domains, settings and languages. In the first part of the tutorial, we introduce how to extract structured facts (i.e., entities and their relations for types of interest) from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> to construct <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, with a focus on methods that are weakly-supervised and domain-independent for timely knowledge base construction across various application domains. In the second part, we introduce how to leverage other knowledge, such as the distributional statistics of characters and words, the annotations for other tasks and other domains, and the linguistics and problem structures, to combat the problem of inadequate supervision, and conduct low-resource information extraction. In the third part, we describe recent advances in knowledge base reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154260 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6004/>The interplay between lexical resources and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6004><div class="card-body p-3 small">Incorporating linguistic, world and common sense knowledge into AI / NLP systems is currently an important research area, with several open problems and challenges. At the same time, processing and storing this knowledge in <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a> is not a straightforward task. We propose to address these complementary goals from two methodological perspectives : the use of NLP methods to help the process of constructing and enriching lexical resources and the use of lexical resources for improving NLP applications. This tutorial may be useful for two main types of audience : those working on language resources who are interested in becoming acquainted with automatic NLP techniques, with the end goal of speeding and/or easing up the process of resource curation ; and on the other hand, researchers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> who would like to benefit from the knowledge of lexical resources to improve their systems and models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-6005/>Socially Responsible NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6005><div class="card-body p-3 small">As <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>methods</a> to adequately mitigate these issues (e.g., adjusting <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling rates</a> or de-biasing through regularization). The tutorial is based on a new course on <a href=https://en.wikipedia.org/wiki/Ethics>Ethics</a> and <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> developed at Carnegie Mellon University.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154253 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6006/>Deep Learning for Conversational AI<span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/n/nikola-mrksic/>Nikola MrkÅ¡iÄ‡</a>
|
<a href=/people/i/inigo-casanueva/>IÃ±igo Casanueva</a>
|
<a href=/people/i/ivan-vulic/>Ivan VuliÄ‡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6006><div class="card-body p-3 small">Spoken Dialogue Systems (SDS) have great commercial potential as they promise to revolutionise the way in which humans interact with machines. The advent of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> led to substantial developments in this area of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>, and the goal of this tutorial is to familiarise the research community with the recent advances in what some call the most difficult problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. From a research perspective, the design of spoken dialogue systems provides a number of significant challenges, as these <a href=https://en.wikipedia.org/wiki/System>systems</a> depend on : a) solving several difficult NLP and decision-making tasks ; and b) combining these into a functional dialogue system pipeline. A key long-term goal of dialogue system research is to enable open-domain systems that can converse about arbitrary topics and assist humans with completing a wide range of tasks. Furthermore, such systems need to autonomously learn on-line to improve their performance and recover from errors using both signals from their environment and from implicit and explicit user feedback. While the design of such systems has traditionally been modular, domain and language-specific, advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have alleviated many of the design problems. The main purpose of this tutorial is to encourage dialogue research in the NLP community by providing the research background, a survey of available resources, and giving key insights to application of state-of-the-art SDS methodology into industry-scale conversational AI systems. We plan to introduce researchers to the pipeline framework for modelling goal-oriented dialogue systems, which includes three key components : 1) <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a> ; 2) Dialogue Management ; and 3) Language Generation. The differences between goal-oriented dialogue systems and chat-bot style conversational agents will be explained in order to show the motivation behind the design of both, with the main focus on the pipeline SDS framework. For each key component, we will define the research problem, provide a brief literature review and introduce the current state-of-the-art approaches. Complementary resources (e.g. available datasets and toolkits) will also be discussed. Finally, future work, outstanding challenges, and current industry practices will be presented. All of the presented material will be made available online for future reference.</div></div></div><hr><div id=w18-05><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-05.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-05/>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0500/>Proceedings of the Thirteenth Workshop on Innovative Use of <span class=acl-fixed-case>NLP</span> for Building Educational Applications</a></strong><br><a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0503/>Predicting misreadings from gaze in children with reading difficulties</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/s/sigrid-klerke/>Sigrid Klerke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0503><div class="card-body p-3 small">We present the first work on predicting reading mistakes in children with <a href=https://en.wikipedia.org/wiki/Reading_disability>reading difficulties</a> based on eye-tracking data from real-world reading teaching. Our approach employs several linguistic and gaze-based features to inform an ensemble of different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, including multi-task learning models that let us transfer knowledge about individual readers to attain better predictions. Notably, the <a href=https://en.wikipedia.org/wiki/Data>data</a> we use in this work stems from noisy readings in the wild, outside of controlled lab conditions. Our experiments show that despite the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and despite the small fraction of misreadings, gaze data improves the performance more than any other feature group and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve good performance. We further show that gaze patterns for misread words do not fully generalize across readers, but that we can transfer some knowledge between readers using <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> at least in some cases. Applications of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> include partial automation of reading assessment as well as personalized text simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0504/>Automatic Input Enrichment for Selecting Reading Material : An Online Study with English Teachers<span class=acl-fixed-case>E</span>nglish Teachers</a></strong><br><a href=/people/m/maria-chinkina/>Maria Chinkina</a>
|
<a href=/people/a/ankita-oswal/>Ankita Oswal</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0504><div class="card-body p-3 small">Input material at the appropriate level is crucial for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a>. Automating the search for such material can systematically and efficiently support teachers in their pedagogical practice. This is the goal of the computational linguistic task of automatic input enrichment (Chinkina & Meurers, 2016): It analyzes and re-ranks a collection of texts in order to prioritize those containing target linguistic forms. In the online study described in the paper, we collected 240 responses from English teachers in order to investigate whether they preferred automatic input enrichment over <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a> when selecting reading material for class. Participants demonstrated a general preference for the material provided by an automatic input enrichment system. It was also rated significantly higher than the texts retrieved by a standard <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engine</a> with regard to the representation of linguistic forms and equivalent with regard to the relevance of the content to the topic. We discuss the implications of the results for <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> and consider the potential strands of future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0506/>Second Language Acquisition Modeling</a></strong><br><a href=/people/b/burr-settles/>Burr Settles</a>
|
<a href=/people/c/chris-brust/>Chris Brust</a>
|
<a href=/people/e/erin-gustafson/>Erin Gustafson</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0506><div class="card-body p-3 small">We present the task of second language acquisition (SLA) modeling. Given a history of errors made by learners of a <a href=https://en.wikipedia.org/wiki/Second_language>second language</a>, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7 M words produced by more than 6k learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a> using <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a>, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>.<i>second language acquisition (SLA) modeling</i>. Given a history of errors made by learners of a\n second language, the task is to predict errors that they are\n likely to make at arbitrary points in the future. We describe a\n large corpus of more than 7M words produced by more than 6k\n learners of English, Spanish, and French using Duolingo, a\n popular online language-learning app. Then we report on the\n results of a shared task challenge aimed studying the SLA task\n via this corpus, which attracted 15 teams and synthesized work\n from various fields including cognitive science, linguistics,\n and machine learning.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0507/>A Report on the Complex Word Identification Shared Task 2018</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/sanja-stajner/>Sanja Å tajner</a>
|
<a href=/people/a/anais-tack/>AnaÃ¯s Tack</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0507><div class="card-body p-3 small">We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT&#8217;2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks : English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks : <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and probabilistic classification. A total of 12 teams submitted their results in different task / track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0508/>Towards Single Word Lexical Complexity Prediction</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0508><div class="card-body p-3 small">In this paper we present work-in-progress where we investigate the usefulness of previously created word lists to the task of single-word lexical complexity analysis and prediction of the complexity level for learners of <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> as a second language. The word lists used map each word to a single CEFR level, and the task consists of predicting CEFR levels for unseen words. In contrast to previous work on word-level lexical complexity, we experiment with topics as additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and show that linking words to topics significantly increases <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0511/>Annotating Student Talk in Text-based Classroom Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/a/amanda-godley/>Amanda Godley</a>
|
<a href=/people/c/christopher-olshefski/>Christopher Olshefski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0511><div class="card-body p-3 small">Classroom discussions in English Language Arts have a positive effect on students&#8217; reading, writing and reasoning skills. Although prior work has largely focused on teacher talk and student-teacher interactions, we focus on three theoretically-motivated aspects of high-quality student talk : <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Knowledge_domain>knowledge domain</a>. We introduce an annotation scheme, then show that the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> can be used to produce reliable <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and that the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are predictive of discussion quality. We also highlight opportunities provided by our scheme for education and natural language processing research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0512/>Toward Automatically Measuring Learner Ability from Human-Machine Dialog Interactions using Novel Psychometric Models</a></strong><br><a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/m/michelle-lamar/>Michelle LaMar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0512><div class="card-body p-3 small">While dialog systems have been widely deployed for computer-assisted language learning (CALL) and formative assessment systems in recent years, relatively limited work has been done with respect to the psychometrics and validity of these technologies in evaluating and providing feedback regarding student learning and conversational ability. This paper formulates a Markov decision process based measurement model, and applies it to text chat data collected from crowdsourced native and non-native English language speakers interacting with an automated dialog agent. We investigate how well the model measures speaker conversational ability, and find that it effectively captures the differences in how native and non-native speakers of English accomplish the dialog task. Such models could have important implications for CALL systems of the future that effectively combine dialog management with measurement of learner conversational ability in real-time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0513/>Generating Feedback for English Foreign Language Exercises<span class=acl-fixed-case>E</span>nglish Foreign Language Exercises</a></strong><br><a href=/people/b/bjorn-rudzewitz/>BjÃ¶rn Rudzewitz</a>
|
<a href=/people/r/ramon-ziai/>Ramon Ziai</a>
|
<a href=/people/k/kordula-de-kuthy/>Kordula De Kuthy</a>
|
<a href=/people/v/verena-moller/>Verena MÃ¶ller</a>
|
<a href=/people/f/florian-nuxoll/>Florian Nuxoll</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0513><div class="card-body p-3 small">While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a>. We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and content errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0523/>Grotoco@SLAM : Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models<span class=acl-fixed-case>SLAM</span>: Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models</a></strong><br><a href=/people/s/sigrid-klerke/>Sigrid Klerke</a>
|
<a href=/people/h/hector-martinez-alonso/>HÃ©ctor MartÃ­nez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0523><div class="card-body p-3 small">We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for the task, including user-derived measures, while examining how far we can get with a simple <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a>. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system : a task-wise (per exercise-format) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0524/>Context Based Approach for Second Language Acquisition</a></strong><br><a href=/people/n/nihal-v-nayak/>Nihal V. Nayak</a>
|
<a href=/people/a/arjun-r-rao/>Arjun R. Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0524><div class="card-body p-3 small">SLAM 2018 focuses on predicting a student&#8217;s mistake while using the Duolingo application. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> we developed for this shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our system uses a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a> to predict the likelihood of a student making a mistake while answering an exercise on <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a> in all three language tracks-English / Spanish (en / es), Spanish / English (es / en) and French / English (fr / en). We conduct an ablation study with several <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> during the development of this system and discover that context based features plays a major role in language acquisition modeling. Our model beats Duolingo&#8217;s baseline scores in all three language tracks (AUROC scores for en / es = 0.821, es / en = 0.790 and fr / en = 0.812). Our work makes a case for providing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>favourable textual context</a> for students while learning second language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0525/>Second Language Acquisition Modeling : An Ensemble Approach</a></strong><br><a href=/people/a/anton-osika/>Anton Osika</a>
|
<a href=/people/s/susanna-nilsson/>Susanna Nilsson</a>
|
<a href=/people/a/andrii-sydorchuk/>Andrii Sydorchuk</a>
|
<a href=/people/f/faruk-sahin/>Faruk Sahin</a>
|
<a href=/people/a/anders-huss/>Anders Huss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0525><div class="card-body p-3 small">Accurate prediction of students&#8217; knowledge is a fundamental building block of <a href=https://en.wikipedia.org/wiki/Personalized_learning>personalized learning systems</a>. Here, we propose an ensemble model to predict student knowledge gaps. Applying our approach to student trace data from the online educational platform Duolingo we achieved highest score on all three datasets in the 2018 Shared Task on Second Language Acquisition Modeling. We describe our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and discuss relevance of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to how it would be setup in a production environment for personalized education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0526/>Modeling Second-Language Learning from a Psychological Perspective</a></strong><br><a href=/people/a/alexander-rich/>Alexander Rich</a>
|
<a href=/people/p/pamela-osborn-popp/>Pamela Osborn Popp</a>
|
<a href=/people/d/david-halpern/>David Halpern</a>
|
<a href=/people/a/anselm-rothe/>Anselm Rothe</a>
|
<a href=/people/t/todd-gureckis/>Todd Gureckis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0526><div class="card-body p-3 small">Psychological research on learning and memory has tended to emphasize small-scale laboratory studies. However, large datasets of people using <a href=https://en.wikipedia.org/wiki/Educational_software>educational software</a> provide opportunities to explore these issues from a new perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well-known class of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> (gradient boosted decision trees), with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> partially informed by theories from the <a href=https://en.wikipedia.org/wiki/Psychology>psychological literature</a>. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0527/>A Memory-Sensitive Classification Model of Errors in Early Second Language Learning</a></strong><br><a href=/people/b/brendan-tomoschuk/>Brendan Tomoschuk</a>
|
<a href=/people/j/jarrett-lovelett/>Jarrett Lovelett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0527><div class="card-body p-3 small">In this paper, we explore a variety of linguistic and cognitive features to better understand <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language acquisition</a> in early users of the language learning app Duolingo. With these features, we trained a random forest classifier to predict errors in early learners of <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Of particular note was our finding that mean and variance in error for each user and token can be a memory efficient replacement for their respective dummy-encoded categorical variables. At test, these models improved over the baseline model with AUROC values of 0.803 for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 0.823 for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and 0.829 for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0529 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0529/>Language Model Based Grammatical Error Correction without Annotated Training Data</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0529><div class="card-body p-3 small">Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0530/>A Semantic Role-based Approach to Open-Domain Automatic Question Generation</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/brian-riordan/>Brian Riordan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0530><div class="card-body p-3 small">We present a novel rule-based system for automatic generation of factual questions from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a>, using semantic role labeling (SRL) as the main form of text analysis. The system is capable of generating both <a href=https://en.wikipedia.org/wiki/Questionnaire>wh-questions</a> and <a href=https://en.wikipedia.org/wiki/Yes&#8211;no_question>yes / no questions</a> from the same <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a>. We present an extensive evaluation of the <a href=https://en.wikipedia.org/wiki/System>system</a> and compare it to a recent neural network architecture for question generation. The SRL-based system outperforms the <a href=https://en.wikipedia.org/wiki/Nervous_system>neural system</a> in both average quality and variety of generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0531/>Automated Content Analysis : A Case Study of Computer Science Student Summaries</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/p/patricia-m-davies/>Patricia M. Davies</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0531><div class="card-body p-3 small">Technology is transforming Higher Education learning and teaching. This paper reports on a project to examine how and why automated content analysis could be used to assess precis writing by university students. We examine the case of one hundred and twenty-two summaries written by computer science freshmen. The texts, which had been hand scored using a teacher-designed rubric, were autoscored using the Natural Language Processing software, PyrEval. Pearson&#8217;s correlation coefficient and <a href=https://en.wikipedia.org/wiki/Spearman_rank_correlation>Spearman rank correlation</a> were used to analyze the relationship between the teacher score and the PyrEval score for each summary. Three content models automatically constructed by PyrEval from different sets of human reference summaries led to consistent correlations, showing that the approach is reliable. Also observed was that, in cases where the focus of student assessment centers on formative feedback, categorizing the PyrEval scores by examining the average and standard deviations could lead to novel interpretations of their relationships. It is suggested that this project has implications for the ways in which automated content analysis could be used to help university students improve their summarization skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0532/>Toward Data-Driven Tutorial Question Answering with Deep Learning Conversational Models</a></strong><br><a href=/people/m/mayank-kulkarni/>Mayank Kulkarni</a>
|
<a href=/people/k/kristy-boyer/>Kristy Boyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0532><div class="card-body p-3 small">There has been an increase in popularity of data-driven question answering systems given their recent success. This pa-per explores the possibility of building a tutorial question answering system for <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java programming</a> from data sampled from a community-based question answering forum. This paper reports on the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset. We investigate how retrieval-based and generative models perform on the given <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The work also investigates the usefulness of using hybrid approaches such as combining retrieval-based and generative models. The results indicate that building data-driven tutorial systems using community-based question answering forums holds significant promise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0533/>Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></strong><br><a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/n/neisarg-dave/>Neisarg Dave</a>
|
<a href=/people/d/drew-wham/>Drew Wham</a>
|
<a href=/people/b/bart-pursel/>Bart Pursel</a>
|
<a href=/people/c/c-lee-giles/>C. Lee Giles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0533><div class="card-body p-3 small">We investigate how <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, specifically <a href=https://en.wikipedia.org/wiki/Ranking>ranking models</a>, can be used to select useful distractors for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice questions</a>. Our proposed models can learn to select distractors that resemble those in actual exam questions, which is different from most existing unsupervised ontology-based and similarity-based methods. We empirically study feature-based and neural net (NN) based ranking models with experiments on the recently released SciQ dataset and our MCQL dataset. Experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outperform both the NN-based method and unsupervised baselines. These two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can also be used as benchmarks for distractor generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0534/>A Portuguese Native Language Identification Dataset<span class=acl-fixed-case>P</span>ortuguese Native Language Identification Dataset</a></strong><br><a href=/people/i/iria-del-rio-gayo/>Iria del RÃ­o Gayo</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0534><div class="card-body p-3 small">In this paper we present NLI-PT, the first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author&#8217;s first language based on their second language writing. The dataset includes 1,868 student essays written by learners of <a href=https://en.wikipedia.org/wiki/European_Portuguese>European Portuguese</a>, native speakers of the following L1s : <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/Tetum_language>Tetum</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. NLI-PT includes the original student text and four different types of annotation : POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>Second Language Acquisition</a> and educational NLP. We discuss possible applications of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and present the results obtained for the first lexical baseline system for Portuguese NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0536 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0536/>The Effect of Adding Authorship Knowledge in Automated Text Scoring</a></strong><br><a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/x/xie-chen/>Xie Chen</a>
|
<a href=/people/r/ronan-cummins/>Ronan Cummins</a>
|
<a href=/people/o/oistein-e-andersen/>Ã˜istein E. Andersen</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0536><div class="card-body p-3 small">Some <a href=https://en.wikipedia.org/wiki/Test_(assessment)>language exams</a> have multiple writing tasks. When a learner writes multiple texts in a language exam, it is not surprising that the quality of these texts tends to be similar, and the existing automated text scoring (ATS) systems do not explicitly model this similarity. In this paper, we suggest that it could be useful to include the other texts written by this learner in the same exam as extra references in an ATS system. We propose various approaches of fusing information from multiple tasks and pass this authorship knowledge into our ATS model on six different datasets. We show that this can positively affect the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance at a global level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0537/>SB@GU at the Complex Word Identification 2018 Shared Task<span class=acl-fixed-case>SB</span>@<span class=acl-fixed-case>GU</span> at the Complex Word Identification 2018 Shared Task</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/i/ildiko-pilan/>IldikÃ³ PilÃ¡n</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0537><div class="card-body p-3 small">In this paper, we describe our experiments for the Shared Task on Complex Word Identification (CWI) 2018 (Yimam et al., 2018), hosted by the 13th Workshop on Innovative Use of NLP for Building Educational Applications (BEA) at NAACL 2018. Our system for <a href=https://en.wikipedia.org/wiki/English_language>English</a> builds on previous work for <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> concerning the classification of words into proficiency levels. We investigate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and compare their usefulness using <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection methods</a>. For the German, Spanish and French data we use simple <a href=https://en.wikipedia.org/wiki/System>systems</a> based on character n-gram models and show that sometimes simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve comparable results to fully feature-engineered systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0539 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0539/>Deep Learning Architecture for Complex Word Identification</a></strong><br><a href=/people/d/dirk-de-hertog/>Dirk De Hertog</a>
|
<a href=/people/a/anais-tack/>AnaÃ¯s Tack</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0539><div class="card-body p-3 small">We describe a system for the CWI-task that includes information on 5 aspects of the (complex) lexical item, namely distributional information of the item itself, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological structure</a>, psychological measures, corpus-counts and topical information. We constructed a deep learning architecture that combines those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and apply it to the probabilistic and binary classification task for all <a href=https://en.wikipedia.org/wiki/English_language>English sets</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We achieved reasonable performance on all sets with best performances seen on the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>probabilistic task</a>, particularly on the English news set (MAE 0.054 and F1-score of 0.872). An analysis of the results shows that reasonable performance can be achieved with a single <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> without any domain-specific tweaking of the parameter settings and that distributional features capture almost all of the information also found in hand-crafted features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0540 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0540/>NILC at CWI 2018 : Exploring <a href=https://en.wikipedia.org/wiki/Feature_engineering>Feature Engineering</a> and Feature Learning<span class=acl-fixed-case>NILC</span> at <span class=acl-fixed-case>CWI</span> 2018: Exploring Feature Engineering and Feature Learning</a></strong><br><a href=/people/n/nathan-hartmann/>Nathan Hartmann</a>
|
<a href=/people/l/leandro-borges-dos-santos/>Leandro Borges dos Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0540><div class="card-body p-3 small">This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches : (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0541/>Complex Word Identification Using Character n-grams</a></strong><br><a href=/people/m/maja-popovic/>Maja PopoviÄ‡</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0541><div class="card-body p-3 small">This paper investigates the use of character n-gram frequencies for identifying complex words in English, German and Spanish texts. The approach is based on the assumption that complex words are likely to contain different <a href=https://en.wikipedia.org/wiki/Character_(computing)>character sequences</a> than simple words. The <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>multinomial Naive Bayes classifier</a> was used with n-grams of different lengths as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and the best results were obtained for the combination of 2-grams and 4-grams. This variant was submitted to the Complex Word Identification Shared Task 2018 for all texts and achieved F-scores between 70 % and 83 %. The system was ranked in the middle range for all English texts, as third of fourteen submissions for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and as tenth of seventeen submissions for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is not very convenient for the cross-language task, achieving only 59 % on the <a href=https://en.wikipedia.org/wiki/French_language>French text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0545/>Deep Factorization Machines for Knowledge Tracing</a></strong><br><a href=/people/j/jill-jenn-vie/>Jill-JÃªnn Vie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0545><div class="card-body p-3 small">This paper introduces our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We used deep factorization machines, a wide and deep learning model of pairwise relationships between users, items, skills, and other entities considered. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> (AUC 0.815) hopefully managed to beat the logistic regression baseline (AUC 0.774) but not the top performing <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (AUC 0.861) and reveals interesting strategies to build upon <a href=https://en.wikipedia.org/wiki/Item_response_theory>item response theory models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0546/>CLUF : a Neural Model for Second Language Acquisition Modeling<span class=acl-fixed-case>CLUF</span>: a Neural Model for Second Language Acquisition Modeling</a></strong><br><a href=/people/s/shuyao-xu/>Shuyao Xu</a>
|
<a href=/people/j/jin-chen/>Jin Chen</a>
|
<a href=/people/l/long-qin/>Long Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0546><div class="card-body p-3 small">Second Language Acquisition Modeling is the task to predict whether a <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learner</a> would respond correctly in future exercises based on their learning history. In this paper, we propose a neural network based system to utilize rich contextual, linguistic and user information. Our neural model consists of a Context encoder, a Linguistic feature encoder, a User information encoder and a Format information encoder (CLUF). Furthermore, a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is introduced to combine such encoded features and make final predictions. Our system ranked in first place in the English track and second place in the Spanish and French track with an AUROC score of 0.861, 0.835 and 0.854 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0547/>Neural sequence modelling for learner error prediction</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0547><div class="card-body p-3 small">This paper describes our use of two recurrent neural network sequence models : sequence labelling and sequence-to-sequence models, for the prediction of future learner errors in our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We show that these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capture complementary information as combining them improves performance. Furthermore, the same network architecture and group of features can be used directly to build competitive prediction models in all three language tracks, demonstrating that our approach generalises well across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0548 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0548/>Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a></a></strong><br><a href=/people/l/le-an-ha/>Le An Ha</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0548><div class="card-body p-3 small">Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process. In this paper we propose a fully automatic method for generating distractor suggestions for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice questions</a> used in high-stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval methods</a>. We frame the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> as a <a href=https://en.wikipedia.org/wiki/Prediction>prediction task</a> where we aim to predict the human-produced distractors used in large sets of <a href=https://en.wikipedia.org/wiki/Medical_research>medical questions</a>, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0549 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0549/>Co-Attention Based Neural Network for Source-Dependent Essay Scoring</a></strong><br><a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0549><div class="card-body p-3 small">This paper presents an investigation of using a co-attention based neural network for source-dependent essay scoring. We use a co-attention mechanism to help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn the importance of each part of the essay more accurately. Also, this paper shows that the co-attention based neural network model provides reliable score prediction of source-dependent responses. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two source-dependent response corpora. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> on both <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>. We also show that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is similar to the <a href=https://en.wikipedia.org/wiki/Expert_witness>expert opinions</a> with examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0550 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0550/>Cross-Lingual Content Scoring</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/s/sebastian-stennmanns/>Sebastian Stennmanns</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0550><div class="card-body p-3 small">We investigate the feasibility of cross-lingual content scoring, a scenario where training and test data in an automatic scoring task are from two different languages. Cross-lingual scoring can contribute to <a href=https://en.wikipedia.org/wiki/Educational_equality>educational equality</a> by allowing answers in multiple languages. Training a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in one language and applying it to another language might also help to overcome data sparsity issues by re-using trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> from other languages. As there is no suitable dataset available for this new task, we create a comparable bi-lingual corpus by extending the English ASAP dataset with German answers. Our experiments with cross-lingual scoring based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-translating</a> either training or test data show a considerable drop in scoring quality.</div></div></div><hr><div id=w18-06><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-06.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-06/>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0600/>Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</a></strong><br><a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prudâ€™hommeaux</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0601/>What type of happiness are you looking for?-A closer look at detecting mental health from language</a></strong><br><a href=/people/a/alina-arseniev-koehler/>Alina Arseniev-Koehler</a>
|
<a href=/people/s/sharon-mozgai/>Sharon Mozgai</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0601><div class="card-body p-3 small">Computational models to detect <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental illnesses</a> from <a href=https://en.wikipedia.org/wiki/Writing>text</a> and <a href=https://en.wikipedia.org/wiki/Speech>speech</a> could enhance our understanding of <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> while offering opportunities for early detection and intervention. However, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are often disconnected from the lived experience of depression and the larger diagnostic debates in <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>. This article investigates these disconnects, primarily focusing on the labels used to diagnose <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>, how these labels are computationally represented, and the performance metrics used to evaluate <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. We also consider how <a href=https://en.wikipedia.org/wiki/Medical_device>medical instruments</a> used to measure <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>, such as the Patient Health Questionnaire (PHQ), contribute to these disconnects. To illustrate our points, we incorporate mixed-methods analyses of 698 interviews on <a href=https://en.wikipedia.org/wiki/Emotional_health>emotional health</a>, which are coupled with self-report PHQ screens for <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>. We propose possible strategies to bridge these gaps between modern <a href=https://en.wikipedia.org/wiki/Psychiatry>psychiatric understandings of depression</a>, <a href=https://en.wikipedia.org/wiki/Laity>lay experience of depression</a>, and <a href=https://en.wikipedia.org/wiki/Computational_psychology>computational representation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0603/>Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online Postings</a></strong><br><a href=/people/h/han-chin-shing/>Han-Chin Shing</a>
|
<a href=/people/s/suraj-nair/>Suraj Nair</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/m/meir-friedenberg/>Meir Friedenberg</a>
|
<a href=/people/h/hal-daume-iii/>Hal DaumÃ© III</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0603><div class="card-body p-3 small">We report on the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>assessment of suicide risk</a> via online postings in <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Evaluation of risk-level annotations by experts yields what is, to our knowledge, the first demonstration of reliability in <a href=https://en.wikipedia.org/wiki/Risk_assessment>risk assessment</a> by clinicians based on social media postings. We also introduce and demonstrate the value of a new, detailed rubric for assessing suicide risk, compare crowdsourced with expert performance, and present baseline predictive modeling experiments using the new dataset, which will be made available to researchers through the American Association of Suicidology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0604/>CLPsych 2018 Shared Task : Predicting Current and Future Psychological Health from Childhood Essays<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays</a></strong><br><a href=/people/v/veronica-lynn/>Veronica Lynn</a>
|
<a href=/people/a/alissa-goodman/>Alissa Goodman</a>
|
<a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0604><div class="card-body p-3 small">We describe the shared task for the CLPsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. Language-based predictions of a person&#8217;s current health have the potential to supplement traditional <a href=https://en.wikipedia.org/wiki/Psychological_evaluation>psychological assessment</a> such as <a href=https://en.wikipedia.org/wiki/Questionnaire>questionnaires</a>, improving intake risk measurement and monitoring. Predictions of future psychological health can aid with both early detection and the development of <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>preventative care</a>. Research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the NLP community. This shared task represents one of the first attempts to evaluate the use of early language to predict future health ; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>prevention</a> and <a href=https://en.wikipedia.org/wiki/Therapy>treatment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0606/>Using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for automatic triage of posts in a <a href=https://en.wikipedia.org/wiki/Internet_forum>peer-support forum</a></a></strong><br><a href=/people/e/edgar-altszyler/>Edgar Altszyler</a>
|
<a href=/people/a/ariel-j-berenstein/>Ariel J. Berenstein</a>
|
<a href=/people/d/david-n-milne/>David Milne</a>
|
<a href=/people/r/rafael-a-calvo/>Rafael A. Calvo</a>
|
<a href=/people/d/diego-fernandez-slezak/>Diego Fernandez Slezak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0606><div class="card-body p-3 small">Mental health forums are online spaces where people can share their experiences anonymously and get peer support. These <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>, require the supervision of moderators to provide support in delicate cases, such as posts expressing <a href=https://en.wikipedia.org/wiki/Suicide_ideation>suicide ideation</a>. The large increase in the number of forum users makes the task of the moderators unmanageable without the help of <a href=https://en.wikipedia.org/wiki/Triage_(medicine)>automatic triage systems</a>. In the present paper, we present a Machine Learning approach for the triage of posts. Most approaches in the literature focus on the content of the posts, but only a few authors take advantage of features extracted from the context in which they appear. Our approach consists of the development and implementation of a large variety of new <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> from both, the content and the context of posts, such as previous messages, interaction with other users and author&#8217;s history. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> has competed in the CLPsych 2017 Shared Task, obtaining the first place for several of the subtasks. Moreover, we also found that models that take advantage of post context improve significantly its performance in the detection of flagged posts (posts that require moderators attention), as well as those that focus on post content outperforms in the detection of most urgent events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0607/>Hierarchical neural model with <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for the classification of social media text related to mental health</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/g/george-gkotsis/>George Gkotsis</a>
|
<a href=/people/r/rina-dutta/>Rina Dutta</a>
|
<a href=/people/r/robert-stewart/>Robert Stewart</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0607><div class="card-body p-3 small">Mental health problems represent a major public health challenge. Automated analysis of text related to <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> is aimed to help <a href=https://en.wikipedia.org/wiki/Medical_decision-making>medical decision-making</a>, <a href=https://en.wikipedia.org/wiki/Public_health_policy>public health policies</a> and to improve <a href=https://en.wikipedia.org/wiki/Health_care>health care</a>. Such <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> may involve <a href=https://en.wikipedia.org/wiki/Categorization>text classification</a>. Traditionally, automated classification has been performed mainly using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> involving costly <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Recently, the performance of those <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> has been dramatically improved by neural methods. However, mainly Convolutional neural networks (CNNs) have been explored. In this paper, we apply a hierarchical Recurrent neural network (RNN) architecture with an attention mechanism on social media data related to <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>. We show that this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> improves overall classification results as compared to previously reported results on the same <a href=https://en.wikipedia.org/wiki/Data>data</a>. Benefitting from the attention mechanism, it can also efficiently select text elements crucial for classification decisions, which can also be used for in-depth analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0609/>Deep Learning for Depression Detection of Twitter Users<span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/p/prasadith-buddhitha/>Prasadith Buddhitha</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0609><div class="card-body p-3 small">Mental illness detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can be considered a complex task, mainly due to the complicated nature of <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental disorders</a>. In recent years, this research area has started to evolve with the continuous increase in popularity of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> that became an integral part of people&#8217;s life. This close relationship between social media platforms and their users has made these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> to reflect the users&#8217; personal life with different limitations. In such an environment, researchers are presented with a wealth of information regarding one&#8217;s life. In addition to the level of complexity in identifying <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental illnesses</a> through social media platforms, adopting supervised machine learning approaches such as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> have not been widely accepted due to the difficulties in obtaining sufficient amounts of annotated training data. Due to these reasons, we try to identify the most effective deep neural network architecture among a few of selected architectures that were successfully used in natural language processing tasks. The chosen architectures are used to detect users with signs of mental illnesses (depression in our case) given limited unstructured text data extracted from the <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social media platform</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0611/>Predicting Psychological Health from Childhood Essays with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for the CLPsych 2018 Shared Task (Team UKNLP)<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task (Team <span class=acl-fixed-case>UKNLP</span>)</a></strong><br><a href=/people/a/anthony-rios/>Anthony Rios</a>
|
<a href=/people/t/tung-tran/>Tung Tran</a>
|
<a href=/people/r/ramakanth-kavuluru/>Ramakanth Kavuluru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0611><div class="card-body p-3 small">This paper describes the systems we developed for tasks A and B of the 2018 CLPsych shared task. The first task (task A) focuses on predicting behavioral health scores at age 11 using childhood essays. The second task (task B) asks participants to predict future psychological distress at ages 23, 33, 42, and 50 using the age 11 essays. We propose two convolutional neural network based methods that map each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression problem</a>. Among seven teams we ranked third on task A with disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we ranked third on task B with an average DPC score of 0.3062.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0612/>A Psychologically Informed Approach to CLPsych Shared Task 2018<span class=acl-fixed-case>CLP</span>sych Shared Task 2018</a></strong><br><a href=/people/a/almog-simchon/>Almog Simchon</a>
|
<a href=/people/m/michael-gilead/>Michael Gilead</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0612><div class="card-body p-3 small">This paper describes our approach to the CLPsych 2018 Shared Task, in which we attempted to predict cross-sectional psychological health at age 11 and future psychological distress based on childhood essays. We attempted several modeling approaches and observed best cross-validated prediction accuracy with relatively simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> based on <a href=https://en.wikipedia.org/wiki/Psychology>psychological theory</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provided reasonable predictions in most outcomes. Notably, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was especially successful in predicting out-of-sample psychological distress (across people and across time) at age 50.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0613/>Predicting Psychological Health from Childhood Essays. The UGent-IDLab CLPsych 2018 Shared Task System.<span class=acl-fixed-case>UG</span>ent-<span class=acl-fixed-case>IDL</span>ab <span class=acl-fixed-case>CLP</span>sych 2018 Shared Task System.</a></strong><br><a href=/people/k/klim-zaporojets/>Klim Zaporojets</a>
|
<a href=/people/l/lucas-sterckx/>Lucas Sterckx</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0613><div class="card-body p-3 small">This paper describes the IDLab system submitted to Task A of the CLPsych 2018 shared task. The goal of this task is predicting psychological health of children based on language used in hand-written essays and socio-demographic control variables. Our entry uses word- and character-based features as well as lexicon-based features and <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> derived from the essays such as the quality of the language. We apply <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a>, <a href=https://en.wikipedia.org/wiki/Gradient_boosting>gradient boosting</a> as well as neural-network based regressors (feed-forward, CNNs and RNNs) to predict scores. We then make ensembles of our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0614/>Can adult mental health be predicted by childhood future-self narratives? Insights from the CLPsych 2018 Shared Task<span class=acl-fixed-case>CLP</span>sych 2018 Shared Task</a></strong><br><a href=/people/k/kylie-radford/>Kylie Radford</a>
|
<a href=/people/l/louise-lavrencic/>Louise Lavrencic</a>
|
<a href=/people/r/ruth-peters/>Ruth Peters</a>
|
<a href=/people/k/kim-kiely/>Kim Kiely</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/s/scott-nowson/>Scott Nowson</a>
|
<a href=/people/w/will-radford/>Will Radford</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0614><div class="card-body p-3 small">The CLPsych 2018 Shared Task B explores how childhood essays can predict <a href=https://en.wikipedia.org/wiki/Distress_(medicine)>psychological distress</a> throughout the author&#8217;s life. Our main aim was to build tools to help our psychologists understand the data, propose <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and interpret predictions. We submitted two linear regression models : ModelA uses simple demographic and word-count features, while ModelB uses linguistic, entity, typographic, expert-gazetteer, and readability features. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and <a href=https://en.wikipedia.org/wiki/Clinical_psychology>clinical psychology</a> is a productive direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0615/>Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/j/jong-yoon/>Jong Yoon</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0615><div class="card-body p-3 small">Schizophrenia is a mental disorder which afflicts an estimated 0.7 % of adults world wide. It affects many areas of <a href=https://en.wikipedia.org/wiki/Cognition>mental function</a>, often evident from <a href=https://en.wikipedia.org/wiki/Speech_disorder>incoherent speech</a>. Diagnosing schizophrenia relies on <a href=https://en.wikipedia.org/wiki/Subjectivity>subjective judgments</a> resulting in disagreements even among trained clinicians. Recent studies have proposed the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for diagnosis by drawing on automatically-extracted linguistic features like discourse coherence and <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>. Here, we present the first benchmark comparison of previously proposed coherence models for detecting symptoms of schizophrenia and evaluate their performance on a new dataset of recorded interviews between subjects and clinicians. We also present two alternative coherence metrics based on modern sentence embedding techniques that outperform the previous methods on our dataset. Lastly, we propose a novel <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> for reference incoherence based on ambiguous pronoun usage and show that it is a highly predictive feature on our <a href=https://en.wikipedia.org/wiki/Data>data</a>. While the number of subjects is limited in this pilot study, our results suggest new directions for diagnosing common symptoms of schizophrenia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0616/>Oral-Motor and Lexical Diversity During Naturalistic Conversations in Adults with Autism Spectrum Disorder</a></strong><br><a href=/people/j/julia-parish-morris/>Julia Parish-Morris</a>
|
<a href=/people/e/evangelos-sariyanidi/>Evangelos Sariyanidi</a>
|
<a href=/people/c/casey-zampella/>Casey Zampella</a>
|
<a href=/people/g/g-keith-bartley/>G. Keith Bartley</a>
|
<a href=/people/e/emily-ferguson/>Emily Ferguson</a>
|
<a href=/people/a/ashley-a-pallathra/>Ashley A. Pallathra</a>
|
<a href=/people/l/leila-bateman/>Leila Bateman</a>
|
<a href=/people/s/samantha-plate/>Samantha Plate</a>
|
<a href=/people/m/meredith-cola/>Meredith Cola</a>
|
<a href=/people/j/juhi-pandey/>Juhi Pandey</a>
|
<a href=/people/e/edward-s-brodkin/>Edward S. Brodkin</a>
|
<a href=/people/r/robert-t-schultz/>Robert T. Schultz</a>
|
<a href=/people/b/birkan-tunc/>Birkan TunÃ§</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0616><div class="card-body p-3 small">Autism spectrum disorder (ASD) is a <a href=https://en.wikipedia.org/wiki/Neurodevelopmental_disorder>neurodevelopmental condition</a> characterized by impaired <a href=https://en.wikipedia.org/wiki/Communication>social communication</a> and the presence of restricted, repetitive patterns of behaviors and interests. Prior research suggests that restricted patterns of behavior in <a href=https://en.wikipedia.org/wiki/Autism_spectrum>ASD</a> may be cross-domain phenomena that are evident in a variety of modalities. Computational studies of language in ASD provide support for the existence of an underlying dimension of restriction that emerges during a conversation. Similar evidence exists for restricted patterns of facial movement. Using tools from <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, and <a href=https://en.wikipedia.org/wiki/Information_theory>information theory</a>, this study tests whether cognitive-motor restriction can be detected across multiple behavioral domains in adults with ASD during a naturalistic conversation. Our methods identify restricted behavioral patterns, as measured by <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> in <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>word use</a> and <a href=https://en.wikipedia.org/wiki/Human_mouth>mouth movement</a>. Results suggest that adults with ASD produce significantly less diverse mouth movements and words than neurotypical adults, with an increased reliance on repeated patterns in both domains. The diversity values of the two domains are not significantly correlated, suggesting that they provide complementary information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0617/>Dynamics of an idiostyle of a Russian suicidal blogger<span class=acl-fixed-case>R</span>ussian suicidal blogger</a></strong><br><a href=/people/t/tatiana-litvinova/>Tatiana Litvinova</a>
|
<a href=/people/o/olga-litvinova/>Olga Litvinova</a>
|
<a href=/people/p/pavel-seredin/>Pavel Seredin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0617><div class="card-body p-3 small">Over 800000 people die of suicide each year. It is es-timated that by the year 2020, this figure will have in-creased to 1.5 million. It is considered to be one of the major causes of mortality during adolescence. Thus there is a growing need for <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of identifying su-icidal individuals. Language analysis is known to be a valuable psychodiagnostic tool, however the material for such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> is not easy to obtain. Currently as the Internet communications are developing, there is an opportunity to study texts of suicidal individuals. Such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> can provide a useful insight into the peculiarities of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal thinking</a>, which can be used to further develop methods for diagnosing the risk of suicidal behavior. The paper analyzes the dynamics of a number of linguistic parameters of an idiostyle of a Russian-language blogger who died by suicide. For the first time such an <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> has been conducted using the material of <a href=https://en.wikipedia.org/wiki/Internet_in_Russia>Russian online texts</a>. For <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, the LIWC program is used. A <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation analysis</a> was performed to identify the relationship between LIWC variables and number of days prior to suicide. Data visualization, as well as comparison with the results of related studies was performed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0618/>RSDD-Time : Temporal Annotation of Self-Reported Mental Health Diagnoses<span class=acl-fixed-case>RSDD</span>-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses</a></strong><br><a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/b/bart-desmet/>Bart Desmet</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0618><div class="card-body p-3 small">Self-reported diagnosis statements have been widely employed in studying language related to mental health in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. However, existing research has largely ignored the temporality of mental health diagnoses. In this work, we introduce RSDD-Time : a new dataset of 598 manually annotated self-reported depression diagnosis posts from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> that include temporal information about the diagnosis. Annotations include whether a <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health condition</a> is present and how recently the diagnosis happened. Furthermore, we include exact temporal spans that relate to the date of diagnosis. This information is valuable for various computational methods to examine <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> because one&#8217;s mental health state is not static. We also test several baseline classification and extraction approaches, which suggest that extracting temporal information from self-reported diagnosis statements is challenging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0620/>Within and Between-Person Differences in Language Used Across Anxiety Support and Neutral Reddit Communities<span class=acl-fixed-case>R</span>eddit Communities</a></strong><br><a href=/people/m/molly-ireland/>Molly Ireland</a>
|
<a href=/people/m/micah-iserman/>Micah Iserman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0620><div class="card-body p-3 small">Although many studies have distinguished between the social media language use of people who do and do not have a <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health condition</a>, within-person context-sensitive comparisons (for example, analyzing individuals&#8217; language use when seeking support or discussing neutral topics) are less common. Two dictionary-based analyses of Reddit communities compared (1) anxious individuals&#8217; comments in <a href=https://en.wikipedia.org/wiki/Internet_forum>anxiety support communities</a> (e.g., /r / PanicParty) with the same users&#8217; comments in <a href=https://en.wikipedia.org/wiki/Internet_forum>neutral communities</a> (e.g., /r / todayilearned), and, (2) within popular neutral communities, comments by members of <a href=https://en.wikipedia.org/wiki/Internet_forum>anxiety subreddits</a> with comments by other users. Each comparison yielded theory-consistent effects as well as unexpected results that suggest novel hypotheses to be tested in the future. Results have relevance for improving researchers&#8217; and practitioners&#8217; ability to unobtrusively assess <a href=https://en.wikipedia.org/wiki/Anxiety>anxiety symptoms</a> in conversations that are not explicitly about <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0621/>Helping or Hurting? Predicting Changes in Usersâ€™ Risk of Self-Harm Through Online Community Interactions</a></strong><br><a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/t/timothy-walsh/>Timothy Walsh</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/j/julien-han/>Julien Han</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0621><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> have formed around <a href=https://en.wikipedia.org/wiki/Suicide>suicide</a> and <a href=https://en.wikipedia.org/wiki/Suicide_prevention>self-harm prevention</a>. While these <a href=https://en.wikipedia.org/wiki/Community>communities</a> offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a>. In this work, we focus on how interaction with others in such a <a href=https://en.wikipedia.org/wiki/Community>community</a> affects the <a href=https://en.wikipedia.org/wiki/Mental_state>mental state</a> of users who are seeking support. We first build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of conversation threads between users in a distressed state and community members offering support. We then show how to construct a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to predict whether distressed users are helped or harmed by the interactions in the thread, and we achieve a macro-F1 score of up to 0.69.</div></div></div><hr><div id=w18-07><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-07.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-07/>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0700/>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0701/>Anaphora Resolution for Twitter Conversations : An Exploratory Study<span class=acl-fixed-case>T</span>witter Conversations: An Exploratory Study</a></strong><br><a href=/people/b/berfin-aktas/>Berfin AktaÅŸ</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0701><div class="card-body p-3 small">We present a corpus study of pronominal anaphora on Twitter conversations. After outlining the specific features of this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, with respect to reference resolution, we explain the construction of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and the annotation steps. From this we derive a list of phenomena that need to be considered when performing anaphora resolution on this type of <a href=https://en.wikipedia.org/wiki/Data>data</a>. Finally, we test the performance of an off-the-shelf resolution system, and provide some qualitative error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0702/>Anaphora Resolution with the ARRAU Corpus<span class=acl-fixed-case>ARRAU</span> Corpus</a></strong><br><a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Moosavi</a>
|
<a href=/people/i/ina-roesiger/>Ina Roesiger</a>
|
<a href=/people/a/adam-roussel/>Adam Roussel</a>
|
<a href=/people/f/fabian-simonjetz/>Fabian Simonjetz</a>
|
<a href=/people/a/alexandra-uma/>Alexandra Uma</a>
|
<a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/h/heike-zinsmeister/>Heike Zinsmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0702><div class="card-body p-3 small">The ARRAU corpus is an <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphorically annotated corpus of English</a> providing rich linguistic information about <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>. The most distinctive feature of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is the annotation of a wide range of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric relations</a>, including bridging references and <a href=https://en.wikipedia.org/wiki/Discourse_deixis>discourse deixis</a> in addition to identity (coreference). Other distinctive features include treating all NPs as markables, including non-referring NPs ; and the annotation of a variety of morphosyntactic and semantic mention and entity attributes, including the genericity status of the entities referred to by markables. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> however has not been extensively used for <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a> research so far. In this paper, we discuss three datasets extracted from the ARRAU corpus to support the three subtasks of the CRAC 2018 Shared Taskidentity anaphora resolution over ARRAU-style markables, bridging references resolution, and discourse deixis ; the evaluation scripts assessing system performance on those datasets ; and preliminary results on these three tasks that may serve as baseline for subsequent research in these phenomena.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0703/>Rule- and Learning-based Methods for Bridging Resolution in the ARRAU Corpus<span class=acl-fixed-case>ARRAU</span> Corpus</a></strong><br><a href=/people/i/ina-roesiger/>Ina Roesiger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0703><div class="card-body p-3 small">We present two systems for bridging resolution, which we submitted to the CRAC shared task on bridging anaphora resolution in the ARRAU corpus (track 2): a rule-based approach following Hou et al. 2014 and a learning-based approach. The re-implementation of Hou et al. 2014 achieves very poor performance when being applied to ARRAU. We found that the reasons for this lie in the different bridging annotations : whereas the rule-based system suggests many referential bridging pairs, ARRAU contains mostly lexical bridging. We describe the differences between these two types of bridging and adapt the rule-based approach to be able to handle lexical bridging. The modified rule-based approach achieves reasonable performance on all (sub)-tasks and outperforms a simple learning-based approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0704/>A <a href=https://en.wikipedia.org/wiki/Predictive_modelling>Predictive Model</a> for <a href=https://en.wikipedia.org/wiki/Anaphora_(rhetoric)>Notional Anaphora</a> in English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0704><div class="card-body p-3 small">Notional anaphors are pronouns which disagree with their antecedents&#8217; grammatical categories for notional reasons, such as plural to singular agreement in : the government... they. Since such cases are rare and conflict with evidence from strictly agreeing cases (the government... it), they present a substantial challenge to both <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Referring_expression_generation>referring expression generation</a>. Using the OntoNotes corpus, this paper takes an ensemble approach to predicting English notional anaphora in context on the basis of the largest empirical data to date. In addition to state of the art prediction accuracy, the results suggest that theoretical approaches positing a plural construal at the antecedent&#8217;s utterance are insufficient, and that circumstances at the anaphor&#8217;s utterance location, as well as global factors such as genre, have a strong effect on the choice of referring expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0706/>Towards Bridging Resolution in <a href=https://en.wikipedia.org/wiki/German_language>German</a> : <a href=https://en.wikipedia.org/wiki/Data_analysis>Data Analysis</a> and Rule-based Experiments<span class=acl-fixed-case>G</span>erman: Data Analysis and Rule-based Experiments</a></strong><br><a href=/people/j/janis-pagel/>Janis Pagel</a>
|
<a href=/people/i/ina-roesiger/>Ina Roesiger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0706><div class="card-body p-3 small">Bridging resolution is the task of recognising bridging anaphors and linking them to their antecedents. While there is some work on bridging resolution for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, there is only little work for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We present two datasets which contain bridging annotations, namely DIRNDL and <a href=https://en.wikipedia.org/wiki/GRAIN>GRAIN</a>, and compare the performance of a rule-based system with a simple baseline approach on these two corpora. The performance for full bridging resolution ranges between an <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>F1 score</a> of 13.6 % for <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>DIRNDL</a> and 11.8 % for <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>GRAIN</a>. An analysis using <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle lists</a> suggests that the <a href=https://en.wikipedia.org/wiki/System>system</a> could, to a certain extent, benefit from ranking and re-ranking antecedent candidates. Furthermore, we investigate the importance of single features and show that the <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> used in our work seem promising for future bridging resolution approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0707/>Detecting and Resolving Shell Nouns in German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/a/adam-roussel/>Adam Roussel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0707><div class="card-body p-3 small">This paper describes the design and evaluation of a <a href=https://en.wikipedia.org/wiki/System>system</a> for the automatic detection and resolution of shell nouns in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Shell nouns are general nouns, such as fact, question, or problem, whose full interpretation relies on a content phrase located elsewhere in a text, which these <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> simultaneously serve to characterize and encapsulate. To accomplish this, the system uses a series of lexico-syntactic patterns in order to extract shell noun candidates and their content in parallel. Each pattern has its own <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>, which makes the final decision as to whether or not a link is to be established and the shell noun resolved. Overall, about 26.2 % of the annotated shell noun instances were correctly identified by the <a href=https://en.wikipedia.org/wiki/System>system</a>, and of these cases, about 72.5 % are assigned the correct content phrase. Though it remains difficult to identify shell noun instances reliably (recall is accordingly low in this regard), this system usually assigns the right content to correctly classified cases. cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0709/>A Fine-grained Large-scale Analysis of Coreference Projection</a></strong><br><a href=/people/m/michal-novak/>Michal NovÃ¡k</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0709><div class="card-body p-3 small">We perform a fine-grained large-scale analysis of coreference projection. By projecting gold coreference from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and vice versa on Prague Czech-English Dependency Treebank 2.0 Coref, we set an upper bound of a proposed projection approach for these two languages. We undertake a detailed thorough analysis that combines the analysis of projection&#8217;s subtasks with analysis of performance on individual mention types. The findings are accompanied with examples from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div></div><hr><div id=w18-08><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-08.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-08/>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0800/>Proceedings of the Second <span class=acl-fixed-case>ACL</span> Workshop on Ethics in Natural Language Processing</a></strong><br><a href=/people/m/mark-alfano/>Mark Alfano</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0801/>On the Utility of Lay Summaries and AI Safety Disclosures : Toward Robust, Open Research Oversight<span class=acl-fixed-case>AI</span> Safety Disclosures: Toward Robust, Open Research Oversight</a></strong><br><a href=/people/a/allen-schmaltz/>Allen Schmaltz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0801><div class="card-body p-3 small">In this position paper, we propose that the community consider encouraging researchers to include two riders, a Lay Summary and an AI Safety Disclosure, as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchersvia a relatively non-intrusive mechanismto consider the societal implications of technologies carrying (un)known and/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers ; the lay summary, at least, has significant precedence in the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> and other areas of science ; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).</div></div></div><hr><div id=w18-09><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-09.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-09/>Proceedings of the Workshop on Figurative Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0900/>Proceedings of the Workshop on Figurative Language Processing</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/p/patricia-lichtenstein/>Patricia Lichtenstein</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/c/chee-wee/>Chee Wee</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0902/>Linguistic Features of <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> and Metaphor Production Quality</a></strong><br><a href=/people/s/stephen-skalicky/>Stephen Skalicky</a>
|
<a href=/people/s/scott-crossley/>Scott Crossley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0902><div class="card-body p-3 small">Using <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to detect <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> has provided a deeper in-sight into <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. The purpose of this study is to assess whether <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> can help explain differences in quality of figurative language. In this study a large corpus of metaphors and sarcastic responses are collected from human subjects and rated for figurative language quality based on <a href=https://en.wikipedia.org/wiki/Metaphor>theoretical components of metaphor</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>. Using natural language processing tools, specific linguistic features related to lexical sophistication and semantic cohesion were used to predict the human ratings of figurative language quality. Results demonstrate <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> were able to predict small amounts of variance in metaphor and sarcasm production quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0905/>Catching Idiomatic Expressions in EFL Essays<span class=acl-fixed-case>EFL</span> Essays</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0905><div class="card-body p-3 small">This paper presents an exploratory study on large-scale detection of idiomatic expressions in <a href=https://en.wikipedia.org/wiki/Essay>essays</a> written by non-native speakers of English. We describe a computational search procedure for automatic detection of idiom-candidate phrases in essay texts. The study used a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of essays</a> written during a standardized examination of English language proficiency. Automatically-flagged candidate expressions were manually annotated for <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>. The study found that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> are widely used in <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>EFL essays</a>. The study also showed that a search algorithm that accommodates the syntactic and lexical exibility of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> can increase the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of idiom instances by 30 %, but it also increases the amount of false positives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0906/>Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0906><div class="card-body p-3 small">We propose a new annotated corpus for metaphor interpretation by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a>, and a novel DNN model for performing this task. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of 200 sets of 5 sentences, with each set containing one reference metaphorical sentence, and four ranked candidate paraphrases. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained for a binary classification of paraphrase candidates, and then used to predict graded paraphrase acceptability. It reaches an encouraging 75 % accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0907/>A Report on the 2018 VUA Metaphor Detection Shared Task<span class=acl-fixed-case>VUA</span> Metaphor Detection Shared Task</a></strong><br><a href=/people/c/chee-wee-leong/>Chee Wee (Ben) Leong</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0907><div class="card-body p-3 small">As the community working on computational approaches to <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative language</a> is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area. One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset. We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0908/>An LSTM-CRF Based Approach to Token-Level Metaphor Detection<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Based Approach to Token-Level Metaphor Detection</a></strong><br><a href=/people/m/malay-pramanick/>Malay Pramanick</a>
|
<a href=/people/a/ashim-gupta/>Ashim Gupta</a>
|
<a href=/people/p/pabitra-mitra/>Pabitra Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0908><div class="card-body p-3 small">Automatic processing of <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative languages</a> is gaining popularity in NLP community for their ubiquitous nature and increasing volume. In this era of <a href=https://en.wikipedia.org/wiki/Web_2.0>web 2.0</a>, automatic analysis of sarcasm and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> is important for their extensive usage. Metaphors are a part of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> that compares different concepts, often on a <a href=https://en.wikipedia.org/wiki/Cognition>cognitive level</a>. Many approaches have been proposed for automatic detection of metaphors, even using <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential models</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In this paper, we propose a method for detection of metaphors at the token level using a hybrid model of Bidirectional-LSTM and CRF. We used fewer <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, as compared to the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a>. On experimentation with VUAMC, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtained an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.674.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0911" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0911/>Bigrams and BiLSTMs Two <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Sequential Metaphor Detection<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>s Two Neural Networks for Sequential Metaphor Detection</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0911><div class="card-body p-3 small">We present and compare two alternative deep neural architectures to perform word-level metaphor detection on text : a bi-LSTM model and a new structure based on recursive feed-forward concatenation of the input. We discuss different versions of such models and the effect that input manipulation-specifically, reducing the length of sentences and introducing concreteness scores for words-have on their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0913/>Neural Metaphor Detecting with CNN-LSTM Model<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0913><div class="card-body p-3 small">Metaphors are <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative languages</a> widely used in daily life and literatures. It&#8217;s an important task to detect the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> evoked by texts. Thus, the metaphor shared task is aimed to extract <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> from plain texts at <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>word level</a>. We propose to use a CNN-LSTM model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information. In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task. We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieved 65.06 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in the all POS testing subtask and 67.15 % in the verbs testing subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0914/>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection<span class=acl-fixed-case>LSTM</span> Contrast : A Deep Neural Network for Metaphor Detection</a></strong><br><a href=/people/k/krishnkant-swarnkar/>Krishnkant Swarnkar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0914><div class="card-body p-3 small">The contrast between the contextual and general meaning of a word serves as an important clue for detecting its <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0915/>Conditional Random Fields for Metaphor Detection</a></strong><br><a href=/people/a/anna-mosolova/>Anna Mosolova</a>
|
<a href=/people/i/ivan-bondarenko/>Ivan Bondarenko</a>
|
<a href=/people/v/vadim-fomin/>Vadim Fomin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0915><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for detecting <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> in sentences which was used in Shared Task on Metaphor Detection by First Workshop on Figurative Language Processing. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0916/>Detecting Figurative Word Occurrences Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/m/malgorzata-marciniak/>Malgorzata Marciniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0916><div class="card-body p-3 small">The paper addresses detection of figurative usage of words in <a href=https://en.wikipedia.org/wiki/English_language>English text</a>. The chosen method was to use <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> fed by pretrained word embeddings. The obtained results show that simple <a href=https://en.wikipedia.org/wiki/Solution_concept>solutions</a>, based on <a href=https://en.wikipedia.org/wiki/Word_embedding>words embeddings</a> only, are comparable to complex solutions, using many sources of information which are not available for languages less-studied than <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0917/>Multi-Module Recurrent Neural Networks with Transfer Learning</a></strong><br><a href=/people/f/filip-skurniak/>Filip Skurniak</a>
|
<a href=/people/m/maria-janicka/>Maria Janicka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0917><div class="card-body p-3 small">This paper describes multiple solutions designed and tested for the problem of word-level metaphor detection. The proposed systems are all based on variants of recurrent neural network architectures. Specifically, we explore multiple sources of information : pre-trained word embeddings (Glove), a dictionary of language concreteness and a transfer learning scenario based on the states of an encoder network from neural network machine translation system. One of the architectures is based on combining all three systems : (1) Neural CRF (Conditional Random Fields), trained directly on the metaphor data set ; (2) Neural Machine Translation encoder of a transfer learning scenario ; (3) a neural network used to predict final labels, trained directly on the metaphor data set. Our results vary between test sets : Neural CRF standalone is the best one on submission data, while combined system scores the highest on a test subset randomly selected from training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0918/>Using Language Learner Data for Metaphor Detection</a></strong><br><a href=/people/e/egon-stemle/>Egon Stemle</a>
|
<a href=/people/a/alexander-onysko/>Alexander Onysko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0918><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in the shared task on metaphor detection on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing figurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018). The system combines a small assertion of trending techniques, which implement matured methods from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>ML</a> ; in particular, the system uses word embeddings from standard corpora and from <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> representing different proficiency levels of language learners in a LSTM BiRNN architecture. The <a href=https://en.wikipedia.org/wiki/System>system</a> is available under the APLv2 open-source license.</div></div></div><hr><div id=w18-10><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-10.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-10/>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1000/>Proceedings of the Workshop on Generalization in the Age of Deep Learning</a></strong><br><a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1002/>Commonsense mining as knowledge base completion? A study on the impact of <a href=https://en.wikipedia.org/wiki/Novelty>novelty</a></a></strong><br><a href=/people/s/stanislaw-jastrzebski/>Stanislaw JastrzÄ™bski</a>
|
<a href=/people/d/dzmitry-bahdanau/>Dzmitry Bahdanau</a>
|
<a href=/people/s/seyedarian-hosseini/>Seyedarian Hosseini</a>
|
<a href=/people/m/michael-noukhovitch/>Michael Noukhovitch</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1002><div class="card-body p-3 small">Commonsense knowledge bases such as <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> represent knowledge in the form of relational triples. Inspired by recent work by Li et al., we analyse if knowledge base completion models can be used to mine <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> from raw text. We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results. We critically analyse the difficulty of mining novel commonsense knowledge, and show that a simple baseline method that outperforms the previous state of the art on predicting more novel triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1003/>Deep learning evaluation using <a href=https://en.wikipedia.org/wiki/Deep_linguistic_processing>deep linguistic processing</a></a></strong><br><a href=/people/a/alexander-kuhnle/>Alexander Kuhnle</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1003><div class="card-body p-3 small">We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing &#8216;deep&#8217; linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail, as compared to a single performance value on a static and monolithic dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1004/>The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Attention Models</a></strong><br><a href=/people/n/noah-weber/>Noah Weber</a>
|
<a href=/people/l/leena-shekhar/>Leena Shekhar</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1004><div class="card-body p-3 small">Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models typically do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution (Ettinger et al., 2017), and as such, are likely to fail on examples from the tail end of the distribution (such as inputs that are noisy (Belinkov and Bisk, 2018), or of different length (Bentivogli et al., 2016)). In this paper we look at a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s ability to generalize this structure beyond the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training distribution</a> depends greatly on the chosen random seed, even when performance on the test set remains the same. This finding suggests that <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to capture generalizable structure is highly sensitive, and more so, this sensitivity may not be apparent when evaluating the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on standard test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1005/>Extrapolation in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/p/pasquale-minervini/>Pasquale Minervini</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1005><div class="card-body p-3 small">We argue that <a href=https://en.wikipedia.org/wiki/Extrapolation>extrapolation</a> to unseen data will often be easier for <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that capture global structures, rather than just maximise their local fit to the training data. We show that this is true for two popular <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> : the Decomposable Attention Model and <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>.</div></div></div><hr><div id=w18-11><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-11.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-11/>Proceedings of the Second Workshop on Computational Modeling of Peopleâ€™s Opinions, Personality, and Emotions in Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1100/>Proceedings of the Second Workshop on Computational Modeling of Peopleâ€™s Opinions, Personality, and Emotions in Social Media</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/c/claudia-wagner/>Claudia Wagner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1102/>Social and Emotional Correlates of Capitalization on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/sophia-chan/>Sophia Chan</a>
|
<a href=/people/a/alona-fyshe/>Alona Fyshe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1102><div class="card-body p-3 small">Social media text is replete with unusual <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization patterns</a>. We posit that capitalizing a token like THIS performs two expressive functions : it marks a person socially, and marks certain parts of an utterance as more salient than others. Focusing on gender and sentiment, we illustrate using a corpus of tweets that <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> appears in more negative than positive contexts, and is used more by females compared to males. Yet we find that both genders use <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> in a similar way when expressing sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1106/>The Social and the Neural Network : How to Make <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> about People again</a></strong><br><a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1106><div class="card-body p-3 small">Over the years, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> has increasingly focused on <a href=https://en.wikipedia.org/wiki/Problem_solving>tasks</a> that can be solved by <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a>, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, but have narrowed our focus and biased the tools demographically. However, with the increased availability of <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1107/>Observational Comparison of Geo-tagged and Randomly-drawn Tweets</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a>
|
<a href=/people/a/annabelle-carrell/>Annabelle Carrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1107><div class="card-body p-3 small">Twitter is a ubiquitous source of micro-blog social media data, providing the academic, industrial, and public sectors real-time access to actionable information. A particularly attractive property of some <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> is * geo-tagging *, where a user account has opted-in to attaching their current location to each message. Unfortunately (from a researcher&#8217;s perspective) only a fraction of Twitter accounts agree to this, and these <a href=https://en.wikipedia.org/wiki/User_(computing)>accounts</a> are likely to have systematic diffences with the general population. This work is an exploratory study of these differences across the full range of Twitter content, and complements previous studies that focus on the <a href=https://en.wikipedia.org/wiki/English_language>English-language subset</a>. Additionally, we compare methods for querying users by self-identified properties, finding that the constrained semantics of the description field provides cleaner, higher-volume results than more complex regular expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1110/>Understanding the Effect of Gender and Stance in Opinion Expression in Debates on Abortion</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1110><div class="card-body p-3 small">In this paper, we focus on understanding linguistic differences across groups with different self-identified gender and stance in expressing opinions about ABORTION. We provide a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of users&#8217; gender, stance on ABORTION as well as the debates in ABORTION drawn from debate.org. We use the gender and stance information to identify significant linguistic differences across individuals with different gender and stance. We show the importance of considering the stance information along with the gender since we observe significant linguistic differences across individuals with different stance even within the same gender group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1111/>Frustrated, Polite, or Formal : Quantifying Feelings and Tone in Email</a></strong><br><a href=/people/n/niyati-chhaya/>Niyati Chhaya</a>
|
<a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/p/projjal-chanda/>Projjal Chanda</a>
|
<a href=/people/j/jaya-singh/>Jaya Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1111><div class="card-body p-3 small">Email conversations are the primary mode of communication in enterprises. The email content expresses an individual&#8217;s needs, requirements and intentions. Affective information in the <a href=https://en.wikipedia.org/wiki/Email>email text</a> can be used to get an insight into the sender&#8217;s mood or emotion. We present a novel approach to model human frustration in text. We identify <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> that influence <a href=https://en.wikipedia.org/wiki/Frustration>human perception of frustration</a> and model it as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning task</a>. The paper provides a detailed comparison across traditional <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and word distribution-based models. We report a <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean-squared error (MSE)</a> of 0.018 against human-annotated frustration for the best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The approach establishes the importance of <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect features</a> in frustration prediction for <a href=https://en.wikipedia.org/wiki/Email>email data</a>. We further evaluate the efficacy of the proposed feature set and model in predicting other tone or affects in text, namely <a href=https://en.wikipedia.org/wiki/Formality>formality</a> and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> ; results demonstrate a comparable performance against the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1112/>Reddit : A Gold Mine for Personality Prediction<span class=acl-fixed-case>R</span>eddit: A Gold Mine for Personality Prediction</a></strong><br><a href=/people/m/matej-gjurkovic/>Matej GjurkoviÄ‡</a>
|
<a href=/people/j/jan-snajder/>Jan Å najder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1112><div class="card-body p-3 small">Automated personality prediction from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is gaining increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and social sciences communities. However, due to high labeling costs and privacy issues, the few publicly available datasets are of limited size and low <a href=https://en.wikipedia.org/wiki/Diversity_(business)>topic diversity</a>. We address this problem by introducing a large-scale dataset derived from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, a source so far overlooked for personality prediction. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is labeled with <a href=https://en.wikipedia.org/wiki/Myers&#8211;Briggs_Type_Indicator>Myers-Briggs Type Indicators (MBTI)</a> and comes with a rich set of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for more than 9k users. We carry out a preliminary feature analysis, revealing marked differences between the MBTI dimensions and <a href=https://en.wikipedia.org/wiki/Zeros_and_poles>poles</a>. Furthermore, we use the dataset to train and evaluate benchmark personality prediction models, achieving macro F1-scores between 67 % and 82 % on the individual dimensions and 82 % accuracy for exact or one-off accurate type prediction. These results are encouraging and comparable with the reliability of standardized tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1113/>Predicting Authorship and Author Traits from Keystroke Dynamics</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1113><div class="card-body p-3 small">Written text transmits a good deal of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>nonverbal information</a> related to the author&#8217;s identity and social factors, such as <a href=https://en.wikipedia.org/wiki/Ageing>age</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Personality>personality</a>. However, it is less known to what extent behavioral biometric traces transmit such <a href=https://en.wikipedia.org/wiki/Information>information</a>. We use typist data to study the predictiveness of <a href=https://en.wikipedia.org/wiki/Author>authorship</a>, and present first experiments on predicting both <a href=https://en.wikipedia.org/wiki/Ageing>age</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a> from <a href=https://en.wikipedia.org/wiki/Keystroke_dynamics>keystroke dynamics</a>. Our results show that the model based on keystroke features, while being two orders of magnitude smaller, leads to significantly higher accuracies for authorship than the text-based system. For user attribute prediction, the best approach is to combine the two, suggesting that extralinguistic factors are disclosed to a larger degree in written text, while author identity is better transmitted in typing behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1114/>Predicting Twitter User Demographics from Names Alone<span class=acl-fixed-case>T</span>witter User Demographics from Names Alone</a></strong><br><a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1114><div class="card-body p-3 small">Social media analysis frequently requires tools that can automatically infer <a href=https://en.wikipedia.org/wiki/Demography>demographics</a> to contextualize trends. These tools often require hundreds of user-authored messages for each user, which may be prohibitive to obtain when analyzing millions of users. We explore character-level neural models that learn a representation of a user&#8217;s name and <a href=https://en.wikipedia.org/wiki/User_(computing)>screen name</a> to predict gender and ethnicity, allowing for demographic inference with minimal data. We release trained models1 which may enable new <a href=https://en.wikipedia.org/wiki/Demography>demographic analyses</a> that would otherwise require enormous amounts of data collection</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1115/>Modeling Personality Traits of Filipino Twitter Users<span class=acl-fixed-case>F</span>ilipino <span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/e/edward-tighe/>Edward Tighe</a>
|
<a href=/people/c/charibeth-cheng/>Charibeth Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1115><div class="card-body p-3 small">Recent studies in the field of text-based personality recognition experiment with different languages, <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction techniques</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to create better and more accurate models ; however, little focus is placed on exploring the language use of a group of individuals defined by nationality. Individuals of the same nationality share certain practices and communicate certain ideas that can become embedded into their <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Many nationals are also not limited to speaking just one language, such as how Filipinos speak <a href=https://en.wikipedia.org/wiki/Filipino_language>Filipino</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the two national languages of the Philippines. The addition of several <a href=https://en.wikipedia.org/wiki/Languages_of_the_Philippines>regional / indigenous languages</a>, along with the commonness of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, allow for a <a href=https://en.wikipedia.org/wiki/Filipino_language>Filipino</a> to have a rich vocabulary. This presents an opportunity to create a text-based personality model based on how Filipinos speak, regardless of the language they use. To do so, data was collected from 250 Filipino Twitter users. Different combinations of <a href=https://en.wikipedia.org/wiki/Data_processing>data processing techniques</a> were experimented upon to create <a href=https://en.wikipedia.org/wiki/Personality_type>personality models</a> for each of the <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five</a>. The results for both <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> show that <a href=https://en.wikipedia.org/wiki/Conscientiousness>Conscientiousness</a> is consistently the easiest trait to model, followed by <a href=https://en.wikipedia.org/wiki/Extraversion_and_introversion>Extraversion</a>. Classification models for <a href=https://en.wikipedia.org/wiki/Agreeableness>Agreeableness</a> and <a href=https://en.wikipedia.org/wiki/Neuroticism>Neuroticism</a> had subpar performances, but performed better than those of <a href=https://en.wikipedia.org/wiki/Openness>Openness</a>. An analysis on personality trait score representation showed that classifying extreme outliers generally produce better results for all traits except for <a href=https://en.wikipedia.org/wiki/Neuroticism>Neuroticism</a> and <a href=https://en.wikipedia.org/wiki/Openness>Openness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-1116/>Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos GÃ³mez-RodrÃ­guez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1116><div class="card-body p-3 small">The usage of part-of-day nouns, such as &#8216;night&#8217;, and their time-specific greetings (&#8216;good night&#8217;), varies across languages and cultures. We show the possibilities that <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> offers for studying the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of these terms and its variability between countries. We mine a worldwide sample of multilingual tweets with temporal greetings, and study how their frequencies vary in relation with local time. The results provide insights into the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of these temporal expressions and the <a href=https://en.wikipedia.org/wiki/Sociology>cultural and sociological factors</a> influencing their usage.</div></div></div><hr><div id=w18-12><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-12.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-12/>Proceedings of the Second Workshop on Subword/Character LEvel Models</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1200/>Proceedings of the Second Workshop on Subword/Character <span class=acl-fixed-case>LE</span>vel Models</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich SchÃ¼tze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1201/>Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings<span class=acl-fixed-case>A</span>rabic Neural Machine Translation in Low-Resource Settings</a></strong><br><a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1201><div class="card-body p-3 small">Neural machine translation has achieved impressive results in the last few years, but its success has been limited to settings with large amounts of parallel data. One way to improve <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> for lower-resource settings is to initialize a word-based NMT model with pretrained word embeddings. However, rare words still suffer from lower quality word embeddings when trained with standard word-level objectives. We introduce <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> that utilize <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological resources</a>, and compare to purely <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised alternatives</a>. We work with <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphologically rich language with available linguistic resources, and perform Ar-to-En MT experiments on a small corpus of <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED subtitles</a>. We find that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> utilizing subword information consistently outperform standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on a word similarity task and as initialization of the source word embeddings in a low-resource NMT system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1202/>Entropy-Based Subword Mining with an Application to Word Embeddings</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/f/frank-f-xu/>Frank Xu</a>
|
<a href=/people/a/aston-zhang/>Aston Zhang</a>
|
<a href=/people/s/stephen-macke/>Stephen Macke</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1202><div class="card-body p-3 small">Recent literature has shown a wide variety of benefits to mapping traditional one-hot representations of words and phrases to lower-dimensional real-valued vectors known as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> by learning distinct <a href=https://en.wikipedia.org/wiki/Embedding>embedding vectors</a> for each word. Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as <a href=https://en.wikipedia.org/wiki/Prefix>prefixes</a>, <a href=https://en.wikipedia.org/wiki/Suffix>suffixes</a>, and <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>root-words</a> as well as <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound words</a>. Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures. This not only leads to poor embeddings for <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> that have <a href=https://en.wikipedia.org/wiki/Long-tail_distribution>long-tail distributions</a>, but also <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic methods</a> for handling out-of-vocabulary words. In this paper we propose SubwordMine, an entropy-based subword mining algorithm that is fast, unsupervised, and fully data-driven. We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1204/>Addressing Low-Resource Scenarios with Character-aware Embeddings</a></strong><br><a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian PadÃ³</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1204><div class="card-body p-3 small">Most modern approaches to computing word embeddings assume the availability of <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> with billions of words. In this paper, we explore a setup where only <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> with millions of words are available, and many words in any new text are out of vocabulary. This setup is both of practical interests modeling the situation for specific domains and low-resource languages and of psycholinguistic interest, since it corresponds much more closely to the actual experiences and challenges of human language learning and use. We compare standard skip-gram word embeddings with character-based embeddings on word relatedness prediction. Skip-grams excel on large corpora, while character-based embeddings do well on small corpora generally and rare and complex words specifically. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can be combined easily.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1205/>Subword-level Composition Functions for Learning Word Embeddings</a></strong><br><a href=/people/b/bofang-li/>Bofang Li</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1205><div class="card-body p-3 small">Subword-level information is crucial for capturing the meaning and morphology of words, especially for out-of-vocabulary entries. We propose CNN- and RNN-based subword-level composition functions for learning word embeddings, and systematically compare them with popular word-level and subword-level models (Skip-Gram and FastText). Additionally, we propose a hybrid training scheme in which a pure subword-level model is trained jointly with a conventional word-level embedding model based on lookup-tables. This increases the fitness of all types of subword-level word embeddings ; the word-level embeddings can be discarded after training, leaving only compact subword-level representation with much smaller data volume. We evaluate these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> on a set of intrinsic and extrinsic tasks, showing that subword-level models have advantage on tasks related to <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and datasets with high OOV rate, and can be combined with other types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1206/>Discovering Phonesthemes with Sparse Regularization</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/g/gina-anne-levow/>Gina-Anne Levow</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1206><div class="card-body p-3 small">We introduce a simple method for extracting non-arbitrary form-meaning representations from a collection of semantic vectors. We treat the problem as one of <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained to predict word vectors from subword features. We apply this model to the problem of automatically discovering phonesthemes, which are submorphemic sound clusters that appear in words with similar meaning. Many of our model-predicted phonesthemes overlap with those proposed in the linguistics literature, and we validate our approach with human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/291466308 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-1209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-1209/>Incorporating Subword Information into Matrix Factorization Word Embeddings</a></strong><br><a href=/people/a/alexandre-salle/>Alexandre Salle</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1209><div class="card-body p-3 small">The positive effect of adding subword information to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> has been demonstrated for <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a>. In this paper we investigate whether similar benefits can also be derived from incorporating <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> into counting models. We evaluate the impact of different types of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1210/>A Multi-Context Character Prediction Model for a Brain-Computer Interface</a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/shaobin-xu/>Shaobin Xu</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1210><div class="card-body p-3 small">Brain-computer interfaces and other augmentative and alternative communication devices introduce language-modeing challenges distinct from other character-entry methods. In particular, the acquired signal of the EEG (electroencephalogram) signal is noisier, which, in turn, makes the user intent harder to decipher. In order to adapt to this condition, we propose to maintain ambiguous history for every time step, and to employ, apart from the character language model, word information to produce a more robust prediction system. We present preliminary results that compare this proposed Online-Context Language Model (OCLM) to current <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that are used in this type of setting. Evaluation on both perplexity and predictive accuracy demonstrates promising results when dealing with ambiguous histories in order to provide to the front end a distribution of the next character the user might type.</div></div></div><hr><div id=w18-13><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-13.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-13/>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1300/>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</a></strong><br><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1301/>Using Hedge Detection to Improve Committed Belief Tagging</a></strong><br><a href=/people/m/morgan-ulinski/>Morgan Ulinski</a>
|
<a href=/people/s/seth-benjamin/>Seth Benjamin</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1301><div class="card-body p-3 small">We describe a novel method for identifying hedge terms using a set of manually constructed rules. We present experiments adding hedge features to a committed belief system to improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. We compare performance of this system (a) without <a href=https://en.wikipedia.org/wiki/Hedge_(finance)>hedging features</a>, (b) with dictionary-based features, and (c) with rule-based features. We find that using hedge features improves performance of the committed belief system, particularly in identifying instances of non-committed belief and reported belief.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1303/>Detecting Sarcasm is Extremely Easy ;-)</a></strong><br><a href=/people/n/natalie-parde/>Natalie Parde</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1303><div class="card-body p-3 small">Detecting sarcasm in text is a particularly challenging problem in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>, and its solution may vary across different types of text. We analyze the performance of a domain-general sarcasm detection system on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from two very different domains : <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon product reviews</a>. We categorize the errors that we identify with each, and make recommendations for addressing these issues in NLP systems in the future.</div></div></div><hr><div id=w18-14><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-14.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-14/>Proceedings of the First International Workshop on Spatial Language Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1400/>Proceedings of the First International Workshop on Spatial Language Understanding</a></strong><br><a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1406/>Points, Paths, and Playscapes : Large-scale Spatial Language Understanding Tasks Set in the Real World</a></strong><br><a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/t/tania-bedrax-weiss/>Tania Bedrax-Weiss</a>
|
<a href=/people/d/daphne-luong/>Daphne Luong</a>
|
<a href=/people/s/srini-narayanan/>Srini Narayanan</a>
|
<a href=/people/b/bo-pang/>Bo Pang</a>
|
<a href=/people/f/fernando-pereira/>Fernando Pereira</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a>
|
<a href=/people/m/michael-tseng/>Michael Tseng</a>
|
<a href=/people/y/yuan-zhang/>Yuan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1406><div class="card-body p-3 small">Spatial language understanding is important for practical applications and as a building block for better abstract language understanding. Much progress has been made through work on understanding spatial relations and values in <a href=https://en.wikipedia.org/wiki/Image>images</a> and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> as well as on giving and following navigation instructions in restricted domains. We argue that the next big advances in spatial language understanding can be best supported by creating large-scale datasets that focus on points and paths based in the real world, and then extending these to create online, persistent playscapes that mix human and bot players, where the bot players must learn, evolve, and survive according to their depth of understanding of scenes, navigation, and interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1408/>The Case for Systematically Derived Spatial Language Usage</a></strong><br><a href=/people/b/bonnie-dorr/>Bonnie Dorr</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1408><div class="card-body p-3 small">This position paper argues that, while prior work in spatial language understanding for tasks such as <a href=https://en.wikipedia.org/wiki/Robot_navigation>robot navigation</a> focuses on mapping <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> into deep conceptual or non-linguistic representations, it is possible to systematically derive regular patterns of spatial language usage from existing lexical-semantic resources. Furthermore, even with access to such resources, effective solutions to many application areas such as <a href=https://en.wikipedia.org/wiki/Robot_navigation>robot navigation</a> and <a href=https://en.wikipedia.org/wiki/Narrative>narrative generation</a> also require additional knowledge at the syntax-semantics interface to cover the wide range of spatial expressions observed and available to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language speakers</a>. We ground our insights in, and present our extensions to, an existing lexico-semantic resource, covering 500 semantic classes of verbs, of which 219 fall within a spatial subset. We demonstrate that these extensions enable systematic derivation of regular patterns of spatial language without requiring manual annotation.</div></div></div><hr><div id=w18-15><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-15.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-15/>Proceedings of the First Workshop on Storytelling</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1500/>Proceedings of the First Workshop on Storytelling</a></strong><br><a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao â€˜Kennethâ€™ Huang</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/i/ishan-misra/>Ishan Misra</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1502/>Linguistic Features of Helpfulness in Automated Support for Creative Writing</a></strong><br><a href=/people/m/melissa-roemmele/>Melissa Roemmele</a>
|
<a href=/people/a/andrew-gordon/>Andrew Gordon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1502><div class="card-body p-3 small">We examine an emerging NLP application that supports <a href=https://en.wikipedia.org/wiki/Creative_writing>creative writing</a> by automatically suggesting continuing sentences in a story. The <a href=https://en.wikipedia.org/wiki/Application_software>application</a> tracks users&#8217; modifications to generated sentences, which can be used to quantify their helpfulness in advancing the story. We explore the task of predicting helpfulness based on automatically detected linguistic features of the suggestions. We illustrate this analysis on a set of user interactions with the <a href=https://en.wikipedia.org/wiki/Application_software>application</a> using an initial selection of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> relevant to <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1505/>Towards Controllable Story Generation</a></strong><br><a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1505><div class="card-body p-3 small">We present a general <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> of analyzing existing story corpora to generate controllable and creative new stories. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>storyline</a>. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>storylines</a>. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.</div></div></div><hr><div id=w18-16><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-16.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-16/>Proceedings of the Second Workshop on Stylistic Variation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1600/>Proceedings of the Second Workshop on Stylistic Variation</a></strong><br><a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/l/lucie-flekova/>Lucie Flekova</a>
|
<a href=/people/m/moshe-koppel/>Moshe Koppel</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1601/>Stylistic variation over 200 years of court proceedings according to gender and social class</a></strong><br><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1601><div class="card-body p-3 small">We present an approach to detect stylistic variation across social variables (here : gender and social class), considering also diachronic change in language use. For detection of stylistic variation, we use <a href=https://en.wikipedia.org/wiki/Relative_entropy>relative entropy</a>, measuring the difference between <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distributions</a> at different linguistic levels (here : <a href=https://en.wikipedia.org/wiki/Lexis_(linguistics)>lexis</a> and grammar). In addition, by <a href=https://en.wikipedia.org/wiki/Relative_entropy>relative entropy</a>, we can determine which linguistic units are related to stylistic variation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1602/>Stylistic Variation in Social Media Part-of-Speech Tagging</a></strong><br><a href=/people/m/murali-raghu-babu-balusu/>Murali Raghu Babu Balusu</a>
|
<a href=/people/t/taha-merghani/>Taha Merghani</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1602><div class="card-body p-3 small">Social media features substantial stylistic variation, raising new challenges for syntactic analysis of online writing. However, this variation is often aligned with author attributes such as age, gender, and <a href=https://en.wikipedia.org/wiki/Geography>geography</a>, as well as more readily-available social network metadata. In this paper, we report new evidence on the link between language and social networks in the task of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. We find that tagger error rates are correlated with <a href=https://en.wikipedia.org/wiki/Flow_network>network structure</a>, with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some parts of the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a>, and lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> elsewhere. As a result, tagger accuracy depends on training from a balanced sample of the network, rather than training on texts from a narrow subcommunity. We also describe our attempts to add robustness to stylistic variation, by building a mixture-of-experts model in which each expert is associated with a region of the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. While prior work found that similar approaches yield performance improvements in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, we were unable to obtain performance improvements in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, despite strong evidence for the link between part-of-speech error rates and social network structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1603/>Detecting Syntactic Features of Translated Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/h/hai-hu/>Hai Hu</a>
|
<a href=/people/w/wen-li/>Wen Li</a>
|
<a href=/people/s/sandra-kubler/>Sandra KÃ¼bler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1603><div class="card-body p-3 small">We present a machine learning approach to distinguish texts translated to Chinese (by humans) from texts originally written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, with a focus on a wide range of syntactic features. Using Support Vector Machines (SVMs) as classifier on a genre-balanced corpus in translation studies of Chinese, we find that constituent parse trees and dependency triples as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> without lexical information perform very well on the task, with an F-measure above 90 %, close to the results of lexical n-gram features, without the risk of learning topic information rather than translation features. Thus, we claim syntactic features alone can accurately distinguish <a href=https://en.wikipedia.org/wiki/Translation>translated</a> from original Chinese. Translated Chinese exhibits an increased use of determiners, subject position pronouns, NP + as NP modifiers, multiple NPs or VPs conjoined by, among other structures. We also interpret the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic features</a> with reference to previous translation studies in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, particularly the usage of <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1604/>Evaluating Creative Language Generation : The Case of Rap Lyric Ghostwriting</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1604><div class="card-body p-3 small">Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>, <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>style</a>, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluations methods for one such <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation methodology</a> that addresses several complementary aspects of this task, and illustrate how such <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> can be used to meaning fully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1605/>Cross-corpus Native Language Identification via Statistical Embedding</a></strong><br><a href=/people/f/francisco-rangel/>Francisco Rangel</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/a/alexandra-l-uitdenbogerd/>Alexandra Uitdenbogerd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1605><div class="card-body p-3 small">In this paper, we approach the task of native language identification in a realistic cross-corpus scenario where a model is trained with available data and has to predict the native language from data of a different corpus. The motivation behind this study is to investigate native language identification in the Australian academic scenario where a majority of students come from <a href=https://en.wikipedia.org/wiki/China>China</a>, <a href=https://en.wikipedia.org/wiki/Indonesia>Indonesia</a>, and <a href=https://en.wikipedia.org/wiki/Arab_world>Arabic-speaking nations</a>. We have proposed a statistical embedding representation reporting a significant improvement over common single-layer approaches of the state of the art, identifying <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and Indonesian in a cross-corpus scenario. The proposed approach was shown to be competitive even when the data is scarce and imbalanced.</div></div></div><hr><div id=w18-17><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-17.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-17/>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1700/>Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs-12)</a></strong><br><a href=/people/g/goran-glavas/>Goran GlavaÅ¡</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1703/>Multi-hop Inference for Sentence-level TextGraphs : How Challenging is Meaningfully Combining Information for Science Question Answering?<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs: How Challenging is Meaningfully Combining Information for Science Question Answering?</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1703><div class="card-body p-3 small">Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question. This multi-hop inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, or the tendency for long chains of facts to quickly drift off topic. This is a major barrier to current <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference models</a>, as even elementary science questions require an average of 4 to 6 facts to answer and explain. In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> built from three free-text corpora (including study guides and Simple Wikipedia). We demonstrate <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> tends to be high and aggregation quality low, at between 0.04 and 3, and highlight scenarios that maximize the likelihood of meaningfully combining information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1704/>Multi-Sentence Compression with Word Vertex-Labeled Graphs and Integer Linear Programming</a></strong><br><a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/s/stephane-huet/>StÃ©phane Huet</a>
|
<a href=/people/t/thiago-gouveia-da-silva/>Thiago Gouveia da Silva</a>
|
<a href=/people/a/andrea-carneiro-linhares/>AndrÃ©a Carneiro Linhares</a>
|
<a href=/people/j/juan-manuel-torres-moreno/>Juan-Manuel Torres-Moreno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1704><div class="card-body p-3 small">Multi-Sentence Compression (MSC) aims to generate a short sentence with key information from a cluster of closely related sentences. MSC enables <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and question-answering systems to generate outputs combining fully formed sentences from one or several documents. This paper describes a new Integer Linear Programming method for MSC using a vertex-labeled graph to select different keywords, and novel 3-gram scores to generate more informative sentences while maintaining their grammaticality. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is of good quality and outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for evaluations led on <a href=https://en.wikipedia.org/wiki/News_media>news dataset</a>. We led both automatic and manual evaluations to determine the informativeness and the grammaticality of compressions for each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Additional tests, which take advantage of the fact that the length of compressions can be modulated, still improve ROUGE scores with shorter output sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1705/>Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs</a></strong><br><a href=/people/k/khiem-pham/>Khiem Pham</a>
|
<a href=/people/g/guangliang-chen/>Guangliang Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1705><div class="card-body p-3 small">Spectral clustering has received a lot of attention due to its ability to separate nonconvex, non-intersecting manifolds, but its high <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> has significantly limited its applicability. Motivated by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach in which we first use the selected landmark set and the given data to form a bipartite graph and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering. We show that our proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks, thus capable of handling large data. Finally, we demonstrate the excellent performance of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> by comparing with the state-of-the-art scalable algorithms on several benchmark data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1706/>Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings</a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/a/amol-agrawal/>Amol Agrawal</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/a/anirudha-desai/>Anirudha Desai</a>
|
<a href=/people/v/vinayak-mathur/>Vinayak Mathur</a>
|
<a href=/people/a/alfred-hough/>Alfred Hough</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1706><div class="card-body p-3 small">Word sense induction (WSI), which addresses <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graph-based method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each polysemous word. By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expensive step of <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor search</a> that plagues other graph-based methods without sacrificing the quality of sense clusters. Experiments on three datasets show that our proposed method produces similar or better sense clusters and embeddings compared with previous state-of-the-art methods while being significantly more efficient.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>