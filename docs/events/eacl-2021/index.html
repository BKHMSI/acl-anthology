<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>European Chapter of the Association for Computational Linguistics (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>European Chapter of the Association for Computational Linguistics (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021eacl-main>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a>
<span class="badge badge-info align-middle ml-1">135&nbsp;papers</span></li><li><a class=align-middle href=#2021eacl-demos>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#2021eacl-srw>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2021eacl-tutorials>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021adaptnlp-1>Proceedings of the Second Workshop on Domain Adaptation for NLP</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2021bea-1>Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021bsnlp-1>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2021dravidianlangtech-1>Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#2021gwc-1>Proceedings of the 11th Global Wordnet Conference</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021hackashop-1>Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021hcinlp-1>Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021humeval-1>Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021lantern-1>Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021louhi-1>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021ltedi-1>Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021vardial-1>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021wanlp-1>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">23&nbsp;papers</span></li><li><a class=align-middle href=#2021wassa-1>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li></ul></div></div><div id=2021eacl-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eacl-main/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.0/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></strong><br><a href=/people/p/paola-merlo/>Paola Merlo</a>
|
<a href=/people/j/jorg-tiedemann/>Jorg Tiedemann</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.1/>Unsupervised Sentence-embeddings by Manifold Approximation and <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>Projection</a></a></strong><br><a href=/people/s/subhradeep-kayal/>Subhradeep Kayal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--1><div class="card-body p-3 small">The concept of unsupervised universal sentence encoders has gained traction recently, wherein pre-trained models generate effective task-agnostic fixed-dimensional representations for phrases, sentences and paragraphs. Such methods are of varying complexity, from simple weighted-averages of word vectors to complex language-models based on bidirectional transformers. In this work we propose a novel technique to generate sentence-embeddings in an unsupervised fashion by projecting the sentences onto a fixed-dimensional manifold with the objective of preserving local neighbourhoods in the original space. To delineate such neighbourhoods we experiment with several set-distance metrics, including the recently proposed Word Mover&#8217;s distance, while the fixed-dimensional projection is achieved by employing a scalable and efficient manifold approximation method rooted in topological data analysis. We test our approach, which we term EMAP or Embeddings by Manifold Approximation and Projection, on six publicly available text-classification datasets of varying size and complexity. Empirical results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consistently performs similar to or better than several alternative state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.3/>Disambiguatory Signals are Stronger in Word-initial Positions</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--3><div class="card-body p-3 small">Psycholinguistic studies of human word processing and <a href=https://en.wikipedia.org/wiki/Lexical_access>lexical access</a> provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjectureas in Wedel et al. (2019b), but common elsewherethat languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.5/>If you’ve got it, flaunt it : Making the most of fine-grained sentiment annotations</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--5><div class="card-body p-3 small">Fine-grained sentiment analysis attempts to extract <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment holders</a>, targets and polar expressions and resolve the relationship between them, but progress has been hampered by the difficulty of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. Targeted sentiment analysis, on the other hand, is a more narrow task, focusing on extracting <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment targets</a> and classifying their polarity. In this paper, we explore whether incorporating holder and expression information can improve target extraction and classification and perform experiments on eight English datasets. We conclude that jointly predicting target and polarity BIO labels improves target extraction, and that augmenting the input text with gold expressions generally improves targeted polarity classification. This highlights the potential importance of annotating expressions for fine-grained sentiment datasets. At the same time, our results show that performance of current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for predicting polar expressions is poor, hampering the benefit of this <a href=https://en.wikipedia.org/wiki/Information>information</a> in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.9/>Telling BERT’s Full Story : from Local Attention to Global Aggregation<span class=acl-fixed-case>BERT</span>’s Full Story: from Local Attention to Global Aggregation</a></strong><br><a href=/people/d/damian-pascual/>Damian Pascual</a>
|
<a href=/people/g/gino-brunner/>Gino Brunner</a>
|
<a href=/people/r/roger-wattenhofer/>Roger Wattenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--9><div class="card-body p-3 small">We take a deep look into the behaviour of self-attention heads in the transformer architecture. In light of recent work discouraging the use of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distributions</a> for explaining a model&#8217;s behaviour, we show that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distributions</a> can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.14/>Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models</a></strong><br><a href=/people/i/itzik-malkiel/>Itzik Malkiel</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--14><div class="card-body p-3 small">Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter- and intra-dataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9 % gain in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.16/>Dictionary-based Debiasing of Pre-trained Word Embeddings</a></strong><br><a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--16><div class="card-body p-3 small">Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.18/>Non-Autoregressive Text Generation with Pre-trained Language Models</a></strong><br><a href=/people/y/yixuan-su/>Yixuan Su</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/s/simon-baker/>Simon Baker</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--18><div class="card-body p-3 small">Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a <a href=https://en.wikipedia.org/wiki/Non-blocking_algorithm>NAG model</a> for a greatly improved performance. Additionally, we devise two mechanisms to alleviate the two common problems of vanilla NAG models : the inflexibility of prefixed output length and the conditional independence of individual token predictions. To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, sentence compression and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.21/>CD2CR : Co-reference resolution across documents and domains<span class=acl-fixed-case>CD</span>ˆ2<span class=acl-fixed-case>CR</span>: Co-reference resolution across documents and domains</a></strong><br><a href=/people/j/james-ravenscroft/>James Ravenscroft</a>
|
<a href=/people/a/amanda-clare/>Amanda Clare</a>
|
<a href=/people/a/arie-cattan/>Arie Cattan</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--21><div class="card-body p-3 small">Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text documents</a>. Current state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific work</a> and <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a> that discuss them. Identifying the same entities and corresponding concepts in both <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a> and <a href=https://en.wikipedia.org/wiki/News>news</a> can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD2CR). The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD2CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.24/>Recipes for Building an Open-Domain Chatbot</a></strong><br><a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/n/naman-goyal/>Naman Goyal</a>
|
<a href=/people/d/da-ju/>Da Ju</a>
|
<a href=/people/m/mary-williamson/>Mary Williamson</a>
|
<a href=/people/y/yinhan-liu/>Yinhan Liu</a>
|
<a href=/people/j/jing-xu/>Jing Xu</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/e/eric-michael-smith/>Eric Michael Smith</a>
|
<a href=/people/y/y-lan-boureau/>Y-Lan Boureau</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--24><div class="card-body p-3 small">Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills : providing engaging talking points, and displaying knowledge, <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> and <a href=https://en.wikipedia.org/wiki/Personality>personality</a> appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> with 90 M, 2.7B and 9.4B parameter models, and make our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.25/>Evaluating the Evaluation of Diversity in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a></a></strong><br><a href=/people/g/guy-tevet/>Guy Tevet</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--25><div class="card-body p-3 small">Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for evaluating diversity metrics. The framework measures the correlation between a proposed <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity metric</a> and a <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity parameter</a>, a single parameter that controls some aspect of <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> in generated text. For example, a diversity parameter might be a <a href=https://en.wikipedia.org/wiki/Binary_variable>binary variable</a> used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by : (a) establishing best practices for eliciting <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity judgments</a> from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> by tuning a decoding parameter mostly affect form but not meaning. Our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.<i>metrics</i>. The framework measures the correlation between a proposed diversity metric and a <i>diversity parameter</i>, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a &#8220;decoding parameter&#8221; mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.26/>Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering</a></strong><br><a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/p/patrick-ng/>Patrick Ng</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--26><div class="card-body p-3 small">Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve-and-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>unified model</a> is then trained under a multi-task learning framework. Experiments show that : (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily ; (2) the BERT model improves the accuracy across all three sub-tasks ; and (3) benefiting from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.<i>retrieve-and-rerank</i> framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.27/>Implicitly Abusive Comparisons A New Dataset and <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Analysis</a></a></strong><br><a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/m/maja-geulig/>Maja Geulig</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--27><div class="card-body p-3 small">We examine the task of detecting implicitly abusive comparisons (e.g. Your hair looks like you have been electrocuted). Implicitly abusive comparisons are abusive comparisons in which <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive words</a> (e.g. dumbass or scum) are absent. We detail the process of creating a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this task via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments that include a range of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> that help us better understand the mechanisms underlying abusive comparisons.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.29/>A Systematic Review of Reproducibility Research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--29><div class="card-body p-3 small">Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility</a> should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>,</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.31/>Semantic Oppositeness Assisted Deep Contextual Modeling for Automatic Rumor Detection in <a href=https://en.wikipedia.org/wiki/Social_network>Social Networks</a></a></strong><br><a href=/people/n/nisansa-de-silva/>Nisansa de Silva</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--31><div class="card-body p-3 small">Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> or reply structure. We show, with extensive experiments on recent data sets for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance. Further, we show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more resistant to the variances in performance introduced by <a href=https://en.wikipedia.org/wiki/Randomness>randomness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.32/>Polarized-VAE : Proximity Based Disentangled Representation Learning for Text Generation<span class=acl-fixed-case>VAE</span>: Proximity Based Disentangled Representation Learning for Text Generation</a></strong><br><a href=/people/v/vikash-balasubramanian/>Vikash Balasubramanian</a>
|
<a href=/people/i/ivan-kobyzev/>Ivan Kobyzev</a>
|
<a href=/people/h/hareesh-bahuleyan/>Hareesh Bahuleyan</a>
|
<a href=/people/i/ilya-shapiro/>Ilya Shapiro</a>
|
<a href=/people/o/olga-vechtomova/>Olga Vechtomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--32><div class="card-body p-3 small">Learning disentangled representations of realworld data is a challenging open problem. Most previous methods have focused on either supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE) by training with task-specific losses. In this work, we propose polarized-VAE, an approach that disentangles select attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to disentangle the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>syntax of sentences</a> and carry out transfer experiments. Polarized-VAE outperforms the VAE baseline and is competitive with state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.36/>FEWS : Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary<span class=acl-fixed-case>FEWS</span>: Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary</a></strong><br><a href=/people/t/terra-blevins/>Terra Blevins</a>
|
<a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--36><div class="card-body p-3 small">Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. FEWS has high sense coverage across different natural language domains and provides : (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few- and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find <a href=https://en.wikipedia.org/wiki/Human>humans</a> outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.37/>MONAH : Multi-Modal Narratives for Humans to analyze conversations<span class=acl-fixed-case>MONAH</span>: Multi-Modal Narratives for Humans to analyze conversations</a></strong><br><a href=/people/j/joshua-y-kim/>Joshua Y. Kim</a>
|
<a href=/people/k/kalina-yacef/>Kalina Yacef</a>
|
<a href=/people/g/greyson-kim/>Greyson Kim</a>
|
<a href=/people/c/chunfeng-liu/>Chunfeng Liu</a>
|
<a href=/people/r/rafael-calvo/>Rafael Calvo</a>
|
<a href=/people/s/silas-taylor/>Silas Taylor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--37><div class="card-body p-3 small">In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of video-recorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. Our feature engineering contributions are two-fold : firstly, we identify the range of multimodal features relevant to detect rapport-building ; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.38/>Does Typological Blinding Impede Cross-Lingual Sharing?</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--38><div class="card-body p-3 small">Bridging the performance gap between high- and low-resource languages has been the focus of much previous work. Typological features from <a href=https://en.wikipedia.org/wiki/Database>databases</a> such as the <a href=https://en.wikipedia.org/wiki/World_Atlas_of_Language_Structures>World Atlas of Language Structures (WALS)</a> are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typological information</a>. Our hypothesis is that a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We verify this hypothesis by blinding a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from exploiting <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> somewhat improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.44/>Improving Factual Consistency Between a Response and Persona Facts</a></strong><br><a href=/people/m/mohsen-mesgar/>Mohsen Mesgar</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--44><div class="card-body p-3 small">Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker&#8217;s persona. These models are trained with fully <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> where the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> barely captures factual consistency. We propose to fine-tune these models by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> and an efficient <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> that explicitly captures the consistency between a response and persona facts as well as semantic plausibility. Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retains the language quality of responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.45/>PolyLM : Learning about <a href=https://en.wikipedia.org/wiki/Polysemy>Polysemy</a> through <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a><span class=acl-fixed-case>P</span>oly<span class=acl-fixed-case>LM</span>: Learning about Polysemy through Language Modeling</a></strong><br><a href=/people/a/alan-ansell/>Alan Ansell</a>
|
<a href=/people/f/felipe-bravo-marquez/>Felipe Bravo-Marquez</a>
|
<a href=/people/b/bernhard-pfahringer/>Bernhard Pfahringer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--45><div class="card-body p-3 small">To avoid the meaning conflation deficiency of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, a number of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses : firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring ; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on <a href=https://en.wikipedia.org/wiki/Wavelength-division_multiplexing>WSI</a>, showing that it performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized <a href=https://en.wikipedia.org/wiki/Wavelength-division_multiplexing>WSI method</a> despite having six times fewer parameters. Code and pre-trained models are available at https://github.com/AlanAnsell/PolyLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.53.Dataset.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.53.Software.txt data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.53/>Cross-lingual Entity Alignment with Incidental Supervision</a></strong><br><a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--53><div class="card-body p-3 small">Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between <a href=https://en.wikipedia.org/wiki/Glossary_of_plant_morphology>KGs</a>. Therefore, we propose a new model, JEANS, which jointly represents multilingual KGs and <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> in a shared embedding scheme, and seeks to improve <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity alignment</a> with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted : (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a self-learning based alignment learning process to iteratively induce the correspondence of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity alignment</a> with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.54/>Query Generation for Multimodal Documents</a></strong><br><a href=/people/k/kyungho-kim/>Kyungho Kim</a>
|
<a href=/people/k/kyungjae-lee/>Kyungjae Lee</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/y/young-in-song/>Young-In Song</a>
|
<a href=/people/s/seungwook-lee/>Seungwook Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--54><div class="card-body p-3 small">This paper studies the problem of generatinglikely queries for multimodal documents withimages. Our application scenario is enablingefficient first-stage retrieval of relevant doc-uments, by attaching generated queries to <a href=https://en.wikipedia.org/wiki/Document>doc-uments</a> before indexing. We can then indexthis expanded text to efficiently narrow downto candidate matches using inverted index, sothat expensive reranking can follow. Our eval-uation results show that our proposed multi-modal representation meaningfully improvesrelevance ranking. More importantly, ourframework can achieve the state of the art inthe first stage retrieval scenarios</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.55/>End-to-End Argument Mining as Biaffine Dependency Parsing</a></strong><br><a href=/people/y/yuxiao-ye/>Yuxiao Ye</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--55><div class="card-body p-3 small">Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, performing at F1 = 73.5 % for component identification and F1 = 46.4 % for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. In a thorough analysis, we investigate the factors that contribute to the success of our model : the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.57" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.57/>CTC-based Compression for Direct Speech Translation<span class=acl-fixed-case>CTC</span>-based Compression for Direct Speech Translation</a></strong><br><a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--57><div class="card-body p-3 small">Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for phone recognition and did not test this solution for direct ST, in which a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> translates the input audio into the target language without intermediate representations. In this work, we propose the first <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> able to perform a <a href=https://en.wikipedia.org/wiki/Dynamic_compression>dynamic compression</a> of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> brings a 1.3-1.5 BLEU improvement over a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on two language pairs (English-Italian and English-German), contextually reducing the <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> by more than 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.60/>Top-down Discourse Parsing via Sequence Labelling</a></strong><br><a href=/people/f/fajri-koto/>Fajri Koto</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--60><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down approach</a> to discourse parsing that is conceptually simpler than its predecessors (Kobayashi et al., 2020 ; Zhang et al., 2020). By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent models</a> and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for <a href=https://en.wikipedia.org/wiki/Top-down_parsing>top-down parsing</a>. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.64/>Neural Data-to-Text Generation with LM-based Text Augmentation<span class=acl-fixed-case>LM</span>-based Text Augmentation</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/h/hui-su/>Hui Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--64><div class="card-body p-3 small">For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the <a href=https://en.wikipedia.org/wiki/Data>data</a> available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with <a href=https://en.wikipedia.org/wiki/Data>data samples</a>. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised sequence-to-sequence models with less than 10 % of the training set. By utilizing all annotated data, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can boost the performance of a standard sequence-to-sequence model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.65/>Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling<span class=acl-fixed-case>A</span>rabic Sequence Labeling</a></strong><br><a href=/people/m/muhammad-khalifa/>Muhammad Khalifa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/k/khaled-shaalan/>Khaled Shaalan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--65><div class="card-body p-3 small">A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> can be costly, especially for <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>multiple language varieties</a> and dialects. We propose to self-train pre-trained language models in zero- and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10 % F_1 (NER) and 2 % accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of <a href=https://en.wikipedia.org/wiki/Label_(computer_science)>labeled data</a>. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.<tex-math>_1</tex-math> (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.67.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.67/>Coordinate Constructions in English Enhanced Universal Dependencies : Analysis and Computational Modeling<span class=acl-fixed-case>E</span>nglish Enhanced <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies: Analysis and Computational Modeling</a></strong><br><a href=/people/s/stefan-grunewald/>Stefan Grünewald</a>
|
<a href=/people/p/prisca-piccirilli/>Prisca Piccirilli</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--67><div class="card-body p-3 small">In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several <a href=https://en.wikipedia.org/wiki/Observational_error>systematic errors</a> in the original data, and propose to also propagate <a href=https://en.wikipedia.org/wiki/Adjuncts>adjuncts</a>. We observe high <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.70/>Continuous Learning in Neural Machine Translation using Bilingual Dictionaries</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--70><div class="card-body p-3 small">While recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> led to significant improvements in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> is often still not able to continuously adapt to the environment. For humans, as well as for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Bilingual_dictionaries>bilingual dictionaries</a> are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges : The system needs to be able to perform <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a> as well as model the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> of source and target language. In this work, we proposed an evaluation framework to assess the ability of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to continuously learn new phrases. We integrate one-shot learning methods for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with different word representations and show that it is important to address both in order to successfully make use of <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionaries</a>. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30 % to up to 70 %. The correct lemma is even generated by more than 90 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.71/>Adv-OLM : Generating Textual Adversaries via OLM<span class=acl-fixed-case>OLM</span>: Generating Textual Adversaries via <span class=acl-fixed-case>OLM</span></a></strong><br><a href=/people/v/vijit-malik/>Vijit Malik</a>
|
<a href=/people/a/ashwani-bhat/>Ashwani Bhat</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--71><div class="card-body p-3 small">Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Analysis of these attacks on the state of the art transformers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.74/>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a></strong><br><a href=/people/g/gautier-izacard/>Gautier Izacard</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--74><div class="card-body p-3 small">Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the <a href=https://en.wikipedia.org/wiki/Natural_Questions>Natural Questions</a> and TriviaQA open benchmarks. Interestingly, we observe that the performance of this <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to efficiently aggregate and combine evidence from multiple passages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.79" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.79/>A Neural Few-Shot Text Classification Reality Check</a></strong><br><a href=/people/t/thomas-dopierre/>Thomas Dopierre</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/w/wilfried-logerais/>Wilfried Logerais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--79><div class="card-body p-3 small">Modern classification models tend to struggle when the amount of annotated data is scarce. To overcome this issue, several neural few-shot classification models have emerged, yielding significant progress over time, both in <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. In the latter, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used to rely on fixed word embeddings, before the advent of <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a>. Additionally, some <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used in <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a> are yet to be tested in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. In this paper, we compare all these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, first adapting those made in the field of <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a> to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and second providing them access to <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a>. We then test these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> equipped with the same transformer-based encoder on the intent detection task, known for having a large amount of classes. Our results reveal that while methods perform almost equally on the ARSC dataset, this is not the case for the Intent Detection task, where most recent and supposedly best competitors perform worse than older and simpler ones (while all are are given access to transformers). We also show that a simple <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> is surprisingly strong. All the new developed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> as well as the evaluation framework are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.80/>Multilingual Machine Translation : Closing the Gap between Shared and Language-specific Encoder-Decoders</a></strong><br><a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a>
|
<a href=/people/m/mikel-artetxe/>Mikel Artetxe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--80><div class="card-body p-3 small">State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, while allowing to add new languages without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoderdecoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.84/>Identifying Named Entities as they are Typed</a></strong><br><a href=/people/r/ravneet-arora/>Ravneet Arora</a>
|
<a href=/people/c/chen-tse-tsai/>Chen-Tse Tsai</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preotiuc-Pietro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--84><div class="card-body p-3 small">Identifying <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> in written text is an essential component of the text processing pipeline used in applications such as <a href=https://en.wikipedia.org/wiki/Text_editor>text editors</a> to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to <a href=https://en.wikipedia.org/wiki/System>systems</a> that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or co-referenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best <a href=https://en.wikipedia.org/wiki/System>systems</a> that are evaluated on each token after its typed, reach performance within 15 F1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this <a href=https://en.wikipedia.org/wiki/Computer_simulation>setup</a> and open up the development of other NLP tools in a similar setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.85/>SANDI : Story-and-Images Alignment<span class=acl-fixed-case>SANDI</span>: Story-and-Images Alignment</a></strong><br><a href=/people/s/sreyasi-nag-chowdhury/>Sreyasi Nag Chowdhury</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--85><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> contains a multitude of social media posts and other of stories where <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> is interspersed with <a href=https://en.wikipedia.org/wiki/Image>images</a>. In these contexts, <a href=https://en.wikipedia.org/wiki/Image>images</a> are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.87/>El Volumen Louder Por Favor : Code-switching in Task-oriented Semantic Parsing</a></strong><br><a href=/people/a/arash-einolghozati/>Arash Einolghozati</a>
|
<a href=/people/a/abhinav-arora/>Abhinav Arora</a>
|
<a href=/people/l/lorena-sainz-maza-lecanda/>Lorena Sainz-Maza Lecanda</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--87><div class="card-body p-3 small">Being able to parse code-switched (CS) utterances, such as <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish+English</a> or <a href=https://en.wikipedia.org/wiki/Hindi>Hindi+English</a>, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only <a href=https://en.wikipedia.org/wiki/English_literature>English corpus</a> alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings : fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.88" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.88/>Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs</a></strong><br><a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--88><div class="card-body p-3 small">Paraphrase generation plays an essential role in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language process (NLP)</a>, and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> to improve the robustness of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.89.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.89.Dataset.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.89" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.89/>Data Augmentation for Hypernymy Detection</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/l/lorenzo-bertolini/>Lorenzo Bertolini</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--89><div class="card-body p-3 small">The automatic detection of hypernymy relationships represents a challenging problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The successful application of state-of-the-art supervised approaches using <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as small dog-dog or small dog-animal, for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by leveraging linguistic resources such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. Using an evaluation across 3 different datasets for hypernymy detection and 2 different <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.90/>Few-shot learning through contextual data augmentation</a></strong><br><a href=/people/f/farid-arthaud/>Farid Arthaud</a>
|
<a href=/people/r/rachel-bawden/>Rachel Bawden</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--90><div class="card-body p-3 small">Machine translation (MT) models used in industries with constantly changing topics, such as <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or <a href=https://en.wikipedia.org/wiki/News_agency>news agencies</a>, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pretrained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.91/>Zero-shot Generalization in Dialog State Tracking through Generative Question Answering</a></strong><br><a href=/people/s/shuyang-li/>Shuyang Li</a>
|
<a href=/people/j/jin-cao/>Jin Cao</a>
|
<a href=/people/m/mukund-sridhar/>Mukund Sridhar</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--91><div class="card-body p-3 small">Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a> for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves joint goal accuracy in zero-shot domain adaptation settings by up to 9 % (absolute) over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the MultiWOZ 2.1 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.94/>MIDAS : A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations<span class=acl-fixed-case>MIDAS</span>: A Dialog Act Annotation Scheme for Open Domain <span class=acl-fixed-case>H</span>uman<span class=acl-fixed-case>M</span>achine Spoken Conversations</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--94><div class="card-body p-3 small">Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> in human-human conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24 K utterances). To validate our scheme, we leveraged <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning methods</a> to train a multi-label dialog act prediction model and reached an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.79.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.99/>Detecting Extraneous Content in Podcasts</a></strong><br><a href=/people/s/sravana-reddy/>Sravana Reddy</a>
|
<a href=/people/y/yongze-yu/>Yongze Yu</a>
|
<a href=/people/a/aasish-pappu/>Aasish Pappu</a>
|
<a href=/people/a/aswin-sivaraman/>Aswin Sivaraman</a>
|
<a href=/people/r/rezvaneh-rezapour/>Rezvaneh Rezapour</a>
|
<a href=/people/r/rosie-jones/>Rosie Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--99><div class="card-body p-3 small">Podcast episodes often contain material extraneous to the main content, such as <a href=https://en.wikipedia.org/wiki/Advertising>advertisements</a>, interleaved within the audio and the written descriptions. We present <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that leverage both textual and listening patterns in order to detect such content in <a href=https://en.wikipedia.org/wiki/Podcast>podcast descriptions</a> and <a href=https://en.wikipedia.org/wiki/Transcript_(law)>audio transcripts</a>. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.102/>Joint Learning of Representations for Web-tables, Entities and Types using Graph Convolutional Network</a></strong><br><a href=/people/a/aniket-pramanick/>Aniket Pramanick</a>
|
<a href=/people/i/indrajit-bhattacharya/>Indrajit Bhattacharya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--102><div class="card-body p-3 small">Existing approaches for table annotation with entities and types either capture the structure of table using graphical models, or learn embeddings of table entries without accounting for the complete syntactic structure. We propose TabGCN, that uses Graph Convolutional Networks to capture the complete structure of tables, <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and the training annotations, and jointly learns embeddings for table elements as well as the entities and types. To account for knowledge incompleteness, TabGCN&#8217;s embeddings can be used to discover new entities and types. Using experiments on 5 benchmark datasets, we show that TabGCN significantly outperforms multiple state-of-the-art baselines for table annotation, while showing promising performance on downstream table-related applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.104/>ECOL-R : Encouraging Copying in Novel Object Captioning with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a><span class=acl-fixed-case>ECOL</span>-<span class=acl-fixed-case>R</span>: Encouraging Copying in Novel Object Captioning with Reinforcement Learning</a></strong><br><a href=/people/y/yufei-wang/>Yufei Wang</a>
|
<a href=/people/i/ian-wood/>Ian Wood</a>
|
<a href=/people/s/stephen-wan/>Stephen Wan</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--104><div class="card-body p-3 small">Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.107/>Debiasing Pre-trained Contextualised Embeddings</a></strong><br><a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--107><div class="card-body p-3 small">In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.112/>Cross-lingual Visual Pre-training for Multimodal Machine Translation</a></strong><br><a href=/people/o/ozan-caglayan/>Ozan Caglayan</a>
|
<a href=/people/m/menekse-kuyu/>Menekse Kuyu</a>
|
<a href=/people/m/mustafa-sercan-amac/>Mustafa Sercan Amac</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/e/erkut-erdem/>Erkut Erdem</a>
|
<a href=/people/a/aykut-erdem/>Aykut Erdem</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--112><div class="card-body p-3 small">Pre-trained language models have been shown to improve performance in many <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a> substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.114.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.114/>An Expert Annotated Dataset for the Detection of Online Misogyny</a></strong><br><a href=/people/e/ella-guest/>Ella Guest</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/a/alexandros-mittos/>Alexandros Mittos</a>
|
<a href=/people/n/nishanth-sastry/>Nishanth Sastry</a>
|
<a href=/people/g/gareth-tyson/>Gareth Tyson</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--114><div class="card-body p-3 small">Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for <a href=https://en.wikipedia.org/wiki/Misogyny>online misogyny</a>, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 6567 labels for <a href=https://en.wikipedia.org/wiki/Reddit>Reddit posts</a> and comments. As previous research has found untrained crowdsourced annotators struggle with identifying <a href=https://en.wikipedia.org/wiki/Misogyny>misogyny</a>, we hired and trained annotators and provided them with robust annotation guidelines. We report <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline classification</a> performance on the <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification task</a>, achieving <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.93 and <a href=https://en.wikipedia.org/wiki/F-number>F1</a> of 0.43. The codebook and datasets are made freely available for future researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.115/>WikiMatrix : Mining 135 M Parallel Sentences in 1620 Language Pairs from Wikipedia<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>M</span>atrix: Mining 135<span class=acl-fixed-case>M</span> Parallel Sentences in 1620 Language Pairs from <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/s/shuo-sun/>Shuo Sun</a>
|
<a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--115><div class="card-body p-3 small">We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135 M <a href=https://en.wikipedia.org/wiki/Parallelism_(grammar)>parallel sentences</a> for 16720 different language pairs, out of which only 34 M are aligned with English. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.116/>ChEMU-Ref : A Corpus for Modeling <a href=https://en.wikipedia.org/wiki/Anaphora_resolution>Anaphora Resolution</a> in the Chemical Domain<span class=acl-fixed-case>C</span>h<span class=acl-fixed-case>EMU</span>-Ref: A Corpus for Modeling Anaphora Resolution in the Chemical Domain</a></strong><br><a href=/people/b/biaoyan-fang/>Biaoyan Fang</a>
|
<a href=/people/c/christian-druckenbrodt/>Christian Druckenbrodt</a>
|
<a href=/people/s/saber-a-akhondi/>Saber A Akhondi</a>
|
<a href=/people/j/jiayuan-he/>Jiayuan He</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--116><div class="card-body p-3 small">Chemical patents contain rich coreference and bridging links, which are the target of this research. Specially, we introduce a novel annotation scheme, based on which we create the ChEMU-Ref dataset from reaction description snippets in English-language chemical patents. We propose a neural approach to <a href=https://en.wikipedia.org/wiki/Anaphora_resolution>anaphora resolution</a>, which we show to achieve strong results, especially when jointly trained over coreference and bridging links.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.118/>Searching for Search Errors in Neural Morphological Inflection</a></strong><br><a href=/people/m/martina-forster/>Martina Forster</a>
|
<a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--118><div class="card-body p-3 small">Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>language generation</a>. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Further, <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> often finds the <a href=https://en.wikipedia.org/wiki/Maxima_and_minima>global optimum</a>. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.119/>Quantifying Appropriateness of Summarization Data for <a href=https://en.wikipedia.org/wiki/Curriculum>Curriculum Learning</a></a></strong><br><a href=/people/r/ryuji-kano/>Ryuji Kano</a>
|
<a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/t/toru-nishino/>Toru Nishino</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--119><div class="card-body p-3 small">Much research has reported the training data of summarization models are noisy ; summaries often do not reflect what is written in the source texts. We propose an effective method of curriculum learning to train <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization models</a> from such <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a>. Curriculum learning is used to train sequence-to-sequence models with <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a>. In translation tasks, previous research quantified <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> of the training data using two <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with noisy and clean corpora. Because such <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> do not exist in summarization fields, we propose a model that can quantify <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> from a single noisy corpus. We conduct experiments on three summarization models ; one pretrained model and two non-pretrained models, and verify our method improves the performance. Furthermore, we analyze how different curricula affect the performance of pretrained and non-pretrained summarization models. Our result on human evaluation also shows our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves the performance of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.124.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.124" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.124/>Civil Rephrases Of Toxic Texts With Self-Supervised Transformers</a></strong><br><a href=/people/l/leo-laugier/>Léo Laugier</a>
|
<a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/j/jeffrey-sorensen/>Jeffrey Sorensen</a>
|
<a href=/people/l/lucas-dixon/>Lucas Dixon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--124><div class="card-body p-3 small">Platforms that support online commentary, from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> to <a href=https://en.wikipedia.org/wiki/Online_newspaper>news sites</a>, are increasingly leveraging <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for <a href=https://en.wikipedia.org/wiki/Moderator_(Internet)>human moderators</a> to do, and <a href=https://en.wikipedia.org/wiki/Computational_model>computational approaches</a> are still nascent. This work focuses on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.125/>Generating Weather Comments from Meteorological Simulations</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/s/sora-tanaka/>Sora Tanaka</a>
|
<a href=/people/m/masatsugu-hangyo/>Masatsugu Hangyo</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/kotaro-funakoshi/>Kotaro Funakoshi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--125><div class="card-body p-3 small">The task of generating weather-forecast comments from meteorological simulations has the following requirements : (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather information</a>, such as sunny and rain, for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performed best against <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in terms of informativeness. We make our code and data publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Long Paper"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.127/>A phonetic model of non-native spoken word processing</a></strong><br><a href=/people/y/yevgen-matusevych/>Yevgen Matusevych</a>
|
<a href=/people/h/herman-kamper/>Herman Kamper</a>
|
<a href=/people/t/thomas-schatz/>Thomas Schatz</a>
|
<a href=/people/n/naomi-feldman/>Naomi Feldman</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--127><div class="card-body p-3 small">Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis : that some of these difficulties can arise from the non-native speakers&#8217; phonetic perception. We train a computational model of phonetic learning, which has no access to <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>, on either one or two languages. We first show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a> may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model&#8217;s lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.128/>Bootstrapping Relation Extractors using Syntactic Search by Examples</a></strong><br><a href=/people/m/matan-eyal/>Matan Eyal</a>
|
<a href=/people/a/asaf-amrami/>Asaf Amrami</a>
|
<a href=/people/h/hillel-taub-tabib/>Hillel Taub-Tabib</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--128><div class="card-body p-3 small">The advent of <a href=https://en.wikipedia.org/wiki/Neural_network>neural-networks</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> over syntactic-graphs (Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and DocRED and show that the resulting <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are competitive with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on manually annotated data and on data obtained from distant supervision. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> also outperform <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.129/>Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR<span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--129><div class="card-body p-3 small">Systems that generate natural language text from abstract meaning representations such as AMR are typically evaluated using automatic surface matching metrics that compare the generated texts to reference texts from which the input meaning representations were constructed. We show that besides well-known issues from which such <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> suffer, an additional problem arises when applying these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for AMR-to-text evaluation, since an abstract meaning representation allows for numerous surface realizations. In this work we aim to alleviate these issues by proposing _, a decomposable metric that builds on two pillars. The first is the principle of meaning preservation : it measures to what extent a given <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AMR</a> can be reconstructed from the generated sentence using SOTA AMR parsers and applying (fine-grained) AMR evaluation metrics to measure the distance between the original and the reconstructed <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AMR</a>. The second pillar builds on a principle of (grammatical) form that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since _ does not necessarily rely on gold AMRs, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> may extend to other text generation tasks.<tex-math>\\mathcal{M}\\mathcal{F}_\\beta</tex-math>, a decomposable metric that builds on two pillars. The first is the <b>principle of meaning preservation <tex-math>\\mathcal{M}</tex-math>\n </b>: it measures to what extent a given AMR can be reconstructed from the generated sentence using SOTA AMR parsers and applying (fine-grained) AMR evaluation metrics to measure the distance between the original and the reconstructed AMR. The second pillar builds on a <b>principle of (grammatical) form <tex-math>\\mathcal{F}</tex-math>\n</b> that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since <tex-math>\\mathcal{M}\\mathcal{F}_\\beta</tex-math> does not necessarily rely on gold AMRs, it may extend to other text generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.130/>The Source-Target Domain Mismatch Problem in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/j/jiajun-shen/>Jiajun Shen</a>
|
<a href=/people/p/peng-jen-chen/>Peng-Jen Chen</a>
|
<a href=/people/m/matthew-le/>Matthew Le</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a>
|
<a href=/people/m/marcaurelio-ranzato/>Marc’Aurelio Ranzato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--130><div class="card-body p-3 small">While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> on low resource languages. While this may severely affect <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, the degradation can be alleviated by combining <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> with self-training and by increasing the amount of target side monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.132/>Understanding Pre-Editing for Black-Box Neural Machine Translation</a></strong><br><a href=/people/r/rei-miyata/>Rei Miyata</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--132><div class="card-body p-3 small">Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing methods</a> for particular settings, thus far, a deep understanding of what <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> to incrementally record the minimum edits for each <a href=https://en.wikipedia.org/wiki/Translation>ST</a> and collected 6,652 instances of <a href=https://en.wikipedia.org/wiki/Pre-editing>pre-editing</a> across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives : the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following : (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.140/>WiC-TSV : An Evaluation Benchmark for Target Sense Verification of Words in Context<span class=acl-fixed-case>WiC-TSV</span>: <span class=acl-fixed-case>A</span>n Evaluation Benchmark for Target Sense Verification of Words in Context</a></strong><br><a href=/people/a/anna-breit/>Anna Breit</a>
|
<a href=/people/a/artem-revenko/>Artem Revenko</a>
|
<a href=/people/k/kiamehr-rezaee/>Kiamehr Rezaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--140><div class="card-body p-3 small">We present WiC-TSV, a new multi-domain evaluation benchmark for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> highly flexible for the evaluation of a diverse set of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We set baseline performance on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Experimental results show that even though these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.141/>Self-Supervised and Controlled Multi-Document Opinion Summarization</a></strong><br><a href=/people/h/hady-elsahar/>Hady Elsahar</a>
|
<a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/j/jos-rozen/>Jos Rozen</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--141><div class="card-body p-3 small">We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard <a href=https://en.wikipedia.org/wiki/Likelihood_function>log-likelihood loss</a> and mainstream models. We address the problem of <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> through the use of <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a>, to steer the generation towards more coherent and relevant summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.145/>Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates<span class=acl-fixed-case>B</span>ayesian Uncertainty Estimates</a></strong><br><a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/d/dmitri-puzyrev/>Dmitri Puzyrev</a>
|
<a href=/people/l/lyubov-kupriyanova/>Lyubov Kupriyanova</a>
|
<a href=/people/d/denis-belyakov/>Denis Belyakov</a>
|
<a href=/people/d/daniil-larionov/>Daniil Larionov</a>
|
<a href=/people/n/nikita-khromov/>Nikita Khromov</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/d/dmitry-v-dylov/>Dmitry V. Dylov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--145><div class="card-body p-3 small">Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Besides, we also demonstrate that to acquire instances during <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.149/>BERT Prescriptions to Avoid Unwanted Headaches : A Comparison of Transformer Architectures for Adverse Drug Event Detection<span class=acl-fixed-case>BERT</span> Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection</a></strong><br><a href=/people/b/beatrice-portelli/>Beatrice Portelli</a>
|
<a href=/people/e/edoardo-lenzi/>Edoardo Lenzi</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/g/giuseppe-serra/>Giuseppe Serra</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--149><div class="card-body p-3 small">Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, tested on two standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. SpanBERT and PubMedBERT emerged as the best models in our evaluation : this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.150/>Semantic Parsing of Disfluent Speech</a></strong><br><a href=/people/p/priyanka-sen/>Priyanka Sen</a>
|
<a href=/people/i/isabel-groves/>Isabel Groves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--150><div class="card-body p-3 small">Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> research to date focuses on <a href=https://en.wikipedia.org/wiki/Written_language>written text</a>. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> does not seamlessly handle <a href=https://en.wikipedia.org/wiki/Disfluency>disfluencies</a>. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39 % but can also outperform adding real disfluencies in the ATIS dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.156/>We Need To Talk About Random Splits</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/s/sebastian-ebert/>Sebastian Ebert</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--156><div class="card-body p-3 small">(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> to simulate real-world drift ; this is known as the covariate shift assumption. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead ; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.158/>Alignment verification to improve NMT translation towards highly inflectional languages with limited resources<span class=acl-fixed-case>NMT</span> translation towards highly inflectional languages with limited resources</a></strong><br><a href=/people/g/george-tambouratzis/>George Tambouratzis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--158><div class="card-body p-3 small">The present article discusses how to improve translation quality when using limited training data to translate towards <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a>. The starting point is a neural MT system, used to train translation models, using solely publicly available <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. An initial analysis of the translation output has shown that quality is sub-optimal, due mainly to an insufficient amount of training data. To improve translation quality, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, combined with an open source module, designed for a low-resource MT system. Experimental results of the proposed hybridized method with multiple independent test sets achieve improvements over (i) both the best individual NMT and (ii) the standard ensemble system provided in the Marian-NMT system. Improvements over Marian-NMT are in many cases statistically significant. Finally, a qualitative analysis of translation results indicates a greater robustness for the hybridized method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.159/>Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase<span class=acl-fixed-case>NLU</span> using <span class=acl-fixed-case>BERT</span>-based Interchangeable Rephrase</a></strong><br><a href=/people/a/akhila-yerukola/>Akhila Yerukola</a>
|
<a href=/people/m/mason-bretan/>Mason Bretan</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--159><div class="card-body p-3 small">We introduce a data augmentation technique based on byte pair encoding and a BERT-like self-attention model to boost performance on spoken language understanding tasks. We compare and evaluate this method with a range of augmentation techniques encompassing generative models such as VAEs and performance-boosting techniques such as synonym replacement and <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>. We show our method performs strongly on domain and intent classification tasks for a voice assistant and in a user-study focused on utterance naturalness and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.160.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.160/>How to Evaluate a Summarizer : Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation</a></strong><br><a href=/people/j/julius-steen/>Julius Steen</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--160><div class="card-body p-3 small">Manual evaluation is essential to judge progress on <a href=https://en.wikipedia.org/wiki/Automatic_text_summarization>automatic text summarization</a>. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries&#8217; linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current <a href=https://en.wikipedia.org/wiki/Statistical_inference>statistical analysis methods</a> can inflate <a href=https://en.wikipedia.org/wiki/Type_I_and_type_II_errors>type I error rates</a> up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Long Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.162/>Error Analysis and the Role of <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphology</a></a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--162><div class="card-body p-3 small">We evaluate two common conjectures in <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> of NLP models : (i) <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> is predictive of errors ; and (ii) the importance of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> increases with the morphological complexity of a language. We show across four different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error prediction</a> across tasks ; however, this effect is less pronounced with morphologically complex languages. We speculate this is because <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> is more discriminative in morphologically simple languages. Across all four tasks, <a href=https://en.wikipedia.org/wiki/Grammatical_case>case</a> and gender are the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> most predictive of error.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.170/>Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction</a></strong><br><a href=/people/j/junfeng-jiang/>Junfeng Jiang</a>
|
<a href=/people/a/an-wang/>An Wang</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--170><div class="card-body p-3 small">Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on <a href=https://en.wikipedia.org/wiki/Syntax>syntactics</a>. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms all of the existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our code is available at https://github.com/wcwowwwww/towe-eacl.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.174/>Acquiring a Formality-Informed Lexical Resource for Style Analysis</a></strong><br><a href=/people/e/elisabeth-eder/>Elisabeth Eder</a>
|
<a href=/people/u/ulrike-krieg-holz/>Ulrike Krieg-Holz</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--174><div class="card-body p-3 small">To track different levels of formality in written discourse, we introduce a novel type of lexicon for the <a href=https://en.wikipedia.org/wiki/German_language>German language</a>, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.175.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.175.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.175/>Probing into the Root : A Dataset for Reason Extraction of Structural Events from Financial Documents</a></strong><br><a href=/people/p/pei-chen/>Pei Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/t/taifeng-wang/>Taifeng Wang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--175><div class="card-body p-3 small">This paper proposes a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text, but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 <a href=https://en.wikipedia.org/wiki/Financial_crisis>financial events</a> and 11,006 reason spans. We also provide the performance of existing canonical methods in <a href=https://en.wikipedia.org/wiki/Event_(computing)>event extraction</a> and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and human performance, and existing methods are far from resolving this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.176/>Language Modelling as a Multi-Task Problem</a></strong><br><a href=/people/l/lucas-weber/>Lucas Weber</a>
|
<a href=/people/j/jaap-jumelet/>Jaap Jumelet</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--176><div class="card-body p-3 small">In this paper, we propose to study <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> as a multi-task problem, bringing together three strands of research : <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. Based on hypotheses derived from <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theory</a>, we investigate whether <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> adhere to learning principles of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> during training. To showcase the idea, we analyse the generalisation behaviour of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. We argue that this insight is valuable for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, linguistics and interpretability research and can lead to exciting new findings in all three domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.177/>ChainCQG : Flow-Aware Conversational Question Generation<span class=acl-fixed-case>C</span>hain<span class=acl-fixed-case>CQG</span>: Flow-Aware Conversational Question Generation</a></strong><br><a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/m/mostafa-mirshekari/>Mostafa Mirshekari</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/a/aaron-sisto/>Aaron Sisto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--177><div class="card-body p-3 small">Conversational systems enable numerous valuable <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a> is an important component underlying many of these. However, conversational question-answering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48 % BLEU-1 improvement). Additionally, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate different types of questions, with improved <a href=https://en.wikipedia.org/wiki/Fluid_dynamics>fluidity</a> and coreference alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.178/>The Interplay of Task Success and Dialogue Quality : An in-depth Evaluation in Task-Oriented Visual Dialogues</a></strong><br><a href=/people/a/alberto-testoni/>Alberto Testoni</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--178><div class="card-body p-3 small">When training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on referential dialogue guessing games, the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is usually chosen based on its task success. We show that in the popular <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approach</a>, this choice prevents the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and task-agnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> if they learn to ground, encode, and decode also words that do not occur frequently in the training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.179.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.179/>Are you kidding me? : Detecting Unpalatable Questions on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/s/sunyam-bagga/>Sunyam Bagga</a>
|
<a href=/people/a/andrew-piper/>Andrew Piper</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--179><div class="card-body p-3 small">Abusive language in <a href=https://en.wikipedia.org/wiki/Online_discourse>online discourse</a> negatively affects a large number of <a href=https://en.wikipedia.org/wiki/Social_media>social media users</a>. Many computational methods have been proposed to address this issue of <a href=https://en.wikipedia.org/wiki/Online_abuse>online abuse</a>. The existing work, however, tends to focus on detecting the more explicit forms of abuse leaving the subtler forms of abuse largely untouched. Our work addresses this gap by making three core contributions. First, inspired by the theory of impoliteness, we propose a novel task of detecting a subtler form of abuse, namely unpalatable questions. Second, we publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, we implement a wide array of <a href=https://en.wikipedia.org/wiki/Machine_learning>learning models</a> and also investigate the benefits of incorporating <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a> into <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. Our results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive. We hope that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.180/>Neural-Driven Search-Based Paraphrase Generation</a></strong><br><a href=/people/b/betty-fabre/>Betty Fabre</a>
|
<a href=/people/t/tanguy-urvoy/>Tanguy Urvoy</a>
|
<a href=/people/j/jonathan-chevelu/>Jonathan Chevelu</a>
|
<a href=/people/d/damien-lolive/>Damien Lolive</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--180><div class="card-body p-3 small">We study a search-based paraphrase generation scheme where candidate paraphrases are generated by iterated transformations from the original sentence and evaluated in terms of syntax quality, <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a>, and lexical distance. The <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> is derived from BERT, and the lexical quality is based on GPT2 perplexity. To solve this multi-objective search problem, we propose two algorithms : Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS). We provide an extensive set of experiments on 5 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with a rigorous reproduction and validation for several state-of-the-art paraphrase generation algorithms. These experiments show that, although being non explicitly supervised, our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> perform well against these <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.185/>FAST : Financial News and Tweet Based Time Aware Network for Stock Trading<span class=acl-fixed-case>FAST</span>: Financial News and Tweet Based Time Aware Network for Stock Trading</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/a/arnav-wadhwa/>Arnav Wadhwa</a>
|
<a href=/people/s/shivam-agarwal/>Shivam Agarwal</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--185><div class="card-body p-3 small">Designing profitable trading strategies is complex as <a href=https://en.wikipedia.org/wiki/Volatility_(finance)>stock movements</a> are highly stochastic ; the market is influenced by large volumes of <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards <a href=https://en.wikipedia.org/wiki/Profit_(economics)>profit-making</a>. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8 % in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks : English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> as a tool for daily stock trading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.186/>Building Representative Corpora from Illiterate Communities : A Reviewof Challenges and Mitigation Strategies for Developing Countries</a></strong><br><a href=/people/s/stephanie-hirmer/>Stephanie Hirmer</a>
|
<a href=/people/a/alycia-leonard/>Alycia Leonard</a>
|
<a href=/people/j/josephine-tumwesige/>Josephine Tumwesige</a>
|
<a href=/people/c/costanza-conforti/>Costanza Conforti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--186><div class="card-body p-3 small">Most well-established data collection methods currently adopted in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora : we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.195/>Content-based Models of Quotation</a></strong><br><a href=/people/a/ansel-maclaughlin/>Ansel MacLaughlin</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--195><div class="card-body p-3 small">We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> through evaluations on five <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, a RoBERTA sequential sentence tagger, achieving an average rho of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.200/>Lexical Normalization for Code-switched Data and its Effect on POS Tagging<span class=acl-fixed-case>POS</span> Tagging</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--200><div class="card-body p-3 small">Lexical normalization, the translation of non-canonical data to standard language, has shown to improve the performance of many natural language processing tasks on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper, we propose three <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization models</a> specifically designed to handle code-switched data which we evaluate for two language pairs : Indonesian-English and Turkish-German. For the latter, we introduce novel <a href=https://en.wikipedia.org/wiki/Data_normalization>normalization layers</a> and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of <a href=https://en.wikipedia.org/wiki/Data_normalization>normalization</a> on POS tagging. Results show that our CS-tailored normalization models significantly outperform monolingual ones, and lead to 5.4 % relative performance increase for POS tagging as compared to unnormalized input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.201/>Structural Encoding and Pre-training Matter : Adapting BERT for Table-Based Fact Verification<span class=acl-fixed-case>BERT</span> for Table-Based Fact Verification</a></strong><br><a href=/people/r/rui-dong/>Rui Dong</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--201><div class="card-body p-3 small">Growing concern with <a href=https://en.wikipedia.org/wiki/Misinformation>online misinformation</a> has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> (Herzig et al., 2020), we find that modeling table structure improves a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> pre-trained on <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a> with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing <a href=https://en.wikipedia.org/wiki/Statistical_classification>statement classification accuracy</a> from 72.2 % to 73.9 % even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering task</a> increases <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> to 76 %. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-to-text generation dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.204/>Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks</a></strong><br><a href=/people/j/jimin-sun/>Jimin Sun</a>
|
<a href=/people/h/hwijeen-ahn/>Hwijeen Ahn</a>
|
<a href=/people/c/chan-young-park/>Chan Young Park</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--204><div class="card-body p-3 small">Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmatically-motivated tasks, such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of <a href=https://en.wikipedia.org/wiki/Pragmatics>language pragmatics</a> : <a href=https://en.wikipedia.org/wiki/Context_(language_use)>language context-level</a>, <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_anthropology>linguistic anthropology</a>. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.205/>PHASE : Learning Emotional Phase-aware Representations for Suicide Ideation Detection on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>PHASE</span>: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/h/harshit-joshi/>Harshit Joshi</a>
|
<a href=/people/l/lucie-flek/>Lucie Flek</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--205><div class="card-body p-3 small">Recent psychological studies indicate that individuals exhibiting <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a> increasingly turn to <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> rather than <a href=https://en.wikipedia.org/wiki/Mental_health_professional>mental health practitioners</a>. Contextualizing the build-up of such ideation is critical for the identification of users at risk. In this work, we focus on identifying <a href=https://en.wikipedia.org/wiki/Suicide>suicidal intent</a> in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> by augmenting linguistic models with <a href=https://en.wikipedia.org/wiki/Emotion>emotional phases</a> modeled from users&#8217; historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user&#8217;s historical emotional spectrum on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users&#8217; historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.206/>Exploiting Definitions for Frame Identification</a></strong><br><a href=/people/t/tianyu-jiang/>Tianyu Jiang</a>
|
<a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--206><div class="card-body p-3 small">Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which <a href=https://en.wikipedia.org/wiki/Film_frame>frame</a> best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>. Our <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frame identification model</a> assesses the suitability of a <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frame</a> for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on three data sets and show that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> consistently achieves better performance than previous <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.207/>ADePT : Auto-encoder based Differentially Private Text Transformation<span class=acl-fixed-case>AD</span>e<span class=acl-fixed-case>PT</span>: Auto-encoder based Differentially Private Text Transformation</a></strong><br><a href=/people/s/satyapriya-krishna/>Satyapriya Krishna</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a>
|
<a href=/people/c/christophe-dupuy/>Christophe Dupuy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--207><div class="card-body p-3 small">Privacy is an important concern when building <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> on data containing personal information. Differential privacy offers a strong definition of <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> and can be used to solve several <a href=https://en.wikipedia.org/wiki/Privacy>privacy concerns</a>. Multiple solutions have been proposed for the differentially-private transformation of datasets containing <a href=https://en.wikipedia.org/wiki/Information_sensitivity>sensitive information</a>. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> added in the process. This paper addresses this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>&#8217;s theoretical privacy guarantee and assess its privacy leakage under Membership Inference Attacks (MIA) on models trained with transformed data. Our results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.210/>Evaluating Neural Model Robustness for Machine Comprehension</a></strong><br><a href=/people/w/winston-wu/>Winston Wu</a>
|
<a href=/people/d/dustin-arendt/>Dustin Arendt</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--210><div class="card-body p-3 small">We evaluate neural model robustness to adversarial attacks using different types of linguistic unit perturbations character and word, and propose a new method for strategic sentence-level perturbations. We experiment with different amounts of perturbations to examine model confidence and misclassification rate, and contrast model performance with different embeddings BERT and ELMo on two benchmark datasets SQuAD and TriviaQA. We demonstrate how to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance during an <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attack</a> by using <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensembles</a>. Finally, we analyze factors that effect model behavior under adversarial attack, and develop a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to predict errors during attacks. Our novel findings reveal that (a) unlike BERT, models that use ELMo embeddings are more susceptible to adversarial attacks, (b) unlike word and paraphrase, character perturbations affect the model the most but are most easily compensated for by adversarial training, (c) word perturbations lead to more high-confidence misclassifications compared to sentence- and character-level perturbations, (d) the type of question and model answer length (the longer the answer the more likely it is to be incorrect) is the most predictive of model errors in adversarial setting, and (e) conclusions about model behavior are dataset-specific.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Long Paper"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.211/>Hidden Biases in Unreliable News Detection Datasets</a></strong><br><a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/h/heba-elfardy/>Heba Elfardy</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/t/thomas-butler/>Thomas Butler</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--211><div class="card-body p-3 small">Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking mechanism</a> or retrieving any supporting evidence. In this work, we take a closer look at these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that <a href=https://en.wikipedia.org/wiki/Selection_bias>selection bias</a> during data collection leads to undesired artifacts in the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong <a href=https://en.wikipedia.org/wiki/Confounding>confounding factor</a> that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can exploit. In the presence of this <a href=https://en.wikipedia.org/wiki/Confounding>confounding factor</a>, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (10 %) in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for all <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> tested in a clean split with no train / test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for the unreliable news detection task. We suggest future dataset creation include a simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as a difficulty / bias probe and future model development use a clean non-overlapping site and date split.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.213/>Unsupervised Extractive Summarization using Pointwise Mutual Information</a></strong><br><a href=/people/v/vishakh-padmakumar/>Vishakh Padmakumar</a>
|
<a href=/people/h/he-he/>He He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--213><div class="card-body p-3 small">Unsupervised approaches to extractive summarization usually rely on a notion of sentence importance defined by the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between a sentence and the document. We propose new metrics of relevance and redundancy using pointwise mutual information (PMI) between sentences, which can be easily computed by a pre-trained language model. Intuitively, a relevant sentence allows readers to infer the document content (high PMI with the document), and a redundant sentence can be inferred from the summary (high PMI with the summary). We then develop a greedy sentence selection algorithm to maximize relevance and minimize redundancy of extracted sentences. We show that our method outperforms similarity-based methods on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in a range of domains including <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Medical_journal>medical journal articles</a>, and <a href=https://en.wikipedia.org/wiki/Anecdote>personal anecdotes</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--215 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.215" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.215/>Deep Subjecthood : Higher-Order Grammatical Features in Multilingual BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/i/isabel-papadimitriou/>Isabel Papadimitriou</a>
|
<a href=/people/e/ethan-a-chi/>Ethan A. Chi</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--215><div class="card-body p-3 small">We investigate how Multilingual BERT (mBERT) encodes <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a subject) is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the <a href=https://en.wikipedia.org/wiki/Morphosyntactic_alignment>morphosyntactic alignment</a> of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as <a href=https://en.wikipedia.org/wiki/Passive_voice>passive voice</a>, <a href=https://en.wikipedia.org/wiki/Animacy>animacy</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_case>case</a> strongly correlate with classification decisions, suggesting that mBERT does not encode <a href=https://en.wikipedia.org/wiki/Subject_(grammar)>subjecthood</a> purely syntactically, but that <a href=https://en.wikipedia.org/wiki/Subject_(grammar)>subjecthood embedding</a> is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.217/>DOCENT : Learning Self-Supervised Entity Representations from Large Document Collections<span class=acl-fixed-case>DOCENT</span>: Learning Self-Supervised Entity Representations from Large Document Collections</a></strong><br><a href=/people/y/yury-zemlyanskiy/>Yury Zemlyanskiy</a>
|
<a href=/people/s/sudeep-gandhe/>Sudeep Gandhe</a>
|
<a href=/people/r/ruining-he/>Ruining He</a>
|
<a href=/people/b/bhargav-kanagal/>Bhargav Kanagal</a>
|
<a href=/people/a/anirudh-ravula/>Anirudh Ravula</a>
|
<a href=/people/j/juraj-gottweis/>Juraj Gottweis</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a>
|
<a href=/people/i/ilya-eckstein/>Ilya Eckstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--217><div class="card-body p-3 small">This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> match or outperform competitive baselines, sometimes with little or no fine-tuning, and are also able to scale to very large corpora. Finally, we make our <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-trained models</a> publicly available. This includes Reviews2Movielens, mapping the ~1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.218" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.218/>Scientific Discourse Tagging for Evidence Extraction</a></strong><br><a href=/people/x/xiangci-li/>Xiangci Li</a>
|
<a href=/people/g/gully-burns/>Gully Burns</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--218><div class="card-body p-3 small">Evidence plays a crucial role in any biomedical research narrative, providing justification for some claims and refutation for others. We seek to build models of scientific argument using <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction methods</a> from <a href=https://en.wikipedia.org/wiki/Academic_publishing>full-text papers</a>. We present the capability of automatically extracting text fragments from primary research papers that describe the evidence presented in that paper&#8217;s figures, which arguably provides the raw material of any scientific argument made within the paper. We apply richly contextualized deep representation learning pre-trained on biomedical domain corpus to the analysis of scientific discourse structures and the extraction of evidence fragments (i.e., the text in the results section describing data presented in a specified subfigure) from a set of biomedical experimental research articles. We first demonstrate our state-of-the-art scientific discourse tagger on two scientific discourse tagging datasets and its transferability to new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We then show the benefit of leveraging scientific discourse tags for downstream tasks such as claim-extraction and evidence fragment detection. Our work demonstrates the potential of using evidence fragments derived from figure spans for improving the quality of scientific claims by cataloging, indexing and reusing evidence fragments as independent documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.220/>StructSum : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> via Structured Representations<span class=acl-fixed-case>S</span>truct<span class=acl-fixed-case>S</span>um: Summarization via Structured Representations</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/a/artidoro-pagnoni/>Artidoro Pagnoni</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--220><div class="card-body p-3 small">Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges : (i) layout bias : they overfit to the style of training corpora ; (ii) limited abstractiveness : they are optimized to copying <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> from the source rather than generating novel abstractive summaries ; (iii) lack of transparency : they are not interpretable. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN / DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.222.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.222.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.222" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.222/>LSOIE : A Large-Scale Dataset for Supervised Open Information Extraction<span class=acl-fixed-case>LSOIE</span>: A Large-Scale Dataset for Supervised Open Information Extraction</a></strong><br><a href=/people/j/jacob-solawetz/>Jacob Solawetz</a>
|
<a href=/people/s/stefan-larson/>Stefan Larson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--222><div class="card-body p-3 small">Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> like knowledge base creation, <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>, and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. However, current OIE datasets are limited in both size and diversity. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset LSOIE. Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, and code are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.224/>Unsupervised Abstractive Summarization of Bengali Text Documents<span class=acl-fixed-case>B</span>engali Text Documents</a></strong><br><a href=/people/r/radia-rayan-chowdhury/>Radia Rayan Chowdhury</a>
|
<a href=/people/m/mir-tafseer-nayeem/>Mir Tafseer Nayeem</a>
|
<a href=/people/t/tahsin-tasnim-mim/>Tahsin Tasnim Mim</a>
|
<a href=/people/m/md-saifur-rahman-chowdhury/>Md. Saifur Rahman Chowdhury</a>
|
<a href=/people/t/taufiqul-jannat/>Taufiqul Jannat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--224><div class="card-body p-3 small">Abstractive summarization systems generally rely on large collections of document-summary pairs. However, the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. To overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents, which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali Language</a>. We conduct experiments on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and compare our <a href=https://en.wikipedia.org/wiki/System>system</a> with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.226/>On the Computational Modelling of Michif Verbal Morphology<span class=acl-fixed-case>M</span>ichif Verbal Morphology</a></strong><br><a href=/people/f/fineen-davis/>Fineen Davis</a>
|
<a href=/people/e/eddie-antonio-santos/>Eddie Antonio Santos</a>
|
<a href=/people/h/heather-souter/>Heather Souter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--226><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state computational model</a> of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>verbal morphology</a> of <a href=https://en.wikipedia.org/wiki/Michif>Michif</a>. Michif, the official language of the Mtis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the Mtis homelands in what is now called <a href=https://en.wikipedia.org/wiki/Canada>Canada</a> and the United States, but <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rule-based approach is necessary as there is insufficient language data for an approach that uses <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the Mtis community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.227.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.227/>A Few Topical Tweets are Enough for Effective User Stance Detection</a></strong><br><a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--227><div class="card-body p-3 small">User stance detection entails ascertaining the position of a user towards a target, such as an entity, topic, or claim. Recent work that employs <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised classification</a> has shown that performing stance detection on vocal Twitter users, who have many tweets on a target, can be highly accurate (+98 %). However, such methods perform poorly or fail completely for less vocal users, who may have authored only a few tweets about a target. In this paper, we tackle stance detection for such <a href=https://en.wikipedia.org/wiki/User_(computing)>users</a> using two approaches. In the first approach, we improve user-level stance detection by representing tweets using contextualized embeddings, which capture latent meanings of words in context. We show that this approach outperforms two strong baselines and achieves 89.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 91.3 % macro F-measure on eight controversial topics. In the second approach, we expand the tweets of a given user using their Twitter timeline tweets, which may not be topically relevant, and then we perform <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised classification</a> of the user, which entails clustering a user with other users in the training set. This approach achieves 95.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 93.1 % <a href=https://en.wikipedia.org/wiki/F-measure>macro F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.228.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.228/>Do Syntax Trees Help Pre-trained Transformers Extract Information?</a></strong><br><a href=/people/d/devendra-sachan/>Devendra Sachan</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/w/william-l-hamilton/>William L. Hamilton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--228><div class="card-body p-3 small">Much recent work suggests that incorporating <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntax information</a> from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> implicitly encode <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. In this work, we systematically study the utility of incorporating dependency trees into pre-trained transformers on three representative information extraction tasks : semantic role labeling (SRL), <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure : a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models : we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.235/>Entity-level Factual Consistency of Abstractive Text Summarization</a></strong><br><a href=/people/f/feng-nan/>Feng Nan</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--235><div class="card-body p-3 small">A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.239/>Diverse Adversaries for Mitigating Bias in Training</a></strong><br><a href=/people/x/xudong-han/>Xudong Han</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--239><div class="card-body p-3 small">Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> based on the use of multiple diverse discriminators, whereby <a href=https://en.wikipedia.org/wiki/Discriminator>discriminators</a> are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.240/>‘Just because you are right, does n’t mean I am wrong’ : Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks<span class=acl-fixed-case>I</span> am wrong’: Overcoming a bottleneck in development and evaluation of Open-Ended <span class=acl-fixed-case>VQA</span> tasks</a></strong><br><a href=/people/m/man-luo/>Man Luo</a>
|
<a href=/people/s/shailaja-keyur-sampat/>Shailaja Keyur Sampat</a>
|
<a href=/people/r/riley-tallman/>Riley Tallman</a>
|
<a href=/people/y/yankai-zeng/>Yankai Zeng</a>
|
<a href=/people/m/manuha-vancha/>Manuha Vancha</a>
|
<a href=/people/a/akarshan-sajja/>Akarshan Sajja</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--240><div class="card-body p-3 small">GQA (CITATION) is a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best vision-language models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.241/>Better <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> by Extracting <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Information</a> from BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/h/hassan-s-shavarani/>Hassan S. Shavarani</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--241><div class="card-body p-3 small">Adding linguistic information (syntax or semantics) to neural machine translation (NMT) have mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT(Devlin et al., 2019) has been marginally useful in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> because effective fine-tuning is difficult to obtain for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> without making training brittle and unreliable. We augment <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer-based NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.242.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.242/>CLiMP : A Benchmark for Chinese Language Model Evaluation<span class=acl-fixed-case>CL</span>i<span class=acl-fixed-case>MP</span>: A Benchmark for <span class=acl-fixed-case>C</span>hinese Language Model Evaluation</a></strong><br><a href=/people/b/beilei-xiang/>Beilei Xiang</a>
|
<a href=/people/c/changbing-yang/>Changbing Yang</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/a/alex-warstadt/>Alex Warstadt</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--242><div class="card-body p-3 small">Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, covering 9 major <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese linguistic phenomena</a>. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8 %. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifiernoun agreement and verb complement selection are the phenomena that models generally perform best at. However, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8 % average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--244 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.244/>Progressively Pretrained Dense Corpus Index for <a href=https://en.wikipedia.org/wiki/Open-domain_question_answering>Open-Domain Question Answering</a></a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/h/hong-wang/>Hong Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--244><div class="card-body p-3 small">Commonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several information-seeking QA datasets. However, the pretraining of the <a href=https://en.wikipedia.org/wiki/Sparse_matrix>dense vector representations</a> is highly resource-demanding, e.g., requires a very large <a href=https://en.wikipedia.org/wiki/Batch_processing>batch size</a> and lots of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training steps</a>. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sample-efficient method</a> to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves more than 4-point absolute improvement in terms of answer exact match.<i>e.g.</i>, requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--245 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.245/>Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs</a></strong><br><a href=/people/d/dora-jambor/>Dora Jambor</a>
|
<a href=/people/k/komal-teru/>Komal Teru</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/w/william-l-hamilton/>William L. Hamilton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--245><div class="card-body p-3 small">Real-world knowledge graphs are often characterized by low-frequency relationsa challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> in this few-shot setting. We find that a simple, zero-shot baseline which ignores any relation-specific information achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Short Paper"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.246/>ProFormer : Towards On-Device LSH Projection Based Transformers<span class=acl-fixed-case>P</span>ro<span class=acl-fixed-case>F</span>ormer: Towards On-Device <span class=acl-fixed-case>LSH</span> Projection Based Transformers</a></strong><br><a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--246><div class="card-body p-3 small">At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a>, <a href=https://en.wikipedia.org/wiki/Watch>watches</a> and IoT. To surmount these challenges, we introduce ProFormer a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N / K representations reducing the computations quadratically by O(K2). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. ProFormer is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings reduces the <a href=https://en.wikipedia.org/wiki/Memory_footprint>embedding memory footprint</a> from 92.16 MB to 1.7 KB and requires 16x less computation overhead, which is very impressive making it the fastest and smallest on-device model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.249/>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO<span class=acl-fixed-case>MS</span>-<span class=acl-fixed-case>COCO</span></a></strong><br><a href=/people/z/zarana-parekh/>Zarana Parekh</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/a/austin-waters/>Austin Waters</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--249><div class="card-body p-3 small">By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Unfortunately, datasets have limited cross-modal associations : images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on <a href=https://en.wikipedia.org/wiki/CxC>CxC</a> for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC&#8217;s value for measuring the influence of intra- and inter-modality learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.251/>ENPAR : Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction<span class=acl-fixed-case>ENPAR</span>:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--251><div class="card-body p-3 small">Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019 ; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for these additional tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity information</a> into the sentence encoder, we further utilize the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair information</a>. Specifically, we devise four novel <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity encoder</a> and an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair encoder</a>. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.255/>Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation</a></strong><br><a href=/people/d/deeksha-varshney/>Deeksha Varshney</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--255><div class="card-body p-3 small">A recent topic of research in <a href=https://en.wikipedia.org/wiki/Natural_language_generation>natural language generation</a> has been the development of automatic response generation modules that can automatically respond to a user&#8217;s utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classes</a> with the input sequences. However, the outputs by these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> may be inconsistent. We employ <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the <a href=https://en.wikipedia.org/wiki/Code>decoders</a>. Human evaluation reveals that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> produces more emotionally pertinent responses. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.261/>Modeling Context in Answer Sentence Selection Systems on a Latency Budget</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--261><div class="card-body p-3 small">Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In this work, we present an approach to efficiently incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode <a href=https://en.wikipedia.org/wiki/Context_(computing)>context</a>, improves 6 % to 11 % over non-contextual state of the art in <a href=https://en.wikipedia.org/wiki/IBM_System_i>AS2</a> with minimal impact on <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>system latency</a>. All experiments in this work were conducted in English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.263/>DISK-CSV : Distilling Interpretable Semantic Knowledge with a Class Semantic Vector<span class=acl-fixed-case>DISK</span>-<span class=acl-fixed-case>CSV</span>: Distilling Interpretable Semantic Knowledge with a Class Semantic Vector</a></strong><br><a href=/people/h/housam-khalifa-bashier/>Housam Khalifa Bashier</a>
|
<a href=/people/m/mi-young-kim/>Mi-Young Kim</a>
|
<a href=/people/r/randy-goebel/>Randy Goebel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--263><div class="card-body p-3 small">Neural networks (NN) applied to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> are becoming deeper and more complex, making them increasingly difficult to understand and interpret. Even in applications of limited scope on fixed data, the creation of these complex black-boxes creates substantial challenges for <a href=https://en.wikipedia.org/wiki/Debugging>debugging</a>, understanding, and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. But rapid development in this <a href=https://en.wikipedia.org/wiki/Field_(physics)>field</a> has now lead to building more straightforward and interpretable models. We propose a new technique (DISK-CSV) to distill knowledge concurrently from any neural network architecture for text classification, captured as a lightweight interpretable / explainable classifier. Across multiple datasets, our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> achieves better performance than the target black-box. In addition, our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> provides better explanations than existing techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.264/>Attention Can Reflect <a href=https://en.wikipedia.org/wiki/Syntactic_structure>Syntactic Structure</a> (If You Let It)</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--264><div class="card-body p-3 small">Since the popularization of the Transformer as a general-purpose feature encoder for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English a language with <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>rigid word order</a> and a lack of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>inflectional morphology</a>. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.266/>CDA : a Cost Efficient Content-based Multilingual Web Document Aligner<span class=acl-fixed-case>CDA</span>: a Cost Efficient Content-based Multilingual Web Document Aligner</a></strong><br><a href=/people/t/thuy-vu/>Thuy Vu</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--266><div class="card-body p-3 small">We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps : (i) projecting documents of a <a href=https://en.wikipedia.org/wiki/Web_domain>web domain</a> to a shared multilingual space ; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TFIDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of <a href=https://en.wikipedia.org/wiki/Computer-aided_design>CDA</a> in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Long Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.270.Software.zip data-toggle=tooltip data-placement=top title=Software>
<i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.270/>Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--270><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers&#8217; representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question : how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.271/>Facilitating Terminology Translation with Target Lemma Annotations</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--271><div class="card-body p-3 small">Most of the recent work on terminology integration in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms ; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> using a source-side data augmentation method that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human evaluation</a> indicate a 47.7 % absolute improvement over the previous work in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>term translation accuracy</a> when translating into <a href=https://en.wikipedia.org/wiki/Latvian_language>Latvian</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.273" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.273/>Summarising Historical Text in Modern Languages</a></strong><br><a href=/people/x/xutan-peng/>Xutan Peng</a>
|
<a href=/people/y/yi-zheng/>Yi Zheng</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/a/advaith-siddharthan/>Advaith Siddharthan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--273><div class="card-body p-3 small">We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding <a href=https://en.wikipedia.org/wiki/Modern_language>modern language</a>. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.274" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.274/>Challenges in <a href=https://en.wikipedia.org/wiki/Debiasing>Automated Debiasing</a> for Toxic Language Detection</a></strong><br><a href=/people/x/xuhui-zhou/>Xuhui Zhou</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--274><div class="card-body p-3 small">Biased associations have been a challenge in the development of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> for detecting toxic language, hindering both fairness and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. As potential solutions, we investigate recently introduced <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing methods</a> for text classification datasets and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, as applied to toxic language detection. Our focus is on lexical (e.g., <a href=https://en.wikipedia.org/wiki/Profanity>swear words</a>, <a href=https://en.wikipedia.org/wiki/List_of_ethnic_slurs>slurs</a>, identity mentions) and dialectal markers (specifically <a href=https://en.wikipedia.org/wiki/African-American_Vernacular_English>African American English</a>). Our comprehensive experiments establish that existing <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> are limited in their ability to prevent <a href=https://en.wikipedia.org/wiki/Bias>biased behavior</a> in current <a href=https://en.wikipedia.org/wiki/Particle_detector>toxicity detectors</a>. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--276 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.276/>Detecting Scenes in Fiction : A new Segmentation Task</a></strong><br><a href=/people/a/albin-zehe/>Albin Zehe</a>
|
<a href=/people/l/leonard-konle/>Leonard Konle</a>
|
<a href=/people/l/lea-katharina-dumpelmann/>Lea Katharina Dümpelmann</a>
|
<a href=/people/e/evelyn-gius/>Evelyn Gius</a>
|
<a href=/people/a/andreas-hotho/>Andreas Hotho</a>
|
<a href=/people/f/fotis-jannidis/>Fotis Jannidis</a>
|
<a href=/people/l/lucas-kaufmann/>Lucas Kaufmann</a>
|
<a href=/people/m/markus-krug/>Markus Krug</a>
|
<a href=/people/f/frank-puppe/>Frank Puppe</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/a/annekea-schreiber/>Annekea Schreiber</a>
|
<a href=/people/n/nathalie-wiedmer/>Nathalie Wiedmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--276><div class="card-body p-3 small">This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> of <a href=https://en.wikipedia.org/wiki/Gamma>gamma</a> = 0.7. Baseline experiments using BERT achieve an F1 score of 24 %, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.279.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--279 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.279 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.279" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.279/>Expanding, Retrieving and Infilling : Diversifying Cross-Domain Question Generation with Flexible Templates</a></strong><br><a href=/people/x/xiaojing-yu/>Xiaojing Yu</a>
|
<a href=/people/a/anxiao-jiang/>Anxiao Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--279><div class="card-body p-3 small">Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>lack of diversity</a> and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>bad sentence structures</a>. In this paper, we focus on <a href=https://en.wikipedia.org/wiki/Question_answering>question generation</a> over <a href=https://en.wikipedia.org/wiki/SQL>SQL database</a> and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with sentence structure guidance. Furthermore, a new activation / deactivation mechanism is proposed for template-based sequence-to-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two large-scale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.280/>Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings</a></strong><br><a href=/people/o/ohjoon-kwon/>Ohjoon Kwon</a>
|
<a href=/people/d/dohyun-kim/>Dohyun Kim</a>
|
<a href=/people/s/soo-ryeon-lee/>Soo-Ryeon Lee</a>
|
<a href=/people/j/junyoung-choi/>Junyoung Choi</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--280><div class="card-body p-3 small">Word embedding is considered an essential factor in improving the performance of various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP) models</a>. However, it is hardly applicable in real-world datasets as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> is generally studied with a well-refined corpus. Notably, in Hangeul (Korean writing system), which has a unique writing system, various kinds of Out-Of-Vocabulary (OOV) appear from typos. In this paper, we propose a robust Hangeul word embedding model against <a href=https://en.wikipedia.org/wiki/Typographical_error>typos</a>, while maintaining high performance. The proposed model utilizes a Convolutional Neural Network (CNN) architecture with a channel attention mechanism that learns to infer the original word embeddings. The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> train with a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that consists of a mix of <a href=https://en.wikipedia.org/wiki/Typographical_error>typos</a> and correct words. To demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a>, we conduct three kinds of intrinsic and extrinsic tasks. While the existing embedding models fail to maintain stable performance as the <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise level</a> increases, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows stable performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.281" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.281/>Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/a/andy-mingren-li/>Andy Mingren Li</a>
|
<a href=/people/y/yishu-miao/>Yishu Miao</a>
|
<a href=/people/o/ozan-caglayan/>Ozan Caglayan</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--281><div class="card-body p-3 small">This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts : (a) adaptive policies to learn a good trade-off between high translation quality and low latency ; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> low.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.283/>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</a></strong><br><a href=/people/y/yixuan-tang/>Yixuan Tang</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/a/anthony-tung/>Anthony Tung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--283><div class="card-body p-3 small">Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these <a href=https://en.wikipedia.org/wiki/System>systems</a> have learned the ability to perform multi-hop reasoning. In this paper, we propose an additional sub-question evaluation for the multi-hop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate <a href=https://en.wikipedia.org/wiki/Questionnaire>sub-questions</a> for a multi-hop question, followed by extracting the corresponding <a href=https://en.wikipedia.org/wiki/Questionnaire>sub-answers</a>. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.285" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.285/>Variational Weakly Supervised Sentiment Analysis with Posterior Regularization</a></strong><br><a href=/people/z/ziqian-zeng/>Ziqian Zeng</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--285><div class="card-body p-3 small">Sentiment analysis is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Most of existing state-of-the-art methods are under the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning paradigm</a>. However, <a href=https://en.wikipedia.org/wiki/Annotation>human annotations</a> can be scarce. Thus, we should leverage more weak supervision for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior distribution</a> of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior distributions</a> of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Long Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.288/>Cognition-aware Cognate Detection</a></strong><br><a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/p/prashant-sharma/>Prashant Sharma</a>
|
<a href=/people/s/sayali-ghodekar/>Sayali Ghodekar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/m/malhar-kulkarni/>Malhar Kulkarni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--288><div class="card-body p-3 small">Automatic detection of cognates helps downstream NLP tasks of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>Cross-lingual Information Retrieval</a>, <a href=https://en.wikipedia.org/wiki/Computational_phylogenetics>Computational Phylogenetics</a> and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with <a href=https://en.wikipedia.org/wiki/Cognition>cognitive features</a> extracted from human readers&#8217; gaze behaviour. We collect gaze behaviour data for a small sample of <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10 % with the collected gaze features, and 12 % using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.295/>Probing the Probing Paradigm : Does Probing Accuracy Entail Task Relevance?</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--295><div class="card-body p-3 small">Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> learned by <a href=https://en.wikipedia.org/wiki/Neural_coding>neural encoders</a>, through the lens of &#8216;probing&#8217; tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.296/>One-class Text Classification with Multi-modal Deep Support Vector Data Description</a></strong><br><a href=/people/c/chenlong-hu/>Chenlong Hu</a>
|
<a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--296><div class="card-body p-3 small">This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.297" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.297/>Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings</a></strong><br><a href=/people/c/christos-xypolopoulos/>Christos Xypolopoulos</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--297><div class="card-body p-3 small">The number of senses of a given word, or <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> based on simple geometry in the contextual embedding space. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, OntoNotes, <a href=https://en.wikipedia.org/wiki/Oxford>Oxford</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.299/>Disfluency Correction using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>Unsupervised and Semi-supervised Learning</a></a></strong><br><a href=/people/n/nikhil-saini/>Nikhil Saini</a>
|
<a href=/people/d/drumil-trivedi/>Drumil Trivedi</a>
|
<a href=/people/s/shreya-khare/>Shreya Khare</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--299><div class="card-body p-3 small">Spoken language is different from the <a href=https://en.wikipedia.org/wiki/Written_language>written language</a> in its style and structure. Disfluencies that appear in transcriptions from <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition systems</a> generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.300/>Complex Question Answering on <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a></a></strong><br><a href=/people/s/saurabh-srivastava/>Saurabh Srivastava</a>
|
<a href=/people/m/mayur-patidar/>Mayur Patidar</a>
|
<a href=/people/s/sudip-chowdhury/>Sudip Chowdhury</a>
|
<a href=/people/p/puneet-agarwal/>Puneet Agarwal</a>
|
<a href=/people/i/indrajit-bhattacharya/>Indrajit Bhattacharya</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--300><div class="card-body p-3 small">Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.304/>Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database</a></strong><br><a href=/people/k/kenichi-iwatsuki/>Kenichi Iwatsuki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--304><div class="card-body p-3 small">Formulaic expressions (FEs), such as &#8216;in this paper, we propose&#8217; are frequently used in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a>. FEs convey a communicative function (CF), i.e. &#8216;showing the aim of the paper&#8217; in the above-mentioned example. Although CF-labelled FEs are helpful in assisting <a href=https://en.wikipedia.org/wiki/Academic_writing>academic writing</a>, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the topdown approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciBERT classifier. We show that the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used to construct FE databases of disciplines that are different from the training data. The <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Medical_classification>in-disciplinary classification</a> was more than 80 %, while <a href=https://en.wikipedia.org/wiki/Medical_classification>cross-disciplinary classification</a> also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65 % of the FEs are correct and useful, which is sufficiently high considering practical use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.309" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.309/>From the Stage to the Audience : Propaganda on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/o/oana-balalau/>Oana Balalau</a>
|
<a href=/people/r/roxana-horincar/>Roxana Horincar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--309><div class="card-body p-3 small">Political discussions revolve around ideological conflicts that often split the audience into two opposing parties. Both parties try to win the argument by bringing forward information. However, often this information is misleading, and its dissemination employs <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda techniques</a>. In this work, we analyze the impact of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> on six major political forums on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> that target a diverse audience in two countries, the US and the UK. We focus on three research questions : who is posting propaganda? how does <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> differ across the <a href=https://en.wikipedia.org/wiki/Political_spectrum>political spectrum</a>? and how is <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> received on political forums?</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.310.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-main.310.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.310/>Probing for <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a> in vector space models</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/t/tiago-kramer-vieira/>Tiago Kramer Vieira</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--310><div class="card-body p-3 small">Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a> in <a href=https://en.wikipedia.org/wiki/Language>language</a>. In this paper, we propose probing measures to assess if some of the expected linguistic properties of <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>noun compounds</a>, especially those related to idiomatic meanings, and their dependence on <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> and sensitivity to <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>noun compounds</a> and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a> is not yet accurately represented by contextualised models</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.311/>Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?</a></strong><br><a href=/people/y/yulong-wu/>Yulong Wu</a>
|
<a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--311><div class="card-body p-3 small">An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. We then introduce <a href=https://en.wikipedia.org/wiki/Ablation>ablation methods</a> that verify whether these skills are required to succeed on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. By observing the drop in performance of neural MRC models evaluated on the original and the modified <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we can measure to what degree the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6 %). These results imply that most of the answered questions in the examined <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> do not require understanding the discourse structure of the text. To specifically probe for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, there is a need to design more challenging <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> that can correctly evaluate the intended skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.314/>Meta-Learning for Effective Multi-task and Multilingual Modelling</a></strong><br><a href=/people/i/ishan-tarunesh/>Ishan Tarunesh</a>
|
<a href=/people/s/sushil-khyalia/>Sushil Khyalia</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/g/ganesh-ramakrishnan/>Ganesh Ramakrishnan</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--314><div class="card-body p-3 small">Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and languages. We also investigate the role of different <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategies</a> used during <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>. We present experiments on five different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and six different languages from the XTREME multilingual benchmark dataset. Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multi-task baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.321/>Two Training Strategies for Improving <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> over Universal Graph</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--321><div class="card-body p-3 small">This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies : (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path ; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/UGDSRE.</div></div></div><hr><div id=2021eacl-demos><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eacl-demos/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.0/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.1/>Using and comparing Rhetorical Structure Theory parsers with rst-workbench<span class=acl-fixed-case>R</span>hetorical <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>T</span>heory parsers with rst-workbench</a></strong><br><a href=/people/a/arne-neumann/>Arne Neumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--1><div class="card-body p-3 small">I present rst-workbench, a <a href=https://en.wikipedia.org/wiki/Package_manager>software package</a> that simplifies the installation and usage of numerous end-to-end Rhetorical Structure Theory (RST) parsers. The tool offers a <a href=https://en.wikipedia.org/wiki/Web_application>web-based interface</a> that allows users to enter text and let multiple RST parsers generate analyses concurrently. The resulting RST trees can be compared visually, manually post-edited (in the browser) and stored for later usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.5/>MATILDA-Multi-AnnoTator multi-language InteractiveLight-weight Dialogue Annotator<span class=acl-fixed-case>MATILDA</span> - Multi-<span class=acl-fixed-case>A</span>nno<span class=acl-fixed-case>T</span>ator multi-language <span class=acl-fixed-case>I</span>nteractive<span class=acl-fixed-case>L</span>ight-weight Dialogue Annotator</a></strong><br><a href=/people/d/davide-cucurnia/>Davide Cucurnia</a>
|
<a href=/people/n/nikolai-rozanov/>Nikolai Rozanov</a>
|
<a href=/people/i/irene-sucameli/>Irene Sucameli</a>
|
<a href=/people/a/augusto-ciuffoletti/>Augusto Ciuffoletti</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--5><div class="card-body p-3 small">Dialogue Systems are becoming ubiquitous in various forms and shapes-virtual assistants(Siri, <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, etc.), chat-bots, customer sup-port, chit-chat systems just to name a few. The advances in <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and their publication have democratised advanced NLP.However, data remains a crucial bottleneck. Our contribution to this essential pillar isMATILDA, to the best of our knowledge the first multi-annotator, multi-language dialogue annotation tool. MATILDA allows the creation of corpora, the management of users, the annotation of dialogues, the quick adaptation of the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> to any language and the resolution of inter-annotator disagreement. We evaluate the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> on ease of use, annotation speed and interannotation resolution for both experts and novices and conclude that this <a href=https://en.wikipedia.org/wiki/Tool>tool</a> not only supports the full pipeline for dialogue annotation, but also allows non-technical people to easily use it. We are completely open-sourcing the tool at https://github.com/wluper/matilda and provide a tutorial video1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.8/>Forum 4.0 : An Open-Source User Comment Analysis Framework</a></strong><br><a href=/people/m/marlo-haering/>Marlo Haering</a>
|
<a href=/people/j/jakob-smedegaard-andersen/>Jakob Smedegaard Andersen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/w/wiebke-loosen/>Wiebke Loosen</a>
|
<a href=/people/b/benjamin-milde/>Benjamin Milde</a>
|
<a href=/people/t/tim-pietz/>Tim Pietz</a>
|
<a href=/people/c/christian-stocker/>Christian Stöcker</a>
|
<a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/o/olaf-zukunft/>Olaf Zukunft</a>
|
<a href=/people/w/walid-maalej/>Walid Maalej</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--8><div class="card-body p-3 small">With the increasing number of user comments in diverse domains, including comments on <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce websites</a>, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of <a href=https://en.wikipedia.org/wiki/Digital_journalism>online journalism</a> and <a href=https://en.wikipedia.org/wiki/App_store>app stores</a>. We outline the underlying container architecture, including the <a href=https://en.wikipedia.org/wiki/Web_interface>web-based user interface</a>, the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning component</a>, and the <a href=https://en.wikipedia.org/wiki/Task_manager>task manager</a> for time-consuming tasks. We finally conduct <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> experiments with simulated annotations and different <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategies</a> on existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from both domains to evaluate Forum 4.0&#8217;s performance. Forum 4.0 achieves promising classification results (ROC-AUC 0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a>. We explain how Forum 4.0&#8217;s architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.9/>SLTEV : Comprehensive Evaluation of Spoken Language Translation<span class=acl-fixed-case>SLTEV</span>: Comprehensive Evaluation of Spoken Language Translation</a></strong><br><a href=/people/e/ebrahim-ansari/>Ebrahim Ansari</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/mohammad-mahmoudi/>Mohammad Mahmoudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--9><div class="card-body p-3 small">Automatic evaluation of Machine Translation (MT) quality has been investigated over several decades. Spoken Language Translation (SLT), esp. when simultaneous, needs to consider additional criteria and does not have a standard evaluation procedure and a widely used toolkit. To fill the gap, we develop SLTev, an open-source tool for assessing SLT in a comprehensive way. SLTev reports the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>, <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>, and <a href=https://en.wikipedia.org/wiki/Software_stability>stability</a> of an SLT candidate output based on the time-stamped transcript and reference translation into a target language. For quality, we rely on sacreBLEU which provides MT evaluation measures such as chrF or <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. For <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>, we propose two new scoring techniques. For <a href=https://en.wikipedia.org/wiki/Stability_theory>stability</a>, we extend the previously defined <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> with a normalized Flicker in our work. We also propose a new averaging of older measures. A preliminary version of SLTev was used in the IWSLT 2020 shared task. Moreover, a growing collection of test datasets directly accessible by SLTev are provided for system evaluation comparable across papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.12/>A Dashboard for Mitigating the COVID-19 Misinfodemic<span class=acl-fixed-case>COVID</span>-19 Misinfodemic</a></strong><br><a href=/people/z/zhengyuan-zhu/>Zhengyuan Zhu</a>
|
<a href=/people/k/kevin-meng/>Kevin Meng</a>
|
<a href=/people/j/josue-caraballo/>Josue Caraballo</a>
|
<a href=/people/i/israa-jaradat/>Israa Jaradat</a>
|
<a href=/people/x/xiao-shi/>Xiao Shi</a>
|
<a href=/people/z/zeyu-zhang/>Zeyu Zhang</a>
|
<a href=/people/f/farahnaz-akrami/>Farahnaz Akrami</a>
|
<a href=/people/h/haojin-liao/>Haojin Liao</a>
|
<a href=/people/f/fatma-arslan/>Fatma Arslan</a>
|
<a href=/people/d/damian-jimenez/>Damian Jimenez</a>
|
<a href=/people/m/mohanmmed-samiul-saeef/>Mohanmmed Samiul Saeef</a>
|
<a href=/people/p/paras-pathak/>Paras Pathak</a>
|
<a href=/people/c/chengkai-li/>Chengkai Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--12><div class="card-body p-3 small">This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the <a href=https://en.wikipedia.org/wiki/Dashboard_(business)>dashboard</a> uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.16/>A description and demonstration of SAFAR framework<span class=acl-fixed-case>SAFAR</span> framework</a></strong><br><a href=/people/k/karim-bouzoubaa/>Karim Bouzoubaa</a>
|
<a href=/people/y/younes-jaafar/>Younes Jaafar</a>
|
<a href=/people/d/driss-namly/>Driss Namly</a>
|
<a href=/people/r/ridouane-tachicart/>Ridouane Tachicart</a>
|
<a href=/people/r/rachida-tajmout/>Rachida Tajmout</a>
|
<a href=/people/h/hakima-khamar/>Hakima Khamar</a>
|
<a href=/people/h/hamid-jaafar/>Hamid Jaafar</a>
|
<a href=/people/l/lhoussain-aouragh/>Lhoussain Aouragh</a>
|
<a href=/people/a/abdellah-yousfi/>Abdellah Yousfi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--16><div class="card-body p-3 small">Several tools and resources have been developed to deal with Arabic NLP. However, a homogenous and flexible Arabic environment that gathers these <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> is rarely available. In this perspective, we introduce SAFAR which is a monolingual framework developed in accordance with <a href=https://en.wikipedia.org/wiki/Software_requirements>software engineering requirements</a> and dedicated to <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>, especially, the modern standard Arabic and Moroccan dialect. After one decade of integration and development, SAFAR possesses today more than 50 tools and resources that can be exploited either using its <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> or using its <a href=https://en.wikipedia.org/wiki/User_interface>web interface</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.19/>LOME : Large Ontology Multilingual Extraction<span class=acl-fixed-case>LOME</span>: Large Ontology Multilingual Extraction</a></strong><br><a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/s/siddharth-vashishtha/>Siddharth Vashishtha</a>
|
<a href=/people/y/yunmo-chen/>Yunmo Chen</a>
|
<a href=/people/t/tongfei-chen/>Tongfei Chen</a>
|
<a href=/people/c/chandler-may/>Chandler May</a>
|
<a href=/people/c/craig-harman/>Craig Harman</a>
|
<a href=/people/k/kyle-rawlins/>Kyle Rawlins</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--19><div class="card-body p-3 small">We present LOME, a <a href=https://en.wikipedia.org/wiki/System>system</a> for performing multilingual information extraction. Given a text document as input, our core system identifies spans of textual entity and event mentions with a FrameNet (Baker et al., 1998) parser. It subsequently performs <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, fine-grained entity typing, and temporal relation prediction between events. By doing so, the <a href=https://en.wikipedia.org/wiki/System>system</a> constructs an event and entity focused knowledge graph. We can further apply <a href=https://en.wikipedia.org/wiki/Modular_programming>third-party modules</a> for other types of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, like <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our (multilingual) first-party modules either outperform or are competitive with the (monolingual) state-of-the-art. We achieve this through the use of multilingual encoders like XLM-R (Conneau et al., 2020) and leveraging multilingual training data. LOME is available as a Docker container on Docker Hub. In addition, a lightweight version of the <a href=https://en.wikipedia.org/wiki/System>system</a> is accessible as a web demo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.21/>Graph Matching and Graph Rewriting : GREW tools for corpus exploration, maintenance and conversion<span class=acl-fixed-case>GREW</span> tools for corpus exploration, maintenance and conversion</a></strong><br><a href=/people/b/bruno-guillaume/>Bruno Guillaume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--21><div class="card-body p-3 small">This article presents a set of tools built around the Graph Rewriting computational framework which can be used to compute complex rule-based transformations on linguistic structures. Application of the graph matching mechanism for <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus exploration</a>, error mining or quantitative typology are also given.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.22/>Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP<span class=acl-fixed-case>M</span>a<span class=acl-fixed-case>C</span>h<span class=acl-fixed-case>A</span>mp): A Toolkit for Multi-task Learning in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/i/ibrahim-sharaf/>Ibrahim Sharaf</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--22><div class="card-body p-3 small">Transfer learning, particularly approaches that combine <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> with pre-trained contextualized embeddings and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, have advanced the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.26/>European Language Grid : A Joint Platform for the European Language Technology Community<span class=acl-fixed-case>E</span>uropean Language Grid: A Joint Platform for the <span class=acl-fixed-case>E</span>uropean Language Technology Community</a></strong><br><a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/v/victoria-arranz/>Victoria Arranz</a>
|
<a href=/people/a/andrejs-vasiljevs/>Andrejs Vasiļjevs</a>
|
<a href=/people/g/gerhard-backfried/>Gerhard Backfried</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>Jose Manuel Gomez-Perez</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/remi-calizzano/>Rémi Calizzano</a>
|
<a href=/people/n/nils-feldhus/>Nils Feldhus</a>
|
<a href=/people/s/stefanie-hegele/>Stefanie Hegele</a>
|
<a href=/people/f/florian-kintzel/>Florian Kintzel</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/j/julian-moreno-schneider/>Julian Moreno-Schneider</a>
|
<a href=/people/d/dimitrios-galanis/>Dimitris Galanis</a>
|
<a href=/people/p/penny-labropoulou/>Penny Labropoulou</a>
|
<a href=/people/m/miltos-deligiannis/>Miltos Deligiannis</a>
|
<a href=/people/k/katerina-gkirtzou/>Katerina Gkirtzou</a>
|
<a href=/people/a/athanasia-kolovou/>Athanasia Kolovou</a>
|
<a href=/people/d/dimitris-gkoumas/>Dimitris Gkoumas</a>
|
<a href=/people/l/leon-voukoutis/>Leon Voukoutis</a>
|
<a href=/people/i/ian-roberts/>Ian Roberts</a>
|
<a href=/people/j/jana-hamrlova/>Jana Hamrlova</a>
|
<a href=/people/d/dusan-varis/>Dusan Varis</a>
|
<a href=/people/l/lukas-kacena/>Lukas Kacena</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/v/valerie-mapelli/>Valérie Mapelli</a>
|
<a href=/people/m/mickael-rigault/>Mickaël Rigault</a>
|
<a href=/people/j/julija-melnika/>Julija Melnika</a>
|
<a href=/people/m/miro-janosik/>Miro Janosik</a>
|
<a href=/people/k/katja-prinz/>Katja Prinz</a>
|
<a href=/people/a/andres-garcia-silva/>Andres Garcia-Silva</a>
|
<a href=/people/c/cristian-berrio/>Cristian Berrio</a>
|
<a href=/people/o/ondrej-klejch/>Ondrej Klejch</a>
|
<a href=/people/s/steve-renals/>Steve Renals</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--26><div class="card-body p-3 small">Europe is a <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual society</a>, in which dozens of languages are spoken. The only option to enable and to benefit from <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> is through <a href=https://en.wikipedia.org/wiki/Language_technology>Language Technologies (LT)</a>, i.e., <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> and <a href=https://en.wikipedia.org/wiki/Speech_technology>Speech Technologies</a>. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eacl-demos.27.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.27/>A New Surprise Measure for Extracting Interesting Relationships between Persons</a></strong><br><a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/j/jingun-kwon/>Jingun Kwon</a>
|
<a href=/people/y/young-in-song/>Young-In Song</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--27><div class="card-body p-3 small">One way to enhance user engagement in <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> is to suggest interesting facts to the user. Although relationships between persons are important as a target for <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprisingness</a>. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is that it does not require any labeled dataset with <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for the surprising personal relationships. The results of the human evaluation show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> could extract more interesting relationships between persons from <a href=https://en.wikipedia.org/wiki/Japanese_Wikipedia>Japanese Wikipedia articles</a> than a popularity-based baseline method. We demonstrate our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> as a chrome plugin on <a href=https://en.wikipedia.org/wiki/Google_Search>google search</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.28/>Paladin : an annotation tool based on active and proactive learning</a></strong><br><a href=/people/m/minh-quoc-nghiem/>Minh-Quoc Nghiem</a>
|
<a href=/people/p/paul-baylis/>Paul Baylis</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--28><div class="card-body p-3 small">In this paper, we present Paladin, an open-source web-based annotation tool for creating high-quality multi-label document-level datasets. By integrating <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and <a href=https://en.wikipedia.org/wiki/Proactive_learning>proactive learning</a> to the annotation task, <a href=https://en.wikipedia.org/wiki/Paladin>Paladin</a> makes the task less time-consuming and requiring less human effort. Although <a href=https://en.wikipedia.org/wiki/Paladin>Paladin</a> is designed for multi-label settings, the <a href=https://en.wikipedia.org/wiki/System>system</a> is flexible and can be adapted to other tasks in single-label settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.29/>Story Centaur : Large Language Model Few Shot Learning as a Creative Writing Tool</a></strong><br><a href=/people/b/ben-swanson/>Ben Swanson</a>
|
<a href=/people/k/kory-mathewson/>Kory Mathewson</a>
|
<a href=/people/b/ben-pietrzak/>Ben Pietrzak</a>
|
<a href=/people/s/sherol-chen/>Sherol Chen</a>
|
<a href=/people/m/monica-dinalescu/>Monica Dinalescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--29><div class="card-body p-3 small">Few shot learning with large language models has the potential to give individuals without formal <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> training the access to a wide range of text to text models. We consider how this applies to creative writers and present Story Centaur, a user interface for prototyping few shot models and a set of recombinable web components that deploy them. Story Centaur&#8217;s goal is to expose creative writers to few shot learning with a simple but powerful interface that lets them compose their own co-creation tools that further their own unique artistic directions. We build out several examples of such tools, and in the process probe the boundaries and issues surrounding generation with large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-demos.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.31/>OCTIS : Comparing and Optimizing Topic models is Simple !<span class=acl-fixed-case>OCTIS</span>: Comparing and Optimizing Topic models is Simple!</a></strong><br><a href=/people/s/silvia-terragni/>Silvia Terragni</a>
|
<a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/b/bruno-giovanni-galuzzi/>Bruno Giovanni Galuzzi</a>
|
<a href=/people/p/pietro-tropeano/>Pietro Tropeano</a>
|
<a href=/people/a/antonio-candelieri/>Antonio Candelieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--31><div class="card-body p-3 small">In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> integrates several state-of-the-art <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> and evaluation metrics. These <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can be targeted as objective by the underlying <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a> to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link : https://github.com/MIND-Lab/OCTIS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.33/>Breaking Writer’s Block : Low-cost Fine-tuning of Natural Language Generation Models</a></strong><br><a href=/people/a/alexandre-duval/>Alexandre Duval</a>
|
<a href=/people/t/thomas-lamson/>Thomas Lamson</a>
|
<a href=/people/g/gael-de-leseleuc-de-kerouara/>Gaël de Léséleuc de Kérouara</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--33><div class="card-body p-3 small">It is standard procedure these days to solve Information Extraction task by fine-tuning large pre-trained language models. This is not the case for generation task, which relies on a variety of techniques for controlled language generation. In this paper, we describe a <a href=https://en.wikipedia.org/wiki/System>system</a> that fine-tunes a <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation model</a> for the problem of solving writer&#8217;s block. The fine-tuning changes the conditioning to also include the right context in addition to the left context, as well as an optional list of entities, the size, the genre and a summary of the paragraph that the human author wishes to generate. Our proposed <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> obtains excellent results, even with a small number of epochs and a total cost of USD 150. The <a href=https://en.wikipedia.org/wiki/System>system</a> can be accessed as a web-service and all the code is released. A video showcasing the interface and the <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> is also available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.35/>Domain Expert Platform for Goal-Oriented Dialog Collection</a></strong><br><a href=/people/d/didzis-gosko/>Didzis Goško</a>
|
<a href=/people/a/arturs-znotins/>Arturs Znotins</a>
|
<a href=/people/i/inguna-skadina/>Inguna Skadina</a>
|
<a href=/people/n/normunds-gruzitis/>Normunds Gruzitis</a>
|
<a href=/people/g/gunta-nespore-berzkalne/>Gunta Nešpore-Bērzkalne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--35><div class="card-body p-3 small">Today, most <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> are fully or partly built using neural network architectures. A crucial prerequisite for the creation of a goal-oriented neural network dialogue system is a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that represents typical dialogue scenarios and includes various <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic annotations</a>, e.g. intents, slots and dialogue actions, that are necessary for training a particular neural network architecture. In this demonstration paper, we present an easy to use interface and its back-end which is oriented to domain experts for the collection of goal-oriented dialogue samples. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> not only allows to collect or write sample dialogues in a structured way, but also provides a means for simple annotation and interpretation of the dialogues. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> itself is language-independent ; it depends only on the availability of particular <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>language processing components</a> for a specific language. It is currently being used to collect dialogue samples in <a href=https://en.wikipedia.org/wiki/Latvian_language>Latvian</a> (a highly inflected language) which represent typical communication between students and the student service.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-demos.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-demos--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-demos.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-demos.36/>Which is Better for <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> : <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> or <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>? Answering Comparative Questions in Natural Language<span class=acl-fixed-case>MATLAB</span>? Answering Comparative Questions in Natural Language</a></strong><br><a href=/people/v/viktoriia-chekalina/>Viktoriia Chekalina</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/m/meriem-beloucif/>Meriem Beloucif</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-demos--36><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for answering comparative questions (Is X better than Y with respect to Z?) in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interface</a> for comparative QA that can be used in <a href=https://en.wikipedia.org/wiki/Personal_assistant>personal assistants</a>, <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, and similar <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP devices</a>. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> by probing several methods, making the three best ones available as an online demo.</div></div></div><hr><div id=2021eacl-srw><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eacl-srw/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.0/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/m/madhumita-sushil/>Madhumita Sushil</a>
|
<a href=/people/e/ece-takmaz/>Ece Takmaz</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.1/>Computationally Efficient Wasserstein Loss for Structured Labels<span class=acl-fixed-case>W</span>asserstein Loss for Structured Labels</a></strong><br><a href=/people/a/ayato-toyokuni/>Ayato Toyokuni</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/h/hisashi-kashima/>Hisashi Kashima</a>
|
<a href=/people/m/makoto-yamada/>Makoto Yamada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--1><div class="card-body p-3 small">The problem of estimating the probability distribution of labels has been widely studied as a label distribution learning (LDL) problem, whose applications include age estimation, emotion analysis, and semantic segmentation. We propose a tree-Wasserstein distance regularized LDL algorithm, focusing on hierarchical text classification tasks. We propose predicting the entire label hierarchy using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, where the similarity between predicted and true labels is measured using the tree-Wasserstein distance. Through experiments using synthetic and real-world datasets, we demonstrate that the proposed method successfully considers the structure of labels during training, and it compares favorably with the Sinkhorn algorithm in terms of <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> and <a href=https://en.wikipedia.org/wiki/Memory_complexity>memory usage</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.2/>Have Attention Heads in BERT Learned Constituency Grammar?<span class=acl-fixed-case>BERT</span> Learned Constituency Grammar?</a></strong><br><a href=/people/z/ziyang-luo/>Ziyang Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--2><div class="card-body p-3 small">With the success of pre-trained language models in recent years, more and more researchers focus on opening the black box of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Following this interest, we carry out a qualitative and quantitative analysis of <a href=https://en.wikipedia.org/wiki/Constituency_grammar>constituency grammar</a> in attention heads of BERT and RoBERTa. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist <a href=https://en.wikipedia.org/wiki/Head_(linguistics)>heads</a> that can induce some grammar types much better than baselines, suggesting that some <a href=https://en.wikipedia.org/wiki/Head_(linguistics)>heads</a> act as a proxy for <a href=https://en.wikipedia.org/wiki/Constituency_grammar>constituency grammar</a>. We also analyze how attention heads&#8217; constituency grammar inducing (CGI) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity (SMS) tasks and natural language inference (NLI) tasks. Our results suggest that SMS tasks decrease the average CGI ability of upper layers, while NLI tasks increase it. Lastly, we investigate the connections between CGI ability and natural language understanding ability on QQP and MNLI tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.3/>Do we read what we hear? Modeling orthographic influences on <a href=https://en.wikipedia.org/wiki/Spoken_word_recognition>spoken word recognition</a></a></strong><br><a href=/people/n/nicole-macher/>Nicole Macher</a>
|
<a href=/people/b/badr-m-abdullah/>Badr M. Abdullah</a>
|
<a href=/people/h/harm-brouwer/>Harm Brouwer</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--3><div class="card-body p-3 small">Theories and models of <a href=https://en.wikipedia.org/wiki/Spoken_word_recognition>spoken word recognition</a> aim to explain the process of accessing lexical knowledge given an acoustic realization of a word form. There is consensus that <a href=https://en.wikipedia.org/wiki/Phonology>phonological and semantic information</a> is crucial for this <a href=https://en.wikipedia.org/wiki/Process_(philosophy)>process</a>. However, there is accumulating evidence that orthographic information could also have an impact on auditory word recognition. This paper presents two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> of <a href=https://en.wikipedia.org/wiki/Spoken_word_recognition>spoken word recognition</a> that instantiate different hypotheses regarding the influence of <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a> on this process. We show that these models reproduce human-like behavior in different ways and provide testable hypotheses for future research on the source of orthographic effects in spoken word recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.7/>Automatically Cataloging Scholarly Articles using Library of Congress Subject Headings</a></strong><br><a href=/people/n/nazmul-kazi/>Nazmul Kazi</a>
|
<a href=/people/n/nathaniel-lane/>Nathaniel Lane</a>
|
<a href=/people/i/indika-kahanda/>Indika Kahanda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--7><div class="card-body p-3 small">Institutes are required to catalog their articles with proper subject headings so that the users can easily retrieve relevant articles from the <a href=https://en.wikipedia.org/wiki/Institutional_repository>institutional repositories</a>. However, due to the rate of proliferation of the number of articles in these repositories, it is becoming a challenge to manually catalog the newly added articles at the same pace. To address this challenge, we explore the feasibility of automatically annotating articles with Library of Congress Subject Headings (LCSH). We first use <a href=https://en.wikipedia.org/wiki/Web_scraping>web scraping</a> to extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> for a collection of articles from the Repository Analytics and Metrics Portal (RAMP). Then, we map these <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to LCSH names for developing a gold-standard dataset. As a case study, using the subset of Biology-related LCSH concepts, we develop <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> by formulating this task as a multi-label classification problem. Our experimental results demonstrate the viability of this <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> for predicting LCSH for <a href=https://en.wikipedia.org/wiki/Academic_publishing>scholarly articles</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.11/>Contrasting distinct structured views to learn sentence embeddings</a></strong><br><a href=/people/a/antoine-simoulin/>Antoine Simoulin</a>
|
<a href=/people/b/benoit-crabbe/>Benoit Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--11><div class="card-body p-3 small">We propose a self-supervised method that builds sentence embeddings from the combination of diverse explicit syntactic structures of a sentence. We assume <a href=https://en.wikipedia.org/wiki/Structure>structure</a> is crucial to building consistent representations as we expect <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence meaning</a> to be a function of both syntax and semantic aspects. In this perspective, we hypothesize that some linguistic representations might be better adapted given the considered task or sentence. We, therefore, propose to learn individual representation functions for different syntactic frameworks jointly. Again, by hypothesis, all such <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> should encode similar semantic information differently and consequently, be complementary for building better sentential semantic embeddings. To assess such hypothesis, we propose an original contrastive multi-view framework that induces an explicit interaction between models during the training phase. We make experiments combining various structures such as dependency, constituency, or sequential schemes. Our results outperform comparable <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on several <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> from standard sentence embedding benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.12/>Discrete Reasoning Templates for Natural Language Understanding</a></strong><br><a href=/people/h/hadeel-al-negheimish/>Hadeel Al-Negheimish</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/a/alessandra-russo/>Alessandra Russo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--12><div class="card-body p-3 small">Reasoning about information from multiple parts of a passage to derive an answer is an open challenge for reading-comprehension models. In this paper, we present an approach that reasons about complex questions by decomposing them to simpler subquestions that can take advantage of single-span extraction reading-comprehension models, and derives the final answer according to instructions in a predefined reasoning template. We focus on subtraction based arithmetic questions and evaluate our approach on a subset of the DROP dataset. We show that our approach is competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> while being interpretable and requires little <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.17/>Development of Conversational AI for Sleep Coaching Programme<span class=acl-fixed-case>AI</span> for Sleep Coaching Programme</a></strong><br><a href=/people/h/heereen-shim/>Heereen Shim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--17><div class="card-body p-3 small">Almost 30 % of the adult population in the world is experiencing or has experience insomnia. Cognitive Behaviour Therapy for insomnia (CBT-I) is one of the most effective treatment, but it has limitations on accessibility and availability. Utilising <a href=https://en.wikipedia.org/wiki/Technology>technology</a> is one of the possible solutions, but existing methods neglect conversational aspects, which plays a critical role in <a href=https://en.wikipedia.org/wiki/Sleep_therapy>sleep therapy</a>. To address this issue, we propose a PhD project exploring potentials of developing conversational artificial intelligence (AI) for a sleep coaching programme, which is motivated by CBT-I treatment. This PhD project aims to develop natural language processing (NLP) algorithms to allow the <a href=https://en.wikipedia.org/wiki/System>system</a> to interact naturally with a user and provide automated analytic system to support human experts. In this paper, we introduce research questions lying under three phases of the sleep coaching programme : triaging, monitoring the progress, and providing <a href=https://en.wikipedia.org/wiki/Coaching>coaching</a>. We expect this research project&#8217;s outcomes could contribute to the research domains of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> but also the healthcare field by providing a more accessible and affordable sleep treatment solution and an automated analytic system to lessen the burden of human experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.18/>Relating Relations : Meta-Relation Extraction from Online Health Forum Posts</a></strong><br><a href=/people/d/daniel-stickley/>Daniel Stickley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--18><div class="card-body p-3 small">Relation extraction is a key task in <a href=https://en.wikipedia.org/wiki/Knowledge_extraction>knowledge extraction</a>, and is commonly defined as the task of identifying relations that hold between entities in text. This thesis proposal addresses the specific task of identifying meta-relations, a higher order family of relations naturally construed as holding between other relations which includes temporal, comparative, and causal relations. More specifically, we aim to develop theoretical underpinnings and practical solutions for the challenges of (1) incorporating meta-relations into conceptualisations and annotation schemes for (lower-order) relations and named entities, (2) obtaining annotations for them with tolerable cognitive load on annotators, (3) creating models capable of reliably extracting meta-relations, and related to that (4) addressing the limited-data problem exacerbated by the introduction of meta-relations into the learning task. We explore recent works in relation extraction and discuss our plans to formally conceptualise meta-relations for the domain of user-generated health texts, and create a new dataset, annotation scheme and models for meta-relation extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-srw.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.24/>Beyond the English Web : Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers<span class=acl-fixed-case>E</span>nglish Web: Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers</a></strong><br><a href=/people/l/liina-repo/>Liina Repo</a>
|
<a href=/people/v/valtteri-skantsi/>Valtteri Skantsi</a>
|
<a href=/people/s/samuel-ronnqvist/>Samuel Rönnqvist</a>
|
<a href=/people/s/saara-hellstrom/>Saara Hellström</a>
|
<a href=/people/m/miika-oinonen/>Miika Oinonen</a>
|
<a href=/people/a/anna-salmela/>Anna Salmela</a>
|
<a href=/people/d/douglas-biber/>Douglas Biber</a>
|
<a href=/people/j/jesse-egbert/>Jesse Egbert</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a>
|
<a href=/people/v/veronika-laippala/>Veronika Laippala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--24><div class="card-body p-3 small">We explore cross-lingual transfer of register classification for <a href=https://en.wikipedia.org/wiki/Web_page>web documents</a>. Registers, that is, text varieties such as <a href=https://en.wikipedia.org/wiki/Blog>blogs</a> or <a href=https://en.wikipedia.org/wiki/News>news</a> are one of the primary predictors of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> and thus affect the automatic processing of language. We introduce two new register-annotated corpora, FreCORE and SweCORE, for <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We demonstrate that deep pre-trained language models perform strongly in these languages and outperform previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>. Specifically, we show 1) that zero-shot cross-lingual transfer from the large English CORE corpus can match or surpass previously published monolingual models, and 2) that lightweight monolingual classification requiring very little training data can reach or surpass our zero-shot performance. We further analyse classification results finding that certain registers continue to pose challenges in particular for cross-lingual transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-srw.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-srw--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-srw.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-srw.26/>Why Find the Right One?</a></strong><br><a href=/people/p/payal-khullar/>Payal Khullar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-srw--26><div class="card-body p-3 small">The present paper investigates the impact of the anaphoric one words in English on the Neural Machine Translation (NMT) process using English-Hindi as source and target language pair. As expected, the experimental results show that the state-of-the-art Google English-Hindi NMT system achieves significantly poorly on sentences containing anaphoric ones as compared to the sentences containing regular, non-anaphoric ones. But, more importantly, we note that amongst the <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric words</a>, the <a href=https://en.wikipedia.org/wiki/Noun_class>noun class</a> is clearly much harder for NMT than the <a href=https://en.wikipedia.org/wiki/Determinative>determinatives</a>. This reaffirms the linguistic disparity of the two phenomenon in recent theoretical syntactic literature, despite the obvious surface similarities.</div></div></div><hr><div id=2021eacl-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eacl-tutorials/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.0/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-tutorials--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-tutorials.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.1/>Unsupervised Natural Language Parsing (Introductory Tutorial)</a></strong><br><a href=/people/k/kewei-tu/>Kewei Tu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-tutorials--1><div class="card-body p-3 small">Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community : a general trend towards <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised training</a> or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models. In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-tutorials--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-tutorials.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.2/>Aggregating and Learning from Multiple Annotators</a></strong><br><a href=/people/s/silviu-paun/>Silviu Paun</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-tutorials--2><div class="card-body p-3 small">The success of NLP research is founded on high-quality annotated datasets, which are usually obtained from multiple expert annotators or crowd workers. The standard practice to training machine learning models is to first adjudicate the disagreements and then perform the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>. To this end, there has been a lot of work on aggregating annotations, particularly for classification tasks. However, many other tasks, particularly in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, have unique characteristics not considered by standard models of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, e.g., label interdependencies in sequence labelling tasks, unrestricted labels for <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric annotation</a>, or preference labels for ranking texts. In recent years, researchers have picked up on this and are covering the gap. A first objective of this tutorial is to connect NLP researchers with state-of-the-art aggregation models for a diverse set of canonical language annotation tasks. There is also a growing body of recent work arguing that following the convention and training with adjudicated labels ignores any uncertainty the labellers had in their classifications, which results in models with poorer generalisation capabilities. Therefore, a second objective of this tutorial is to teach NLP workers how they can augment their (deep) neural models to learn from data with multiple interpretations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-tutorials.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-tutorials--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-tutorials.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-tutorials.3/>Tutorial Proposal : End-to-End Speech Translation</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-tutorials--3><div class="card-body p-3 small">Speech translation is the translation of speech in one language typically to text in another, traditionally accomplished through a combination of <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Speech translation has attracted interest for many years, but the recent successful applications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to both individual tasks have enabled new opportunities through joint modeling, in what we today call &#8216;end-to-end speech translation.&#8217; In this tutorial we will introduce the techniques used in cutting-edge research on <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. Starting from the traditional cascaded approach, we will given an overview on data sources and model architectures to achieve state-of-the art performance with end-to-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications.</div></div></div><hr><div id=2021adaptnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.adaptnlp-1/>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.0/>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></strong><br><a href=/people/e/eyal-ben-david/>Eyal Ben-David</a>
|
<a href=/people/s/shay-b-cohen/>Shay Cohen</a>
|
<a href=/people/r/ryan-mcdonald/>Ryan McDonald</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/g/guy-rotman/>Guy Rotman</a>
|
<a href=/people/y/yftah-ziser/>Yftah Ziser</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.adaptnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.6/>Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data<span class=acl-fixed-case>F</span>risian-<span class=acl-fixed-case>D</span>utch Data</a></strong><br><a href=/people/a/anouck-braggaar/>Anouck Braggaar</a>
|
<a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--6><div class="card-body p-3 small">While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind. In this paper we focus on the <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> specifically tailored towards the target domain, by selecting instances from multiple <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams. We use a deep biaffine parser initialized with mBERT. The best single source treebank (nl_alpino) resulted in an <a href=https://en.wikipedia.org/wiki/Greatest_common_divisor>LAS</a> of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 <a href=https://en.wikipedia.org/wiki/Greatest_common_divisor>LAS</a> on the test data. Additional experiments consisted of removing <a href=https://en.wikipedia.org/wiki/Diacritic>diacritics</a> from our Frisian data, creating more similar training data by cropping sentences and running our best <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using XLM-R. These experiments did not lead to a better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.9/>Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--9><div class="card-body p-3 small">Achieving satisfying performance in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.12/>BERTologiCoMix : How does Code-Mixing interact with Multilingual BERT?<span class=acl-fixed-case>BERT</span>ologi<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>M</span>ix: How does Code-Mixing interact with Multilingual <span class=acl-fixed-case>BERT</span>?</a></strong><br><a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--12><div class="card-body p-3 small">Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using <a href=https://en.wikipedia.org/wiki/Synthetic_genomics>synthetically generated data</a> along with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>naturally occurring data</a> to improve their performance. Finetuning mBERT on such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a> improves it&#8217;s code-mixed performance, but the benefits of using the different types of Code-Mixed data are n&#8217;t clear. In this paper, we study the impact of <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with different types of code-mixed data and outline the changes that occur to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> during such <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> and that <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with any type of code-mixed text improves the responsivity of it&#8217;s attention heads to code-mixed text inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.13/>Locality Preserving Loss : Neighbors that Live together, Align together</a></strong><br><a href=/people/a/ashwinkumar-ganesan/>Ashwinkumar Ganesan</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/tim-oates/>Tim Oates</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--13><div class="card-body p-3 small">We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> and maintain its local neighborhood while aligning one <a href=https://en.wikipedia.org/wiki/Manifold>manifold</a> to another. This reduces the overall size of the dataset required to align the two in <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as crosslingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a>, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL-optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16 % improvement over our baseline in lower resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.15/>Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning</a></strong><br><a href=/people/g/gordon-buck/>Gordon Buck</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--15><div class="card-body p-3 small">Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> can improve results obtained. However, the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.adaptnlp-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.17/>An Empirical Study of Compound PCFGs<span class=acl-fixed-case>PCFG</span>s</a></strong><br><a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--17><div class="card-body p-3 small">Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for phrase-structure grammar induction. However, due to the high <a href=https://en.wikipedia.org/wiki/Time_complexity>time-complexity</a> of chart-based representation and inference, it is difficult to investigate them comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION). We highlight three key findings : (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on <a href=https://en.wikipedia.org/wiki/English_language>English</a> do not always generalize to morphology-rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.adaptnlp-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.20/>Effective Distant Supervision for Temporal Relation Extraction</a></strong><br><a href=/people/x/xinyu-zhao/>Xinyu Zhao</a>
|
<a href=/people/s/shih-ting-lin/>Shih-Ting Lin</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--20><div class="card-body p-3 small">A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more. We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> of automatically collecting distantly-supervised examples of temporal relations. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.adaptnlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.21/>Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation</a></strong><br><a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--21><div class="card-body p-3 small">Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a>, we also provide a deep view of properties of contextual embeddings, i.e., the <a href=https://en.wikipedia.org/wiki/Anisotropy>anisotropy problem</a> and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.adaptnlp-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.22/>Gradual Fine-Tuning for Low-Resource Domain Adaptation</a></strong><br><a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/s/seth-ebner/>Seth Ebner</a>
|
<a href=/people/m/mahsa-yarmohammadi/>Mahsa Yarmohammadi</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/k/kenton-murray/>Kenton Murray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--22><div class="card-body p-3 small">Fine-tuning is known to improve NLP models by adapting an initial <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on more plentiful but less domain-salient examples to data in a target domain. Such <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> is typically done using one stage of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> or learning objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.25/>Semantic Parsing of Brief and Multi-Intent Natural Language Utterances</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/c/charles-newton/>Charles Newton</a>
|
<a href=/people/v/victor-hung/>Victor Hung</a>
|
<a href=/people/b/beth-atkinson/>Beth Atkinson</a>
|
<a href=/people/j/john-killilea/>John Killilea</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--25><div class="card-body p-3 small">Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new projection and reduction method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.</div></div></div><hr><div id=2021bea-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.bea-1/>Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.0/>Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</a></strong><br><a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/r/ronja-laarmann-quante/>Ronja Laarmann-Quante</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bea-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.2/>Text Simplification by Tagging<span class=acl-fixed-case>T</span>ext <span class=acl-fixed-case>S</span>implification by <span class=acl-fixed-case>T</span>agging</a></strong><br><a href=/people/k/kostiantyn-omelianchuk/>Kostiantyn Omelianchuk</a>
|
<a href=/people/v/vipul-raheja/>Vipul Raheja</a>
|
<a href=/people/o/oleksandr-skurzhanskyi/>Oleksandr Skurzhanskyi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--2><div class="card-body p-3 small">Edit-based approaches have recently shown promising results on multiple monolingual sequence transduction tasks. In contrast to conventional sequence-to-sequence (Seq2Seq) models, which learn to generate text from scratch as they are trained on parallel corpora, these methods have proven to be much more effective since they are able to learn to make fast and accurate transformations while leveraging powerful pre-trained language models. Inspired by these ideas, we present TST, a simple and efficient Text Simplification system based on sequence Tagging, leveraging pre-trained Transformer-based encoders. Our system makes simplistic data augmentations and tweaks in training and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> on a pre-existing system, which makes it less reliant on large amounts of parallel training data, provides more control over the outputs and enables faster <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference speeds</a>. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves near state-of-the-art performance on benchmark test datasets for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is fully non-autoregressive, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves faster inference speeds by over 11 times than the current state-of-the-art text simplification system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.5/>Broad Linguistic Complexity Analysis for Greek Readability Classification<span class=acl-fixed-case>G</span>reek Readability Classification</a></strong><br><a href=/people/s/savvas-chatzipanagiotidis/>Savvas Chatzipanagiotidis</a>
|
<a href=/people/m/maria-giagkou/>Maria Giagkou</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--5><div class="card-body p-3 small">This paper explores the linguistic complexity of Greek textbooks as a readability classification task. We analyze textbook corpora for different school subjects and textbooks for <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> as a Second Language, covering a very wide spectrum of school age groups and proficiency levels. A broad range of quantifiable linguistic complexity features (lexical, morphological and syntactic) are extracted and calculated. Conducting experiments with different <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>feature subsets</a>, we show that the different linguistic dimensions contribute orthogonal information, each contributing towards the highest result achieved using all linguistic feature subsets. A readability classifier trained on this basis reaches a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>classification accuracy</a> of 88.16 % for the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> as a Second Language corpus. To investigate the generalizability of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a>, we also perform cross-corpus evaluations. We show that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> trained on the most varied text collection (for <a href=https://en.wikipedia.org/wiki/Ancient_Greek>Greek</a> as a school subject) generalizes best. In addition to advancing the state of the art for Greek readability analysis, the paper also contributes insights on the role of different feature sets and training setups for generalizable readability classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bea-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.10/>Parsing Argumentative Structure in English-as-Foreign-Language Essays<span class=acl-fixed-case>E</span>nglish-as-Foreign-Language Essays</a></strong><br><a href=/people/j/jan-wira-gotama-putra/>Jan Wira Gotama Putra</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--10><div class="card-body p-3 small">This paper presents a study on parsing the argumentative structure in English-as-foreign-language (EFL) essays, which are inherently noisy. The <a href=https://en.wikipedia.org/wiki/Parsing>parsing process</a> consists of two steps, linking related sentences and then labelling their relations. We experiment with several deep learning architectures to address each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> independently. In the sentence linking task, a biaffine model performed the best. In the relation labelling task, a fine-tuned BERT model performed the best. Two sentence encoders are employed, and we observed that non-fine-tuning models generally performed better when using Sentence-BERT as opposed to BERT encoder. We trained our models using two types of parallel texts : original noisy EFL essays and those improved by annotators, then evaluate them on the original <a href=https://en.wikipedia.org/wiki/Essay>essays</a>. The experiment shows that an end-to-end in-domain system achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of.341. On the other hand, the cross-domain system achieved 94 % performance of the in-domain system. This signals that well-written texts can also be useful to train argument mining system for noisy texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.11/>Training and Domain Adaptation for Supervised Text Segmentation</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--11><div class="card-body p-3 small">Unlike traditional unsupervised text segmentation methods, recent supervised segmentation models rely on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> as the source of large-scale segmentation supervision. These models have, however, predominantly been evaluated on the in-domain (Wikipedia-based) test sets, preventing conclusions about their general segmentation efficacy. In this work, we focus on the domain transfer performance of supervised neural text segmentation in the educational domain. To this end, we first introduce K12Seg, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for evaluation of supervised segmentation, created from educational reading material for grade-1 to college-level students. We then benchmark a hierarchical text segmentation model (HITS), based on RoBERTa, in both in-domain and domain-transfer segmentation experiments. While HITS produces state-of-the-art in-domain performance (on three Wikipedia-based test sets), we show that, subject to the standard full-blown fine-tuning, it is susceptible to domain overfitting. We identify adapter-based fine-tuning as a remedy that substantially improves transfer performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.19/>C-Test Collector : A Proficiency Testing Application to Collect Training Data for C-Tests<span class=acl-fixed-case>C</span>-Test Collector: A Proficiency Testing Application to Collect Training Data for <span class=acl-fixed-case>C</span>-Tests</a></strong><br><a href=/people/c/christian-haring/>Christian Haring</a>
|
<a href=/people/r/rene-lehmann/>Rene Lehmann</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--19><div class="card-body p-3 small">We present the C-Test Collector, a <a href=https://en.wikipedia.org/wiki/Web_application>web-based tool</a> that allows <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a> to test their proficiency level using c-tests. Our tool collects anonymized data on test performance, which allows teachers to gain insights into common error patterns. At the same time, it allows NLP researchers to collect training data for being able to generate c-test variants at the desired difficulty level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bea-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bea-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bea-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bea-1.22/>Sharks are not the threat humans are : Argument Component Segmentation in School Student Essays</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/d/debanjan-ghosh/>Debanjan Ghosh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bea-1--22><div class="card-body p-3 small">Argument mining is often addressed by a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline method</a> where segmentation of text into argumentative units is conducted first and proceeded by an argument component identification task. In this research, we apply a token-level classification to identify claim and premise tokens from a new corpus of argumentative essays written by middle school students. To this end, we compare a variety of state-of-the-art models such as discrete features and deep learning architectures (e.g., BiLSTM networks and BERT-based architectures) to identify the argument components. We demonstrate that a BERT-based multi-task learning architecture (i.e., token and sentence level classification) adaptively pretrained on a relevant unlabeled dataset obtains the best results.</div></div></div><hr><div id=2021bsnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.bsnlp-1/>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.0/>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></strong><br><a href=/people/b/bogdan-babych/>Bogdan Babych</a>
|
<a href=/people/o/olga-kanishcheva/>Olga Kanishcheva</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/v/vasyl-starko/>Vasyl Starko</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.1/>HerBERT : Efficiently Pretrained Transformer-based Language Model for Polish<span class=acl-fixed-case>H</span>er<span class=acl-fixed-case>BERT</span>: Efficiently Pretrained Transformer-based Language Model for <span class=acl-fixed-case>P</span>olish</a></strong><br><a href=/people/r/robert-mroczkowski/>Robert Mroczkowski</a>
|
<a href=/people/p/piotr-rybak/>Piotr Rybak</a>
|
<a href=/people/a/alina-wroblewska/>Alina Wróblewska</a>
|
<a href=/people/i/ireneusz-gawlik/>Ireneusz Gawlik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--1><div class="card-body p-3 small">BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. A <a href=https://en.wikipedia.org/wiki/Procedure_(term)>training procedure</a> designed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, which, unlike the isolating English language, is a <a href=https://en.wikipedia.org/wiki/Fusional_language>fusional language</a>. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other <a href=https://en.wikipedia.org/wiki/Factor_analysis>factors</a> that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model HerBERT is trained. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art results on multiple downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.4/>Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company’s Reputation</a></strong><br><a href=/people/n/nikolay-babakov/>Nikolay Babakov</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--4><div class="card-body p-3 small">Not all topics are equally flammable in terms of <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> : a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or <a href=https://en.wikipedia.org/wiki/Sexual_minority>sexual minorities</a>. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> of collecting and labelling a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for appropriateness. While <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in user-generated data is well-studied, we aim at defining a more fine-grained notion of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a>. The core of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is that it can harm the reputation of a speaker. This is different from <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in two respects : (i) <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> : a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bsnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.6/>RuSentEval : Linguistic Source, Encoder Force !<span class=acl-fixed-case>R</span>u<span class=acl-fixed-case>S</span>ent<span class=acl-fixed-case>E</span>val: Linguistic Source, Encoder Force!</a></strong><br><a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/e/ekaterina-taktasheva/>Ekaterina Taktasheva</a>
|
<a href=/people/e/elina-sigdel/>Elina Sigdel</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--6><div class="card-body p-3 small">The success of pre-trained transformer language models has brought a great deal of interest on how these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> work, and what they learn about language. However, prior research in the field is mainly devoted to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and little is known regarding other languages. To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, including ones that have not been explored yet. We apply a combination of complementary probing methods to explore the distribution of various linguistic properties in five multilingual transformers for two typologically contrasting languages Russian and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.7/>Exploratory Analysis of News Sentiment Using Subgroup Discovery</a></strong><br><a href=/people/a/anita-valmarska/>Anita Valmarska</a>
|
<a href=/people/l/luis-adrian-cabrera-diego/>Luis Adrián Cabrera-Diego</a>
|
<a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--7><div class="card-body p-3 small">In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> in the <a href=https://en.wikipedia.org/wiki/News>news</a>. We propose a methodology that combines Named Entity Recognition and Subgroup Discovery-a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features-Named Entities). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.8/>Creating an Aligned Russian Text Simplification Dataset from Language Learner Data<span class=acl-fixed-case>R</span>ussian Text Simplification Dataset from Language Learner Data</a></strong><br><a href=/people/a/anna-dmitrieva/>Anna Dmitrieva</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--8><div class="card-body p-3 small">Parallel language corpora where regular texts are aligned with their simplified versions can be used in both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistic studies</a>. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Simple English, but many other languages lack such <a href=https://en.wikipedia.org/wiki/Data>data</a>. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian literature texts</a> adapted for learners of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.9/>Multilingual Named Entity Recognition and Matching Using <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and Dedupe for <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>BERT</span> and Dedupe for <span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/m/marko-prelevikj/>Marko Prelevikj</a>
|
<a href=/people/s/slavko-zitnik/>Slavko Zitnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--9><div class="card-body p-3 small">This paper describes the University of Ljubljana (UL FRI) Group&#8217;s submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data. We perform training iteratively and on the concatenated data of previously available NER datasets. For the normalization task we use Stanza lemmatizer, while for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching</a> we implemented a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> using the Dedupe library. The performance of evaluations suggests that multi-source settings outperform less-resourced approaches. The best NER models achieve 0.91 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. In multi-lingual NER setting we achieve <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of 0.82 and 0.74.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bsnlp-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.13/>Benchmarking Pre-trained Language Models for Multilingual NER : TraSpaS at the BSNLP2021 Shared Task<span class=acl-fixed-case>NER</span>: <span class=acl-fixed-case>T</span>ra<span class=acl-fixed-case>S</span>pa<span class=acl-fixed-case>S</span> at the <span class=acl-fixed-case>BSNLP</span>2021 Shared Task</a></strong><br><a href=/people/m/marek-suppa/>Marek Suppa</a>
|
<a href=/people/o/ondrej-jariabka/>Ondrej Jariabka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--13><div class="card-body p-3 small">In this paper we describe TraSpaS, a submission to the third shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop. In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits : character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data. Our code is available at.<url>https://github.com/NaiveNeuron/slavner-2021</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.14/>Named Entity Recognition and Linking Augmented with Large-Scale Structured Data</a></strong><br><a href=/people/p/pawel-rychlikowski/>Paweł Rychlikowski</a>
|
<a href=/people/b/bartlomiej-najdecki/>Bartłomiej Najdecki</a>
|
<a href=/people/a/adrian-lancucki/>Adrian Lancucki</a>
|
<a href=/people/a/adam-kaczmarek/>Adam Kaczmarek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--14><div class="card-body p-3 small">In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the analysis of Named Entities in multilingual Web documents in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a> with rich inflection. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> takes advantage of large collections of both unstructured and structured documents. The former serve as data for unsupervised training of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and embeddings of lexical units. The latter refers to <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and its structured counterpart-Wikidata, our source of <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization rules</a>, and real-world entities. With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.15/>Slav-NER : the 3rd Cross-lingual Challenge on Recognition, Normalization, <a href=https://en.wikipedia.org/wiki/Language_classification>Classification</a>, and Linking of Named Entities across <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>NER</span>: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across <span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/b/bogdan-babych/>Bogdan Babych</a>
|
<a href=/people/z/zara-kancheva/>Zara Kancheva</a>
|
<a href=/people/o/olga-kanishcheva/>Olga Kanishcheva</a>
|
<a href=/people/m/maria-lebedeva/>Maria Lebedeva</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/i/ivaylo-radev/>Ivaylo Radev</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Sikonja</a>
|
<a href=/people/v/vasyl-starko/>Vasyl Starko</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--15><div class="card-body p-3 small">This paper describes Slav-NER : the 3rd Multilingual Named Entity Challenge in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>. The tasks involve recognizing mentions of named entities in <a href=https://en.wikipedia.org/wiki/Web_page>Web documents</a>, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. Detailed valuation information is available on the shared task web page.</div></div></div><hr><div id=2021dravidianlangtech-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.dravidianlangtech-1/>Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.0/>Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/r/ruba-priyadharshini/>Ruba Priyadharshini</a>
|
<a href=/people/a/anand-kumar-m/>Anand Kumar M</a>
|
<a href=/people/p/parameswari-krishnamurthy/>Parameswari Krishnamurthy</a>
|
<a href=/people/e/elizabeth-sherly/>Elizabeth Sherly</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.3.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dravidianlangtech-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.3/>Towards Offensive Language Identification for Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/s/siva-sai/>Siva Sai</a>
|
<a href=/people/y/yashvardhan-sharma/>Yashvardhan Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--3><div class="card-body p-3 small">Offensive speech identification in countries like <a href=https://en.wikipedia.org/wiki/India>India</a> poses several challenges due to the usage of code-mixed and romanized variants of multiple languages by the users in their posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The challenge of offensive language identification on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> is harder, considering the low resources available for the same. In this paper, we explored the zero-shot learning and few-shot learning paradigms based on multilingual language models for offensive speech detection in code-mixed and romanized variants of three Dravidian languages-Malayalam, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, and <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>. We propose a novel and flexible approach of selective translation and <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> to reap better results from fine-tuning and ensembling multilingual transformer networks like XLMRoBERTa and mBERT. We implemented pretrained, fine-tuned, and ensembled versions of XLM-RoBERTa for offensive speech classification. Further, we experimented with interlanguage, inter-task, and multi-task transfer learning techniques to leverage the rich resources available for offensive speech identification in the <a href=https://en.wikipedia.org/wiki/English_language>English language</a> and to enrich the models with <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from related tasks. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yielded good results and are promising for effective offensive speech identification in low resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.4.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.4/>Sentiment Classification of Code-Mixed Tweets using Bi-Directional RNN and Language Tags<span class=acl-fixed-case>RNN</span> and Language Tags</a></strong><br><a href=/people/s/sainik-mahata/>Sainik Mahata</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--4><div class="card-body p-3 small">Sentiment analysis tools and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have been developed extensively throughout the years, for <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>. In contrast, similar <a href=https://en.wikipedia.org/wiki/Tool>tools</a> for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a> are scarce. This is because, state-of-the-art pre-processing tools like POS tagger, shallow parsers, etc., are not readily available for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a>. Although, such working tools for Indian languages, like <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, that are spoken by the majority of the population, are available, finding the same for less spoken languages like, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, is difficult. Moreover, due to the advent of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, the multi-lingual population of India, who are comfortable with both <a href=https://en.wikipedia.org/wiki/English_language>English</a> ad their regional language, prefer to communicate by mixing both languages. This gives rise to massive code-mixed content and automatically annotating them with their respective sentiment labels becomes a challenging task. In this work, we take up a similar challenge of developing a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis model</a> that can work with English-Tamil code-mixed data. The proposed work tries to solve this by using bi-directional LSTMs along with <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>language tagging</a>. Other traditional methods, based on classical machine learning algorithms have also been discussed in the literature, and they also act as the baseline systems to which we will compare our Neural Network based model. The performance of the developed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, based on Neural Network architecture, garnered <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, and F1 scores of 0.59, 0.66, and 0.58 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.11.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.11/>Task-Oriented Dialog Systems for Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/t/tushar-kanakagiri/>Tushar Kanakagiri</a>
|
<a href=/people/k/karthik-radhakrishnan/>Karthik Radhakrishnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--11><div class="card-body p-3 small">Task-oriented dialog systems help a user achieve a particular goal by parsing user requests to execute a particular action. These systems typically require copious amounts of training data to effectively understand the user intent and its corresponding slots. Acquiring large training corpora requires significant manual effort in annotation, rendering its construction infeasible for low-resource languages. In this paper, we present a two-step approach for automatically constructing task-oriented dialogue data in such <a href=https://en.wikipedia.org/wiki/Programming_language>languages</a> by making use of annotated data from high resource languages. First, we use a machine translation (MT) system to translate the utterance and slot information to the target language. Second, we use token prefix matching and mBERT based semantic matching to align the slot tokens to the corresponding tokens in the utterance. We hand-curate a new test dataset in two low-resource Dravidian languages and show the significance and impact of our training dataset construction using a state-of-the-art mBERT model-achieving a Slot F1 of 81.51 (Kannada) and 78.82 (Tamil) on our test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.12/>A Survey on <a href=https://en.wikipedia.org/wiki/Paralinguistics>Paralinguistics</a> in Tamil Speech Processing<span class=acl-fixed-case>T</span>amil Speech Processing</a></strong><br><a href=/people/a/anosha-ignatius/>Anosha Ignatius</a>
|
<a href=/people/u/uthayasanker-thayasivam/>Uthayasanker Thayasivam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--12><div class="card-body p-3 small">Speech carries not only the semantic content but also the paralinguistic information which captures the <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>speaking style</a>. Speaker traits and <a href=https://en.wikipedia.org/wiki/Emotion>emotional states</a> affect how words are being spoken. The research on paralinguistic information is an emerging field in speech and language processing and it has many potential applications including <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, speaker identification and verification, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> and accent recognition. Among them, there is a significant interest in <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> from <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. A detailed study of paralinguistic information present in speech signal and an overview of research work related to speech emotion for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil Language</a> is presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.13.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.13/>Is this Enough?-Evaluation of Malayalam Wordnet<span class=acl-fixed-case>M</span>alayalam <span class=acl-fixed-case>W</span>ordnet</a></strong><br><a href=/people/n/nandu-chandran-nair/>Nandu Chandran Nair</a>
|
<a href=/people/m/maria-chiara-giangregorio/>Maria-chiara Giangregorio</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--13><div class="card-body p-3 small">Quality of a product is the degree to which a product meets the customer&#8217;s expectation, which must also be valid for the case of lexical semantic resources. Conducting a periodic evaluation of resources is essential to ensure if the resources meet a native speaker&#8217;s expectations and free from errors. This paper defines the possible mistakes in a lexical semantic resource and explains the steps applied to quantify Malayalam wordnet quality. Malayalam is one of the classical languages of India. We hope to subset the less quality part of the <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> and perform <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to make it better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.14.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.14/>LA-SACo : A Study of Learning Approaches for Sentiments Analysis inCode-Mixing Texts<span class=acl-fixed-case>LA</span>-<span class=acl-fixed-case>SAC</span>o: A Study of Learning Approaches for Sentiments Analysis in<span class=acl-fixed-case>C</span>ode-Mixing Texts</a></strong><br><a href=/people/f/fazlourrahman-balouchzahi/>Fazlourrahman Balouchzahi</a>
|
<a href=/people/h/h-l-shashirekha/>H L Shashirekha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--14><div class="card-body p-3 small">Substantial amount of text data which is increasingly being generated and shared on the internet and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> every second affect the society positively or negatively almost in any aspect of online world and also business and industries. Sentiments / opinions / reviews&#8217; of users posted on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> are the valuable information that have motivated researchers to analyze them to get better insight and feedbacks about any product such as a video in <a href=https://en.wikipedia.org/wiki/Instagram>Instagram</a>, a movie in <a href=https://en.wikipedia.org/wiki/Netflix>Netflix</a>, or even new brand car introduced by BMW. Sentiments are usually written using a combination of languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a> which is resource rich and regional languages such as <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, etc. which are resource poor. However, due to technical constraints, many users prefer to pen their opinions in <a href=https://en.wikipedia.org/wiki/Roman_script>Roman script</a>. These kinds of texts written in two or more languages using a common language script or different language scripts are called code-mixing texts. Code-mixed texts are increasing day-by-day with the increase in the number of users depending on various online platforms. Analyzing such texts pose a real challenge for the researchers. In view of the challenges posed by the code-mixed texts, this paper describes three proposed models namely, SACo-Ensemble, SACo-Keras, and SACo-ULMFiT using Machine Learning (ML), Deep Learning (DL), and Transfer Learning (TL) approaches respectively for the task of Sentiments Analysis in Tamil-English and Malayalam-English code-mixed texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.16/>Findings of the Shared Task on Troll Meme Classification in Tamil<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--16><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>internet</a> has facilitated its user-base with a platform to communicate and express their views without any censorship. On the other hand, this <a href=https://en.wikipedia.org/wiki/Freedom_of_speech>freedom of expression</a> or <a href=https://en.wikipedia.org/wiki/Freedom_of_speech>free speech</a> can be abused by its user or a troll to demean an individual or a group. Demeaning people based on their gender, <a href=https://en.wikipedia.org/wiki/Sexual_orientation>sexual orientation</a>, religious believes or any other characteristics trolling could cause great distress in the online community. Hence, the content posted by a <a href=https://en.wikipedia.org/wiki/Internet_troll>troll</a> needs to be identified and dealt with before causing any more damage. Amongst all the forms of troll content, <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are most prevalent due to their popularity and ability to propagate across cultures. A <a href=https://en.wikipedia.org/wiki/Internet_troll>troll</a> uses a <a href=https://en.wikipedia.org/wiki/Internet_meme>meme</a> to demean, attack or offend its targetted audience. In this shared task, we provide a resource (TamilMemes) that could be used to train a system capable of identifying a troll meme in the <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil language</a>. In our TamilMemes dataset, each meme has been categorized into either a <a href=https://en.wikipedia.org/wiki/Internet_troll>troll</a> or a not_troll class. Along with the meme images, we also provided the Latin transcripted text from memes. We received 10 system submissions from the participants which were evaluated using the weighted average F1-score. The <a href=https://en.wikipedia.org/wiki/System>system</a> with the weighted average F1-score of 0.55 secured the first rank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.17/>Findings of the Shared Task on Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a><span class=acl-fixed-case>T</span>amil, <span class=acl-fixed-case>M</span>alayalam, and <span class=acl-fixed-case>K</span>annada</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/r/ruba-priyadharshini/>Ruba Priyadharshini</a>
|
<a href=/people/n/navya-jose/>Navya Jose</a>
|
<a href=/people/a/anand-kumar-m/>Anand Kumar M</a>
|
<a href=/people/t/thomas-mandl/>Thomas Mandl</a>
|
<a href=/people/p/prasanna-kumar-kumaresan/>Prasanna Kumar Kumaresan</a>
|
<a href=/people/r/rahul-ponnusamy/>Rahul Ponnusamy</a>
|
<a href=/people/h/hariharan-r-l/>Hariharan R L</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/e/elizabeth-sherly/>Elizabeth Sherly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--17><div class="card-body p-3 small">Detecting <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in <a href=https://en.wikipedia.org/wiki/Language_localisation>local languages</a> is critical for moderating user-generated content. Thus, the field of offensive language identification in under-resourced Tamil, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a> and <a href=https://en.wikipedia.org/wiki/Kannada>Kannada languages</a> are essential. As the <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> is more code-mixed and not well studied for under-resourced languages, it is imperative to create resources and conduct benchmarking studies to encourage research in under-resourced Dravidian languages. We created a shared task on offensive language detection in <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>. We summarize here the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this challenge which are openly available at https://competitions.codalab.org/competitions/27654, and present an overview of the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> and the results of the competing systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.18.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.18.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.18/>GX@DravidianLangTech-EACL2021 : Multilingual Neural Machine Translation and <a href=https://en.wikipedia.org/wiki/Back-translation>Back-translation</a><span class=acl-fixed-case>GX</span>@<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Multilingual Neural Machine Translation and Back-translation</a></strong><br><a href=/people/w/wanying-xie/>Wanying Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--18><div class="card-body p-3 small">In this paper, we describe the GX system in the EACL2021 shared task on machine translation in <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>. Given the low amount of parallel training data, We adopt two methods to improve the overall performance : (1) multilingual translation, we use a shared encoder-decoder multilingual translation model handling multiple languages simultaneously to facilitate the translation performance of these languages ; (2) back-translation, we collected other open-source parallel and monolingual data and apply back-translation to benefit from the monolingual data. The experimental results show that we can achieve satisfactory <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results in these <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> and rank first in English-Telugu and Tamil-Telugu translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.20.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.20/>Simon @ DravidianLangTech-EACL2021 : Detecting Offensive Content in Kannada Language<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Detecting Offensive Content in <span class=acl-fixed-case>K</span>annada Language</a></strong><br><a href=/people/q/qinyu-que/>Qinyu Que</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--20><div class="card-body p-3 small">This article introduces the system for the shared task of Offensive Language Identification in Dravidian Languages-EACL 2021. The world&#8217;s information technology develops at a high speed. People are used to expressing their views and opinions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This leads to a lot of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. As people become more dependent on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, the detection of offensive language becomes more and more necessary. This shared task is in three languages : <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>. Our team takes part in the Kannada language task. To accomplish the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we use the XLM-Roberta model for pre-training. But the capabilities of the XLM-Roberta model do not satisfy us in terms of statement information collection. So we made some tweaks to the output of this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and experiments for accomplishing the task of the <a href=https://en.wikipedia.org/wiki/Kannada>Kannada language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.26.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.26/>Hypers@DravidianLangTech-EACL2021 : Offensive language identification in Dravidian code-mixed YouTube Comments and Posts<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Offensive language identification in <span class=acl-fixed-case>D</span>ravidian code-mixed <span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube Comments and Posts</a></strong><br><a href=/people/c/charangan-vasantharajan/>Charangan Vasantharajan</a>
|
<a href=/people/u/uthayasanker-thayasivam/>Uthayasanker Thayasivam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--26><div class="card-body p-3 small">Code-Mixed Offensive contents are used pervasively in social media posts in the last few years. Consequently, gained the significant attraction of the research community for identifying the different forms of such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a> (e.g., <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, and sentiments) and contributed to the creation of datasets. Most of the recent studies deal with high-resource languages (e.g., English) due to many publicly available datasets, and by the lack of dataset in low-resource anguages, those studies are slightly involved in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Therefore, this study has the focus on offensive language identification on code-mixed low-resourced Dravidian languages such as <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a> using the bidirectional approach and fine-tuning strategies. According to the leaderboard, the proposed model got a 0.96 <a href=https://en.wikipedia.org/wiki/FIVB_World_Rankings>F1-score</a> for <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, 0.73 <a href=https://en.wikipedia.org/wiki/FIVB_World_Rankings>F1-score</a> for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, and 0.70 <a href=https://en.wikipedia.org/wiki/FIVB_World_Rankings>F1-score</a> for <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a> in the bench-mark. Moreover, in the view of multilingual models, this modal ranked 3rd and achieved favorable results and confirmed the model as the best among all systems submitted to these shared tasks in these three languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.29.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.29/>ZYJ123@DravidianLangTech-EACL2021 : Offensive Language Identification based on XLM-RoBERTa with DPCNN<span class=acl-fixed-case>ZYJ</span>123@<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Offensive Language Identification based on <span class=acl-fixed-case>XLM</span>-<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a with <span class=acl-fixed-case>DPCNN</span></a></strong><br><a href=/people/y/yingjia-zhao/>Yingjia Zhao</a>
|
<a href=/people/x/xin-tao/>Xin Tao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--29><div class="card-body p-3 small">The development of online media platforms has given users more opportunities to post and comment freely, but the negative impact of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> has become increasingly apparent. It is very necessary for the automatic identification system of offensive language. This paper describes our work on the task of Offensive Language Identification in Dravidian language-EACL 2021. To complete this task, we propose a <a href=https://en.wikipedia.org/wiki/System>system</a> based on the multilingual model XLM-Roberta and DPCNN. The test results on the official test data set confirm the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a>. The <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average F1-score</a> of <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and <a href=https://en.wikipedia.org/wiki/Tami_language>Tami language</a> are 0.69, 0.92, and 0.76 respectively, ranked 6th, 6th, and 3rd</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.32.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.32/>CUSATNLP@DravidianLangTech-EACL2021 : Language Agnostic Classification of Offensive Content in Tweets<span class=acl-fixed-case>CUSATNLP</span>@<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021:Language Agnostic Classification of Offensive Content in Tweets</a></strong><br><a href=/people/s/sara-renjit/>Sara Renjit</a>
|
<a href=/people/s/sumam-mary-idicula/>Sumam Mary Idicula</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--32><div class="card-body p-3 small">Identifying offensive information from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> is a vital <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing task</a>. This task concentrated more on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other foreign languages these days. In this shared task on Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian Languages</a>, in the First Workshop of Speech and Language Technologies for <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian Languages</a> in EACL 2021, the aim is to identify offensive content from code mixed Dravidian Languages Kannada, Malayalam, and Tamil. Our team used language agnostic BERT (Bidirectional Encoder Representation from Transformers) for <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a> and a Softmax classifier. The language-agnostic representation based classification helped obtain good performance for all the three languages, out of which results for the <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam language</a> are good enough to obtain a third position among the participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.40.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.40.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.40/>Maoqin @ DravidianLangTech-EACL2021 : The Application of Transformer-Based Model<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: The Application of Transformer-Based Model</a></strong><br><a href=/people/m/maoqin-yang/>Maoqin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--40><div class="card-body p-3 small">This paper describes the result of team-Maoqin at DravidianLangTech-EACL2021. The provided task consists of three languages(Tamil, Malayalam, and Kannada), I only participate in one of the language task-Malayalam. The goal of this task is to identify offensive language content of the code-mixed dataset of comments / posts in Dravidian Languages (Tamil-English, Malayalam-English, and Kannada-English) collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This is a <a href=https://en.wikipedia.org/wiki/Comparison_and_contrast_of_classification_schemes_in_linguistics_and_metadata>classification task</a> at the comment / post level. Given a Youtube comment, systems have to classify it into Not-offensive, Offensive-untargeted, Offensive-targeted-individual, Offensive-targeted-group, Offensive-targeted-other, or Not-in-indented-language. I use the transformer-based language model with BiGRU-Attention to complete this task. To prove the validity of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, I also use some other neural network models for comparison. And finally, the team ranks 5th in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with a weighted average F1 score of 0.93 on the private leader board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.41.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.41/>Simon @ DravidianLangTech-EACL2021 : Meme Classification for Tamil with BERT<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Meme Classification for <span class=acl-fixed-case>T</span>amil with <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/q/qinyu-que/>Qinyu Que</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--41><div class="card-body p-3 small">In this paper, we introduce the <a href=https://en.wikipedia.org/wiki/System>system</a> for the task of meme classification for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, submitted by our team. In today&#8217;s society, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has become an important platform for people to communicate. We use <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to share information about ourselves and express our views on things. It has gradually developed a unique form of <a href=https://en.wikipedia.org/wiki/Emotional_expression>emotional expression</a> on <a href=https://en.wikipedia.org/wiki/Internet_meme>social media meme</a>. The <a href=https://en.wikipedia.org/wiki/Meme>meme</a> is an expression that is often ironic. This also gives the meme a unique sense of humor. But it&#8217;s not just positive content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. There&#8217;s also a lot of <a href=https://en.wikipedia.org/wiki/Profanity>offensive content</a>. Meme&#8217;s unique expression makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> often used by some users to post offensive content. Therefore, it is very urgent to detect the offensive content of the meme. Our team uses the natural language processing method to classify the offensive content of the meme. Our team combines the BERT model with the CNN to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to collect statement information. Finally, the F1-score of our team in the official test set is 0.49, and our method ranks 5th.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.45.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.45/>SSNCSE_NLP@DravidianLangTech-EACL2021 : Offensive Language Identification on Multilingual Code Mixing Text<span class=acl-fixed-case>SSNCSE</span>_<span class=acl-fixed-case>NLP</span>@<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021: Offensive Language Identification on Multilingual Code Mixing Text</a></strong><br><a href=/people/b/bharathi-b/>Bharathi B</a>
|
<a href=/people/a/agnusimmaculate-silvia-a/>Agnusimmaculate Silvia A</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--45><div class="card-body p-3 small">Social networks made a huge impact in almost all fields in recent years. Text messaging through the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> or <a href=https://en.wikipedia.org/wiki/Mobile_phone>cellular phones</a> has become a major medium of personal and commercial communication. Everyday we have to deal with <a href=https://en.wikipedia.org/wiki/Text_messaging>texts</a>, <a href=https://en.wikipedia.org/wiki/Email>emails</a> or different types of messages in which there are a variety of attacks and abusive phrases. It is the moderator&#8217;s decision which comments to remove from the platform because of violations and which ones to keep but an automatic software for detecting abusive languages would be useful in recent days. In this paper we describe an automatic offensive language identification from <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> with various <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>. This is work is shared task in DravidanLangTech-EACL2021. The goal of this task is to identify offensive language content of the code-mixed dataset of comments / posts in Dravidian Languages ((Tamil-English, Malayalam-English, and Kannada-English)) collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This work explains the submissions made by SSNCSE_NLP in DravidanLangTech-EACL2021 Code-mix tasks for Offensive language detection. We achieve F1 scores of 0.95 for <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, 0.7 for <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a> and 0.73 for task2-Tamil on the test-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.dravidianlangtech-1.47.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.47/>MUCS@DravidianLangTech-EACL2021 : COOLI-Code-Mixing Offensive Language Identification<span class=acl-fixed-case>MUCS</span>@<span class=acl-fixed-case>D</span>ravidian<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EACL</span>2021:<span class=acl-fixed-case>COOLI</span>-Code-Mixing Offensive Language Identification</a></strong><br><a href=/people/f/fazlourrahman-balouchzahi/>Fazlourrahman Balouchzahi</a>
|
<a href=/people/a/aparna-b-k/>Aparna B K</a>
|
<a href=/people/h/h-l-shashirekha/>H L Shashirekha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--47><div class="card-body p-3 small">This paper describes the models submitted by the team MUCS for Offensive Language Identification in Dravidian Languages-EACL 2021 shared task that aims at identifying and classifying code-mixed texts of three language pairs namely, Kannada-English (Kn-En), Malayalam-English (Ma-En), and Tamil-English (Ta-En) into six predefined categories (5 categories in Ma-En language pair). Two models, namely, COOLI-Ensemble and COOLI-Keras are trained with the char sequences extracted from the sentences combined with words as features. Out of the two proposed models, COOLI-Ensemble model (best among our models) obtained first rank for Ma-En language pair with 0.97 weighted F1-score and fourth and sixth ranks with 0.75 and 0.69 weighted F1-score for Ta-En and Kn-En language pairs respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dravidianlangtech-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dravidianlangtech-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dravidianlangtech-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dravidianlangtech-1.51/>OffTamil@DravideanLangTech-EASL2021 : Offensive Language Identification in Tamil Text<span class=acl-fixed-case>O</span>ff<span class=acl-fixed-case>T</span>amil@<span class=acl-fixed-case>D</span>ravidean<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>T</span>ech-<span class=acl-fixed-case>EASL</span>2021: Offensive Language Identification in <span class=acl-fixed-case>T</span>amil Text</a></strong><br><a href=/people/d/disne-sivalingam/>Disne Sivalingam</a>
|
<a href=/people/s/sajeetha-thavareesan/>Sajeetha Thavareesan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dravidianlangtech-1--51><div class="card-body p-3 small">In the last few decades, Code-Mixed Offensive texts are used penetratingly in social media posts. Social media platforms and <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> showed much interest on offensive text identification in recent years. Consequently, research community is also interested in identifying such content and also contributed to the development of corpora. Many publicly available corpora are there for research on identifying offensive text written in <a href=https://en.wikipedia.org/wiki/English_language>English language</a> but rare for low resourced languages like <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>. The first code-mixed offensive text for <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> are developed by shared task organizers which is used for this study. This study focused on offensive language identification on code-mixed low-resourced Dravidian language Tamil using four classifiers (Support Vector Machine, random forest, k- Nearest Neighbour and Naive Bayes) using chi2 feature selection technique along with BoW and TF-IDF feature representation techniques using different combinations of n-grams. This proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 76.96 % while using linear SVM with TF-IDF feature representation technique.</div></div></div><hr><div id=2021gwc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.gwc-1/>Proceedings of the 11th Global Wordnet Conference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.0/>Proceedings of the 11th Global Wordnet Conference</a></strong><br><a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/c/christiane-fellbaum/>Christiane Fellbaum</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.1/>On Universal Colexifications</a></strong><br><a href=/people/h/hongchang-bao/>Hongchang Bao</a>
|
<a href=/people/b/bradley-hauer/>Bradley Hauer</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--1><div class="card-body p-3 small">Colexification occurs when two distinct concepts are lexified by the same word. The term covers both <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> and <a href=https://en.wikipedia.org/wiki/Homonym>homonymy</a>. We posit and investigate the hypothesis that no pair of <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> are colexified in every language. We test our hypothesis by analyzing colexification data from <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>, Open Multilingual WordNet, and CLICS. The results show that our hypothesis is supported by over 99.9 % of colexified concept pairs in these three <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gwc-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.3/>Practical Approach on Implementation of WordNets for <a href=https://en.wikipedia.org/wiki/Languages_of_South_Africa>South African Languages</a><span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>ets for <span class=acl-fixed-case>S</span>outh <span class=acl-fixed-case>A</span>frican Languages</a></strong><br><a href=/people/t/tshephisho-joseph-sefara/>Tshephisho Joseph Sefara</a>
|
<a href=/people/t/tumisho-billson-mokgonyane/>Tumisho Billson Mokgonyane</a>
|
<a href=/people/v/vukosi-marivate/>Vukosi Marivate</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--3><div class="card-body p-3 small">This paper proposes the implementation of WordNets for five South African languages, namely, <a href=https://en.wikipedia.org/wiki/Northern_Sotho_language>Sepedi</a>, <a href=https://en.wikipedia.org/wiki/Tswana_language>Setswana</a>, <a href=https://en.wikipedia.org/wiki/Tshivenda_language>Tshivenda</a>, <a href=https://en.wikipedia.org/wiki/Zulu_language>isiZulu</a> and <a href=https://en.wikipedia.org/wiki/Xhosa_language>isiXhosa</a> to be added to open multilingual WordNets (OMW) on natural language toolkit (NLTK). The African WordNets are converted from Princeton WordNet (PWN) 2.0 to 3.0 to match the synsets in PWN 3.0. After conversion, there were 7157, 11972, 1288, 6380, and 9460 lemmas for <a href=https://en.wikipedia.org/wiki/Northern_Sotho_language>Sepedi</a>, <a href=https://en.wikipedia.org/wiki/Tswana_language>Setswana</a>, <a href=https://en.wikipedia.org/wiki/Tshivenda_language>Tshivenda</a>, <a href=https://en.wikipedia.org/wiki/Zulu_language>isiZulu</a> and isiX- hosa respectively. Setswana, <a href=https://en.wikipedia.org/wiki/Xhosa_language>isiXhosa</a>, <a href=https://en.wikipedia.org/wiki/Northern_Sotho_language>Sepedi</a> contains more lemmas compared to 8 languages in OMW and <a href=https://en.wikipedia.org/wiki/Zulu_language>isiZulu</a> contains more lemmas compared to 7 languages in OMW. A library has been published for continuous development of African WordNets in OMW using <a href=https://en.wikipedia.org/wiki/NLTK>NLTK</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.6/>Ask2Transformers : Zero-Shot Domain labelling with Pretrained Language Models<span class=acl-fixed-case>A</span>sk2<span class=acl-fixed-case>T</span>ransformers: Zero-Shot Domain labelling with Pretrained Language Models</a></strong><br><a href=/people/o/oscar-sainz/>Oscar Sainz</a>
|
<a href=/people/g/german-rigau/>German Rigau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--6><div class="card-body p-3 small">In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision. Furthermore, the <a href=https://en.wikipedia.org/wiki/System>system</a> is not restricted to use a particular set of <a href=https://en.wikipedia.org/wiki/Domain_name>domain labels</a>. We exploit the knowledge encoded within different off-the-shelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-the-art on the English dataset used in the evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.9/>Monolingual Word Sense Alignment as a Classification Problem</a></strong><br><a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--9><div class="card-body p-3 small">Words are defined based on their meanings in various ways in different resources. Aligning word senses across monolingual lexicographic resources increases domain coverage and enables integration and incorporation of data. In this paper, we explore the application of classification methods using manually-extracted features along with representation learning techniques in the task of word sense alignment and semantic relationship detection. We demonstrate that the performance of classification methods dramatically varies based on the type of semantic relationships due to the nature of the task but outperforms the previous experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gwc-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.11/>The GlobalWordNet Formats : Updates for 2020<span class=acl-fixed-case>G</span>lobal<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Formats: Updates for 2020</a></strong><br><a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/m/michael-wayne-goodman/>Michael Wayne Goodman</a>
|
<a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/a/alexandre-rademaker/>Alexandre Rademaker</a>
|
<a href=/people/e/ewa-rudnicka/>Ewa Rudnicka</a>
|
<a href=/people/l/luis-morgado-da-costa/>Luis Morgado Da Costa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--11><div class="card-body p-3 small">The Global Wordnet Formats have been introduced to enable wordnets to have a common representation that can be integrated through the Global WordNet Grid. As a result of their adoption, a number of shortcomings of the <a href=https://en.wikipedia.org/wiki/File_format>format</a> were identified, and in this paper we describe the extensions to the <a href=https://en.wikipedia.org/wiki/File_format>formats</a> that address these issues. These include : ordering of senses, dependencies between wordnets, pronunciation, syntactic modelling, relations, sense keys, <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> and RDF support. Furthermore, we provide some perspectives on how these changes help in the integration of <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.13/>Semantic Analysis of Verb-Noun Derivation in Princeton WordNet<span class=acl-fixed-case>P</span>rinceton <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/v/verginica-mititelu/>Verginica Mititelu</a>
|
<a href=/people/s/svetlozara-leseva/>Svetlozara Leseva</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--13><div class="card-body p-3 small">We present here the results of a morphosemantic analysis of the verb-noun pairs in the Princeton WordNet as reflected in the standoff file containing pairs annotated with a set of 14 semantic relations. We have automatically distinguished between <a href=https://en.wikipedia.org/wiki/Zero-derivation>zero-derivation</a> and affixal derivation in the <a href=https://en.wikipedia.org/wiki/Data>data</a> and identified the <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> and manually checked the results. The data show that for each semantic relation an <a href=https://en.wikipedia.org/wiki/Affix>affix</a> prevails in creating new words, although we can not talk about their specificity with respect to such a <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a>. Moreover, certain pairs of verb-noun semantic primes are better represented for each semantic relation, and some semantic clusters (in the form of WordNet subtrees) take shape as a result. We thus employ a large-scale data-driven linguistically motivated analysis afforded by the rich derivational and morphosemantic description in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to the end of capturing finer regularities in the process of derivation as represented in the semantic properties of the words involved and as reflected in the structure of the lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.22/>OdeNet : Compiling a GermanWordNet from other Resources<span class=acl-fixed-case>O</span>de<span class=acl-fixed-case>N</span>et: Compiling a <span class=acl-fixed-case>G</span>erman<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et from other Resources</a></strong><br><a href=/people/m/melanie-siegel/>Melanie Siegel</a>
|
<a href=/people/f/francis-bond/>Francis Bond</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--22><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Princeton_WordNet>Princeton WordNet</a> for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a> has been used worldwide in NLP projects for many years. With the OMW initiative, <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a> for different languages of the world are being linked via identifiers. The parallel development and linking allows new multilingual application perspectives. The development of a <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> for the <a href=https://en.wikipedia.org/wiki/German_language>German language</a> is also in this context. To save development time, existing resources were combined and recompiled. The result was then evaluated and improved. In a relatively short time a resource was created that can be used in projects and continuously improved and extended.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.24/>Text Document Clustering : <a href=https://en.wikipedia.org/wiki/Wordnet>Wordnet</a> vs. TF-IDF vs. Word Embeddings<span class=acl-fixed-case>W</span>ordnet vs. <span class=acl-fixed-case>TF</span>-<span class=acl-fixed-case>IDF</span> vs. Word Embeddings</a></strong><br><a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/m/mateusz-gniewkowski/>Mateusz Gniewkowski</a>
|
<a href=/people/t/tomasz-walkowiak/>Tomasz Walkowiak</a>
|
<a href=/people/m/marcin-bedkowski/>Marcin Będkowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--24><div class="card-body p-3 small">In the paper, we deal with the problem of unsupervised text document clustering for the <a href=https://en.wikipedia.org/wiki/Polish_language>Polish language</a>. Our goal is to compare the modern approaches based on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> (doc2vec and BERT) with the classical <a href=https://en.wikipedia.org/wiki/List_of_programming_languages_by_type>ones</a>, i.e., <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a> and wordnet-based. The experiments are conducted on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing qualification descriptions. The experiments&#8217; results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.26/>Neural Language Models vs Wordnet-based Semantically Enriched Representation in CST Relation Recognition<span class=acl-fixed-case>W</span>ordnet-based Semantically Enriched Representation in <span class=acl-fixed-case>CST</span> Relation Recognition</a></strong><br><a href=/people/a/arkadiusz-janz/>Arkadiusz Janz</a>
|
<a href=/people/m/maciej-piasecki/>Maciej Piasecki</a>
|
<a href=/people/p/piotr-watorski/>Piotr Wątorski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--26><div class="card-body p-3 small">Neural language models, including transformer-based models, that are pre-trained on very large corpora became a common way to represent text in various tasks, including recognition of textual semantic relations, e.g. Cross-document Structure Theory. Pre-trained models are usually fine tuned to downstream tasks and the obtained vectors are used as an input for deep neural classifiers. No linguistic knowledge obtained from resources and tools is utilised. In this paper we compare such universal approaches with a combination of rich graph-based linguistically motivated sentence representation and a typical neural network classifier applied to a task of recognition of CST relation in <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>. The representation describes selected levels of the sentence structure including description of lexical meanings on the basis of the wordnet (plWordNet) synsets and connected SUMO concepts. The obtained results show that in the case of difficult relations and medium size training corpus semantically enriched text representation leads to significantly better results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.27/>What is on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> that is not in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>? A Preliminary Analysis on the TwitterAAE Corpus<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et? A Preliminary Analysis on the <span class=acl-fixed-case>T</span>witter<span class=acl-fixed-case>AAE</span> Corpus</a></strong><br><a href=/people/c/cecilia-domingo/>Cecilia Domingo</a>
|
<a href=/people/t/tatiana-gonzalez-ferrero/>Tatiana Gonzalez-Ferrero</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--27><div class="card-body p-3 small">Natural Language Processing tools and resources have been so far mainly created and trained for standard varieties of language. Nowadays, with the use of large amounts of data gathered from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, other varieties and registers need to be processed, which may present other challenges and difficulties. In this work, we focus on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and we present a preliminary analysis by comparing the TwitterAAE corpus, which is annotated for ethnicity, and <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> by quantifying and explaining the online language that <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> misses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.30/>Toward the creation of <a href=https://en.wikipedia.org/wiki/WordNet>WordNets</a> for ancient Indo-European languages<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>ets for ancient <span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>E</span>uropean languages</a></strong><br><a href=/people/e/erica-biagetti/>Erica Biagetti</a>
|
<a href=/people/c/chiara-zanchi/>Chiara Zanchi</a>
|
<a href=/people/w/william-michael-short/>William Michael Short</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--30><div class="card-body p-3 small">This paper presents the work in progress toward the creation of a family of WordNets for <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, <a href=https://en.wikipedia.org/wiki/Ancient_Greek>Ancient Greek</a>, and <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>. Building on previous attempts in the field, we elaborate these efforts bridging together WordNet relational semantics with theories of meaning from <a href=https://en.wikipedia.org/wiki/Cognitive_linguistics>Cognitive Linguistics</a>. We discuss some of the innovations we have introduced to the WordNet architecture, to better capture the polysemy of words, as well as Indo-European language family-specific features. We conclude the paper framing our work within the larger picture of resources available for ancient languages and showing that WordNet-backed search tools have the potential to re-define the kinds of questions that can be asked of ancient language corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gwc-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gwc-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gwc-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gwc-1.32/>Teaching Through Tagging Interactive Lexical Semantics</a></strong><br><a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/a/andrew-devadason/>Andrew Devadason</a>
|
<a href=/people/m/melissa-rui-lin-teo/>Melissa Rui Lin Teo</a>
|
<a href=/people/l/luis-morgado-da-costa/>Luís Morgado da Costa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gwc-1--32><div class="card-body p-3 small">In this paper we discuss an ongoing effort to enrich students&#8217; learning by involving them in sense tagging. The main goal is to lead students to discover how we can represent meaning and where the limits of our current theories lie. A subsidiary goal is to create sense tagged corpora and an accompanying linked lexicon (in our case wordnets). We present the results of tagging several texts and suggest some ways in which the tagging process could be improved. Two authors of this paper present their own experience as students. Overall, students reported that they found the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a> an enriching experience. The annotated corpora and changes to the <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> are made available through the NTU multilingual corpus and associated wordnets (NTU-MC).</div></div></div><hr><div id=2021hackashop-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.hackashop-1/>Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.0/>Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation</a></strong><br><a href=/people/h/hannu-toivonen/>Hannu Toivonen</a>
|
<a href=/people/m/michele-boggia/>Michele Boggia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.1/>Adversarial Training for News Stance Detection : Leveraging Signals from a Multi-Genre Corpus.</a></strong><br><a href=/people/c/costanza-conforti/>Costanza Conforti</a>
|
<a href=/people/j/jakob-berndt/>Jakob Berndt</a>
|
<a href=/people/m/marco-basaldella/>Marco Basaldella</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/c/chryssi-giannitsarou/>Chryssi Giannitsarou</a>
|
<a href=/people/f/flavio-toxvaerd/>Flavio Toxvaerd</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--1><div class="card-body p-3 small">Cross-target generalization constitutes an important issue for news Stance Detection (SD). In this short paper, we investigate adversarial cross-genre SD, where knowledge from annotated user-generated data is leveraged to improve news SD on targets unseen during training. We implement a BERT-based adversarial network and show experimental performance improvements over a set of strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Given the abundance of user-generated data, which are considerably less expensive to retrieve and annotate than <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, this constitutes a promising research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.10/>Aligning Estonian and Russian news industry keywords with the help of subtitle translations and an environmental thesaurus<span class=acl-fixed-case>E</span>stonian and <span class=acl-fixed-case>R</span>ussian news industry keywords with the help of subtitle translations and an environmental thesaurus</a></strong><br><a href=/people/a/andraz-repar/>Andraž Repar</a>
|
<a href=/people/a/andrej-shumakov/>Andrej Shumakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--10><div class="card-body p-3 small">This paper presents the implementation of a bilingual term alignment approach developed by Repar et al. (2019) to a dataset of unaligned Estonian and Russian keywords which were manually assigned by journalists to describe the article topic. We started by separating the dataset into Estonian and Russian tags based on whether they are written in the Latin or Cyrillic script. Then we selected the available language-specific resources necessary for the <a href=https://en.wikipedia.org/wiki/Alignment_system>alignment system</a> to work. Despite the domains of the language-specific resources (subtitles and environment) not matching the domain of the dataset (news articles), we were able to achieve respectable results with manual evaluation indicating that almost 3/4 of the aligned keyword pairs are at least partial matches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.12/>Comment Section <a href=https://en.wikipedia.org/wiki/Personalization>Personalization</a> : Algorithmic, Interface, and Interaction Design</a></strong><br><a href=/people/y/yixue-wang/>Yixue Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--12><div class="card-body p-3 small">Comment sections allow users to share their personal experiences, discuss and form different opinions, and build communities out of organic conversations. However, many comment sections present chronological ranking to all users. In this paper, I discuss personalization approaches in comment sections based on different objectives for newsrooms and researchers to consider. I propose algorithmic and interface designs when personalizing the presentation of comments based on different objectives including <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a>, and education / background information. I further explain how <a href=https://en.wikipedia.org/wiki/Transparency_(behavior)>transparency</a>, user control, and comment type diversity could help users most benefit from the personalized interacting experience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.14/>EMBEDDIA Tools, Datasets and Challenges : Resources and Hackathon Contributions<span class=acl-fixed-case>EMBEDDIA</span> Tools, Datasets and Challenges: Resources and Hackathon Contributions</a></strong><br><a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/m/michele-boggia/>Michele Boggia</a>
|
<a href=/people/r/ravi-shekhar/>Ravi Shekhar</a>
|
<a href=/people/m/marko-pranjic/>Marko Pranjić</a>
|
<a href=/people/s/salla-salmela/>Salla Salmela</a>
|
<a href=/people/i/ivar-krustok/>Ivar Krustok</a>
|
<a href=/people/t/tarmo-paju/>Tarmo Paju</a>
|
<a href=/people/c/carl-gustav-linden/>Carl-Gustav Linden</a>
|
<a href=/people/l/leo-leppanen/>Leo Leppänen</a>
|
<a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/m/matej-ulcar/>Matej Ulčar</a>
|
<a href=/people/l/linda-freienthal/>Linda Freienthal</a>
|
<a href=/people/s/silver-traat/>Silver Traat</a>
|
<a href=/people/l/luis-adrian-cabrera-diego/>Luis Adrián Cabrera-Diego</a>
|
<a href=/people/m/matej-martinc/>Matej Martinc</a>
|
<a href=/people/n/nada-lavrac/>Nada Lavrač</a>
|
<a href=/people/b/blaz-skrlj/>Blaž Škrlj</a>
|
<a href=/people/m/martin-znidarsic/>Martin Žnidaršič</a>
|
<a href=/people/a/andraz-pelicon/>Andraž Pelicon</a>
|
<a href=/people/b/boshko-koloski/>Boshko Koloski</a>
|
<a href=/people/v/vid-podpecan/>Vid Podpečan</a>
|
<a href=/people/j/janez-kranjc/>Janez Kranjc</a>
|
<a href=/people/s/shane-sheehan/>Shane Sheehan</a>
|
<a href=/people/e/emanuela-boros/>Emanuela Boros</a>
|
<a href=/people/j/jose-g-moreno/>Jose G. Moreno</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a>
|
<a href=/people/h/hannu-toivonen/>Hannu Toivonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--14><div class="card-body p-3 small">This paper presents tools and data sources collected and released by the EMBEDDIA project, supported by the European Union&#8217;s Horizon 2020 research and innovation program. The collected resources were offered to participants of a <a href=https://en.wikipedia.org/wiki/Hackathon>hackathon</a> organized as part of the EACL Hackashop on News Media Content Analysis and Automated Report Generation in February 2021. The <a href=https://en.wikipedia.org/wiki/Hackathon>hackathon</a> had six participating teams who addressed different challenges, either from the list of proposed challenges or their own news-industry-related tasks. This paper goes beyond the scope of the <a href=https://en.wikipedia.org/wiki/Hackathon>hackathon</a>, as it brings together in a coherent and compact form most of the resources developed, collected and released by the EMBEDDIA project. Moreover, it constitutes a handy source for <a href=https://en.wikipedia.org/wiki/News_media>news media industry</a> and researchers in the fields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> and <a href=https://en.wikipedia.org/wiki/Social_science>Social Science</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.15/>A COVID-19 news coverage mood map of Europe<span class=acl-fixed-case>COVID</span>-19 news coverage mood map of <span class=acl-fixed-case>E</span>urope</a></strong><br><a href=/people/f/frankie-robertson/>Frankie Robertson</a>
|
<a href=/people/j/jarkko-lagus/>Jarkko Lagus</a>
|
<a href=/people/k/kaisla-kajava/>Kaisla Kajava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--15><div class="card-body p-3 small">We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different languages across Europe. The dashboard shows analyses for positive / neutral / negative sentiment and <a href=https://en.wikipedia.org/wiki/Morality>moral sentiment</a> for <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> across countries and languages. First we extract <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> from news-crawl. Then we use a pre-trained multilingual BERT model for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of news article headlines and a dictionary and word vectors -based method for moral sentiment analysis of news articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall sentiment, and the region and language of publication from the period starting from the beginning of January 2020 to the end of January 2021.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hackashop-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hackashop-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hackashop-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hackashop-1.16/>Interesting cross-border news discovery using cross-lingual article linking and document similarity</a></strong><br><a href=/people/b/boshko-koloski/>Boshko Koloski</a>
|
<a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/t/timen-stepisnik-perdih/>Timen Stepišnik-Perdih</a>
|
<a href=/people/b/blaz-skrlj/>Blaž Škrlj</a>
|
<a href=/people/t/tarmo-paju/>Tarmo Paju</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hackashop-1--16><div class="card-body p-3 small">Team Name : team-8 Embeddia Tool : Cross-Lingual Document Retrieval Zosa et al. Dataset : Estonian and Latvian news datasets abstract : Contemporary news media face increasing amounts of available data that can be of use when prioritizing, selecting and discovering new news. In this work we propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for retrieving interesting articles in a cross-border news discovery setting. More specifically, we explore how a set of seed documents in <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a> can be projected in Latvian document space and serve as a basis for discovery of novel interesting pieces of Latvian news that would interest Estonian readers. The proposed methodology was evaluated by Estonian journalist who confirmed that in the best setting, from top 10 retrieved Latvian documents, half of them represent news that are potentially interesting to be taken by the Estonian media house and presented to Estonian readers.</div></div></div><hr><div id=2021hcinlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.hcinlp-1/>Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.0/>Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing</a></strong><br><a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/m/michael-madaio/>Michael Madaio</a>
|
<a href=/people/b/brendan-o-connor/>Brendan O'Connor</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/q/qian-yang/>Qian Yang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hcinlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hcinlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.4/>Towards Human-Centered Summarization : A Case Study on Financial News</a></strong><br><a href=/people/t/tatiana-passali/>Tatiana Passali</a>
|
<a href=/people/a/alexios-gidiotis/>Alexios Gidiotis</a>
|
<a href=/people/e/efstathios-chatzikyriakidis/>Efstathios Chatzikyriakidis</a>
|
<a href=/people/g/grigorios-tsoumakas/>Grigorios Tsoumakas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hcinlp-1--4><div class="card-body p-3 small">Recent Deep Learning (DL) summarization models greatly outperform traditional summarization methodologies, generating high-quality summaries. Despite their success, there are still important open issues, such as the limited engagement and trust of users in the whole process. In order to overcome these issues, we reconsider the task of summarization from a human-centered perspective. We propose to integrate a <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> with an underlying DL model, instead of tackling <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> as an isolated task from the end user. We present a novel <a href=https://en.wikipedia.org/wiki/System>system</a>, where the user can actively participate in the whole <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization process</a>. We also enable the user to gather insights into the causative factors that drive the model&#8217;s behavior, exploiting the self-attention mechanism. We focus on the financial domain, in order to demonstrate the efficiency of generic DL models for domain-specific applications. Our work takes a first step towards a model-interface co-design approach, where DL models evolve along user needs, paving the way towards human-computer text summarization interfaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hcinlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hcinlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.5/>Methods for the Design and Evaluation of HCI+NLP Systems<span class=acl-fixed-case>HCI</span>+<span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/h/hendrik-heuer/>Hendrik Heuer</a>
|
<a href=/people/d/daniel-buschek/>Daniel Buschek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hcinlp-1--5><div class="card-body p-3 small">HCI and <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> traditionally focus on different evaluation methods. While <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>HCI</a> involves a small number of people directly and deeply, <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> traditionally relies on standardized benchmark evaluations that involve a larger number of people indirectly. We present five methodological proposals at the intersection of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>HCI</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and situate them in the context of ML-based NLP models. Our goal is to foster interdisciplinary collaboration and progress in both fields by emphasizing what the fields can learn from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hcinlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hcinlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.11/>Challenges in Designing Natural Language Interfaces for Complex Visual Models</a></strong><br><a href=/people/h/henrik-voigt/>Henrik Voigt</a>
|
<a href=/people/m/monique-meuschke/>Monique Meuschke</a>
|
<a href=/people/k/kai-lawonn/>Kai Lawonn</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hcinlp-1--11><div class="card-body p-3 small">Intuitive interaction with visual models becomes an increasingly important task in the field of Visualization (VIS) and verbal interaction represents a significant aspect of it. Vice versa, modeling verbal interaction in visual environments is a major trend in ongoing research in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. To date, research on Language & Vision, however, mostly happens at the intersection of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and Computer Vision (CV), and much less at the intersection of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>Visualization</a>, which is an important area in Human-Computer Interaction (HCI). This paper presents a brief survey of recent work on interactive tasks and set-ups in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>Visualization</a>. We discuss the respective methods, show interesting gaps, and conclude by suggesting neural, visually grounded dialogue modeling as a promising potential for NLIs for visual models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hcinlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hcinlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.17/>RE-AIMing Predictive Text<span class=acl-fixed-case>RE</span>-<span class=acl-fixed-case>AIM</span>ing Predictive Text</a></strong><br><a href=/people/m/matthew-higgs/>Matthew Higgs</a>
|
<a href=/people/c/claire-mccallum/>Claire McCallum</a>
|
<a href=/people/s/selina-sutton/>Selina Sutton</a>
|
<a href=/people/m/mark-warner/>Mark Warner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hcinlp-1--17><div class="card-body p-3 small">Our increasing reliance on <a href=https://en.wikipedia.org/wiki/Mobile_app>mobile applications</a> means much of our communication is mediated with the support of <a href=https://en.wikipedia.org/wiki/Predictive_text>predictive text systems</a>. How do these <a href=https://en.wikipedia.org/wiki/System>systems</a> impact <a href=https://en.wikipedia.org/wiki/Interpersonal_communication>interpersonal communication</a> and broader society? In what ways are <a href=https://en.wikipedia.org/wiki/Predictive_text>predictive text systems</a> harmful, to whom, and why? In this paper, we focus on <a href=https://en.wikipedia.org/wiki/Predictive_text>predictive text systems</a> on <a href=https://en.wikipedia.org/wiki/Mobile_device>mobile devices</a> and attempt to answer these questions. We introduce the concept of a &#8216;text entry intervention&#8217; as a way to evaluate <a href=https://en.wikipedia.org/wiki/Predictive_text>predictive text systems</a> through an interventional lens, and consider the Reach, Effectiveness, Adoption, Implementation, and Maintenance (RE-AIM) of <a href=https://en.wikipedia.org/wiki/Predictive_text>predictive text systems</a>. We finish with a discussion of opportunities for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.hcinlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--hcinlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.hcinlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.hcinlp-1.18/>How do people interact with biased text prediction models while writing?</a></strong><br><a href=/people/a/advait-bhat/>Advait Bhat</a>
|
<a href=/people/s/saaket-agashe/>Saaket Agashe</a>
|
<a href=/people/a/anirudha-joshi/>Anirudha Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--hcinlp-1--18><div class="card-body p-3 small">Recent studies have shown that a bias in thetext suggestions system can percolate in theuser&#8217;s writing. In this pilot study, we ask thequestion : How do people interact with text pre-diction models, in an inline next phrase sugges-tion interface and how does introducing senti-ment bias in the text prediction model affecttheir writing? We present a pilot study as afirst step to answer this question.</div></div></div><hr><div id=2021humeval-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.humeval-1/>Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.humeval-1.0/>Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</a></strong><br><a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--humeval-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.humeval-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=P0SWVm30MFM" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.humeval-1.3/>Trading Off Diversity and Quality in Natural Language Generation</a></strong><br><a href=/people/h/hugh-zhang/>Hugh Zhang</a>
|
<a href=/people/d/daniel-duckworth/>Daniel Duckworth</a>
|
<a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--humeval-1--3><div class="card-body p-3 small">For open-ended language generation tasks such as <a href=https://en.wikipedia.org/wiki/Storytelling>storytelling</a> or <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, choosing the right decoding algorithm is vital for controlling the tradeoff between generation quality and diversity. However, there presently exists no consensus on which <a href=https://en.wikipedia.org/wiki/Decoding_methods>decoding procedure</a> is best or even the criteria by which to compare them. In this paper, we cast <a href=https://en.wikipedia.org/wiki/Decoding_methods>decoding</a> as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of <a href=https://en.wikipedia.org/wiki/Decoding_methods>decoding methods</a> along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap : the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms.<i>quality</i> and <i>diversity</i>. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--humeval-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.humeval-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.humeval-1.5.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=myG72lA2hpo" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.humeval-1.5/>Is This Translation Error Critical? : Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors</a></strong><br><a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/k/kosuke-takahashi/>Kosuke Takahashi</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--humeval-1--5><div class="card-body p-3 small">This paper discusses a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification-based approach</a> to machine translation evaluation, as opposed to a common <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression-based approach</a> in the WMT Metrics task. Recent <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. The human evaluation corpus will be publicly available upon publication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--humeval-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.humeval-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.humeval-1.6/>Towards Objectively Evaluating the Quality of Generated Medical Summaries</a></strong><br><a href=/people/f/francesco-moramarco/>Francesco Moramarco</a>
|
<a href=/people/d/damir-juric/>Damir Juric</a>
|
<a href=/people/a/aleksandar-savkov/>Aleksandar Savkov</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--humeval-1--6><div class="card-body p-3 small">We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, <a href=https://en.wikipedia.org/wiki/F-score>f-score</a>, and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is of paramount importance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--humeval-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.humeval-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.humeval-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.humeval-1.10/>Reliability of Human Evaluation for Text Summarization : Lessons Learned and Challenges Ahead</a></strong><br><a href=/people/n/neslihan-iskender/>Neslihan Iskender</a>
|
<a href=/people/t/tim-polzehl/>Tim Polzehl</a>
|
<a href=/people/s/sebastian-moller/>Sebastian Möller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--humeval-1--10><div class="card-body p-3 small">Only a small portion of research papers with human evaluation for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> provide information about the participant demographics, task design, and experiment protocol. Additionally, many researchers use human evaluation as gold standard without questioning the reliability or investigating the factors that might affect the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the human evaluation. As a result, there is a lack of best practices for reliable human summarization evaluation grounded by empirical evidence. To investigate human evaluation reliability, we conduct a series of human evaluation experiments, provide an overview of participant demographics, <a href=https://en.wikipedia.org/wiki/Design_of_experiments>task design</a>, experimental set-up and compare the results from different experiments. Based on our empirical analysis, we provide guidelines to ensure the reliability of expert and non-expert evaluations, and we determine the factors that might affect the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.humeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--humeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.humeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=z-O6zZJDxOY" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.humeval-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.humeval-1.15/>Interrater Disagreement Resolution : A Systematic Procedure to Reach Consensus in Annotation Tasks</a></strong><br><a href=/people/y/yvette-oortwijn/>Yvette Oortwijn</a>
|
<a href=/people/t/thijs-ossenkoppele/>Thijs Ossenkoppele</a>
|
<a href=/people/a/arianna-betti/>Arianna Betti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--humeval-1--15><div class="card-body p-3 small">We present a systematic procedure for interrater disagreement resolution. The <a href=https://en.wikipedia.org/wiki/Subroutine>procedure</a> is general, but of particular use in multiple-annotator tasks geared towards ground truth construction. We motivate our proposal by arguing that, barring cases in which the researchers&#8217; goal is to elicit different viewpoints, interrater disagreement is a sign of poor quality in the design or the description of a task. Consensus among annotators, we maintain, should be striven for, through a systematic procedure for disagreement resolution such as the one we describe.</div></div></div><hr><div id=2021lantern-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.lantern-1/>Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lantern-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lantern-1.0/>Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></strong><br><a href=/people/m/marius-mosbach/>Marius Mosbach</a>
|
<a href=/people/m/michael-a-hedderich/>Michael A. Hedderich</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/a/aditya-mogadala/>Aditya Mogadala</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/z/zeynep-akata/>Zeynep Akata</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lantern-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lantern-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lantern-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lantern-1.2/>Visual Grounding Strategies for Text-Only Natural Language Processing</a></strong><br><a href=/people/d/damien-sileo/>Damien Sileo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lantern-1--2><div class="card-body p-3 small">Visual grounding is a promising path toward more robust and accurate Natural Language Processing (NLP) models. Many multimodal extensions of BERT (e.g., VideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that lead to state-of-the-art results on multimodal tasks such as Visual Question Answering. Here, we leverage multimodal modeling for purely textual tasks (language modeling and classification) with the expectation that the multimodal pretraining provides a grounding that can improve text processing accuracy. We propose possible <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategies</a> in this respect. A first type of strategy, referred to as transferred grounding consists in applying multimodal models to text-only tasks using a <a href=https://en.wikipedia.org/wiki/Placeholder_name>placeholder</a> to replace image input. The second one, which we call associative grounding, harnesses <a href=https://en.wikipedia.org/wiki/Image_retrieval>image retrieval</a> to match texts with related images during both pretraining and text-only downstream tasks. We draw further distinctions into both strategies and then compare them according to their impact on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and commonsense-related downstream tasks, showing improvement over text-only baselines.<i>transferred grounding</i> consists in applying multimodal models to text-only tasks using a placeholder to replace image input. The second one, which we call <i>associative grounding</i>, harnesses image retrieval to match texts with related images during both pretraining and text-only downstream tasks. We draw further distinctions into both strategies and then compare them according to their impact on language modeling and commonsense-related downstream tasks, showing improvement over text-only baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lantern-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lantern-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lantern-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lantern-1.5/>What Did This Castle Look like before? Exploring Referential Relations in Naturally Occurring Multimodal Texts</a></strong><br><a href=/people/r/ronja-utescher/>Ronja Utescher</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lantern-1--5><div class="card-body p-3 small">Multi-modal texts are abundant and diverse in structure, yet Language & Vision research of these naturally occurring texts has mostly focused on genres that are comparatively light on text, like <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. In this paper, we discuss the challenges and potential benefits of a L&V framework that explicitly models referential relations, taking Wikipedia articles about buildings as an example. We briefly survey existing related tasks in L&V and propose multi-modal information extraction as a general direction for future research.</div></div></div><hr><div id=2021louhi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.louhi-1/>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.0/>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.3/>Understanding Social Support Expressed in a COVID-19 Online Forum<span class=acl-fixed-case>COVID</span>-19 Online Forum</a></strong><br><a href=/people/a/anietie-andy/>Anietie Andy</a>
|
<a href=/people/b/brian-chu/>Brian Chu</a>
|
<a href=/people/r/ramie-fathy/>Ramie Fathy</a>
|
<a href=/people/b/barrington-bennett/>Barrington Bennett</a>
|
<a href=/people/d/daniel-stokes/>Daniel Stokes</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--3><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a> focused on health and wellbeing, individuals tend to seek and give the following <a href=https://en.wikipedia.org/wiki/Social_support>social support</a> : emotional and informational support. Understanding the expressions of these social supports in an online COVID- 19 forum is important for : (a) the forum and its members to provide the right type of support to individuals and (b) determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum : (a) <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a> given (b) <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a> sought (c) informational support given, and (d) informational support sought. Using these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we aim to : (i) determine if there is a correlation between the different <a href=https://en.wikipedia.org/wiki/Social_support>social supports</a> expressed in posts e.g. when members of the <a href=https://en.wikipedia.org/wiki/Internet_forum>forum</a> give emotional support in posts, do they also tend to give or seek informational support in the same post? (ii) determine how these social supports sought and given changes over time in published posts. We find that (i) there is a positive correlation between the informational support given in posts and the <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a> given and <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a> sought, respectively, in these posts and (ii) over time, users tended to seek more <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a> and give less <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.louhi-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.6/>Integrating Higher-Level Semantics into Robust Biomedical Name Representations</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Suster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--6><div class="card-body p-3 small">Neural encoders of biomedical names are typically considered robust if representations can be effectively exploited for various downstream NLP tasks. To achieve this, encoders need to model domain-specific biomedical semantics while rivaling the universal applicability of pretrained self-supervised representations. Previous work on robust representations has focused on learning low-level distinctions between names of fine-grained biomedical concepts. These fine-grained concepts can also be clustered together to reflect higher-level, more general semantic distinctions, such as grouping the names nettle sting and tick-borne fever together under the description puncture wound of skin. It has not yet been empirically confirmed that training biomedical name encoders on fine-grained distinctions automatically leads to <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up encoding</a> of such higher-level semantics. In this paper, we show that this bottom-up effect exists, but that it is still relatively limited. As a solution, we propose a scalable multi-task training regime for biomedical name encoders which can also learn robust representations using only higher-level semantic classes. These representations can generalise both bottom-up as well as top-down among various semantic hierarchies. Moreover, we show how they can be used out-of-the-box for improved unsupervised detection of hypernyms, while retaining robust performance on various semantic relatedness benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.louhi-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.9/>FuzzyBIO : A Proposal for Fuzzy Representation of Discontinuous Entities<span class=acl-fixed-case>F</span>uzzy<span class=acl-fixed-case>BIO</span>: A Proposal for Fuzzy Representation of Discontinuous Entities</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a>
|
<a href=/people/s/suzan-verberne/>Suzan Verberne</a>
|
<a href=/people/w/wessel-kraaij/>Wessel Kraaij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--9><div class="card-body p-3 small">Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a>. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative ; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-to-end performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.louhi-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--louhi-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.louhi-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.louhi-1.11/>Scientific Claim Verification with VerT5erini<span class=acl-fixed-case>V</span>er<span class=acl-fixed-case>T</span>5erini</a></strong><br><a href=/people/r/ronak-pradeep/>Ronak Pradeep</a>
|
<a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--louhi-1--11><div class="card-body p-3 small">This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> in each of the three sub-tasks. We further show VerT5erini&#8217;s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.</div></div></div><hr><div id=2021ltedi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.ltedi-1/>Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.0/>Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/m/manel-zarrouk/>Manel Zarrouk</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.1/>Impact of COVID-19 in Natural Language Processing Publications : a Disaggregated Study in Gender, Contribution and Experience<span class=acl-fixed-case>COVID</span>-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience</a></strong><br><a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--1><div class="card-body p-3 small">This study sheds light on the effects of COVID-19 in the particular field of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> within <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a>. We provide an inter-sectional study on gender, contribution, and experience that considers one school year (from August 2019 to August 2020) as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While the trend in publications increased with the crisis, the results show that the ratio between female and male publications decreased. This only helps to reduce the importance of the female role in the scientific contributions of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> (it is now far below its peak of 0.24). The pandemic has a particularly negative effect on the production of female senior researchers in the first position of authors (maximum work), followed by the female junior researchers in the last position of authors (supervision or collaborative work).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.4/>hBERT + BiasCorp-Fighting Racism on the Web<span class=acl-fixed-case>BERT</span> + <span class=acl-fixed-case>B</span>ias<span class=acl-fixed-case>C</span>orp - Fighting Racism on the Web</a></strong><br><a href=/people/o/olawale-onabola/>Olawale Onabola</a>
|
<a href=/people/z/zhuang-ma/>Zhuang Ma</a>
|
<a href=/people/x/xie-yang/>Xie Yang</a>
|
<a href=/people/b/benjamin-akera/>Benjamin Akera</a>
|
<a href=/people/i/ibraheem-abdulrahman/>Ibraheem Abdulrahman</a>
|
<a href=/people/j/jia-xue/>Jia Xue</a>
|
<a href=/people/d/dianbo-liu/>Dianbo Liu</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--4><div class="card-body p-3 small">Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we&#8217;re tackling this societal issue with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources-Fox News, <a href=https://en.wikipedia.org/wiki/Breitbart_News>BreitbartNews</a> and <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>. The first batch (45,000 manually annotated) is ready for publication. We are currently in the final phase of manually labeling the remaining <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a>. BERT has been used widely in several downstream tasks. In this work, we present hBERT, where we modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. We are also releasing a JavaScript library 3 and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.5/>An Overview of Fairness in Data Illuminating the Bias in Data Pipeline</a></strong><br><a href=/people/s/senthil-kumar-b/>Senthil Kumar B</a>
|
<a href=/people/a/aravindan-chandrabose/>Aravindan Chandrabose</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--5><div class="card-body p-3 small">Data in general encodes human biases by default ; being aware of this is a good start, and the research around how to handle it is ongoing. The term &#8216;<a href=https://en.wikipedia.org/wiki/Bias>bias</a>&#8217; is extensively used in various contexts in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. In our research the focus is specific to biases such as <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Racism>racism</a>, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. These <a href=https://en.wikipedia.org/wiki/Bias>biases</a> exacerbate the lack of <a href=https://en.wikipedia.org/wiki/Social_equality>equality</a>, <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> and inclusion of specific population while utilizing the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. The tools and technology at the intermediate level utilize <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>biased data</a>, and transfer or amplify this <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>bias</a> to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology instead, we should take a conscious effort by designing a unified framework to measure and benchmark the <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.6/>GEPSA, a tool for monitoring social challenges in <a href=https://en.wikipedia.org/wiki/Digital_media>digital press</a><span class=acl-fixed-case>GEPSA</span>, a tool for monitoring social challenges in digital press</a></strong><br><a href=/people/i/inaki-san-vicente/>Iñaki San Vicente</a>
|
<a href=/people/x/xabier-saralegi/>Xabier Saralegi</a>
|
<a href=/people/n/nerea-zubia/>Nerea Zubia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--6><div class="card-body p-3 small">This papers presents a platform for monitoring press narratives with respect to several social challenges, including <a href=https://en.wikipedia.org/wiki/Gender_equality>gender equality</a>, <a href=https://en.wikipedia.org/wiki/Human_migration>migrations</a> and <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a>. As narratives are encoded in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>, we have to use natural processing techniques to automate their analysis. Thus, crawled news are processed by means of several NLP modules, including <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, keyword extraction, <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> for social challenge detection, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. A Flask powered interface provides <a href=https://en.wikipedia.org/wiki/Data_visualization>data visualization</a> for a user-based analysis of the data. This paper presents the architecture of the <a href=https://en.wikipedia.org/wiki/System>system</a> and describes in detail its different components. Evaluation is provided for the <a href=https://en.wikipedia.org/wiki/Modular_design>modules</a> related to extraction and classification of information regarding <a href=https://en.wikipedia.org/wiki/Social_issue>social challenges</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.7.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.ltedi-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.7/>Finding Spoiler Bias in Tweets by Zero-shot Learning and Knowledge Distilling from Neural Text Simplification</a></strong><br><a href=/people/a/avi-bleiweiss/>Avi Bleiweiss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--7><div class="card-body p-3 small">Automatic detection of critical plot information in reviews of media items poses unique challenges to both <a href=https://en.wikipedia.org/wiki/Social_computing>social computing</a> and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. In this paper we propose to cast the problem of discovering spoiler bias in online discourse as a text simplification task. We conjecture that for an item-user pair, the simpler the user review we learn from an item summary the higher its likelihood to present a spoiler. Our neural model incorporates the advanced transformer network to rank the severity of a spoiler in user tweets. We constructed a sustainable high-quality movie dataset scraped from unsolicited review tweets and paired with a title summary and meta-data extracted from a movie specific domain. To a large extent, our quantitative and qualitative results weigh in on the performance impact of named entity presence in <a href=https://en.wikipedia.org/wiki/Plot_(graphics)>plot summaries</a>. Pretrained on a split-and-rephrase corpus with knowledge distilled from <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> and fine-tuned on our movie dataset, our neural model shows to outperform both a language modeler and monolingual translation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.13.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.ltedi-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.13/>IIITT@LT-EDI-EACL2021-Hope Speech Detection : There is always hope in Transformers<span class=acl-fixed-case>IIITT</span>@<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021-Hope Speech Detection: There is always hope in Transformers</a></strong><br><a href=/people/k/karthik-puranik/>Karthik Puranik</a>
|
<a href=/people/a/adeep-hande/>Adeep Hande</a>
|
<a href=/people/r/ruba-priyadharshini/>Ruba Priyadharshini</a>
|
<a href=/people/s/sajeetha-thavareesan/>Sajeetha Thavareesan</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--13><div class="card-body p-3 small">In a world with serious challenges like <a href=https://en.wikipedia.org/wiki/Climate_change>climate change</a>, religious and political conflicts, global pandemics, <a href=https://en.wikipedia.org/wiki/Terrorism>terrorism</a>, and racial discrimination, an <a href=https://en.wikipedia.org/wiki/Internet>internet</a> full of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a>. We work with several transformer-based models to classify social media comments as hope speech or not hope speech in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021. The codes for our best submission can be viewed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.16.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.16/>ZYJ@LT-EDI-EACL2021 : XLM-RoBERTa-Based Model with Attention for Hope Speech Detection<span class=acl-fixed-case>ZYJ</span>@<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021:<span class=acl-fixed-case>XLM</span>-<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a-Based Model with Attention for Hope Speech Detection</a></strong><br><a href=/people/y/yingjia-zhao/>Yingjia Zhao</a>
|
<a href=/people/x/xin-tao/>Xin Tao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--16><div class="card-body p-3 small">Due to the development of modern computer technology and the increase in the number of online media users, we can see all kinds of posts and comments everywhere on the internet. Hope speech can not only inspire the creators but also make other viewers pleasant. It is necessary to effectively and automatically detect hope speech. This paper describes the approach of our team in the task of hope speech detection. We use the attention mechanism to adjust the weight of all the output layers of XLM-RoBERTa to make full use of the information extracted from each layer, and use the weighted sum of all the output layers to complete the classification task. And we use the Stratified-K-Fold method to enhance the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data set</a>. We achieve a weighted average F1-score of 0.59, 0.84, and 0.92 for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, ranked 3rd, 2nd, and 2nd.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.20.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.20/>TeamUNCC@LT-EDI-EACL2021 : Hope Speech Detection using Transfer Learning with Transformers<span class=acl-fixed-case>T</span>eam<span class=acl-fixed-case>UNCC</span>@<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021: Hope Speech Detection using Transfer Learning with Transformers</a></strong><br><a href=/people/k/khyati-mahajan/>Khyati Mahajan</a>
|
<a href=/people/e/erfan-al-hossami/>Erfan Al-Hossami</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--20><div class="card-body p-3 small">In this paper, we describe our approach towards utilizing pre-trained models for the task of hope speech detection. We participated in Task 2 : Hope Speech Detection for Equality, Diversity and Inclusion at LT-EDI-2021 @ EACL2021. The goal of this task is to predict the presence of hope speech, along with the presence of samples that do not belong to the same language in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We describe our approach to fine-tuning RoBERTa for Hope Speech detection in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and our approach to fine-tuning XLM-RoBERTa for Hope Speech detection in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, two low resource Indic languages. We demonstrate the performance of our approach on classifying text into hope-speech, non-hope and not-language. Our approach ranked 1st in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (F1 = 0.93), 1st in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> (F1 = 0.61) and 3rd in <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a> (F1 = 0.83).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.21.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.21/>Autobots@LT-EDI-EACL2021 : One World, One Family : Hope Speech Detection with BERT Transformer Model<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021: One World, One Family: Hope Speech Detection with <span class=acl-fixed-case>BERT</span> Transformer Model</a></strong><br><a href=/people/s/sunil-gundapu/>Sunil Gundapu</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--21><div class="card-body p-3 small">The rapid rise of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>online social networks</a> like <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> allows people to express their views more widely online. However, at the same time, it can lead to an increase in <a href=https://en.wikipedia.org/wiki/Conflict_(process)>conflict</a> and hatred among consumers in the form of <a href=https://en.wikipedia.org/wiki/Freedom_of_speech>freedom of speech</a>. Therefore, it is essential to take a positive strengthening method to research on encouraging, positive, helping, and supportive social media content. In this paper, we describe a Transformer-based BERT model for Hope speech detection for equality, diversity, and inclusion, submitted for LT-EDI-2021 Task 2. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a weighted averaged f1-score of 0.93 on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.24/>Hopeful NLP@LT-EDI-EACL2021 : Finding Hope in YouTube Comment Section<span class=acl-fixed-case>NLP</span>@<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021: Finding Hope in <span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube Comment Section</a></strong><br><a href=/people/v/vasudev-awatramani/>Vasudev Awatramani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--24><div class="card-body p-3 small">The proliferation of <a href=https://en.wikipedia.org/wiki/Hate_speech>Hate Speech</a> and <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is fast becoming a menace to society. In compliment, the dissemination of hate-diffusing, promising and anti-oppressive messages become a unique alternative. Unfortunately, due to its complex nature as well as the relatively limited manifestation in comparison to hostile and neutral content, the identification of Hope Speech becomes a challenge. This work revolves around the detection of Hope Speech in Youtube comments, for the Shared Task on Hope Speech Detection for <a href=https://en.wikipedia.org/wiki/Social_equality>Equality</a>, <a href=https://en.wikipedia.org/wiki/Multiculturalism>Diversity</a>, and <a href=https://en.wikipedia.org/wiki/Inclusion_(disability_rights)>Inclusion</a>. We achieve an <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> of 0.93, ranking 1st on the <a href=https://en.wikipedia.org/wiki/Glossary_of_French_expressions_in_English>leaderboard</a> for English comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.25.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.ltedi-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.25/>NLP-CUET@LT-EDI-EACL2021 : Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>CUET</span>@<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021: Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner</a></strong><br><a href=/people/e/eftekhar-hossain/>Eftekhar Hossain</a>
|
<a href=/people/o/omar-sharif/>Omar Sharif</a>
|
<a href=/people/m/mohammed-moshiul-hoque/>Mohammed Moshiul Hoque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--25><div class="card-body p-3 small">In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post / comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (SVM, LR, ensemble), deep learning (CNN+BiLSTM) and transformer (m-BERT, Indic-BERT, XLNet, XLM-R) based methods. Results indicate that XLM-R outdoes all other techniques by gaining a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted f_1-score</a> of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Our team has achieved 1st, 2nd and 1st rank in these three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ltedi-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ltedi-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ltedi-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.ltedi-1.28.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.ltedi-1.28/>Spartans@LT-EDI-EACL2021 : Inclusive Speech Detection using Pretrained Language Models<span class=acl-fixed-case>LT</span>-<span class=acl-fixed-case>EDI</span>-<span class=acl-fixed-case>EACL</span>2021: Inclusive Speech Detection using Pretrained Language Models</a></strong><br><a href=/people/m/megha-sharma/>Megha Sharma</a>
|
<a href=/people/g/gaurav-arora/>Gaurav Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ltedi-1--28><div class="card-body p-3 small">We describe our system that ranked first in Hope Speech Detection (HSD) shared task and fourth in Offensive Language Identification (OLI) shared task, both in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil language</a>. The goal of HSD and OLI is to identify if a code-mixed comment or post contains hope speech or offensive content respectively. We pre-train a transformer-based model RoBERTa using synthetically generated code-mixed data and use it in an ensemble along with their pre-trained ULMFiT model available from iNLTK.</div></div></div><hr><div id=2021vardial-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.vardial-1/>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.0/>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.2/>Hierarchical Transformer for Multilingual Machine Translation</a></strong><br><a href=/people/a/albina-khusainova/>Albina Khusainova</a>
|
<a href=/people/a/adil-khan/>Adil Khan</a>
|
<a href=/people/a/adin-ramirez-rivera/>Adín Ramírez Rivera</a>
|
<a href=/people/v/vitaly-romanov/>Vitaly Romanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--2><div class="card-body p-3 small">The choice of parameter sharing strategy in multilingual machine translation models determines how optimally <a href=https://en.wikipedia.org/wiki/Parameter_space>parameter space</a> is used and hence, directly influences ultimate translation quality. Inspired by <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>linguistic trees</a> that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture : the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.4/>Representations of Language Varieties Are Reliable Given Corpus Similarity Measures</a></strong><br><a href=/people/j/jonathan-dunn/>Jonathan Dunn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--4><div class="card-body p-3 small">This paper measures <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> both within and between 84 <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> across nine languages. These <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> are drawn from digital sources (the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and tweets), allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. The basic idea is that, if each source adequately represents a single underlying <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a>, then the similarity between these sources should be stable across all languages and countries. The paper shows that there is a consistent agreement between these <a href=https://en.wikipedia.org/wiki/Source_text>sources</a> using frequency-based corpus similarity measures. This provides further evidence that digital geo-referenced corpora consistently represent <a href=https://en.wikipedia.org/wiki/Dialect>local language varieties</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.5/>Whit’s the Richt Pairt o Speech : PoS tagging for Scots<span class=acl-fixed-case>P</span>o<span class=acl-fixed-case>S</span> tagging for <span class=acl-fixed-case>S</span>cots</a></strong><br><a href=/people/h/harm-lameris/>Harm Lameris</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--5><div class="card-body p-3 small">In this paper we explore PoS tagging for the <a href=https://en.wikipedia.org/wiki/Scots_language>Scots language</a>. Scots is spoken in <a href=https://en.wikipedia.org/wiki/Scotland>Scotland</a> and Northern Ireland, and is closely related to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Combining the Scots and English data led to further improvements, with a <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation method</a> giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> containing a mix of genres when combined with Scots data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.vardial-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.8/>Discriminating Between Similar Nordic Languages</a></strong><br><a href=/people/r/rene-haas/>René Haas</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--8><div class="card-body p-3 small">Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for <a href=https://en.wikipedia.org/wiki/Automatic_language_identification>automatic language identification</a> for the <a href=https://en.wikipedia.org/wiki/North_Germanic_languages>Nordic languages</a>, which often suffer <a href=https://en.wikipedia.org/wiki/Miscategorization>miscategorisation</a> by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages : <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, Norwegian (Nynorsk), Norwegian (Bokml), <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> and <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.11/>Optimizing a Supervised Classifier for a Difficult Language Identification Problem</a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--11><div class="card-body p-3 small">This paper describes the system developed by the Laboratoire d&#8217;analyse statistique des textes for the Dravidian Language Identification (DLI) shared task of VarDial 2021. This task is particularly difficult because the materials consists of short YouTube comments, written in <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a>, from three closely related <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>, and a fourth category consisting of several other languages in varying proportions, all mixed with English. The proposed system is made up of a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a> which uses as only features n-grams of characters with a maximum length of 5. After its optimization both in terms of the feature weighting and the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier parameters</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> ranked first in the challenge. The additional analyses carried out underline the importance of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a>, especially when the measure of effectiveness is the Macro-F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--vardial-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.vardial-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.14/>Comparing <a href=https://en.wikipedia.org/wiki/Dialectic>Approaches</a> to Dravidian Language Identification<span class=acl-fixed-case>D</span>ravidian Language Identification</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--vardial-1--14><div class="card-body p-3 small">This paper describes the submissions by team HWR to the Dravidian Language Identification (DLI) shared task organized at VarDial 2021 workshop. The DLI training set includes 16,674 YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages : <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>. We submitted results generated using two models, a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes classifier</a> with adaptive language models, which has shown to obtain competitive performance in many language and dialect identification tasks, and a transformer-based model which is widely regarded as the state-of-the-art in a number of NLP tasks. Our first submission was sent in the closed submission track using only the training set provided by the shared task organisers, whereas the second submission is considered to be open as it used a pretrained model trained with external data. Our team attained shared second position in the shared task with the submission based on Naive Bayes. Our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks.</div></div></div><hr><div id=2021wanlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.wanlp-1/>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.0/>Proceedings of the Sixth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/s/samia-touileb/>Samia Touileb</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.3/>Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection<span class=acl-fixed-case>A</span>rabic Sentiment and Sarcasm Detection</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--3><div class="card-body p-3 small">The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> were initially developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we evaluate the performance of 24 of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Arabic sentiment</a> and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.5/>Kawarith : an Arabic Twitter Corpus for Crisis Events<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>T</span>witter Corpus for Crisis Events</a></strong><br><a href=/people/a/alaa-alharbi/>Alaa Alharbi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--5><div class="card-body p-3 small">Social media (SM) platforms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> provide large quantities of <a href=https://en.wikipedia.org/wiki/Real-time_data>real-time data</a> that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.9/>ArCOV-19 : The First Arabic COVID-19 Twitter Dataset with Propagation Networks<span class=acl-fixed-case>A</span>r<span class=acl-fixed-case>COV</span>-19: The First <span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>T</span>witter Dataset with Propagation Networks</a></strong><br><a href=/people/f/fatima-haouari/>Fatima Haouari</a>
|
<a href=/people/m/maram-hasanain/>Maram Hasanain</a>
|
<a href=/people/r/reem-suwaileh/>Reem Suwaileh</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--9><div class="card-body p-3 small">In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7 M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand <a href=https://en.wikipedia.org/wiki/Conversation>conversational threads</a> (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, and <a href=https://en.wikipedia.org/wiki/Social_computing>social computing</a>. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the <a href=https://en.wikipedia.org/wiki/Arab_world>Arab world</a>. In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.18/>ALUE : Arabic Language Understanding Evaluation<span class=acl-fixed-case>ALUE</span>: <span class=acl-fixed-case>A</span>rabic Language Understanding Evaluation</a></strong><br><a href=/people/h/haitham-seelawi/>Haitham Seelawi</a>
|
<a href=/people/i/ibraheem-tuffaha/>Ibraheem Tuffaha</a>
|
<a href=/people/m/mahmoud-gzawi/>Mahmoud Gzawi</a>
|
<a href=/people/w/wael-farhan/>Wael Farhan</a>
|
<a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/r/riham-badawi/>Riham Badawi</a>
|
<a href=/people/z/zyad-sober/>Zyad Sober</a>
|
<a href=/people/o/oday-al-dweik/>Oday Al-Dweik</a>
|
<a href=/people/a/abed-alhakim-freihat/>Abed Alhakim Freihat</a>
|
<a href=/people/h/hussein-al-natsheh/>Hussein Al-Natsheh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--18><div class="card-body p-3 small">The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels. Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community, we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online, and publicly accessible leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.19/>Quranic Verses Semantic Relatedness Using AraBERT<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/abdullah-alsaleh/>Abdullah Alsaleh</a>
|
<a href=/people/e/eric-atwell/>Eric Atwell</a>
|
<a href=/people/a/abdulrahman-altahhan/>Abdulrahman Altahhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--19><div class="card-body p-3 small">Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92 % accuracy score using a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprised of label &#8216;2&#8217; and label&#8217; -1&#8217;, the latter was generated outside of QurSim dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.20/>AraELECTRA : Pre-Training Text Discriminators for Arabic Language Understanding<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>ELECTRA</span>: Pre-Training Text Discriminators for <span class=acl-fixed-case>A</span>rabic Language Understanding</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--20><div class="card-body p-3 small">Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a> and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.21/>AraGPT2 : Pre-Trained Transformer for Arabic Language Generation<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>GPT</span>2: Pre-Trained Transformer for <span class=acl-fixed-case>A</span>rabic Language Generation</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--21><div class="card-body p-3 small">Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> of 29.8 on <a href=https://en.wikipedia.org/wiki/Wikipedia>held-out Wikipedia articles</a>. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98 % percent <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in detecting model-generated text. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.24/>SERAG : Semantic Entity Retrieval from Arabic Knowledge Graphs<span class=acl-fixed-case>SERAG</span>: Semantic Entity Retrieval from <span class=acl-fixed-case>A</span>rabic Knowledge Graphs</a></strong><br><a href=/people/s/saher-esmeir/>Saher Esmeir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--24><div class="card-body p-3 small">Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity retrieval</a> from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for entity retrieval have shown a significant improvement over traditional methods. These <a href=https://en.wikipedia.org/wiki/Physical_model>models</a>, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses <a href=https://en.wikipedia.org/wiki/Random_walk>random walks</a> to generate <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity embeddings</a>. DBpedia-Entity v2 is considered the standard test collection for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity retrieval</a>. We discuss the challenges of using <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> for non-English languages in general and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in particular. We provide an Arabic version of this standard <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a>, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.25/>Introducing A large Tunisian Arabizi Dialectal Dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>T</span>unisian <span class=acl-fixed-case>A</span>rabizi Dialectal Dataset for Sentiment Analysis</a></strong><br><a href=/people/c/chayma-fourati/>Chayma Fourati</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/a/abir-messaoudi/>Abir Messaoudi</a>
|
<a href=/people/m/moez-benhajhmida/>Moez BenHajhmida</a>
|
<a href=/people/a/aymen-ben-elhaj-mabrouk/>Aymen Ben Elhaj Mabrouk</a>
|
<a href=/people/m/malek-naski/>Malek Naski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--25><div class="card-body p-3 small">On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments : their local dialects. In <a href=https://en.wikipedia.org/wiki/Africa>Africa</a>, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a> and numbers rather than <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic ones</a>. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of a total of 100k comments (about <a href=https://en.wikipedia.org/wiki/Film>movies</a>, <a href=https://en.wikipedia.org/wiki/Politics>politic</a>, <a href=https://en.wikipedia.org/wiki/Sport>sport</a>, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.27/>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--27><div class="card-body p-3 small">We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a> to demonstrate the effectiveness of the proposed method for CEAE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.31/>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification<span class=acl-fixed-case>BERT</span>-based Multi-Task Model for Country and Province Level <span class=acl-fixed-case>MSA</span> and Dialectal <span class=acl-fixed-case>A</span>rabic Identification</a></strong><br><a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/a/abdelkader-el-mahdaouy/>Abdelkader El Mahdaouy</a>
|
<a href=/people/k/kabil-essefar/>Kabil Essefar</a>
|
<a href=/people/n/nabil-el-mamoun/>Nabil El Mamoun</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a>
|
<a href=/people/a/ahmed-khoumsi/>Ahmed Khoumsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--31><div class="card-body p-3 small">Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA / DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA / DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.32/>Country-level Arabic Dialect Identification using RNNs with and without <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a><span class=acl-fixed-case>A</span>rabic Dialect Identification using <span class=acl-fixed-case>RNN</span>s with and without Linguistic Features</a></strong><br><a href=/people/e/elsayed-issa/>Elsayed Issa</a>
|
<a href=/people/m/mohammed-alshakhori1/>Mohammed AlShakhori1</a>
|
<a href=/people/r/reda-al-bahrani/>Reda Al-Bahrani</a>
|
<a href=/people/g/gus-hahn-powell/>Gus Hahn-Powell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--32><div class="card-body p-3 small">This work investigates the value of augmenting <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> with <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2 : Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.33/>Arabic Dialect Identification based on a Weighted Concatenation of TF-IDF Features<span class=acl-fixed-case>A</span>rabic Dialect Identification based on a Weighted Concatenation of <span class=acl-fixed-case>TF</span>-<span class=acl-fixed-case>IDF</span> Features</a></strong><br><a href=/people/m/mohamed-lichouri/>Mohamed Lichouri</a>
|
<a href=/people/m/mourad-abbas/>Mourad Abbas</a>
|
<a href=/people/k/khaled-lounnas/>Khaled Lounnas</a>
|
<a href=/people/b/besma-benaziz/>Besma Benaziz</a>
|
<a href=/people/a/aicha-zitouni/>Aicha Zitouni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--33><div class="card-body p-3 small">In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two <a href=https://en.wikipedia.org/wiki/Design_of_experiments>subtasks</a> : subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), <a href=https://en.wikipedia.org/wiki/Perceptron>Perceptron</a>, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our <a href=https://en.wikipedia.org/wiki/System>system</a> gives F1 scores of 14.87 % and 21.49 %, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> and recorded for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a> (18.70 % and 24.23 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.34/>Machine Learning-Based Approach for Arabic Dialect Identification<span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/h/hamada-nayel/>Hamada Nayel</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan</a>
|
<a href=/people/m/mahmoud-sobhi/>Mahmoud Sobhi</a>
|
<a href=/people/a/ahmed-el-sawy/>Ahmed El-Sawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--34><div class="card-body p-3 small">This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four <a href=https://en.wikipedia.org/wiki/Russian_language>subtasks</a>, two subtasks for country-level identification and the other two <a href=https://en.wikipedia.org/wiki/Russian_language>subtasks</a> for province-level identification. The data in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> covers a total of 100 provinces from all 21 Arab countries and come from the <a href=https://en.wikipedia.org/wiki/Twitter>Twitter domain</a>. The proposed systems depend on five machine-learning approaches namely Complement Nave Bayes, <a href=https://en.wikipedia.org/wiki/Support-vector_machine>Support Vector Machine</a>, <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Tree</a>, <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a> and Random Forest Classifiers. F1 macro-averaged score of Nave Bayes classifier outperformed all other <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> for development and test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wanlp-1.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.36/>Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic<span class=acl-fixed-case>WANLP</span> 2021 Shared Task on Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--36><div class="card-body p-3 small">This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. The shared task has two subtasks : sarcasm detection (subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> and <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a>. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> and 0.748 <a href=https://en.wikipedia.org/wiki/F-number>F1-PN</a> respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.38/>Sarcasm and Sentiment Detection In Arabic Tweets Using BERT-based Models and Data Augmentation<span class=acl-fixed-case>A</span>rabic Tweets Using <span class=acl-fixed-case>BERT</span>-based Models and Data Augmentation</a></strong><br><a href=/people/a/abeer-abuzayed/>Abeer Abuzayed</a>
|
<a href=/people/h/hend-al-khalifa/>Hend Al-Khalifa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--38><div class="card-body p-3 small">In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Abu Farha et al., 2021). The shared <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> consists of two sub-tasks : Sarcasm Detection (Subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> outperformed other models with an increase of the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> by 15 % for both tasks which shows the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.39/>Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for Arabic Sarcasm Detection and Sentiment Analysis<span class=acl-fixed-case>A</span>rabic Sarcasm Detection and Sentiment Analysis</a></strong><br><a href=/people/a/abdullah-i-alharbi/>Abdullah I. Alharbi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--39><div class="card-body p-3 small">Sarcasm detection and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> are important tasks in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a>. Sarcasm is a type of <a href=https://en.wikipedia.org/wiki/Emotional_expression>expression</a> where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wanlp-1.41.Dataset.pdf data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wanlp-1.41.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.41/>Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> : investigating the interest of character-level features<span class=acl-fixed-case>A</span>rabic: investigating the interest of character-level features</a></strong><br><a href=/people/d/dhaou-ghoul/>Dhaou Ghoul</a>
|
<a href=/people/g/gael-lejeune/>Gaël Lejeune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--41><div class="card-body p-3 small">We present three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> developed for the Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We present a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that uses character n-gram features. We also propose two more sophisticated methods : a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with a word level representation and an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifier</a> relying on word and character-level features. We chose to present results from an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifier</a> but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> could have been improved and beat those results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.44/>SarcasmDet at Sarcasm Detection Task 2021 in Arabic using AraBERT Pretrained Model<span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>D</span>et at Sarcasm Detection Task 2021 in <span class=acl-fixed-case>A</span>rabic using <span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span> Pretrained Model</a></strong><br><a href=/people/d/dalya-faraj/>Dalya Faraj</a>
|
<a href=/people/d/dalya-faraj/>Dalya Faraj</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--44><div class="card-body p-3 small">This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Subtask-1 Sarcasm Detection). The goal of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to identify whether a tweet is sarcastic or not. Our solution has been developed using <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble technique</a> with AraBERT pre-trained model. We describe the architecture of the submitted <a href=https://en.wikipedia.org/wiki/Solution>solution</a> in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that we trained or tested to achieve a better score in a table design. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.7830</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.45/>Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> A Hybrid Approach Combining Embeddings and Rule-based Features<span class=acl-fixed-case>A</span>rabic language A Hybrid Approach Combining Embeddings and Rule-based Features</a></strong><br><a href=/people/k/kamel-gaanoun/>Kamel Gaanoun</a>
|
<a href=/people/i/imade-benelallam/>Imade Benelallam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--45><div class="card-body p-3 small">This paper presents the ArabicProcessors team&#8217;s system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using <a href=https://en.wikipedia.org/wiki/Linear_transform>transformers</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The system&#8217;s architecture is an ensemble of <a href=https://en.wikipedia.org/wiki/Naive_bayes>Naive bayes</a>, MarBERT and Mazajak embedding. This process scored an F1-score of 51 % on <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and 71 % for sentiment detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.50/>iCompass at Shared Task on <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> and Sentiment Detection in Arabic<span class=acl-fixed-case>C</span>ompass at Shared Task on Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/malek-naski/>Malek Naski</a>
|
<a href=/people/a/abir-messaoudi/>Abir Messaoudi</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/m/moez-benhajhmida/>Moez BenHajhmida</a>
|
<a href=/people/c/chayma-fourati/>Chayma Fourati</a>
|
<a href=/people/a/aymen-ben-elhaj-mabrouk/>Aymen Ben Elhaj Mabrouk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--50><div class="card-body p-3 small">We describe our submitted <a href=https://en.wikipedia.org/wiki/System>system</a> to the 2021 Shared Task on Sarcasm and Sentiment Detection in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google&#8217;s multilingual BERT and then other Arabic variants : AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wanlp-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wanlp-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wanlp-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wanlp-1.53/>AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in Arabic Tweets<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span> and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in <span class=acl-fixed-case>A</span>rabic Tweets</a></strong><br><a href=/people/a/anshul-wadhawan/>Anshul Wadhawan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wanlp-1--53><div class="card-body p-3 small">This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2 : Sarcasm and Sentiment Detection. One of the subtasks aims at developing a <a href=https://en.wikipedia.org/wiki/System>system</a> that identifies whether a given <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweet</a> is sarcastic in nature or not, while the other aims to identify the sentiment of the <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweet</a>. We approach the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in two steps. The first step involves pre processing the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively.</div></div></div><hr><div id=2021wassa-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.wassa-1/>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.0/>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/j/joao-sedoc/>Joao Sedoc</a>
|
<a href=/people/v/valentin-barriere/>Valentin Barriere</a>
|
<a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.5/>Emotion Ratings : How Intensity, Annotation Confidence and Agreements are Entangled</a></strong><br><a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--5><div class="card-body p-3 small">When humans judge the affective content of texts, they also implicitly assess the correctness of such judgment, that is, their <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a>. We hypothesize that people&#8217;s (in)confidence that they performed well in an annotation task leads to (dis)agreements among each other. If this is true, <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a> may serve as a diagnostic tool for systematic differences in annotations. To probe our assumption, we conduct a study on a subset of the Corpus of Contemporary American English, in which we ask raters to distinguish neutral sentences from emotion-bearing ones, while scoring the confidence of their answers. Confidence turns out to approximate inter-annotator disagreements. Further, we find that <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a> is correlated to emotion intensity : perceiving stronger affect in text prompts annotators to more certain <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performances. This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> actually predict intensity, or rather human&#8217;s self-perceived confidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.6/>Disentangling Document Topic and Author Gender in Multiple Languages : Lessons for Adversarial Debiasing</a></strong><br><a href=/people/e/erenay-dayanik/>Erenay Dayanik</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--6><div class="card-body p-3 small">Text classification is a central tool in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up wrong <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, leading to bad generalization and biases. In <a href=https://en.wikipedia.org/wiki/Social_media_analytics>social media analysis</a>, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are : (a) individual classifiers for topic and author gender are indeed biased ; (b) <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> with adversarial training works for topic, but breaks down for author gender ; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.7/>Universal Joy A Data Set and Results for Classifying Emotions Across Languages</a></strong><br><a href=/people/s/sotiris-lamprinidis/>Sotiris Lamprinidis</a>
|
<a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/d/daniel-hardt/>Daniel Hardt</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--7><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are universal aspects of <a href=https://en.wikipedia.org/wiki/Psychology>human psychology</a>, <a href=https://en.wikipedia.org/wiki/They_(2017_film)>they</a> are expressed differently across different languages and cultures. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of over 530k anonymized public Facebook posts across 18 languages, labeled with five different <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>. Using multilingual BERT embeddings, we show that <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> can be reliably inferred both within and across languages. Zero-shot learning produces promising results for low-resource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the <a href=https://en.wikipedia.org/wiki/Data_anonymization>anonymized data</a> for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wassa-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.9/>An End-to-End Network for Emotion-Cause Pair Extraction</a></strong><br><a href=/people/a/aaditya-singh/>Aaditya Singh</a>
|
<a href=/people/s/shreeshail-hingane/>Shreeshail Hingane</a>
|
<a href=/people/s/saim-wani/>Saim Wani</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--9><div class="card-body p-3 small">The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all potential clause-pairs of emotions and their corresponding causes in a document. Unlike the more well-studied task of Emotion Cause Extraction (ECE), ECPE does not require the emotion clauses to be provided as annotations. Previous works on ECPE have either followed a multi-stage approach where emotion extraction, cause extraction, and <a href=https://en.wikipedia.org/wiki/Pairing>pairing</a> are done independently or use complex architectures to resolve its limitations. In this paper, we propose an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a> for the ECPE task. Due to the unavailability of an English language ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline for the ECPE task on this dataset. On this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> produces significant performance improvements (6.5 % increase in F1 score) over the multi-stage approach and achieves comparable performance to the <a href=https://en.wikipedia.org/wiki/Methodology>state-of-the-art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wassa-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.11/>PVG at WASSA 2021 : A Multi-Input, Multi-Task, Transformer-Based Architecture for Empathy and Distress Prediction<span class=acl-fixed-case>PVG</span> at <span class=acl-fixed-case>WASSA</span> 2021: A Multi-Input, Multi-Task, Transformer-Based Architecture for Empathy and Distress Prediction</a></strong><br><a href=/people/a/atharva-kulkarni/>Atharva Kulkarni</a>
|
<a href=/people/s/sunanda-somwase/>Sunanda Somwase</a>
|
<a href=/people/s/shivam-rajput/>Shivam Rajput</a>
|
<a href=/people/m/manisha-marathe/>Manisha Marathe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--11><div class="card-body p-3 small">Active research pertaining to the affective phenomenon of empathy and <a href=https://en.wikipedia.org/wiki/Distress_(medicine)>distress</a> is invaluable for improving <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-machine interaction</a>. Predicting intensities of such complex emotions from textual data is difficult, as these <a href=https://en.wikipedia.org/wiki/Construct_(philosophy)>constructs</a> are deeply rooted in the <a href=https://en.wikipedia.org/wiki/Psychology>psychological theory</a>. Consequently, for better prediction, it becomes imperative to take into account ancillary factors such as the psychological test scores, demographic features, underlying latent primitive emotions, along with the text&#8217;s undertone and its psychological complexity. This paper proffers team PVG&#8217;s solution to the WASSA 2021 Shared Task on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging the textual data, demographic features, psychological test score, and the intrinsic interdependencies of primitive emotions and <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, we propose a multi-input, multi-task framework for the task of <a href=https://en.wikipedia.org/wiki/Empathy>empathy score prediction</a>. Here, the empathy score prediction is considered the primary task, while emotion and empathy classification are considered secondary auxiliary tasks. For the distress score prediction task, the <a href=https://en.wikipedia.org/wiki/System>system</a> is further boosted by the addition of lexical features. Our submission ranked 1st based on the average correlation (0.545) as well as the distress correlation (0.574), and 2nd for the empathy Pearson correlation (0.517).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wassa-1.12.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.wassa-1.12/>WASSA@IITK at WASSA 2021 : Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction<span class=acl-fixed-case>WASSA</span>@<span class=acl-fixed-case>IITK</span> at <span class=acl-fixed-case>WASSA</span> 2021: Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction</a></strong><br><a href=/people/j/jay-mundra/>Jay Mundra</a>
|
<a href=/people/r/rohan-gupta/>Rohan Gupta</a>
|
<a href=/people/s/sagnik-mukherjee/>Sagnik Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--12><div class="card-body p-3 small">This paper describes our contribution to the WASSA 2021 shared task on Empathy Prediction and <a href=https://en.wikipedia.org/wiki/Emotion_classification>Emotion Classification</a>. The broad goal of this task was to model an empathy score, a distress score and the overall level of emotion of an essay written in response to a newspaper article associated with harm to someone. We have used the ELECTRA model abundantly and also advanced deep learning approaches like <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Additionally, we also leveraged standard machine learning techniques like ensembling. Our system achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson Correlation Coefficient</a> of 0.533 on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> I and a macro F1 score of 0.5528 on sub-task II. We ranked 1st in <a href=https://en.wikipedia.org/wiki/Emotion_classification>Emotion Classification sub-task</a> and 3rd in Empathy Prediction sub-task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.16/>Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--16><div class="card-body p-3 small">In this paper, we describe experiments designed to evaluate the impact of stylometric and emotion-based features on hate speech detection : the task of classifying textual content into hate or non-hate speech classes. Our experiments are conducted for three languages English, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, and <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> both in in-domain and cross-domain setups, and aim to investigate <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> using features that model two linguistic phenomena : the writing style of hateful social media content operationalized as function word usage on the one hand, and emotion expression in hateful messages on the other hand. The results of experiments with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that model different combinations of these phenomena support our hypothesis that stylometric and emotion-based features are robust indicators of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>. Their contribution remains persistent with respect to <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>domain and language variation</a>. We show that the combination of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that model the targeted phenomena outperforms words and character n-gram features under cross-domain conditions, and provides a significant boost to <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, which currently obtain the best results, when combined with them in an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.20/>Creating and Evaluating Resources for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> in the Low-resource Language : Sindhi<span class=acl-fixed-case>S</span>indhi</a></strong><br><a href=/people/w/wazir-ali/>Wazir Ali</a>
|
<a href=/people/n/naveed-ali/>Naveed Ali</a>
|
<a href=/people/y/yong-dai/>Yong Dai</a>
|
<a href=/people/j/jay-kumar/>Jay Kumar</a>
|
<a href=/people/s/saifullah-tumrani/>Saifullah Tumrani</a>
|
<a href=/people/z/zenglin-xu/>Zenglin Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--20><div class="card-body p-3 small">In this paper, we develop Sindhi subjective lexicon using a merger of existing English resources : NRC lexicon, list of opinion words, SentiWordNet, Sindhi-English bilingual dictionary, and collection of Sindhi modifiers. The positive or negative sentiment score is assigned to each <a href=https://en.wikipedia.org/wiki/Sindhi_language>Sindhi opinion word</a>. Afterwards, we determine the coverage of the proposed <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> with subjectivity analysis. Moreover, we crawl multi-domain tweet corpus of news, sports, and finance. The crawled corpus is annotated by experienced annotators using the Doccano text annotation tool. The sentiment annotated corpus is evaluated by employing <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine (SVM)</a>, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN) variants</a>, and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network (CNN)</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wassa-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.23/>L3CubeMahaSent : A Marathi Tweet-based Sentiment Analysis Dataset<span class=acl-fixed-case>L</span>3<span class=acl-fixed-case>C</span>ube<span class=acl-fixed-case>M</span>aha<span class=acl-fixed-case>S</span>ent: A <span class=acl-fixed-case>M</span>arathi Tweet-based Sentiment Analysis Dataset</a></strong><br><a href=/people/a/atharva-kulkarni/>Atharva Kulkarni</a>
|
<a href=/people/m/meet-mandhane/>Meet Mandhane</a>
|
<a href=/people/m/manali-likhitkar/>Manali Likhitkar</a>
|
<a href=/people/g/gayatri-kshirsagar/>Gayatri Kshirsagar</a>
|
<a href=/people/r/raviraj-joshi/>Raviraj Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--23><div class="card-body p-3 small">Sentiment analysis is one of the most fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Popular languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>, and also Indian languages such as <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> have seen a significant amount of work in this area. However, the <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi language</a> which is the third most popular language in India still lags behind due to the absence of proper datasets. In this paper, we present the first major publicly available Marathi Sentiment Analysis Dataset-L3CubeMahaSent. It is curated using tweets extracted from various Maharashtrian personalities&#8217; Twitter accounts. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of ~16,000 distinct tweets classified in three broad classes viz. positive, negative, and neutral. We also present the guidelines using which we annotated the tweets. Finally, we present the statistics of our dataset and baseline classification results using CNN, LSTM, ULMFiT, and BERT based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.26/>Me, myself, and ire : Effects of automatic transcription quality on <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and personality detection</a></strong><br><a href=/people/j/john-culnan/>John Culnan</a>
|
<a href=/people/s/seongjin-park/>Seongjin Park</a>
|
<a href=/people/m/meghavarshini-krishnaswamy/>Meghavarshini Krishnaswamy</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--26><div class="card-body p-3 small">In deployment, <a href=https://en.wikipedia.org/wiki/System>systems</a> that use <a href=https://en.wikipedia.org/wiki/Speech>speech</a> as input must make use of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>automated transcriptions</a>. Yet, typically when these <a href=https://en.wikipedia.org/wiki/System>systems</a> are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets : <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and <a href=https://en.wikipedia.org/wiki/Personality_test>personality detection</a>. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of <a href=https://en.wikipedia.org/wiki/Audio_signal>audio features</a> partially mitigates <a href=https://en.wikipedia.org/wiki/Transcription_error>transcription errors</a>, but that a naive usage of a multi-task setup does not.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wassa-1.27/>Emotional RobBERT and Insensitive BERTje : Combining Transformers and Affect Lexica for Dutch Emotion Detection<span class=acl-fixed-case>R</span>ob<span class=acl-fixed-case>BERT</span> and Insensitive <span class=acl-fixed-case>BERT</span>je: Combining Transformers and Affect Lexica for <span class=acl-fixed-case>D</span>utch Emotion Detection</a></strong><br><a href=/people/l/luna-de-bruyne/>Luna De Bruyne</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--27><div class="card-body p-3 small">In a first step towards improving Dutch emotion detection, we try to combine the Dutch transformer models BERTje and RobBERT with lexicon-based methods. We propose two architectures : one in which lexicon information is directly injected into the transformer model and a meta-learning approach where predictions from transformers are combined with lexicon features. The models are tested on 1,000 Dutch tweets and 1,000 captions from <a href=https://en.wikipedia.org/wiki/Television_show>TV-shows</a> which have been manually annotated with <a href=https://en.wikipedia.org/wiki/Emotion>emotion categories</a> and dimensions. We find that RobBERT clearly outperforms BERTje, but that directly adding lexicon information to transformers does not improve performance. In the meta-learning approach, lexicon information does have a positive effect on BERTje, but not on RobBERT. This suggests that more emotional information is already contained within this latter <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wassa-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wassa-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wassa-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wassa-1.28.OptionalSupplementaryMaterial.html data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.wassa-1.28/>EmpNa at WASSA 2021 : A Lightweight Model for the Prediction of <a href=https://en.wikipedia.org/wiki/Empathy>Empathy</a>, <a href=https://en.wikipedia.org/wiki/Psychological_stress>Distress</a> and <a href=https://en.wikipedia.org/wiki/Emotion>Emotions</a> from Reactions to News Stories<span class=acl-fixed-case>E</span>mp<span class=acl-fixed-case>N</span>a at <span class=acl-fixed-case>WASSA</span> 2021: A Lightweight Model for the Prediction of Empathy, Distress and Emotions from Reactions to News Stories</a></strong><br><a href=/people/g/giuseppe-vettigli/>Giuseppe Vettigli</a>
|
<a href=/people/a/antonio-sorgente/>Antonio Sorgente</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wassa-1--28><div class="card-body p-3 small">This paper describes our submission for the WASSA 2021 shared task regarding the prediction of empathy, distress and emotions from news stories. The solution is based on combining the frequency of words, lexicon-based information, demographics of the annotators and personality of the annotators into a <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a>. The prediction of empathy and distress is performed using <a href=https://en.wikipedia.org/wiki/Linear_regression>Linear Regression</a> while the prediction of emotions is performed using <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a>. Both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are performed using the same <a href=https://en.wikipedia.org/wiki/Software_feature>features</a>. Our models rank 4th for the prediction of emotions and 2nd for the prediction of empathy and distress. These results are particularly interesting when considered that the computational requirements of the <a href=https://en.wikipedia.org/wiki/Solution>solution</a> are minimal.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>