<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Joint Conference on Lexical and Computational Semantics (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Joint Conference on Lexical and Computational Semantics (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#s19-1>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li></ul></div></div><div id=s19-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/S19-1/>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1000/>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*<span class=acl-fixed-case>SEM</span> 2019)</a></strong><br><a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1001/>SURel : A Gold Standard for Incorporating Meaning Shifts into Term Extraction<span class=acl-fixed-case>SUR</span>el: A Gold Standard for Incorporating Meaning Shifts into Term Extraction</a></strong><br><a href=/people/a/anna-hatty/>Anna HÃ¤tty</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1001><div class="card-body p-3 small">We introduce SURel, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with human-annotated meaning shifts between general-language and domain-specific contexts. We show that meaning shifts of term candidates cause errors in <a href=https://en.wikipedia.org/wiki/Term_extraction>term extraction</a>, and demonstrate that the SURel annotation reflects these errors. Furthermore, we illustrate that SURel enables us to assess optimisations of term extraction techniques when incorporating meaning shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1007/>Second-order contexts from lexical substitutes for few-shot learning of word representations</a></strong><br><a href=/people/q/qianchu-liu/>Qianchu Liu</a>
|
<a href=/people/d/diana-mccarthy/>Diana McCarthy</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1007><div class="card-body p-3 small">There is a growing awareness of the need to handle rare and unseen words in word representation modelling. In this paper, we focus on few-shot learning of emerging concepts that fully exploits only a few available contexts. We introduce a substitute-based context representation technique that can be applied on an existing word embedding space. Previous context-based approaches to modelling unseen words only consider bag-of-word first-order contexts, whereas our method aggregates contexts as second-order substitutes that are produced by a sequence-aware sentence completion model. We experimented with three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that aim to test the modelling of emerging concepts. We found that these tasks show different emphasis on first and second order contexts, and our substitute-based method achieves superior performance on naturally-occurring contexts from corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1008/>Pre-trained Contextualized Character Embeddings Lead to Major Improvements in Time Normalization : a Detailed Analysis</a></strong><br><a href=/people/d/dongfang-xu/>Dongfang Xu</a>
|
<a href=/people/e/egoitz-laparra/>Egoitz Laparra</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1008><div class="card-body p-3 small">Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art : 51 % error reduction in news and 33 % in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1009/>Bot2Vec : Learning Representations of Chatbots<span class=acl-fixed-case>B</span>ot2<span class=acl-fixed-case>V</span>ec: Learning Representations of Chatbots</a></strong><br><a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/t/tommy-sandbank/>Tommy Sandbank</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/d/david-konopnicki/>David Konopnicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1009><div class="card-body p-3 small">Chatbots (i.e., bots) are becoming widely used in multiple domains, along with supporting bot programming platforms. These <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> are equipped with novel testing tools aimed at improving the quality of individual <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. Doing so requires an understanding of what sort of <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> are being built (captured by their underlying conversation graphs) and how well they perform (derived through analysis of conversation logs). In this paper, we propose a new model, Bot2Vec, that embeds <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> to a compact representation based on their structure and usage logs. Then, we utilize Bot2Vec representations to improve the quality of two bot analysis tasks. Using conversation data and <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graphs</a> of over than 90 bots, we show that Bot2Vec representations improve detection performance by more than 16 % for both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1011/>A Semantic Cover Approach for Topic Modeling</a></strong><br><a href=/people/r/rajagopal-venkatesaramani/>Rajagopal Venkatesaramani</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a>
|
<a href=/people/b/bradley-malin/>Bradley Malin</a>
|
<a href=/people/y/yevgeniy-vorobeychik/>Yevgeniy Vorobeychik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1011><div class="card-body p-3 small">We introduce a novel topic modeling approach based on constructing a semantic set cover for clusters of similar documents. Specifically, our approach first clusters documents using their Tf-Idf representation, and then covers each cluster with a set of topic words based on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, defined in terms of a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. Computing a topic cover amounts to solving a minimum set cover problem. Our evaluation compares our topic modeling approach to Latent Dirichlet Allocation (LDA) on three metrics : 1) qualitative topic match, measured using evaluations by Amazon Mechanical Turk (MTurk) workers, 2) performance on classification tasks using each topic model as a sparse feature representation, and 3) topic coherence. We find that qualitative judgments significantly favor our approach, the method outperforms LDA on topic coherence, and is comparable to LDA on document classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1012/>MCScript2.0 : A Machine Comprehension Corpus Focused on Script Events and Participants<span class=acl-fixed-case>MCS</span>cript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants</a></strong><br><a href=/people/s/simon-ostermann/>Simon Ostermann</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1012><div class="card-body p-3 small">We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of <a href=https://en.wikipedia.org/wiki/Scripting_language>script knowledge</a>. MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on a new collection process that results in challenging questions. Half of the questions can not be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge. We give a thorough analysis of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the <a href=https://en.wikipedia.org/wiki/Data>data</a>, even if they make use of a commonsense knowledge base. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available at http://www.sfb1102. uni-saarland.de/?page_id=2582</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1013/>Deconstructing <a href=https://en.wikipedia.org/wiki/Multimodality>multimodality</a> : <a href=https://en.wikipedia.org/wiki/Visual_system>visual properties</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> in human semantic processing</a></strong><br><a href=/people/c/christopher-davis/>Christopher Davis</a>
|
<a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/a/anita-lilla-vero/>Anita Lilla Vero</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1013><div class="card-body p-3 small">Multimodal semantic models that extend linguistic representations with additional perceptual input have proved successful in a range of natural language processing (NLP) tasks. Recent research has successfully used <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> to automatically create <a href=https://en.wikipedia.org/wiki/Mental_image>visual representations</a> for words. However, these works have extracted visual features from complete images, and have not examined how different kinds of visual information impact performance. In contrast, we construct multimodal models that differentiate between internal visual properties of the objects and their external visual context. We evaluate the models on the task of decoding brain activity associated with the meanings of nouns, demonstrating their advantage over those based on complete images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S19-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S19-1015/>Neural User Factor Adaptation for Text Classification : Learning to Generalize Across Author Demographics</a></strong><br><a href=/people/x/xiaolei-huang/>Xiaolei Huang</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1015><div class="card-body p-3 small">Language use varies across different <a href=https://en.wikipedia.org/wiki/Demography>demographic factors</a>, such as <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Ageing>age</a>, and <a href=https://en.wikipedia.org/wiki/Location>geographic location</a>. However, most existing document classification methods ignore demographic variability. In this study, we examine empirically how text data can vary across four <a href=https://en.wikipedia.org/wiki/Demography>demographic factors</a> : <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, age, country, and <a href=https://en.wikipedia.org/wiki/Region>region</a>. We propose a multitask neural model to account for <a href=https://en.wikipedia.org/wiki/Demography>demographic variations</a> via <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. In experiments on four English-language social media datasets, we find that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance improves when adapting for user factors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1016/>Abstract Graphs and Abstract Paths for Knowledge Graph Completion</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/b/bhushan-kotnis/>Bhushan Kotnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1016><div class="card-body p-3 small">Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> e.g. entity embeddings, relation representations or patterns will be affected by the imbalance in the information captured in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1018/>Enthymemetic Conditionals</a></strong><br><a href=/people/e/eimear-maguire/>Eimear Maguire</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1018><div class="card-body p-3 small">To model conditionals in a way that reflects their acceptability, we must include some means of making judgements about whether antecedent and consequent are meaningfully related or not. Enthymemes are non-logical arguments which do not hold up by themselves, but are acceptable through their relation to a <a href=https://en.wikipedia.org/wiki/Topos>topos</a>, an already-known general principle or <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> for reasoning. This paper uses <a href=https://en.wikipedia.org/wiki/Enthymeme>enthymemes</a> and <a href=https://en.wikipedia.org/wiki/Theory_of_forms>topoi</a> as a way to model the world-knowledge behind these judgements. In doing so, it provides a reformalisation (in TTR) of enthymemes and topoi as <a href=https://en.wikipedia.org/wiki/Flow_network>networks</a> rather than <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>, and information state update rules for <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditionals</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1019/>Acquiring Structured Temporal Representation via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> : A Feasibility Study</a></strong><br><a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1019><div class="card-body p-3 small">Temporal Dependency Trees are a structured temporal representation that represents temporal relations among time expressions and events in a text as a dependency tree structure. Compared to traditional pair-wise temporal relation representations, temporal dependency trees facilitate efficient <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, higher inter-annotator agreement, and efficient computations. However, <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> on temporal dependency trees so far have only been done by expert annotators, which is costly and time-consuming. In this paper, we introduce a method to crowdsource temporal dependency tree annotations, and show that this representation is intuitive and can be collected with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>agreement</a> through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We produce a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of temporal dependency trees</a>, and present a baseline temporal dependency parser, trained and evaluated on this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1021/>Improving Generalization in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> via Adversarial Training</a></strong><br><a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1021><div class="card-body p-3 small">In order for coreference resolution systems to be useful in practice, they must be able to generalize to new text. In this work, we demonstrate that the performance of the state-of-the-art system decreases when the names of PER and GPE named entities in the CoNLL dataset are changed to names that do not occur in the training set. We use the technique of adversarial gradient-based training to retrain the state-of-the-art system and demonstrate that the retrained system achieves higher performance on the CoNLL dataset (both with and without the change of named entities) and the GAP dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1022/>Improving Human Needs Categorization of Events with Semantic Classification</a></strong><br><a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/e/ellen-riloff/>Ellen Riloff</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1022><div class="card-body p-3 small">Human Needs categories have been used to characterize the reason why an <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective event</a> is positive or negative. For example, I got the flu and I got fired are both negative (undesirable) events, but getting the flu is a Health problem while getting fired is a Financial problem. Previous work created learning models to assign <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> to Human Needs categories based on their words and contexts. In this paper, we introduce an intermediate step that assigns words to relevant semantic concepts. We create lightly supervised models that learn to label words with respect to 10 semantic concepts associated with Human Needs categories, and incorporate these labels as features for event categorization. Our results show that recognizing relevant semantic concepts improves both the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of Human Needs categorization for <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1024/>Automatic Accuracy Prediction for AMR Parsing<span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1024><div class="card-body p-3 small">Abstract Meaning Representation (AMR) represents sentences as directed, acyclic and rooted graphs, aiming at capturing their meaning in a machine readable format. AMR parsing converts natural language sentences into such <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. However, evaluating a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on new data by means of comparison to manually created AMR graphs is very costly. Also, we would like to be able to detect <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> of questionable quality, or preferring results of alternative systems by selecting the ones for which we can assess good quality. We propose AMR accuracy prediction as the task of predicting several metrics of correctness for an automatically generated AMR parse in absence of the corresponding gold parse. We develop a neural end-to-end multi-output regression model and perform three case studies : firstly, we evaluate the model&#8217;s capacity of predicting AMR parse accuracies and test whether it can reliably assign high scores to gold parses. Secondly, we perform parse selection based on predicted parse accuracies of candidate parses from alternative systems, with the aim of improving overall results. Finally, we predict system ranks for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages. All experiments are carried out across two different domains and show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1025/>An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1025><div class="card-body p-3 small">Semantic proto-role labeling (SPRL) is an alternative to semantic role labeling (SRL) that moves beyond a categorical definition of roles, following Dowty&#8217;s feature-based view of proto-roles. This <a href=https://en.wikipedia.org/wiki/Theory>theory</a> determines agenthood vs. patienthood based on a participant&#8217;s instantiation of more or less typical agent vs. patient properties, such as, for example, <a href=https://en.wikipedia.org/wiki/Volition_(psychology)>volition</a> in an event. To perform SPRL, we develop an ensemble of hierarchical models with self-attention and concurrently learned predicate-argument markers. Our method is competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the art</a>, overall outperforming previous work in two formulations of the task (multi-label and multi-variate Likert scale pre- diction). In contrast to previous work, our results do not depend on gold argument heads derived from supplementary gold tree banks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S19-1029" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S19-1029/>Bayesian Inference Semantics : A Modelling System and A Test Suite<span class=acl-fixed-case>B</span>ayesian Inference Semantics: A Modelling System and A Test Suite</a></strong><br><a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/r/rasmus-blanck/>Rasmus Blanck</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a>
|
<a href=/people/a/aleksandre-maskharashvili/>Aleksandre Maskharashvili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1029><div class="card-body p-3 small">We present BIS, a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian Inference Semantics</a>, for probabilistic reasoning in natural language. The current <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the framework of Bernardy et al. (2018), but departs from <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> in important respects. BIS makes use of <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian learning</a> for inferring a hypothesis from premises. This involves estimating the probability of the hypothesis, given the data supplied by the premises of an argument. It uses a syntactic parser to generate typed syntactic structures that serve as input to a model generation system. Sentences are interpreted compositionally to probabilistic programs, and the corresponding truth values are estimated using <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling methods</a>. BIS successfully deals with various probabilistic semantic phenomena, including frequency adverbs, generalised quantifiers, generics, and vague predicates. It performs well on a number of interesting probabilistic reasoning tasks. It also sustains most classically valid inferences (instantiation, <a href=https://en.wikipedia.org/wiki/De_Morgan&#8217;s_laws>de Morgan&#8217;s laws</a>, etc.). To test BIS we have built an experimental test suite with examples of a range of probabilistic and classical inference patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1031/>Incivility Detection in Online Comments</a></strong><br><a href=/people/f/farig-sadeque/>Farig Sadeque</a>
|
<a href=/people/s/stephen-rains/>Stephen Rains</a>
|
<a href=/people/y/yotam-shmargad/>Yotam Shmargad</a>
|
<a href=/people/k/kate-kenski/>Kate Kenski</a>
|
<a href=/people/k/kevin-coe/>Kevin Coe</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1031><div class="card-body p-3 small">Incivility in public discourse has been a major concern in recent times as it can affect the quality and tenacity of the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> negatively. In this paper, we present neural models that can learn to detect <a href=https://en.wikipedia.org/wiki/Name_calling>name-calling</a> and <a href=https://en.wikipedia.org/wiki/Vulgarity>vulgarity</a> from a <a href=https://en.wikipedia.org/wiki/Internet_forum>newspaper comment section</a>. We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like <a href=https://en.wikipedia.org/wiki/Name_calling>namecalling</a> can not be accurately detected by simple models like <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. We apply the models trained on the newspaper comments data to detect uncivil comments in a Russian troll dataset, and find that despite the change of domain, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> makes accurate predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-1032/>Generating Animations from Screenplays</a></strong><br><a href=/people/y/yeyao-zhang/>Yeyao Zhang</a>
|
<a href=/people/e/eleftheria-tsipidi/>Eleftheria Tsipidi</a>
|
<a href=/people/s/sasha-schriber/>Sasha Schriber</a>
|
<a href=/people/m/mubbasir-kapadia/>Mubbasir Kapadia</a>
|
<a href=/people/m/markus-gross/>Markus Gross</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-1032><div class="card-body p-3 small">Automatically generating animation from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> finds application in a number of areas e.g. movie script writing, instructional videos, and <a href=https://en.wikipedia.org/wiki/Public_security>public safety</a>. However, translating <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a> into <a href=https://en.wikipedia.org/wiki/Animation>animation</a> is a challenging task. Existing text-to-animation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>complex sentences</a>. We achieve this by introducing a text simplification step into the <a href=https://en.wikipedia.org/wiki/Process_(computing)>process</a>. Building on an existing animation generation system for <a href=https://en.wikipedia.org/wiki/Screenwriting>screenwriting</a>, we create a robust NLP pipeline to extract information from <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a> and map them to the system&#8217;s knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics. We further evaluated our <a href=https://en.wikipedia.org/wiki/System>system</a> via a user study : 68 % participants believe that our <a href=https://en.wikipedia.org/wiki/System>system</a> generates reasonable <a href=https://en.wikipedia.org/wiki/Animation>animation</a> from input <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>