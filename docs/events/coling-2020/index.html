<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Conference on Computational Linguistics (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Conference on Computational Linguistics (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020coling-main>Proceedings of the 28th International Conference on Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">194&nbsp;papers</span></li><li><a class=align-middle href=#2020coling-demos>Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020coling-tutorials>Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#2020coling-industry>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020argmining-1>Proceedings of the 7th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020cogalex-1>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2020crac-1>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2020dmr-1>Proceedings of the Second International Workshop on Designing Meaning Representations</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020ecomnlp-1>Proceedings of Workshop on Natural Language Processing in E-Commerce</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020fnp-1>Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2020gebnlp-1>Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020lantern-1>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020latechclfl-1>Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2020law-1>Proceedings of the 14th Linguistic Annotation Workshop</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020msr-1>Proceedings of the Third Workshop on Multilingual Surface Realisation</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020mwe-1>Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2020nlp4if-1>Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020peoples-1>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020rdsm-1>Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020semeval-1>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a>
<span class="badge badge-info align-middle ml-1">100&nbsp;papers</span></li><li><a class=align-middle href=#2020smm4h-1>Proceedings of the Fifth Social Media Mining for Health Applications Workshop & Shared Task</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#2020starsem-1>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020textgraphs-1>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020udw-1>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020vardial-1>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2020wanlp-1>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li></ul></div></div><div id=2020coling-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.coling-main/>Proceedings of the 28th International Conference on Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.0/>Proceedings of the 28th International Conference on Computational Linguistics</a></strong><br><a href=/people/d/donia-scott/>Donia Scott</a>
|
<a href=/people/n/nuria-bel/>Nuria Bel</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.4/>CharBERT : Character-aware Pre-trained Language Model<span class=acl-fixed-case>C</span>har<span class=acl-fixed-case>BERT</span>: Character-aware Pre-trained Language Model</a></strong><br><a href=/people/w/wentao-ma/>Wentao Ma</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/c/chenglei-si/>Chenglei Si</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--4><div class="card-body p-3 small">Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can significantly improve the performance and robustness of PLMs simultaneously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.5/>A Graph Representation of Semi-structured Data for Web Question Answering</a></strong><br><a href=/people/x/xingyao-zhang/>Xingyao Zhang</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/l/lijie-wen/>Lijie Wen</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--5><div class="card-body p-3 small">The abundant <a href=https://en.wikipedia.org/wiki/Semi-structured_data>semi-structured data</a> on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, such as HTML-based tables and lists, provide commercial search engines a rich information source for question answering (QA). Different from plain text passages in Web documents, Web tables and lists have inherent structures, which carry semantic correlations among various elements in tables and lists. Many existing studies treat tables and lists as flat documents with pieces of text and do not make good use of semantic information hidden in structures. In this paper, we propose a novel graph representation of Web tables and lists based on a systematic categorization of the components in <a href=https://en.wikipedia.org/wiki/Semi-structured_data>semi-structured data</a> as well as their relations. We also develop pre-training and reasoning techniques on the graph model for the QA task. Extensive experiments on several real datasets collected from a commercial engine verify the effectiveness of our approach. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> by 3.90 points over the state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.10/>Is Killed More Significant than Fled? A Contextual Model for Salient Event Detection</a></strong><br><a href=/people/d/disha-jindal/>Disha Jindal</a>
|
<a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--10><div class="card-body p-3 small">Identifying the key events in a document is critical to holistically understanding its important information. Although measuring the salience of events is highly contextual, most previous work has used a limited representation of events that omits essential information. In this work, we propose a highly contextual model of event salience that uses a rich representation of events, incorporates document-level information and allows for interactions between latent event encodings. Our experimental results on an event salience dataset demonstrate that our model improves over previous work by an absolute 2-4 % on standard metrics, establishing a new state-of-the-art performance for the task. We also propose a new evaluation metric that addresses flaws in previous evaluation methodologies. Finally, we discuss the importance of salient event detection for the downstream task of summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.11/>Appraisal Theories for Emotion Classification in Text</a></strong><br><a href=/people/j/jan-hofmann/>Jan Hofmann</a>
|
<a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/k/kai-sassenberg/>Kai Sassenberg</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--11><div class="card-body p-3 small">Automatic emotion categorization has been predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory, for instance following the fundamental emotion classes proposed by Paul Ekman (fear, joy, anger, disgust, sadness, surprise) or Robert Plutchik (adding trust, anticipation). This approach ignores existing <a href=https://en.wikipedia.org/wiki/Psychology>psychological theories</a> to some degree, which provide explanations regarding the <a href=https://en.wikipedia.org/wiki/Perception>perception of events</a>. For instance, the description that somebody discovers a snake is associated with <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, based on the appraisal as being an unpleasant and non-controllable situation. This emotion reconstruction is even possible without having access to explicit reports of a subjective feeling (for instance expressing this with the words I am afraid.). Automatic classification approaches therefore need to learn properties of events as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> (for instance that the <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> and the mental or physical effort associated with the encounter of a snake leads to fear). With this paper, we propose to make such interpretations of events explicit, following theories of cognitive appraisal of events, and show their potential for <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a> when being encoded in classification models. Our results show that high quality appraisal dimension assignments in event descriptions lead to an improvement in the classification of discrete emotion categories. We make our corpus of appraisal-annotated emotion-associated event descriptions publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.12/>A Symmetric Local Search Network for Emotion-Cause Pair Extraction</a></strong><br><a href=/people/z/zifeng-cheng/>Zifeng Cheng</a>
|
<a href=/people/z/zhiwei-jiang/>Zhiwei Jiang</a>
|
<a href=/people/y/yafeng-yin/>Yafeng Yin</a>
|
<a href=/people/h/hua-yu/>Hua Yu</a>
|
<a href=/people/q/qing-gu/>Qing Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--12><div class="card-body p-3 small">Emotion-cause pair extraction (ECPE) is a new task which aims at extracting the potential clause pairs of emotions and corresponding causes in a document. To tackle this task, a two-step method was proposed by previous study which first extracted emotion clauses and cause clauses individually, then paired the emotion and cause clauses, and filtered out the pairs without <a href=https://en.wikipedia.org/wiki/Causality>causality</a>. Different from this method that separated the <a href=https://en.wikipedia.org/wiki/Detection>detection</a> and the matching of emotion and cause into two steps, we propose a Symmetric Local Search Network (SLSN) model to perform the <a href=https://en.wikipedia.org/wiki/Detection>detection</a> and matching simultaneously by local search. SLSN consists of two symmetric subnetworks, namely the emotion subnetwork and the cause subnetwork. Each <a href=https://en.wikipedia.org/wiki/Subnetwork>subnetwork</a> is composed of a clause representation learner and a local pair searcher. The local pair searcher is a specially-designed cross-subnetwork component which can extract the local emotion-cause pairs. Experimental results on the ECPE corpus demonstrate the superiority of our SLSN over existing state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.14/>METNet : A Mutual Enhanced Transformation Network for Aspect-based Sentiment Analysis<span class=acl-fixed-case>METN</span>et: A Mutual Enhanced Transformation Network for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/b/bin-jiang/>Bin Jiang</a>
|
<a href=/people/j/jing-hou/>Jing Hou</a>
|
<a href=/people/w/wanyue-zhou/>Wanyue Zhou</a>
|
<a href=/people/c/chao-yang/>Chao Yang</a>
|
<a href=/people/s/shihan-wang/>Shihan Wang</a>
|
<a href=/people/l/liang-pang/>Liang Pang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--14><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) aims to determine the sentiment polarity of each specific aspect in a given sentence. Existing researches have realized the importance of the aspect for the ABSA task and have derived many interactive learning methods that model context based on specific aspect. However, current interaction mechanisms are ill-equipped to learn complex sentences with multiple aspects, and these methods underestimate the representation learning of the aspect. In order to solve the two problems, we propose a mutual enhanced transformation network (METNet) for the ABSA task. First, the <a href=https://en.wikipedia.org/wiki/Aspect_(linguistics)>aspect enhancement module</a> in METNet improves the representation learning of the <a href=https://en.wikipedia.org/wiki/Aspect_(linguistics)>aspect</a> with contextual semantic features, which gives the aspect more abundant information. Second, METNet designs and implements a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a>, which enhances the representations of aspect and context iteratively. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of METNet, and we further prove that METNet is outstanding in multi-aspect scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.20/>Affective and Contextual Embedding for Sarcasm Detection</a></strong><br><a href=/people/n/nastaran-babanejad/>Nastaran Babanejad</a>
|
<a href=/people/h/heidar-davoudi/>Heidar Davoudi</a>
|
<a href=/people/a/aijun-an/>Aijun An</a>
|
<a href=/people/m/manos-papagelis/>Manos Papagelis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--20><div class="card-body p-3 small">Automatic sarcasm detection from text is an important classification task that can help identify the actual sentiment in user-generated data, such as <a href=https://en.wikipedia.org/wiki/Review>reviews</a> or <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Despite its usefulness, sarcasm detection remains a challenging task, due to a lack of any <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>vocal intonation</a> or facial gestures in textual data. To date, most of the approaches to addressing the problem have relied on hand-crafted affect features, or pre-trained models of non-contextual word embeddings, such as <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a>. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> inherit limitations that render them inadequate for the task of sarcasm detection. In this paper, we propose two novel <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network models</a> for sarcasm detection, namely ACE 1 and ACE 2. Given as input a text passage, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> predict whether it is sarcastic (or not). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> extend the architecture of BERT by incorporating both affective and contextual features. To the best of our knowledge, this is the first attempt to directly alter BERT&#8217;s architecture and train it from scratch to build a sarcasm classifier. Extensive experiments on different datasets demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for sarcasm detection with significant margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.21/>Understanding Pre-trained BERT for Aspect-based Sentiment Analysis<span class=acl-fixed-case>BERT</span> for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--21><div class="card-body p-3 small">This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for ABSA. The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.27/>Integrating External Event Knowledge for Script Learning</a></strong><br><a href=/people/s/shangwen-lv/>Shangwen Lv</a>
|
<a href=/people/f/fuqing-zhu/>Fuqing Zhu</a>
|
<a href=/people/s/songlin-hu/>Songlin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--27><div class="card-body p-3 small">Script learning aims to predict the subsequent event according to the existing <a href=https://en.wikipedia.org/wiki/Event_chain>event chain</a>. Recent studies focus on event co-occurrence to solve this problem. However, few studies integrate external event knowledge to solve this problem. With our observations, external event knowledge can provide additional knowledge like temporal or causal knowledge for understanding event chain better and predicting the right subsequent event. In this work, we integrate event knowledge from ASER (Activities, States, Events and their Relations) knowledge base to help predict the next event. We propose a new <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> consisting of knowledge retrieval stage and knowledge integration stage. In the knowledge retrieval stage, we select relevant external event knowledge from <a href=https://en.wikipedia.org/wiki/Autostereoscopy>ASER</a>. In the knowledge integration stage, we propose three methods to integrate external knowledge into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and infer final answers. Experiments on the widely-used Multi- Choice Narrative Cloze (MCNC) task show our approach achieves state-of-the-art performance compared to other methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.29/>Heterogeneous Graph Neural Networks to Predict What Happen Next</a></strong><br><a href=/people/j/jianming-zheng/>Jianming Zheng</a>
|
<a href=/people/f/fei-cai/>Fei Cai</a>
|
<a href=/people/y/yanxiang-ling/>Yanxiang Ling</a>
|
<a href=/people/h/honghui-chen/>Honghui Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--29><div class="card-body p-3 small">Given an incomplete event chain, script learning aims to predict the missing event, which can support a series of NLP applications. Existing work can not well represent the heterogeneous relations and capture the discontinuous event segments that are common in the <a href=https://en.wikipedia.org/wiki/Event_chain>event chain</a>. To address these issues, we introduce a heterogeneous-event (HeterEvent) graph network. In particular, we employ each unique word and individual event as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, and explore three kinds of <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> based on <a href=https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences>realistic relations</a> (e.g., the relations of word-and-word, word-and-event, event-and-event). We also design a <a href=https://en.wikipedia.org/wiki/Message_passing>message passing process</a> to realize <a href=https://en.wikipedia.org/wiki/Information_exchange>information interactions</a> among <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>homo or heterogeneous nodes</a>. And the discontinuous event segments could be explicitly modeled by finding the specific path between corresponding nodes in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. The experimental results on one-step and multi-step inference tasks demonstrate that our ensemble model HeterEvent[W+E ] can outperform existing baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.35/>Predicting Stance Change Using Modular Architectures</a></strong><br><a href=/people/a/aldo-porco/>Aldo Porco</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--35><div class="card-body p-3 small">The ability to change a person&#8217;s mind on a given issue depends both on the arguments they are presented with and on their underlying perspectives and biases on that issue. Predicting stance changes require characterizing both aspects and the interaction between them, especially in realistic settings in which <a href=https://en.wikipedia.org/wiki/Stance_(martial_arts)>stance changes</a> are very rare. In this paper, we suggest a modular learning approach, which decomposes the task into multiple modules, focusing on different aspects of the interaction between users, their beliefs, and the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.37/>Multimodal Review Generation with Privacy and Fairness Awareness</a></strong><br><a href=/people/x/xuan-son-vu/>Xuan-Son Vu</a>
|
<a href=/people/t/thanh-son-nguyen/>Thanh-Son Nguyen</a>
|
<a href=/people/d/duc-trong-le/>Duc-Trong Le</a>
|
<a href=/people/l/lili-jiang/>Lili Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--37><div class="card-body p-3 small">Users express their opinions towards entities (e.g., restaurants) via <a href=https://en.wikipedia.org/wiki/Review_site>online reviews</a> which can be in diverse forms such as <a href=https://en.wikipedia.org/wiki/Written_language>text</a>, <a href=https://en.wikipedia.org/wiki/Audience_measurement>ratings</a>, and <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>. Modeling reviews are advantageous for user behavior understanding which, in turn, supports various user-oriented tasks such as <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and review generation. In this paper, we propose MG-PriFair, a multimodal neural-based framework, which generates personalized reviews with privacy and fairness awareness. Motivated by the fact that reviews might contain personal information and sentiment bias, we propose a novel differentially private (dp)-embedding model for training privacy guaranteed embeddings and an evaluation approach for sentiment fairness in the food-review domain. Experiments on our novel review dataset show that MG-PriFair is capable of generating plausibly long reviews while controlling the amount of exploited user data and using the least sentiment biased word embeddings. To the best of our knowledge, we are the first to bring user privacy and sentiment fairness into the review generation task. The dataset and source codes are available at https://github.com/ReML-AI/MG-PriFair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.39/>Improving Abstractive Dialogue Summarization with <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>Graph Structures</a> and Topic Words</a></strong><br><a href=/people/l/lulu-zhao/>Lulu Zhao</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/j/jun-guo/>Jun Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--39><div class="card-body p-3 small">Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms most of the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.42/>Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems : A Survey</a></strong><br><a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--42><div class="card-body p-3 small">In recent years, fostered by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> and by the high demand for conversational AI, various approaches have been proposed that address the capacity to elicit and understand user&#8217;s needs in task-oriented dialogue systems. We focus on two core tasks, slot filling (SF) and intent classification (IC), and survey how neural based models have rapidly evolved to address <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> in dialogue systems. We introduce three neural architectures : independent models, which model SF and IC separately, joint models, which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models, that scale the model to new domains. We discuss the current state of the research in <a href=https://en.wikipedia.org/wiki/Silicon_carbide>SF</a> and <a href=https://en.wikipedia.org/wiki/Silicon_carbide>IC</a>, and highlight challenges that still require attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.43/>Re-framing Incremental Deep Language Models for <a href=https://en.wikipedia.org/wiki/Dialogue_processing>Dialogue Processing</a> with <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a></a></strong><br><a href=/people/m/morteza-rohanian/>Morteza Rohanian</a>
|
<a href=/people/j/julian-hough/>Julian Hough</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--43><div class="card-body p-3 small">We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance and is beneficial for future use in conversational agents in psychiatric treatment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.58/>TIMBERT : Toponym Identifier For The Medical Domain Based on BERT<span class=acl-fixed-case>TIMBERT</span>: Toponym Identifier For The Medical Domain Based on <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/mohammadreza-davari/>MohammadReza Davari</a>
|
<a href=/people/l/leila-kosseim/>Leila Kosseim</a>
|
<a href=/people/t/tien-bui/>Tien Bui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--58><div class="card-body p-3 small">In this paper, we propose an approach to automate the process of place name detection in the medical domain to enable epidemiologists to better study and model the spread of viruses. We created a family of Toponym Identification Models based on BERT (TIMBERT), in order to learn in an end-to-end fashion the mapping from an input sentence to the associated sentence labeled with <a href=https://en.wikipedia.org/wiki/Toponymy>toponyms</a>. When evaluated with the SemEval 2019 task 12 test set (Weissenbacher et al., 2019), our best TIMBERT model achieves an F1 score of 90.85 %, a significant improvement compared to the state-of-the-art of 89.13 % (Wang et al., 2019).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.61/>Identifying Depressive Symptoms from Tweets : Figurative Language Enabled Multitask Learning Framework</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/j/jainish-chauhan/>Jainish Chauhan</a>
|
<a href=/people/j/joy-prakash-sain/>Joy Prakash Sain</a>
|
<a href=/people/k/krishnaprasad-thirunarayan/>Krishnaprasad Thirunarayan</a>
|
<a href=/people/a/amit-sheth/>Amit Sheth</a>
|
<a href=/people/j/jeremiah-schumm/>Jeremiah Schumm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--61><div class="card-body p-3 small">Existing studies on using <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for deriving <a href=https://en.wikipedia.org/wiki/Mental_health>mental health status</a> of users focus on the depression detection task. However, for <a href=https://en.wikipedia.org/wiki/Case_management_(mental_health)>case management</a> and referral to psychiatrists, health-care workers require practical and scalable depressive disorder screening and triage system. This study aims to design and evaluate a decision support system (DSS) to reliably determine the depressive triage level by capturing fine-grained depressive symptoms expressed in user tweets through the emulation of the Patient Health Questionnaire-9 (PHQ-9) that is routinely used in clinical practice. The reliable detection of depressive symptoms from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> is challenging because the 280-character limit on tweets incentivizes the use of creative artifacts in the utterances and figurative usage contributes to effective expression. We propose a novel BERT based robust multi-task learning framework to accurately identify the depressive symptoms using the auxiliary task of figurative usage detection. Specifically, our proposed novel task sharing mechanism, co-task aware attention, enables automatic selection of optimal information across the BERT lay-ers and tasks by soft-sharing of parameters. Our results show that modeling figurative usage can demonstrably improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s robustness and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> for distinguishing the <a href=https://en.wikipedia.org/wiki/Major_depressive_disorder>depression symptoms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.64" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.64/>Probing Multimodal Embeddings for Linguistic Properties : the Visual-Semantic Case</a></strong><br><a href=/people/a/adam-dahlgren-lindstrom/>Adam Dahlgren Lindström</a>
|
<a href=/people/j/johanna-bjorklund/>Johanna Björklund</a>
|
<a href=/people/s/suna-bensch/>Suna Bensch</a>
|
<a href=/people/f/frank-drewes/>Frank Drewes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--64><div class="card-body p-3 small">Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion ofprobing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for <a href=https://en.wikipedia.org/wiki/Embedding>embeddings of image-caption pairs</a>, (ii) define three concrete probing tasks within our general framework, (iii) train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to probe for those properties, and (iv) compare various state-of-the-art <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> under the lens of the proposed probing tasks. Our experiments reveal an up to 16 % increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.72/>Aspect-Category based Sentiment Analysis with Hierarchical Graph Convolutional Network</a></strong><br><a href=/people/h/hongjie-cai/>Hongjie Cai</a>
|
<a href=/people/y/yaofeng-tu/>Yaofeng Tu</a>
|
<a href=/people/x/xiangsheng-zhou/>Xiangsheng Zhou</a>
|
<a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--72><div class="card-body p-3 small">Most of the aspect based sentiment analysis research aims at identifying the sentiment polarities toward some explicit aspect terms while ignores implicit aspects in text. To capture both explicit and implicit aspects, we focus on aspect-category based sentiment analysis, which involves joint aspect category detection and category-oriented sentiment classification. However, currently only a few simple studies have focused on this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. The shortcomings in the way they defined the task make their approaches difficult to effectively learn the inner-relations between categories and the inter-relations between categories and sentiments. In this work, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a hierarchy output structure to first identify multiple aspect categories in a piece of text, and then predict the sentiment for each of the identified categories. Specifically, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where a lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the inter-relations between aspect categories and sentiments. Extensive evaluations demonstrate that our hierarchy output structure is superior over existing ones, and the Hier-GCN model can consistently achieve the best results on four benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.73/>Constituency Lattice Encoding for Aspect Term Extraction</a></strong><br><a href=/people/y/yunyi-yang/>Yunyi Yang</a>
|
<a href=/people/k/kun-li/>Kun Li</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a>
|
<a href=/people/w/weizhou-shen/>Weizhou Shen</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--73><div class="card-body p-3 small">One of the remaining challenges for aspect term extraction in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> resides in the extraction of phrase-level aspect terms, which is non-trivial to determine the boundaries of such terms. In this paper, we aim to address this issue by incorporating the span annotations of constituents of a sentence to leverage the syntactic information in neural network models. To this end, we first construct a constituency lattice structure based on the constituents of a constituency tree. Then, we present two approaches to encoding the constituency lattice using BiLSTM-CRF and BERT as the base models, respectively. We experimented on two benchmark datasets to evaluate the two models, and the results confirm their superiority with respective 3.17 and 1.35 points gained in F1-Measure over the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>. The improvements justify the effectiveness of the constituency lattice for aspect term extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.81" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.81/>A Dataset and Evaluation Framework for Complex Geographical Description Parsing</a></strong><br><a href=/people/e/egoitz-laparra/>Egoitz Laparra</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--81><div class="card-body p-3 small">Much previous work on <a href=https://en.wikipedia.org/wiki/Geoparsing>geoparsing</a> has focused on identifying and resolving individual <a href=https://en.wikipedia.org/wiki/Toponymy>toponyms</a> in text like Adrano, S.Maria di Licodia or Catania. However, geographical locations occur not only as individual toponyms, but also as compositions of reference geolocations joined and modified by connectives, e.g.,. between the towns of Adrano and S.Maria di Licodia, 32 kilometres northwest of Catania. Ideally, a geoparser should be able to take such text, and the geographical shapes of the toponyms referenced within it, and parse these into a geographical shape, formed by a set of coordinates, that represents the location described. But creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this complex geoparsing task is difficult and, if done manually, would require a huge amount of effort to annotate the geographical shapes of not only the <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> described but also the reference toponyms. We present an approach that automates most of the process by combining <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and <a href=https://en.wikipedia.org/wiki/OpenStreetMap>OpenStreetMap</a>. As a result, we have gathered a <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.82/>DocBank : A Benchmark Dataset for <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>Document Layout Analysis</a><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>B</span>ank: A Benchmark Dataset for Document Layout Analysis</a></strong><br><a href=/people/m/minghao-li/>Minghao Li</a>
|
<a href=/people/y/yiheng-xu/>Yiheng Xu</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--82><div class="card-body p-3 small">Document layout analysis usually relies on <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision models</a> to understand documents while ignoring <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual information</a> that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present DocBank, a benchmark dataset that contains 500 K document pages with fine-grained token-level annotations for <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>document layout analysis</a>. DocBank is constructed using a simple yet effective way with weak supervision from the LaTeX documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of <a href=https://en.wikipedia.org/wiki/Document_layout_analysis>document layout analysis</a>. We build several strong baselines and manually split train / dev / test sets for evaluation. Experiment results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at https://github.com/doc-analysis/DocBank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.84/>A High Precision Pipeline for Financial Knowledge Graph Construction</a></strong><br><a href=/people/s/sarah-elhammadi/>Sarah Elhammadi</a>
|
<a href=/people/l/laks-v-s-lakshmanan/>Laks V.S. Lakshmanan</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a>
|
<a href=/people/m/michael-simpson/>Michael Simpson</a>
|
<a href=/people/b/baoxing-huai/>Baoxing Huai</a>
|
<a href=/people/z/zhefeng-wang/>Zhefeng Wang</a>
|
<a href=/people/l/lanjun-wang/>Lanjun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--84><div class="card-body p-3 small">Motivated by applications such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>, and <a href=https://en.wikipedia.org/wiki/Data_integration>data integration</a>, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject, predicate, object) triples in a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, <a href=https://en.wikipedia.org/wiki/Market_(economics)>markets</a>, <a href=https://en.wikipedia.org/wiki/Currency>currencies</a>, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> automatically by <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the <a href=https://en.wikipedia.org/wiki/Financial_services>financial domain</a>. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78 % at the top-100 extractions. The extracted triples are stored in a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> making them readily available for use in downstream applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.88/>Automatic Charge Identification from Facts : A Few Sentence-Level Charge Annotations is All You Need</a></strong><br><a href=/people/s/shounak-paul/>Shounak Paul</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/s/saptarshi-ghosh/>Saptarshi Ghosh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--88><div class="card-body p-3 small">Automatic Charge Identification (ACI) is the task of identifying the relevant charges given the facts of a situation and the statutory laws that define these <a href=https://en.wikipedia.org/wiki/Criminal_charge>charges</a>, and is a crucial aspect of the <a href=https://en.wikipedia.org/wiki/Procedural_law>judicial process</a>. Existing works focus on learning charge-side representations by modeling relationships between the charges, but not much effort has been made in improving fact-side representations. We observe that only a small fraction of sentences in the facts actually indicates the charges. We show that by using a very small subset (3 %) of fact descriptions annotated with sentence-level charges, we can achieve an improvement across a range of different ACI models, as compared to modeling just the main document-level task on a much larger dataset. Additionally, we propose a novel model that utilizes sentence-level charge labels as an auxiliary task, coupled with the main task of document-level charge identification in a multi-task learning framework. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> comprehensively outperforms a large number of recent <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for ACI. The improvement in performance is particularly noticeable for the rare charges which are known to be especially challenging to identify.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.89/>Context-Aware Text Normalisation for Historical Dialects</a></strong><br><a href=/people/m/maria-sukhareva/>Maria Sukhareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--89><div class="card-body p-3 small">Context-aware historical text normalisation is a severely under-researched area. To fill the gap we propose a context-aware normalisation approach that relies on the state-of-the-art methods in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We propose a multidialect normaliser with a context-aware reranking of the candidates. The reranker relies on a word-level n-gram language model that is applied to the five best normalisation candidates. The results are evaluated on the historical multidialect datasets of <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>. We show that incorporating dialectal information into the training leads to an accuracy improvement on all the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. The context-aware reranking gives further improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. For three out of six datasets, we reach a significantly higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than reported in the previous studies. The other three results are comparable with the current state-of-the-art. The code for the reranker is published as open-source.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.91/>Exploring Amharic Sentiment Analysis from Social Media Texts : Building Annotation Tools and Classification Models<span class=acl-fixed-case>A</span>mharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/h/hizkiel-mitiku-alemayehu/>Hizkiel Mitiku Alemayehu</a>
|
<a href=/people/a/abinew-ayele/>Abinew Ayele</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--91><div class="card-body p-3 small">This paper presents the study of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> for Amharic social media texts. As the number of social media users is ever-increasing, social media platforms would like to understand the latent meaning and sentiments of a text to enhance decision-making procedures. However, low-resource languages such as <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> have received less attention due to several reasons such as lack of well-annotated datasets, unavailability of computing resources, and fewer or no expert researchers in the area. This research addresses three main research questions. We first explore the suitability of existing <a href=https://en.wikipedia.org/wiki/Programming_tool>tools</a> for the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis task</a>. Annotation tools are scarce to support large-scale annotation tasks in <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a>. Also, the existing <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platforms</a> do not support Amharic text annotation. Hence, we build a social-network-friendly annotation tool called &#8216;ASAB&#8217; using the <a href=https://en.wikipedia.org/wiki/Telegram_(software)>Telegram bot</a>. We collect 9.4k tweets, where each tweet is annotated by three Telegram users. Moreover, we explore the suitability of machine learning approaches for Amharic sentiment analysis. The FLAIR deep learning text classifier, based on network embeddings that are computed from a distributional thesaurus, outperforms other <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a>. We further investigate the challenges in building a sentiment analysis system for <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and we found that the widespread usage of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative speech</a> are the main issues in dealing with the problem. To advance the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> research in <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and other related low-resource languages, we release the dataset, the annotation tool, source code, and models publicly under a permissive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.92/>Effective Few-Shot Classification with Transfer Learning</a></strong><br><a href=/people/a/aakriti-gupta/>Aakriti Gupta</a>
|
<a href=/people/k/kapil-thadani/>Kapil Thadani</a>
|
<a href=/people/n/neil-ohare/>Neil O’Hare</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--92><div class="card-body p-3 small">Few-shot learning addresses the the problem of <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> based on a small amount of training data. Although more well-studied in the domain of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, recent work has adapted the Amazon Review Sentiment Classification (ARSC) text dataset for use in the few-shot setting. In this work, we use the ARSC dataset to study a simple application of transfer learning approaches to few-shot classification. We train a single binary classifier to learn all few-shot classes jointly by prefixing class identifiers to the input text. Given the text and class, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then makes a binary prediction for that text / class pair. Our results show that this simple approach can outperform most published results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Surprisingly, we also show that including domain information as part of the task definition only leads to a modest improvement in model accuracy, and zero-shot classification, without further fine-tuning on few-shot domains, performs equivalently to few-shot classification. These results suggest that the classes in the ARSC few-shot task, which are defined by the intersection of domain and rating, are actually very similar to each other, and that a more suitable dataset is needed for the study of few-shot text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.97/>Meet Changes with Constancy : Learning Invariance in Multi-Source Translation</a></strong><br><a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/ling-luo/>Ling Luo</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/j/jian-ye/>Jian Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--97><div class="card-body p-3 small">Multi-source neural machine translation aims to translate from parallel sources of information (e.g. languages, <a href=https://en.wikipedia.org/wiki/Image_(journal)>images</a>, etc.) to a single target language, which has shown better performance than most one-to-one systems. Despite the remarkable success of existing models, they usually neglect the fact that multiple source inputs may have inconsistencies. Such differences might bring noise to the task and limit the performance of existing multi-source NMT approaches due to their indiscriminate usage of input sources for target word predictions. In this paper, we attempt to leverage the potential complementary information among distinct sources and alleviate the occasional conflicts of them. To accomplish that, we propose a source invariance network to learn the <a href=https://en.wikipedia.org/wiki/Invariant_(mathematics)>invariant information</a> of parallel sources. Such <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> can be easily integrated with multi-encoder based multi-source NMT methods (e.g. multi-encoder RNN and transformer) to enhance the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results. Extensive experiments on two multi-source translation tasks demonstrate that the proposed approach not only achieves clear gains in translation quality but also captures implicit invariance between different sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.109/>How Relevant Are Selectional Preferences for Transformer-based Language Models?</a></strong><br><a href=/people/e/eleni-metheniti/>Eleni Metheniti</a>
|
<a href=/people/t/tim-van-de-cruys/>Tim Van de Cruys</a>
|
<a href=/people/n/nabil-hathout/>Nabil Hathout</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--109><div class="card-body p-3 small">Selectional preference is defined as the tendency of a predicate to favor particular arguments within a certain <a href=https://en.wikipedia.org/wiki/Context_(language_use)>linguistic context</a>, and likewise, reject others that result in conflicting or implausible meanings. The stellar success of contextual word embedding models such as BERT in NLP tasks has led many to question whether these models have learned linguistic information, but up till now, most research has focused on syntactic information. We investigate whether <a href=https://en.wikipedia.org/wiki/Bert_Diaries>Bert</a> contains information on the selectional preferences of words, by examining the probability it assigns to the dependent word given the presence of a <a href=https://en.wikipedia.org/wiki/Headword>head word</a> in a sentence. We are using word pairs of head-dependent words in five different syntactic relations from the SP-10 K corpus of selectional preference (Zhang et al., 2019b), in sentences from the ukWaC corpus, and we are calculating the correlation of the plausibility score (from SP-10 K) and the model probabilities. Our results show that overall, there is no strong positive or negative correlation in any syntactic relation, but we do find that certain head words have a strong correlation and that masking all words but the head word yields the most positive correlations in most scenarios which indicates that the semantics of the predicate is indeed an integral and influential factor for the selection of the argument.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.111/>A Retrofitting Model for Incorporating <a href=https://en.wikipedia.org/wiki/Semantic_relation>Semantic Relations</a> into Word Embeddings</a></strong><br><a href=/people/s/sapan-shah/>Sapan Shah</a>
|
<a href=/people/s/sreedhar-reddy/>Sreedhar Reddy</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--111><div class="card-body p-3 small">We present a novel retrofitting model that can leverage relational knowledge available in a knowledge resource to improve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The knowledge is captured in terms of relation inequality constraints that compare similarity of related and unrelated entities in the context of an anchor entity. These <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> are used as training data to learn a non-linear transformation function that maps original word vectors to a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> respecting these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>. The transformation function is learned in a similarity metric learning setting using Triplet network architecture. We applied our model to synonymy, antonymy and hypernymy relations in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.112/>Lexical Relation Mining in Neural Word Embeddings</a></strong><br><a href=/people/a/aishwarya-jadhav/>Aishwarya Jadhav</a>
|
<a href=/people/y/yifat-amir/>Yifat Amir</a>
|
<a href=/people/z/zachary-pardos/>Zachary Pardos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--112><div class="card-body p-3 small">Work with neural word embeddings and lexical relations has largely focused on confirmatory experiments which use human-curated examples of semantic and syntactic relations to validate against. In this paper, we explore the degree to which lexical relations, such as those found in popular validation sets, can be derived and extended from a variety of neural embeddings using classical clustering methods. We show that the Word2Vec space of word-pairs (i.e., offset vectors) significantly outperforms other more contemporary methods, even in the presence of a large number of noisy offsets. Moreover, we show that via a simple <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor approach</a> in the offset space, new examples of known relations can be discovered. Our results speak to the amenability of offset vectors from non-contextual neural embeddings to find semantically coherent clusters. This simple approach has implications for the exploration of emergent regularities and their examples, such as emerging trends on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and their related posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.114/>BERT-based Cohesion Analysis of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese Texts</a><span class=acl-fixed-case>BERT</span>-based Cohesion Analysis of <span class=acl-fixed-case>J</span>apanese Texts</a></strong><br><a href=/people/n/nobuhiro-ueda/>Nobuhiro Ueda</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--114><div class="card-body p-3 small">The meaning of natural language text is supported by <a href=https://en.wikipedia.org/wiki/Cohesion_(linguistics)>cohesion</a> among various kinds of entities, including <a href=https://en.wikipedia.org/wiki/Coreference>coreference relations</a>, predicate-argument structures, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>bridging anaphora relations</a>. However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult. Recent advances in neural networks, in particular, self training-based language models including <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text. In this study, we tackle an integrated analysis of cohesion in <a href=https://en.wikipedia.org/wiki/Japanese_literature>Japanese texts</a>. Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution. Furthermore, we also showed that <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is different in nature from the other <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and should be treated specially.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.115/>Schema Aware Semantic Reasoning for Interpreting Natural Language Queries in Enterprise Settings</a></strong><br><a href=/people/j/jaydeep-sen/>Jaydeep Sen</a>
|
<a href=/people/t/tanaya-babtiwale/>Tanaya Babtiwale</a>
|
<a href=/people/k/kanishk-saxena/>Kanishk Saxena</a>
|
<a href=/people/y/yash-butala/>Yash Butala</a>
|
<a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--115><div class="card-body p-3 small">Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, <a href=https://en.wikipedia.org/wiki/Computer_data_storage>data storage</a>, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with 4 benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of ~30 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.117/>What Can We Learn from Noun Substitutions in Revision Histories?</a></strong><br><a href=/people/t/talita-anthonio/>Talita Anthonio</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--117><div class="card-body p-3 small">In community-edited resources such as <a href=https://en.wikipedia.org/wiki/WikiHow>wikiHow</a>, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> and validate in how far the assumption that &#8216;revised means better&#8217; actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>, <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a> and <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hypernymy</a>. Despite the high <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>, we find that a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> can distinguish the revised version of a sentence from an original version with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> close to 70 %, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the &#8216;better version&#8217; with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.118/>Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--118><div class="card-body p-3 small">Unsupervised pretraining models have been shown to facilitate a wide range of downstream NLP applications. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, however, retain some of the limitations of traditional static word embeddings. In particular, they encode only the distributional knowledge available in raw text corpora, incorporated through language modeling objectives. In this work, we complement such distributional knowledge with external lexical knowledge, that is, we integrate the discrete knowledge on word-level semantic similarity into pretraining. To this end, we generalize the standard BERT model to a multi-task learning setting where we couple BERT&#8217;s masked language modeling and next sentence prediction objectives with an auxiliary task of binary word relation classification. Our experiments suggest that our Lexically Informed BERT (LIBERT), specialized for the word-level semantic similarity, yields better performance than the lexically blind vanilla BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.125/>A Deep Generative Distance-Based Classifier for Out-of-Domain Detection with Mahalanobis Space</a></strong><br><a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/s/sihong-liu/>Sihong Liu</a>
|
<a href=/people/z/zijun-liu/>Zijun Liu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--125><div class="card-body p-3 small">Detecting out-of-domain (OOD) input intents is critical in the task-oriented dialog system. Different from most existing methods that rely heavily on manually labeled OOD samples, we focus on the unsupervised OOD detection scenario where there are no labeled OOD samples except for labeled in-domain data. In this paper, we propose a simple but strong generative distance-based classifier to detect OOD samples. We estimate the class-conditional distribution on feature spaces of DNNs via Gaussian discriminant analysis (GDA) to avoid over-confidence problems. And we use two distance functions, Euclidean and Mahalanobis distances, to measure the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence score</a> of whether a test sample belongs to OOD. Experiments on four benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can consistently outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.126/>Contrastive Zero-Shot Learning for Cross-Domain Slot Filling with Adversarial Attack</a></strong><br><a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--126><div class="card-body p-3 small">Zero-shot slot filling has widely arisen to cope with data scarcity in target domains. However, previous approaches often ignore constraints between slot value representation and related slot description representation in the latent space and lack enough <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>. In this paper, we propose a Contrastive Zero-Shot Learning with Adversarial Attack (CZSL-Adv) method for the cross-domain slot filling. The contrastive loss aims to map slot value contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>model robustness</a>. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.128/>Contextual Argument Component Classification for Class Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--128><div class="card-body p-3 small">Argument mining systems often consider <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>, i.e. information outside of an argumentative discourse unit, when trained to accomplish tasks such as argument component identification, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, and relation extraction. However, prior work has not carefully analyzed the utility of different <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual properties</a> in context-aware models. In this work, we show how two different types of contextual information, local discourse context and speaker context, can be incorporated into a computational model for classifying argument components in multi-party classroom discussions. We find that both context types can improve performance, although the improvements are dependent on context size and position.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.130/>Pre-trained Language Model Based Active Learning for Sentence Matching</a></strong><br><a href=/people/g/guirong-bai/>Guirong Bai</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/z/zaiqing-nie/>Zaiqing Nie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--130><div class="card-body p-3 small">Active learning is able to significantly reduce the annotation cost for <a href=https://en.wikipedia.org/wiki/Data-driven_learning>data-driven techniques</a>. However, previous active learning approaches for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with fewer labeled training instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.132/>Using a Penalty-based Loss Re-estimation Method to Improve Implicit Discourse Relation Classification</a></strong><br><a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/h/huibin-ruan/>Huibin Ruan</a>
|
<a href=/people/z/zhen-huang/>Zhen Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--132><div class="card-body p-3 small">We tackle implicit discourse relation classification, a task of automatically determining semantic relationships between arguments. The attention-worthy words in arguments are crucial clues for classifying the discourse relations. Attention mechanisms have been proven effective in highlighting the attention-worthy words during <a href=https://en.wikipedia.org/wiki/Encoding_(memory)>encoding</a>. However, our survey shows that some inessential words are unintentionally misjudged as the attention-worthy words and, therefore, assigned heavier attention weights than should be. We propose a penalty-based loss re-estimation method to regulate the attention learning process, integrating penalty coefficients into the computation of loss by means of overstability of attention weight distributions. We conduct experiments on the Penn Discourse TreeBank (PDTB) corpus. The test results show that our loss re-estimation method leads to substantial improvements for a variety of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, and it obtains highly competitive performance compared to the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.134/>Knowledge Graph Embedding with Atrous Convolution and Residual Learning</a></strong><br><a href=/people/f/feiliang-ren/>Feiliang Ren</a>
|
<a href=/people/j/juchen-li/>Juchen Li</a>
|
<a href=/people/h/huihui-zhang/>Huihui Zhang</a>
|
<a href=/people/s/shilei-liu/>Shilei Liu</a>
|
<a href=/people/b/bochao-li/>Bochao Li</a>
|
<a href=/people/r/ruicheng-ming/>Ruicheng Ming</a>
|
<a href=/people/y/yujia-bai/>Yujia Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--134><div class="card-body p-3 small">Knowledge graph embedding is an important task and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>methods</a> are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> has following main characteristics. First, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> effectively increases <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature interactions</a> by using atrous convolutions. Second, to address the original information forgotten issue and vanishing / exploding gradient issue, it uses the residual learning method. Third, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has simpler structure but much higher parameter efficiency. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on six benchmark datasets with different evaluation metrics. Extensive experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is very effective. On these diverse datasets, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> could be found at https://github.com/neukg/AcrE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.138/>TPLinker : Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking<span class=acl-fixed-case>TPL</span>inker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a></strong><br><a href=/people/y/yucheng-wang/>Yucheng Wang</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/y/yueyang-zhang/>Yueyang Zhang</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/h/hongsong-zhu/>Hongsong Zhu</a>
|
<a href=/people/l/limin-sun/>Limin Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--138><div class="card-body p-3 small">Extracting entities and relations from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in identifying overlapping relations with shared entities. Prior works show that joint learning can result in a noticeable performance gain. However, they usually involve sequential interrelated steps and suffer from the problem of <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. At training time, they predict with the ground truth conditions while at inference it has to make extraction from scratch. This discrepancy leads to error accumulation. To mitigate the issue, we propose in this paper a one-stage joint extraction model, namely, TPLinker, which is capable of discovering overlapping relations sharing one or both entities while being immune from the <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. TPLinker formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Experiment results show that TPLinker performs significantly better on overlapping and multiple relation extraction, and achieves state-of-the-art performance on two public datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.141/>Unsupervised Deep Language and Dialect Identification for Short Texts</a></strong><br><a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/r/rajdeep-sarkar/>Rajdeep Sarkar</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--141><div class="card-body p-3 small">Automatic Language Identification (LI) or Dialect Identification (DI) of short texts of closely related languages or dialects, is one of the primary steps in many natural language processing pipelines. Language identification is considered a solved task in many cases ; however, in the case of very closely related languages, or in an unsupervised scenario (where the languages are not known in advance), performance is still poor. In this paper, we propose the Unsupervised Deep Language and Dialect Identification (UDLDI) method, which can simultaneously learn sentence embeddings and cluster assignments from short texts. The UDLDI model understands the sentence constructions of languages by applying attention to character relations which helps to optimize the clustering of languages. We have performed our experiments on three short-text datasets for different <a href=https://en.wikipedia.org/wiki/Language_family>language families</a>, each consisting of closely related languages or dialects, with very minimal training sets. Our experimental evaluations on these datasets have shown significant improvement over state-of-the-art <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> and our model has outperformed state-of-the-art LI and DI systems in <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised settings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.145/>Improving Long-Tail Relation Extraction with Collaborating Relation-Augmented Attention</a></strong><br><a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/t/tao-shen/>Tao Shen</a>
|
<a href=/people/g/guodong-long/>Guodong Long</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/t/tianyi-zhou/>Tianyi Zhou</a>
|
<a href=/people/c/chengqi-zhang/>Chengqi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--145><div class="card-body p-3 small">Wrong labeling problem and long-tail relations are two main challenges caused by distant supervision in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Recent works alleviate the wrong labeling by selective attention via multi-instance learning, but can not well handle long-tail relations even if hierarchies of the relations are introduced to share knowledge. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, Collaborating Relation-augmented Attention (CoRA), to handle both the wrong labeling and long-tail relations. Particularly, we first propose relation-augmented attention network as base model. It operates on sentence bag with a sentence-to-relation attention to minimize the effect of wrong labeling. Then, facilitated by the proposed base model, we introduce collaborating relation features shared among relations in the hierarchies to promote the relation-augmenting process and balance the training data for long-tail relations. Besides the main training objective to predict the relation of a sentence bag, an auxiliary objective is utilized to guide the relation-augmenting process for a more accurate bag-level representation. In the experiments on the popular benchmark dataset NYT, the proposed CoRA improves the prior state-of-the-art performance by a large margin in terms of <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision@N</a>, <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a> and Hits@K. Further analyses verify its superior capability in handling long-tail relations in contrast to the competitors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.146/>ToHRE : A Top-Down Classification Strategy with Hierarchical Bag Representation for Distantly Supervised Relation Extraction<span class=acl-fixed-case>T</span>o<span class=acl-fixed-case>HRE</span>: A Top-Down Classification Strategy with Hierarchical Bag Representation for Distantly Supervised Relation Extraction</a></strong><br><a href=/people/e/erxin-yu/>Erxin Yu</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yuan-tian/>Yuan Tian</a>
|
<a href=/people/y/yi-chang/>Yi Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--146><div class="card-body p-3 small">Distantly Supervised Relation Extraction (DSRE) has proven to be effective to find relational facts from texts, but it still suffers from two main problems : the wrong labeling problem and the long-tail problem. Most of the existing approaches address these two problems through flat classification, which lacks hierarchical information of relations. To leverage the informative relation hierarchies, we formulate DSRE as a hierarchical classification task and propose a novel hierarchical classification framework, which extracts the relation in a top-down manner. Specifically, in our proposed framework, 1) we use a hierarchically-refined representation method to achieve hierarchy-specific representation ; 2) a top-down classification strategy is introduced instead of training a set of local classifiers. The experiments on NYT dataset demonstrate that our approach significantly outperforms other state-of-the-art approaches, especially for the long-tail problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.156/>Combining Event Semantics and Degree Semantics for Natural Language Inference</a></strong><br><a href=/people/i/izumi-haruta/>Izumi Haruta</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--156><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)>formal semantics</a>, there are two well-developed semantic frameworks : event semantics, which treats verbs and adverbial modifiers using the notion of event, and degree semantics, which analyzes adjectives and comparatives using the notion of degree. However, it is not obvious whether these <a href=https://en.wikipedia.org/wiki/Conceptual_framework>frameworks</a> can be combined to handle cases in which the phenomena in question are interacting with each other. Here, we study this issue by focusing on natural language inference (NLI). We implement a logic-based NLI system that combines event semantics and degree semantics and their interaction with lexical knowledge. We evaluate the <a href=https://en.wikipedia.org/wiki/System>system</a> on various NLI datasets containing linguistically challenging problems. The results show that the system achieves high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracies</a> on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in comparison with previous <a href=https://en.wikipedia.org/wiki/Logic_programming>logic-based systems</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning-based systems</a>. This suggests that the two semantic frameworks can be combined consistently to handle various combinations of linguistic phenomena without compromising the advantage of either <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.163/>Detecting de minimis Code-Switching in Historical German Books<span class=acl-fixed-case>G</span>erman Books</a></strong><br><a href=/people/s/shijia-liu/>Shijia Liu</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--163><div class="card-body p-3 small">Code-switching has long interested linguists, with computational work in particular focusing on speech and social media data (Sitaram et al., 2019). This paper contrasts these informal instances of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> to its appearance in more formal registers, by examining the mixture of languages in the Deutsches Textarchiv (DTA), a corpus of 1406 primarily German books from the 17th to 19th centuries. We automatically annotate and manually inspect spans of six embedded languages (Latin, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and Greek) in the corpus. We quantitatively analyze the differences between code-switching patterns in these books and those in more typically studied speech and social media corpora. Furthermore, we address the practical task of predicting code-switching from <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of the matrix language alone in the DTA corpus. Such <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> can help reduce errors when <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition</a> or <a href=https://en.wikipedia.org/wiki/Speech_synthesis>speech transcription</a> is applied to a large corpus with rare embedded languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.165/>Connecting the Dots Between <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact Verification</a> and Fake News Detection</a></strong><br><a href=/people/q/qifei-li/>Qifei Li</a>
|
<a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--165><div class="card-body p-3 small">Fact verification models have enjoyed a fast advancement in the last two years with the development of pre-trained language models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and the release of large scale datasets such as <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a>. However, the challenging problem of fake news detection has not benefited from the improvement of fact verification models, which is closely related to fake news detection. In this paper, we propose a simple yet effective approach to connect the dots between <a href=https://en.wikipedia.org/wiki/Fact-checking>fact verification</a> and fake news detection. Our approach first employs a text summarization model pre-trained on news corpora to summarize the long news article into a short claim. Then we use a fact verification model pre-trained on the FEVER dataset to detect whether the input <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a> is real or fake. Our approach makes use of the recent success of fact verification models and enables zero-shot fake news detection, alleviating the need of large scale training data to train fake news detection models. Experimental results on FakenewsNet, a benchmark dataset for fake news detection, demonstrate the effectiveness of our proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.167/>Reasoning Step-by-Step : Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network</a></strong><br><a href=/people/d/daizong-liu/>Daizong Liu</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/j/jianfeng-dong/>Jianfeng Dong</a>
|
<a href=/people/p/pan-zhou/>Pan Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--167><div class="card-body p-3 small">Temporal sentence localization in videos aims to ground the best matched segment in an untrimmed video according to a given sentence query. Previous works in this field mainly rely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attentional frameworks</a> to align the temporal boundaries by a soft selection. Although they focus on the visual content relevant to the query, these single-step attention are insufficient to model complex video contents and restrict the higher-level reasoning demand for this task. In this paper, we propose a novel deep rectification-modulation network (RMN), transforming this task into a multi-step reasoning process by repeating rectification and <a href=https://en.wikipedia.org/wiki/Modulation>modulation</a>. In each rectification-modulation layer, unlike existing methods directly conducting the cross-modal interaction, we first devise a rectification module to correct implicit attention misalignment which focuses on the wrong position during the cross-interaction process. Then, a modulation module is developed to capture the frame-to-frame relation with the help of sentence information for better correlating and composing the video contents over time. With multiple such layers cascaded in depth, our RMN progressively refines video and query interactions, thus enabling a further precise localization. Experimental evaluations on three public datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.174/>Language-Driven Region Pointer Advancement for Controllable Image Captioning</a></strong><br><a href=/people/a/annika-lindh/>Annika Lindh</a>
|
<a href=/people/r/robert-ross/>Robert Ross</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--174><div class="card-body p-3 small">Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of 86.55 % and a recall of 97.92 %. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.182/>An Enhanced Knowledge Injection Model for Commonsense Generation</a></strong><br><a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/y/yameng-huang/>Yameng Huang</a>
|
<a href=/people/j/jian-jiao/>Jian Jiao</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--182><div class="card-body p-3 small">Commonsense generation aims at generating plausible everyday scenario description based on a set of provided concepts. Digging the relationship of concepts from scratch is non-trivial, therefore, we retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation. We integrate two additional modules into the pretrained encoder-decoder model for prototype modeling to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.191/>How Positive Are You : Text Style Transfer using Adaptive Style Embedding</a></strong><br><a href=/people/h/heejin-kim/>Heejin Kim</a>
|
<a href=/people/k/kyung-ah-sohn/>Kyung-Ah Sohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--191><div class="card-body p-3 small">The prevalent approach for unsupervised text style transfer is disentanglement between content and style. However, it is difficult to completely separate <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style information</a> from the content. Other approaches allow the latent text representation to contain style and the target style to affect the generated output more than the latent representation does. In both <a href=https://en.wikipedia.org/wiki/Composition_(visual_arts)>approaches</a>, however, it is impossible to adjust the strength of the style in the generated output. Moreover, those previous approaches typically perform both the sentence reconstruction and style control tasks in a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, which complicates the overall architecture. In this paper, we address these issues by separating the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> into a sentence reconstruction module and a style module. We use the Transformer-based autoencoder model for sentence reconstruction and the adaptive style embedding is learned directly in the style module. Because of this separation, each <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> can better focus on its own <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Moreover, we can vary the style strength of the generated sentence by changing the style of the embedding expression. Therefore, our approach not only controls the strength of the style, but also simplifies the model architecture. Experimental results show that our approach achieves better style transfer performance and <a href=https://en.wikipedia.org/wiki/Preservation_(library_and_archival_science)>content preservation</a> than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.195/>Grammatical error detection in transcriptions of spoken English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/c/christian-bentz/>Christian Bentz</a>
|
<a href=/people/k/kate-knill/>Kate Knill</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--195><div class="card-body p-3 small">We describe the collection of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription corrections</a> and grammatical error annotations for the CrowdED Corpus of spoken English monologues on business topics. The corpus recordings were crowdsourced from native speakers of English and learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a> with <a href=https://en.wikipedia.org/wiki/German_language>German</a> as their first language. The new transcriptions and annotations are obtained from different crowdworkers : we analyse the 1108 new crowdworker submissions and propose that they can be used for automatic transcription post-editing and grammatical error correction for speech. To further explore the data we train grammatical error detection models with various configurations including pre-trained and contextual word representations as input, additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and auxiliary objectives, and extra <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> from written error-annotated corpora. We find that a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> concatenating pre-trained and contextual word representations as input performs best, and that additional information does not lead to further performance gains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.197/>Style versus Content : A distinction without a (learnable) difference?</a></strong><br><a href=/people/s/somayeh-jafaritazehjani/>Somayeh Jafaritazehjani</a>
|
<a href=/people/g/gwenole-lecorve/>Gwénolé Lecorvé</a>
|
<a href=/people/d/damien-lolive/>Damien Lolive</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--197><div class="card-body p-3 small">Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use of a probing methodology to analyse how these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal the inherent trade-off between the multiple style transfer objectives which indicates that style can not be usefully separated from content within these style-transfer systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.199/>Heterogeneous Recycle Generation for Chinese Grammatical Error Correction<span class=acl-fixed-case>C</span>hinese Grammatical Error Correction</a></strong><br><a href=/people/c/charles-hinson/>Charles Hinson</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--199><div class="card-body p-3 small">Most recent works in the field of grammatical error correction (GEC) rely on neural machine translation-based models. Although these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> boast impressive performance, they require a massive amount of data to properly train. Furthermore, NMT-based systems treat GEC purely as a translation task and overlook the editing aspect of it. In this work we propose a heterogeneous approach to Chinese GEC, composed of a NMT-based model, a sequence editing model, and a <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checker</a>. Our methodology not only achieves a new state-of-the-art performance for Chinese GEC, but also does so without relying on <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> or GEC-specific architecture changes. We further experiment with all possible configurations of our <a href=https://en.wikipedia.org/wiki/System>system</a> with respect to model composition order and number of rounds of correction. A detailed analysis of each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and their contributions to the correction process is performed by adapting the ERRANT scorer to be able to score Chinese sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.203/>Formality Style Transfer with Shared Latent Space</a></strong><br><a href=/people/y/yunli-wang/>Yunli Wang</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/w/wenhan-chao/>WenHan Chao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--203><div class="card-body p-3 small">Conventional approaches for formality style transfer borrow models from <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and <a href=https://en.wikipedia.org/wiki/Grammar>grammars</a>. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.204/>Keep it Consistent : Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication</a></strong><br><a href=/people/r/ruize-wang/>Ruize Wang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/y/ying-cheng/>Ying Cheng</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/h/haijun-shan/>Haijun Shan</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--204><div class="card-body p-3 small">Visual storytelling aims to generate a narrative paragraph from a sequence of <a href=https://en.wikipedia.org/wiki/Image>images</a> automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for <a href=https://en.wikipedia.org/wiki/Visual_storytelling>visual storytelling</a> by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the <a href=https://en.wikipedia.org/wiki/Story_generator>story generator</a> as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method&#8217;s good ability in generating stories with higher quality compared to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.205/>Referring to what you know and do not know : Making Referring Expression Generation Models Generalize To Unseen Entities</a></strong><br><a href=/people/r/rossana-cunha/>Rossana Cunha</a>
|
<a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/a/adriana-pagano/>Adriana Pagano</a>
|
<a href=/people/f/fabio-alves/>Fabio Alves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--205><div class="card-body p-3 small">Data-to-text Natural Language Generation (NLG) is the computational process of generating <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> in the form of text or voice from non-linguistic data. A core micro-planning task within NLG is referring expression generation (REG), which aims to automatically generate <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> to refer to entities mentioned as discourse unfolds. A limitation of novel REG models is not being able to generate referring expressions to entities not encountered during the training process. To solve this problem, we propose two extensions to NeuralREG, a state-of-the-art encoder-decoder REG model. The first is a copy mechanism, whereas the second consists of representing the gender and type of the referent as inputs to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Drawing on the results of automatic and human evaluation as well as an ablation study using the WebNLG corpus, we contend that our proposal contributes to the generation of more meaningful referring expressions to unseen entities than the original system and related work. Code and all produced data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.208/>Automatic Detection of Machine Generated Text : A Critical Survey</a></strong><br><a href=/people/g/ganesh-jawahar/>Ganesh Jawahar</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-lakshmanan-v-s/>Laks Lakshmanan, V.S.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--208><div class="card-body p-3 small">Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by <a href=https://en.wikipedia.org/wiki/Time-division_multiple_access>TGM</a> from human written text play a vital role in mitigating such misuse of <a href=https://en.wikipedia.org/wiki/Time-division_multiple_access>TGMs</a>. Recently, there has been a flurry of works from both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning (ML) communities</a> to build accurate detectors for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.213/>Learning with Contrastive Examples for Data-to-Text Generation</a></strong><br><a href=/people/y/yui-uehara/>Yui Uehara</a>
|
<a href=/people/t/tatsuya-ishigaki/>Tatsuya Ishigaki</a>
|
<a href=/people/k/kasumi-aoki/>Kasumi Aoki</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--213><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for data-to-text tasks generate fluent but sometimes incorrect sentences e.g., Nikkei gains is generated when Nikkei drops is expected. We investigate <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> trained on contrastive examples i.e., incorrect sentences or terms, in addition to correct ones to reduce such errors. We first create <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> to produce contrastive examples from correct ones by replacing frequent crucial terms such as gain or drop. We then use <a href=https://en.wikipedia.org/wiki/Machine_learning>learning methods</a> with several <a href=https://en.wikipedia.org/wiki/Loss_function>losses</a> that exploit contrastive examples. Experiments on the market comment generation task show that 1) exploiting contrastive examples improves the capability of generating sentences with better lexical choice, without degrading the fluency, 2) the choice of the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> is an important factor because the performances on different metrics depend on the types of loss functions, and 3) the use of the examples produced by some specific rules further improves performance. Human evaluation also supports the effectiveness of using <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrastive examples</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.214/>MedWriter : Knowledge-Aware Medical Text Generation<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>W</span>riter: Knowledge-Aware Medical Text Generation</a></strong><br><a href=/people/y/youcheng-pan/>Youcheng Pan</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/w/weihua-peng/>Weihua Peng</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a>
|
<a href=/people/b/baotian-hu/>Baotian Hu</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/j/junying-chen/>Junying Chen</a>
|
<a href=/people/w/wenxiu-zhou/>Wenxiu Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--214><div class="card-body p-3 small">To exploit the <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to guarantee the correctness of generated text has been a hot topic in recent years, especially for high professional domains such as <a href=https://en.wikipedia.org/wiki/Medicine>medical</a>. However, most of recent works only consider the information of unstructured text rather than structured information of the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. In this paper, we focus on the medical topic-to-text generation task and adapt a knowledge-aware text generation model to the medical domain, named MedWriter, which not only introduces the specific knowledge from the external MKG but also is capable of learning graph-level representation. We conduct experiments on a medical literature dataset collected from <a href=https://en.wikipedia.org/wiki/Medical_journal>medical journals</a>, each of which has a set of topic words, an abstract of medical literature and a corresponding <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> from CMeKG. Experimental results demonstrate incorporating <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> into generation model can improve the quality of the generated text and has robust superiority over the competitor methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.226" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.226/>Context Dependent Semantic Parsing : A Survey</a></strong><br><a href=/people/z/zhuang-li/>Zhuang Li</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--226><div class="card-body p-3 small">Semantic parsing is the task of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>translating natural language utterances</a> into <a href=https://en.wikipedia.org/wiki/Machine-readable_data>machine-readable meaning representations</a>. Currently, most semantic parsing methods are not able to utilize the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> (e.g. dialogue and comments history), which has a great potential to boost the semantic parsing systems. To address this issue, context dependent semantic parsing has recently drawn a lot of attention. In this survey, we investigate progress on the methods for the context dependent semantic parsing, together with the current datasets and tasks. We then point out open problems and challenges for future research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.227/>A Survey of Unsupervised Dependency Parsing</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--227><div class="card-body p-3 small">Syntactic dependency parsing is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Unsupervised dependency parsing aims to learn a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> from sentences that have no annotation of their correct parse trees. Despite its difficulty, unsupervised parsing is an interesting research direction because of its capability of utilizing almost unlimited unannotated text data. It also serves as the basis for other research in low-resource parsing. In this paper, we survey existing <a href=https://en.wikipedia.org/wiki/Parsing>approaches</a> to unsupervised dependency parsing, identify two major classes of <a href=https://en.wikipedia.org/wiki/Parsing>approaches</a>, and discuss recent trends. We hope that our survey can provide insights for researchers and facilitate future research on this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.228/>Exploring Question-Specific Rewards for Generating Deep Questions</a></strong><br><a href=/people/y/yuxi-xie/>Yuxi Xie</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/d/dongzhe-wang/>Dongzhe Wang</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--228><div class="card-body p-3 small">Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to improve question quality. We design three different rewards that target to improve the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the <a href=https://en.wikipedia.org/wiki/Motivational_salience>rewards</a> that correlate well with <a href=https://en.wikipedia.org/wiki/Judgement>human judgement</a> (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.229/>CHIME : Cross-passage Hierarchical Memory Network for Generative Review Question Answering<span class=acl-fixed-case>CHIME</span>: Cross-passage Hierarchical Memory Network for Generative Review Question Answering</a></strong><br><a href=/people/j/junru-lu/>Junru Lu</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/b/binyang-li/>Binyang Li</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--229><div class="card-body p-3 small">We introduce <a href=https://en.wikipedia.org/wiki/CHIME>CHIME</a>, a cross-passage hierarchical memory network for question answering (QA) via text generation. It extends XLNet introducing an auxiliary memory module consisting of two components : the context memory collecting cross-passage evidences, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> introduced by the <a href=https://en.wikipedia.org/wiki/Memory_module>memory module</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.235/>Bi-directional CognitiveThinking Network for Machine Reading Comprehension<span class=acl-fixed-case>C</span>ognitive<span class=acl-fixed-case>T</span>hinking Network for Machine Reading Comprehension</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/y/yue-hu/>Yue Hu</a>
|
<a href=/people/l/luxi-xing/>Luxi Xing</a>
|
<a href=/people/y/yuqiang-xie/>Yuqiang Xie</a>
|
<a href=/people/j/jing-yu/>Jing Yu</a>
|
<a href=/people/y/yajing-sun/>Yajing Sun</a>
|
<a href=/people/x/xiangpeng-wei/>Xiangpeng Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--235><div class="card-body p-3 small">We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and <a href=https://en.wikipedia.org/wiki/Inertial_frame_of_reference>inertial thinking</a>. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network (BCTN) to encode the passage and generate a question (answer) given an answer (question) and decouple the bi-directional knowledge. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has the ability to reverse reasoning questions which can assist <a href=https://en.wikipedia.org/wiki/Inertial_frame_of_reference>inertial thinking</a> to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> shows an interesting perspective on machine reading comprehension and <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.238/>Molweni : A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure</a></strong><br><a href=/people/j/jiaqi-li/>Jiaqi Li</a>
|
<a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/z/zihao-zheng/>Zihao Zheng</a>
|
<a href=/people/z/zekun-wang/>Zekun Wang</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--238><div class="card-body p-3 small">Research into the area of multiparty dialog has grown considerably over recent years. We present the Molweni dataset, a machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni&#8217;s source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. We annotate 30,066 questions on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, including both answerable and unanswerable questions. Molweni also uniquely contributes discourse dependency annotations in a modified Segmented Discourse Representation Theory (SDRT ; Asher et al., 2016) style for all of its multiparty dialogs, contributing large-scale (78,245 annotated discourse relations) data to bear on the task of multiparty dialog discourse parsing. Our experiments show that Molweni is a challenging dataset for current MRC models : BERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7 % F1 on Molweni&#8217;s questions, a 20+% significant drop as compared against its SQuAD 2.0 performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.247/>Conversational Machine Comprehension : a Literature Review</a></strong><br><a href=/people/s/somil-gupta/>Somil Gupta</a>
|
<a href=/people/b/bhanu-pratap-singh-rawat/>Bhanu Pratap Singh Rawat</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--247><div class="card-body p-3 small">Conversational Machine Comprehension (CMC), a research track in conversational AI, expects the machine to understand an open-domain natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>CMC models</a> while highlighting the differences in recent approaches and intends to serve as a compendium of <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>CMC</a> for future researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.249/>Reinforced Multi-task Approach for Multi-hop Question Generation</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/h/hardik-chauhan/>Hardik Chauhan</a>
|
<a href=/people/r/ravi-tej-akella/>Ravi Tej Akella</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--249><div class="card-body p-3 small">Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.254" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.254/>Does Chinese BERT Encode Word Structure?<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>BERT</span> Encode Word Structure?</a></strong><br><a href=/people/y/yile-wang/>Yile Wang</a>
|
<a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--254><div class="card-body p-3 small">Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>. However, little work has investigated <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>word features</a> for character languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT ; (2) word-level features are mostly in the middle representation layers ; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.259/>One Comment from One Perspective : An Effective Strategy for Enhancing Automatic Music Comment</a></strong><br><a href=/people/t/tengfei-huo/>Tengfei Huo</a>
|
<a href=/people/z/zhiqiang-liu/>Zhiqiang Liu</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--259><div class="card-body p-3 small">The automatic generation of music comments is of great significance for increasing the popularity of music and the music platform&#8217;s activity. In human music comments, there exists high distinction and diverse perspectives for the same song. In other words, for a song, different comments stem from different musical perspectives. However, to date, this characteristic has not been considered well in research on automatic comment generation. The existing methods tend to generate common and meaningless comments. In this paper, we propose an effective multi-perspective strategy to enhance the <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> of the generated comments. The experiment results on two music comment datasets show that our proposed model can effectively generate a series of diverse music comments based on different perspectives, which outperforms state-of-the-art baselines by a substantial margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.260" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.260/>A Tale of Two Linkings : Dynamically Gating between Schema Linking and Structural Linking for Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/s/sanxing-chen/>Sanxing Chen</a>
|
<a href=/people/a/aidan-san/>Aidan San</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--260><div class="card-body p-3 small">In Text-to-SQL semantic parsing, selecting the correct entities (tables and columns) for the generated <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a> is both crucial and challenging ; the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is required to connect the natural language (NL) question and the <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a> to the structured knowledge in the database. We formulate two linking processes to address this challenge : schema linking which links explicit NL mentions to the database and structural linking which links the entities in the output SQL with their structural relationships in the <a href=https://en.wikipedia.org/wiki/Database_schema>database schema</a>. Intuitively, the effectiveness of these two linking processes changes based on the entity being generated, thus we propose to dynamically choose between them using a gating mechanism. Integrating the proposed method with two graph neural network-based semantic parsers together with BERT representations demonstrates substantial gains in parsing accuracy on the challenging Spider dataset. Analyses show that our proposed method helps to enhance the structure of the model output when generating complicated SQL queries and offers more explainable predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.261" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.261/>Autoregressive Affective Language Forecasting : A Self-Supervised Task</a></strong><br><a href=/people/m/matthew-matero/>Matthew Matero</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--261><div class="card-body p-3 small">Human natural language is mentioned at a specific point in time while <a href=https://en.wikipedia.org/wiki/Emotion>human emotions</a> change over time. While much work has established a strong link between <a href=https://en.wikipedia.org/wiki/Language>language use</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotional states</a>, few have attempted to model emotional language in time. Here, we introduce the task of affective language forecasting predicting future change in language based on past changes of language, a task with real-world applications such as treating mental health or forecasting trends in consumer confidence. We establish some of the fundamental autoregressive characteristics of the task (necessary history size, static versus dynamic length, varying time-step resolutions) and then build on popular sequence models for words to instead model sequences of language-based emotion in time. Over a novel Twitter dataset of 1,900 users and weekly + daily scores for 6 <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (r =.66) significantly out-predicting, e.g., a moving averaging based on the past time-steps (r =.49). We make our anonymized dataset as well as task setup and evaluation code available for others to build on.<i>affective language forecasting</i> &#8211; predicting future change in language based on past changes of language, a task with real-world applications such as treating mental health or forecasting trends in consumer confidence. We establish some of the fundamental autoregressive characteristics of the task (necessary history size, static versus dynamic length, varying time-step resolutions) and then build on popular sequence models for <i>words</i> to instead model sequences of <i>language-based emotion in time</i>. Over a novel Twitter dataset of 1,900 users and weekly + daily scores for 6 emotions and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (<tex-math>r = .66</tex-math>) significantly out-predicting, e.g., a moving averaging based on the past time-steps (<tex-math>r = .49</tex-math>). We make our anonymized dataset as well as task setup and evaluation code available for others to build on.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.263/>End to End Chinese Lexical Fusion Recognition with Sememe Knowledge<span class=acl-fixed-case>C</span>hinese Lexical Fusion Recognition with Sememe Knowledge</a></strong><br><a href=/people/y/yijiang-liu/>Yijiang Liu</a>
|
<a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/d/donghong-ji/>Donghong Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--263><div class="card-body p-3 small">In this paper, we present Chinese lexical fusion recognition, a new task which could be regarded as one kind of coreference recognition. First, we introduce the task in detail, showing the relationship with coreference recognition and differences from the existing tasks. Second, we propose an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a> for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, handling mentions as well as coreference relationship jointly. The model exploits the state-of-the-art contextualized BERT representations as an encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and then conduct experiments on it. Results demonstrate that our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective and competitive for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Detailed analysis is offered for comprehensively understanding the new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.264/>Comparison by Conversion : Reverse-Engineering UCCA from Syntax and Lexical Semantics<span class=acl-fixed-case>UCCA</span> from Syntax and Lexical Semantics</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/d/dotan-dvir/>Dotan Dvir</a>
|
<a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--264><div class="card-body p-3 small">Building robust <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a> will require a clear characterization of whether and how various linguistic meaning representations complement each other. To perform a systematic comparative analysis, we evaluate the mapping between meaning representations from different frameworks using two complementary methods : (i) a rule-based converter, and (ii) a supervised delexicalized parser that parses to one framework using only information from the other as features. We apply these methods to convert the STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a graph-structured full-sentence meaning representation). Both methods yield surprisingly accurate target representations, close to fully supervised UCCA parser qualityindicating that UCCA annotations are partially redundant with STREUSLE annotations. Despite this substantial convergence between <a href=https://en.wikipedia.org/wiki/Conceptual_framework>frameworks</a>, we find several important areas of divergence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.267/>Normalizing Compositional Structures Across Graphbanks</a></strong><br><a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/j/jonas-groschwitz/>Jonas Groschwitz</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/p/pia-weissenhorn/>Pia Weißenhorn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--267><div class="card-body p-3 small">The emergence of a variety of graph-based meaning representations (MRs) has sparked an important conversation about how to adequately represent semantic structure. MRs exhibit structural differences that reflect different theoretical and design considerations, presenting challenges to uniform linguistic analysis and cross-framework semantic parsing. Here, we ask the question of which design differences between MRs are meaningful and semantically-rooted, and which are superficial. We present a methodology for normalizing discrepancies between MRs at the compositional level (Lindemann et al., 2019), finding that we can normalize the majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--278 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.278/>Finding the Evidence : Localization-aware Answer Prediction for Text Visual Question Answering</a></strong><br><a href=/people/w/wei-han/>Wei Han</a>
|
<a href=/people/h/hantao-huang/>Hantao Huang</a>
|
<a href=/people/t/tao-han/>Tao Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--278><div class="card-body p-3 small">Image text carries essential information to understand the scene and perform <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.289/>Multi-level Alignment Pretraining for Multi-lingual Semantic Parsing</a></strong><br><a href=/people/b/bo-shao/>Bo Shao</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/w/weizhen-qi/>Weizhen Qi</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/x/xiaola-lin/>Xiaola Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--289><div class="card-body p-3 small">In this paper, we present a multi-level alignment pretraining method in a unified architecture formulti-lingual semantic parsing. In this architecture, we use an adversarial training method toalign the space of different languages and use sentence level and word level parallel corpus assupervision information to align the semantic of different languages. Finally, we jointly train themulti-level alignment and semantic parsing tasks. We conduct experiments on a publicly avail-able multi-lingual semantic parsing dataset ATIS and a newly constructed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Experimentalresults show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art methods on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.291/>Conception : Multilingually-Enhanced, Human-Readable Concept Vector Representations</a></strong><br><a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--291><div class="card-body p-3 small">To date, the most successful <a href=https://en.wikipedia.org/wiki/Word_sense>word</a>, word sense, and concept modelling techniques have used large corpora and knowledge resources to produce dense vector representations that capture semantic similarities in a relatively low-dimensional space. Most current approaches, however, suffer from a monolingual bias, with their strength depending on the amount of data available across languages. In this paper we address this issue and propose Conception, a novel technique for building language-independent vector representations of concepts which places <a href=https://en.wikipedia.org/wiki/Multilinguality>multilinguality</a> at its core while retaining explicit relationships between concepts. Our approach results in high-coverage representations that outperform the state of the art in multilingual and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on low-resource languages. Conception its software and the complete set of representations is available at https://github.com/SapienzaNLP/conception.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--293 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.293/>Sentence Matching with Syntax- and Semantics-Aware BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/c/chengguo-lv/>Chengguo Lv</a>
|
<a href=/people/r/ranran-zhen/>Ranran Zhen</a>
|
<a href=/people/g/guohong-fu/>Guohong Fu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--293><div class="card-body p-3 small">Sentence matching aims to identify the special relationship between two sentences, and plays a key role in many natural language processing tasks. However, previous studies mainly focused on exploiting either syntactic or semantic information for sentence matching, and no studies consider integrating both of them. In this study, we propose integrating <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and semantics into BERT with sentence matching. In particular, we use an implicit syntax and semantics integration method that is less sensitive to the output structure information. Thus the implicit integration can alleviate the error propagation problem. The experimental results show that our approach has achieved state-of-the-art or competitive performance on several sentence matching datasets, demonstrating the benefits of implicitly integrating syntactic and semantic features in sentence matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.295/>Homonym normalisation by word sense clustering : a case in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a><span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/y/yo-sato/>Yo Sato</a>
|
<a href=/people/k/kevin-heffernan/>Kevin Heffernan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--295><div class="card-body p-3 small">This work presents a method of word sense clustering that differentiates <a href=https://en.wikipedia.org/wiki/Homonym>homonyms</a> and merge <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a>, taking <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> as an example, where orthographical variation causes problem for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a>. It uses contextualised embeddings (BERT) to cluster tokens into distinct sense groups, and we use these <a href=https://en.wikipedia.org/wiki/Group_(mathematics)>groups</a> to normalise synonymous instances to a single representative form. We see the benefit of this normalisation in <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, as well as in <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.297/>An Unsupervised Method for Learning Representations of Multi-word Expressions for Semantic Classification</a></strong><br><a href=/people/r/robert-vacareanu/>Robert Vacareanu</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--297><div class="card-body p-3 small">This paper explores an unsupervised approach to learning a compositional representation function for multi-word expressions (MWEs), and evaluates it on the Tratz dataset, which associates two-word expressions with the semantic relation between the compound constituents (e.g. the label employer is associated with the noun compound government agency) (Tratz, 2011). The <a href=https://en.wikipedia.org/wiki/Composition_function>composition function</a> is based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, and is trained using the Skip-Gram objective to predict the words in the context of MWEs. Thus our approach can naturally leverage large unlabeled text sources. Further, our method can make use of provided MWEs when available, but can also function as a completely <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised algorithm</a>, using MWE boundaries predicted by a single, domain-agnostic part-of-speech pattern. With pre-defined MWE boundaries, our method outperforms the previous state-of-the-art performance on the coarse-grained evaluation of the Tratz dataset (Tratz, 2011), with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 50.4 %. The unsupervised version of our method approaches the performance of the supervised one, and even outperforms it in some configurations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.300/>Sentence Analogies : Linguistic Regularities in Sentence Embeddings</a></strong><br><a href=/people/x/xunjie-zhu/>Xunjie Zhu</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--300><div class="card-body p-3 small">While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> differ substantially in their ability to reflect such regularities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.301/>Manifold Learning-based Word Representation Refinement Incorporating Global and Local Information</a></strong><br><a href=/people/w/wenyu-zhao/>Wenyu Zhao</a>
|
<a href=/people/d/dong-zhou/>Dong Zhou</a>
|
<a href=/people/l/lin-li/>Lin Li</a>
|
<a href=/people/j/jinjun-chen/>Jinjun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--301><div class="card-body p-3 small">Recent studies show that word embedding models often underestimate similarities between similar words and overestimate similarities between distant words. This results in word similarity results obtained from <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> inconsistent with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. Manifold learning-based methods are widely utilized to refine word representations by re-embedding word vectors from the original embedding space to a new refined semantic space. These methods mainly focus on preserving local geometry information through performing weighted locally linear combination between words and their neighbors twice. However, these reconstruction weights are easily influenced by different selections of neighboring words and the whole combination process is time-consuming. In this paper, we propose two novel word representation refinement methods leveraging isometry feature mapping and local tangent space respectively. Unlike previous methods, our first method corrects pre-trained word embeddings by preserving global geometry information of all words instead of local geometry information between words and their neighbors. Our second method refines word representations by aligning original and re-fined embedding spaces based on local tangent space instead of performing weighted locally linear combination twice. Experimental results obtained from standard semantic relatedness and semantic similarity tasks show that our methods outperform various state-of-the-art baselines for word representation refinement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.304/>Optimizing Transformer for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/a/ali-araabi/>Ali Araabi</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--304><div class="card-body p-3 small">Language pairs with limited amounts of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>, also known as low-resource languages, remain a challenge for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.308/>Towards the First Machine Translation System for <a href=https://en.wikipedia.org/wiki/Sumerian_language>Sumerian Transliterations</a><span class=acl-fixed-case>S</span>umerian Transliterations</a></strong><br><a href=/people/r/ravneet-punia/>Ravneet Punia</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/e/emilie-page-perron/>Émilie Pagé-Perron</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--308><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Sumerian_cuneiform>Sumerian cuneiform script</a> was invented more than 5,000 years ago and represents one of the oldest in history. We present the first attempt to translate <a href=https://en.wikipedia.org/wiki/Sumerian_language>Sumerian texts</a> into English automatically. We publicly release high-quality corpora for standardized training and evaluation and report results on experiments with supervised, phrase-based, and transfer learning techniques for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Quantitative and qualitative evaluations indicate the usefulness of the <a href=https://en.wikipedia.org/wiki/Translation>translations</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> provides a broader audience of researchers with novel access to the data, accelerates the costly and time-consuming manual translation process, and helps them better explore the relationships between <a href=https://en.wikipedia.org/wiki/Cuneiform>Sumerian cuneiform</a> and <a href=https://en.wikipedia.org/wiki/Mesopotamia>Mesopotamian culture</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.309/>Using Bilingual Patents for Translation Training</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/b/benjamin-k-tsou/>Benjamin Tsou</a>
|
<a href=/people/t/tianyuan-cai/>Tianyuan Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--309><div class="card-body p-3 small">While bilingual corpora have been instrumental for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, their utility for training translators has been less explored. We investigate the use of bilingual corpora as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that <a href=https://en.wikipedia.org/wiki/Concordancing>concordancing</a> with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.314/>Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</a></strong><br><a href=/people/h/hang-le/>Hang Le</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--314><div class="card-body p-3 small">We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these <a href=https://en.wikipedia.org/wiki/Code>decoders</a> interact with each other : one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the <a href=https://en.wikipedia.org/wiki/Code>decoders</a>, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.315/>Multitask Learning-Based Neural Bridging Reference Resolution</a></strong><br><a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--315><div class="card-body p-3 small">We propose a multi task learning-based neural model for resolving bridging references tackling two key challenges. The first challenge is the lack of large corpora annotated with bridging references. To address this, we use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to help bridging reference resolution with <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We show that substantial improvements of up to 8 p.p. can be achieved on full bridging resolution with this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>. The second challenge is the different definitions of bridging used in different corpora, meaning that hand-coded systems or <a href=https://en.wikipedia.org/wiki/System>systems</a> using special features designed for one corpus do not work well with other corpora. Our neural model only uses a small number of corpus independent features, thus can be applied to different corpora. Evaluations with very different bridging corpora (ARRAU, ISNOTES, BASHI and SCICORP) suggest that our architecture works equally well on all corpora, and achieves the SoTA results on full bridging resolution for all corpora, outperforming the best reported results by up to 36.3 p.p..</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.317/>Automatic Discovery of Heterogeneous Machine Learning Pipelines : An Application to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/s/suilan-estevez-velarde/>Suilan Estevez-Velarde</a>
|
<a href=/people/y/yoan-gutierrez/>Yoan Gutiérrez</a>
|
<a href=/people/a/andres-montoyo/>Andres Montoyo</a>
|
<a href=/people/y/yudivian-almeida-cruz/>Yudivián Almeida Cruz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--317><div class="card-body p-3 small">This paper presents AutoGOAL, a system for automatic machine learning (AutoML) that uses heterogeneous techniques. In contrast with existing AutoML approaches, our contribution can automatically build machine learning pipelines that combine techniques and algorithms from different frameworks, including shallow classifiers, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tools</a>, and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We define the heterogeneous AutoML optimization problem as the search for the best sequence of algorithms that transforms specific input data into the desired output. This provides a novel theoretical and practical approach to <a href=https://en.wikipedia.org/wiki/AutoML>AutoML</a>. Our proposal is experimentally evaluated in diverse machine learning problems and compared with alternative approaches, showing that it is competitive with other AutoML alternatives in standard benchmarks. Furthermore, it can be applied to novel scenarios, such as several NLP tasks, where existing alternatives can not be directly deployed. The <a href=https://en.wikipedia.org/wiki/System>system</a> is freely available and includes in-built compatibility with a large number of popular machine learning frameworks, which makes our approach useful for solving practical problems with relative ease and effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--319 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.319/>Incorporating Noisy Length Constraints into Transformer with Length-aware Positional Encodings</a></strong><br><a href=/people/y/yui-oka/>Yui Oka</a>
|
<a href=/people/k/katsuki-chousa/>Katsuki Chousa</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--319><div class="card-body p-3 small">Neural Machine Translation often suffers from an under-translation problem due to its limited modeling of output sequence lengths. In this work, we propose a novel approach to training a Transformer model using length constraints based on length-aware positional encoding (PE). Since length constraints with exact target sentence lengths degrade translation performance, we add random noise within a certain window size to the length constraints in the PE during the training. In the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference step</a>, we predict the output lengths using input sequences and a BERT-based length prediction model. Experimental results in an ASPEC English-to-Japanese translation showed the proposed method produced translations with lengths close to the reference ones and outperformed a vanilla Transformer (especially in short sentences) by 3.22 points in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. The average translation results using our length prediction model were also better than another baseline method using input lengths for the length constraints. The proposed noise injection improved robustness for length prediction errors, especially within the window size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.322/>Deep Inside-outside Recursive Autoencoder with All-span Objective</a></strong><br><a href=/people/r/ruyue-hong/>Ruyue Hong</a>
|
<a href=/people/j/jiong-cai/>Jiong Cai</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--322><div class="card-body p-3 small">Deep inside-outside recursive autoencoder (DIORA) is a neural-based model designed for unsupervised constituency parsing. During its forward computation, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> provides phrase and contextual representations for all spans in the input sentence. By utilizing the contextual representation of each leaf-level span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.325/>Picking BERT’s Brain : Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis<span class=acl-fixed-case>BERT</span>’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis</a></strong><br><a href=/people/m/michael-lepori/>Michael Lepori</a>
|
<a href=/people/r/r-thomas-mccoy/>R. Thomas McCoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--325><div class="card-body p-3 small">As the name implies, contextualized representations of language are typically motivated by their ability to encode context. Which aspects of context are captured by such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>? We introduce an approach to address this question using Representational Similarity Analysis (RSA). As case studies, we investigate the degree to which a verb embedding encodes the verb&#8217;s subject, a pronoun embedding encodes the pronoun&#8217;s antecedent, and a full-sentence representation encodes the sentence&#8217;s head word (as determined by a dependency parse). In all cases, we show that BERT&#8217;s contextualized embeddings reflect the linguistic dependency being studied, and that BERT encodes these dependencies to a greater degree than it encodes less linguistically-salient controls. These results demonstrate the ability of our approach to adjudicate between hypotheses about which aspects of context are encoded in representations of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.326/>The Devil is in the Details : Evaluating Limitations of Transformer-based Methods for Granular Tasks</a></strong><br><a href=/people/b/brihi-joshi/>Brihi Joshi</a>
|
<a href=/people/n/neil-shah/>Neil Shah</a>
|
<a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--326><div class="card-body p-3 small">Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and textual similarity in recent years. Extensive work shows how accurately such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives : matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that use contextual embeddings, achieving relative improvements of up to 36 % on granular tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--327 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.327" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.327/>CoLAKE : Contextualized Language and Knowledge Embedding<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>LAKE</span>: Contextualized Language and Knowledge Embedding</a></strong><br><a href=/people/t/tianxiang-sun/>Tianxiang Sun</a>
|
<a href=/people/y/yunfan-shao/>Yunfan Shao</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/y/yaru-hu/>Yaru Hu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--327><div class="card-body p-3 small">With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the <a href=https://en.wikipedia.org/wiki/Context_(computing)>knowledge context</a> of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>language context</a>, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--330 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.330/>Target Word Masking for Location Metonymy Resolution</a></strong><br><a href=/people/h/haonan-li/>Haonan Li</a>
|
<a href=/people/m/maria-vasardani/>Maria Vasardani</a>
|
<a href=/people/m/martin-tomko/>Martin Tomko</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--330><div class="card-body p-3 small">Existing metonymy resolution approaches rely on features extracted from external resources like <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and hand-crafted lexical resources. In this paper, we propose an end-to-end word-level classification approach based only on BERT, without dependencies on <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>taggers</a>, <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, curated dictionaries of place names, or other external resources. We show that our approach achieves the state-of-the-art on 5 datasets, surpassing conventional BERT models and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> by a large margin. We also show that our approach generalises well to unseen data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.333/>What Meaning-Form Correlation Has to Compose With : A Study of MFC on Artificial and Natural Language<span class=acl-fixed-case>MFC</span> on Artificial and Natural Language</a></strong><br><a href=/people/t/timothee-mickus/>Timothee Mickus</a>
|
<a href=/people/t/timothee-bernard/>Timothée Bernard</a>
|
<a href=/people/d/denis-paperno/>Denis Paperno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--333><div class="card-body p-3 small">Compositionality is a widely discussed property of <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a>, although its exact definition has been elusive. We focus on the proposal that <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> can be assessed by measuring meaning-form correlation. We analyze meaning-form correlation on three sets of <a href=https://en.wikipedia.org/wiki/Language>languages</a> : (i) artificial toy languages tailored to be compositional, (ii) a set of English dictionary definitions, and (iii) a set of English sentences drawn from literature. We find that linguistic phenomena such as <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a> and ungrounded stop-words weigh on MFC measurements, and that straightforward methods to mitigate their effects have widely varying results depending on the dataset they are applied to. Data and code are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.334/>Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition</a></strong><br><a href=/people/c/cedric-lothritz/>Cedric Lothritz</a>
|
<a href=/people/k/kevin-allix/>Kevin Allix</a>
|
<a href=/people/l/lisa-veiber/>Lisa Veiber</a>
|
<a href=/people/t/tegawende-f-bissyande/>Tegawendé F. Bissyandé</a>
|
<a href=/people/j/jacques-klein/>Jacques Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--334><div class="card-body p-3 small">Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and has remained an active research field. In recent years, transformer models and more specifically the BERT model developed at Google revolutionised the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While the performance of transformer-based approaches such as BERT has been studied for NER, there has not yet been a study for the fine-grained Named Entity Recognition (FG-NER) task. In this paper, we compare three transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based models (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to a multitude of distinct domains. We find that transformer-based models incrementally outperform the studied non-transformer-based models in most domains with respect to the F1 score. Furthermore, we find that the choice of domains significantly influenced the performance regardless of the respective data size or the model chosen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--336 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.336" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.336/>A Unifying Theory of Transition-based and Sequence Labeling Parsing</a></strong><br><a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a>
|
<a href=/people/m/michalina-strzyz/>Michalina Strzyz</a>
|
<a href=/people/d/david-vilares/>David Vilares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--336><div class="card-body p-3 small">We define a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> from transition-based parsing algorithms that read sentences from left to right to sequence labeling encodings of syntactic trees. This not only establishes a theoretical relation between transition-based parsing and sequence-labeling parsing, but also provides a method to obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--338 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.338/>Semi-supervised Domain Adaptation for Dependency Parsing via Improved Contextualized Word Representations</a></strong><br><a href=/people/y/ying-li/>Ying Li</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--338><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance is dramatically improved on in-domain texts thanks to the rapid progress of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network models</a>. The major challenge for current parsing research is to improve <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on out-of-domain texts that are very different from the in-domain training data when there is only a small-scale out-domain labeled data. To deal with this problem, we propose to improve the contextualized word representations via <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> and fine-tuning BERT processes. Concretely, we apply <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> to three representative semi-supervised domain adaption methods, i.e., direct concatenation (CON), feature augmentation (FA), and domain embedding (DE) with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints, thus enabling to model more pure yet effective domain-specific and domain-invariant representations. Simultaneously, we utilize a large-scale target-domain unlabeled data to fine-tune BERT with only the language model loss, thus obtaining reliable contextualized word representations that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvement, and fine-tuning BERT further boosts parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.341/>Learning to Prune Dependency Trees with Rethinking for Neural Relation Extraction</a></strong><br><a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/x/xue-mengge/>Xue Mengge</a>
|
<a href=/people/z/zhenyu-zhang/>Zhenyu Zhang</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/w/wang-yubin/>Wang Yubin</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--341><div class="card-body p-3 small">Dependency trees have been shown to be effective in capturing long-range relations between target entities. Nevertheless, how to selectively emphasize target-relevant information and remove irrelevant content from the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> is still an open problem. Existing approaches employing pre-defined rules to eliminate <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> may not always yield optimal results due to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> and variability of natural language. In this paper, we present a novel architecture named Dynamically Pruned Graph Convolutional Network (DP-GCN), which learns to prune the dependency tree with rethinking in an end-to-end scheme. In each layer of DP-GCN, we employ a selection module to concentrate on nodes expressing the target relation by a set of binary gates, and then augment the pruned tree with a pruned semantic graph to ensure the connectivity. After that, we introduce a rethinking mechanism to guide and refine the pruning operation by feeding back the high-level learned features repeatedly. Extensive experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves impressive results compared to strong <a href=https://en.wikipedia.org/wiki/Competition>competitors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.342/>How Far Does BERT Look At : Distance-based Clustering and Analysis of BERT’s Attention<span class=acl-fixed-case>BERT</span> Look At: Distance-based Clustering and Analysis of <span class=acl-fixed-case>BERT</span>’s Attention</a></strong><br><a href=/people/y/yue-guan/>Yue Guan</a>
|
<a href=/people/j/jingwen-leng/>Jingwen Leng</a>
|
<a href=/people/c/chao-li/>Chao Li</a>
|
<a href=/people/q/quan-chen/>Quan Chen</a>
|
<a href=/people/m/minyi-guo/>Minyi Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--342><div class="card-body p-3 small">Recent research on the multi-head attention mechanism, especially that in pre-trained models such as BERT, has shown us heuristics and clues in analyzing various aspects of the mechanism. As most of the research focus on probing tasks or hidden states, previous works have found some primitive patterns of attention head behavior by <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristic analytical methods</a>, but a more systematic analysis specific on the <a href=https://en.wikipedia.org/wiki/Attention>attention patterns</a> still remains primitive. In this work, we clearly cluster the attention heatmaps into significantly different patterns through <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised clustering</a> on top of a set of proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, which corroborates with previous observations. We further study their corresponding functions through <a href=https://en.wikipedia.org/wiki/Analytical_chemistry>analytical study</a>. In addition, our proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> can be used to explain and calibrate different attention heads in Transformer models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.343/>An Analysis of Simple Data Augmentation for Named Entity Recognition</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--343><div class="card-body p-3 small">Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (i2b2-2010 and MaSciP), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.348/>Integrating Domain Terminology into Neural Machine Translation</a></strong><br><a href=/people/e/elise-michon/>Elise Michon</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--348><div class="card-body p-3 small">This paper extends existing work on terminology integration into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>, a common industrial practice to dynamically adapt <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to a specific domain. Our method, based on the use of placeholders complemented with morphosyntactic annotation, efficiently taps into the ability of the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to deal with symbolic knowledge to surpass the surface generalization shown by alternative techniques. We compare our approach to state-of-the-art systems and benchmark them through a well-defined evaluation framework, focusing on actual application of terminology and not just on the overall performance. Results indicate the suitability of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in the use-case where <a href=https://en.wikipedia.org/wiki/Terminology>terminology</a> is used in a <a href=https://en.wikipedia.org/wiki/System>system</a> trained on generic data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--351 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.351" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.351/>Neural Machine Translation Models with Back-Translation for the Extremely Low-Resource Indigenous Language Bribri<span class=acl-fixed-case>B</span>ribri</a></strong><br><a href=/people/i/isaac-feldman/>Isaac Feldman</a>
|
<a href=/people/r/rolando-coto-solano/>Rolando Coto-Solano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--351><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation model</a> and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for the <a href=https://en.wikipedia.org/wiki/Bribri_language>Chibchan language Bribri</a>, with an average performance of BLEU 16.91.7. This was trained on an extremely small dataset (5923 Bribri-Spanish pairs), providing evidence for the applicability of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> in extremely low-resource environments. We discuss the challenges entailed in managing training input from languages without standard orthographies, we provide evidence of successful learning of Bribri grammar, and also examine the translations of structures that are infrequent in major <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>, such as positional verbs, ergative markers, numerical classifiers and complex demonstrative systems. In addition to this, we perform an experiment of augmenting the dataset through iterative back-translation (Sennrich et al., 2016a ; Hoang et al., 2018) by using Spanish sentences to create synthetic Bribri sentences. This improves the score by an average of 1.0 BLEU, but only when the new Spanish sentences belong to the same domain as the other Spanish examples. This contributes to the small but growing body of research on Chibchan NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.352/>Dynamic Curriculum Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/c/chen-xu/>Chen Xu</a>
|
<a href=/people/b/bojie-hu/>Bojie Hu</a>
|
<a href=/people/y/yufan-jiang/>Yufan Jiang</a>
|
<a href=/people/k/kai-feng/>Kai Feng</a>
|
<a href=/people/z/zeyang-wang/>Zeyang Wang</a>
|
<a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--352><div class="card-body p-3 small">Large amounts of data has made neural machine translation (NMT) a big success in recent years. But it is still a challenge if we train these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on small-scale corpora. In this case, the way of using data appears to be more important. Here, we investigate the effective use of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for low-resource NMT. In particular, we propose a dynamic curriculum learning (DCL) method to reorder training samples in training. Unlike previous work, we do not use a static scoring function for <a href=https://en.wikipedia.org/wiki/Order_of_operations>reordering</a>. Instead, the order of training samples is dynamically determined in two ways-loss decline and model competence. This eases training by highlighting easy samples that the current <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has enough competence to learn. We test our DCL method in a Transformer-based system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT&#8217;16 En-De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.356/>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural Text<span class=acl-fixed-case>LSTM</span> Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a></strong><br><a href=/people/c/chihiro-shibata/>Chihiro Shibata</a>
|
<a href=/people/k/kei-uchiumi/>Kei Uchiumi</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--356><div class="card-body p-3 small">Long Short-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. However, how such <a href=https://en.wikipedia.org/wiki/Information>information</a> are reflected in its internal vectors for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a> has not yet been sufficiently investigated. We analyze them by learning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> where <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> are implicitly given. We empirically show that the <a href=https://en.wikipedia.org/wiki/Context_(computing)>context update vectors</a>, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as <a href=https://en.wikipedia.org/wiki/Phrase_structure>VP</a> and <a href=https://en.wikipedia.org/wiki/Phrase_structure>NP</a>. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structure</a> or not from a small number of components of the context vector. Even for the case of <a href=https://en.wikipedia.org/wiki/Learning>learning</a> from raw text, context vectors are shown to still correlate well with the <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structures</a>. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--358 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.358/>When and Who? Conversation Transition Based on Bot-Agent Symbiosis Learning Network</a></strong><br><a href=/people/y/yipeng-yu/>Yipeng Yu</a>
|
<a href=/people/r/ran-guan/>Ran Guan</a>
|
<a href=/people/j/jie-ma/>Jie Ma</a>
|
<a href=/people/z/zhuoxuan-jiang/>Zhuoxuan Jiang</a>
|
<a href=/people/j/jingchang-huang/>Jingchang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--358><div class="card-body p-3 small">In online customer service applications, multiple <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> that are specialized in various topics are typically developed separately and are then merged with other <a href=https://en.wikipedia.org/wiki/Intelligent_agent>human agents</a> to a single <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, presenting to the users with a unified interface. Ideally the conversation can be transparently transferred between different sources of <a href=https://en.wikipedia.org/wiki/Customer_support>customer support</a> so that domain-specific questions can be answered timely and this is what we coined as a Bot-Agent symbiosis. Conversation transition is a major challenge in such online customer service and our work formalises the challenge as two core problems, namely, when to transfer and which bot or agent to transfer to and introduces a deep neural networks based approach that addresses these problems. Inspired by the net promoter score (NPS), our research reveals how the problems can be effectively solved by providing user feedback and developing deep neural networks that predict the conversation category distribution and the NPS of the dialogues. Experiments on realistic data generated from an online service support platform demonstrate that the proposed approach outperforms state-of-the-art methods and shows promising perspective for transparent conversation transition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.363/>Translation vs. Dialogue : A Comparative Analysis of Sequence-to-Sequence Modeling</a></strong><br><a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/r/ran-le/>Ran Le</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/j/jinwen-ma/>Jinwen Ma</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--363><div class="card-body p-3 small">Understanding neural models is a major topic of interest in the deep learning community. In this paper, we propose to interpret a general neural model comparatively. Specifically, we study the sequence-to-sequence (Seq2Seq) model in the contexts of two mainstream NLP tasksmachine translation and dialogue response generationas they both use the seq2seq model. We investigate how the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are different and how their task difference results in major differences in the behaviors of the resulting translation and dialogue generation systems. This study allows us to make several interesting observations and gain valuable insights, which can be used to help develop better translation and dialogue generation models. To our knowledge, no such comparative study has been done so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.364/>Diverse dialogue generation with context dependent dynamic loss function</a></strong><br><a href=/people/a/ayaka-ueyama/>Ayaka Ueyama</a>
|
<a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--364><div class="card-body p-3 small">Dialogue systems using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have achieved generation of fluent response sentences to user utterances. Nevertheless, they tend to produce responses that are not diverse and which are less context-dependent. To address these shortcomings, we propose a new <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, an Inverse N-gram loss (INF), which incorporates contextual fluency and diversity at the same time by a simple formula. Our INF loss can adjust its <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> dynamically by a weight using the inverse frequency of the tokens&#8217; n-gram applied to Softmax Cross-Entropy loss, so that rare tokens appear more likely while retaining the fluency of the generated sentences. We trained Transformer using English and Japanese Twitter replies as single-turn dialogues using different <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a>. Our INF loss model outperformed the baselines of SCE loss and ITF loss models in automatic evaluations such as DIST-N and ROUGE, and also achieved higher scores on our human evaluations of coherence and richness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.365/>Towards Topic-Guided Conversational Recommender System</a></strong><br><a href=/people/k/kun-zhou/>Kun Zhou</a>
|
<a href=/people/y/yuanhang-zhou/>Yuanhang Zhou</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/x/xiaoke-wang/>Xiaoke Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--365><div class="card-body p-3 small">Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. To develop an effective <a href=https://en.wikipedia.org/wiki/Computational_fluid_dynamics>CRS</a>, the support of high-quality datasets is essential. Existing CRS datasets mainly focus on immediate requests from users, while lack proactive guidance to the recommendation scenario. In this paper, we contribute a new CRS dataset named TG-ReDial (Recommendation through Topic-Guided Dialog). Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> has two major features. First, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> incorporates topic threads to enforce natural semantic transitions towards the recommendation scenario. Second, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is created in a semi-automatic way, hence human annotation is more reasonable and controllable. Based on TG-ReDial, we present the task of topic-guided conversational recommendation, and propose an effective approach to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Extensive experiments have demonstrated the effectiveness of our approach on three sub-tasks, namely topic prediction, item recommendation and response generation. TG-ReDial is available at blue.<b>TG-ReDial</b> (<b>Re</b>commendation through <b>T</b>opic-<b>G</b>uided <b>Dial</b>og). Our dataset has two major features. First, it incorporates topic threads to enforce natural semantic transitions towards the recommendation scenario. Second, it is created in a semi-automatic way, hence human annotation is more reasonable and controllable. Based on TG-ReDial, we present the task of topic-guided conversational recommendation, and propose an effective approach to this task. Extensive experiments have demonstrated the effectiveness of our approach on three sub-tasks, namely topic prediction, item recommendation and response generation. TG-ReDial is available at blue<url>https://github.com/RUCAIBox/TG-ReDial</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.367/>Summarize before Aggregate : A Global-to-local Heterogeneous Graph Inference Network for Conversational Emotion Recognition</a></strong><br><a href=/people/d/dongming-sheng/>Dongming Sheng</a>
|
<a href=/people/d/dong-wang/>Dong Wang</a>
|
<a href=/people/y/ying-shen/>Ying Shen</a>
|
<a href=/people/h/haitao-zheng/>Haitao Zheng</a>
|
<a href=/people/h/haozhuang-liu/>Haozhuang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--367><div class="card-body p-3 small">Conversational Emotion Recognition (CER) is a crucial task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> with wide applications. Prior works in CER generally focus on modeling emotion influences solely with utterance-level features, with little attention paid on phrase-level semantic connection between utterances. Phrases carry sentiments when they are referred to emotional events under certain topics, providing a global semantic connection between utterances throughout the entire conversation. In this work, we propose a two-stage Summarization and Aggregation Graph Inference Network (SumAggGIN), which seamlessly integrates inference for topic-related emotional phrases and local dependency reasoning over neighbouring utterances in a global-to-local fashion. Topic-related emotional phrases, which constitutes the global topic-related emotional connections, are recognized by our proposed heterogeneous Summarization Graph. Local dependencies, which captures short-term emotional effects between neighbouring utterances, are further injected via an Aggregation Graph to distinguish the subtle differences between utterances containing emotional phrases. The two steps of graph inference are tightly-coupled for a comprehensively understanding of emotional fluctuation. Experimental results on three CER benchmark datasets verify the effectiveness of our proposed model, which outperforms the state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.368/>Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems</a></strong><br><a href=/people/v/vitou-phy/>Vitou Phy</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--368><div class="card-body p-3 small">Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, and the importance of each aspect differs according to the task. For instance, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a> is mandatory in a food-ordering dialogue task, whereas <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> is preferred in a language-teaching dialogue system. However, existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between reference and candidate response. Thus, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not guaranteed to capture the required <a href=https://en.wikipedia.org/wiki/Complex_system>aspects</a>, i.e., <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups : understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--370 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.370/>HiTrans : A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations<span class=acl-fixed-case>H</span>i<span class=acl-fixed-case>T</span>rans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations</a></strong><br><a href=/people/j/jingye-li/>Jingye Li</a>
|
<a href=/people/d/donghong-ji/>Donghong Ji</a>
|
<a href=/people/f/fei-li/>Fei Li</a>
|
<a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/y/yijiang-liu/>Yijiang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--370><div class="card-body p-3 small">Emotion detection in conversations (EDC) is to detect the emotion for each utterance in conversations that have multiple speakers. Different from the traditional non-conversational emotion detection, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for EDC should be context-sensitive (e.g., understanding the whole conversation rather than one utterance) and speaker-sensitive (e.g., understanding which utterance belongs to which speaker). In this paper, we propose a transformer-based context- and speaker-sensitive model for EDC, namely HiTrans, which consists of two hierarchical transformers. We utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer so that utterance representations could be sensitive to the global context of the conversation. Moreover, we exploit an auxiliary task to make our model speaker-sensitive, called pairwise utterance speaker verification (PUSV), which aims to classify whether two utterances belong to the same speaker. We evaluate our model on three benchmark datasets, namely EmoryNLP, <a href=https://en.wikipedia.org/wiki/MELD>MELD</a> and IEMOCAP. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.371.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--371 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.371 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.371" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.371/>A Co-Attentive Cross-Lingual Neural Model for Dialogue Breakdown Detection</a></strong><br><a href=/people/q/qian-lin/>Qian Lin</a>
|
<a href=/people/s/souvik-kundu/>Souvik Kundu</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--371><div class="card-body p-3 small">Ensuring smooth communication is essential in a chat-oriented dialogue system, so that a user can obtain meaningful responses through interactions with the <a href=https://en.wikipedia.org/wiki/System>system</a>. Most prior work on dialogue research does not focus on preventing dialogue breakdown. One of the major challenges is that a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> may generate an undesired utterance leading to a dialogue breakdown, which degrades the overall interaction quality. Hence, it is crucial for a machine to detect dialogue breakdowns in an ongoing conversation. In this paper, we propose a novel dialogue breakdown detection model that jointly incorporates a pretrained cross-lingual language model and a co-attention network. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> leverages effective <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained on one hundred different languages to generate contextualized representations. Co-attention aims to capture the interaction between the latest utterance and the conversation history, and thereby determines whether the latest utterance causes a dialogue breakdown. Experimental results show that our proposed model outperforms all previous approaches on all evaluation metrics in both the Japanese and English tracks in Dialogue Breakdown Detection Challenge 4 (DBDC4 at IWSDS2019).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.376/>Improving Low-Resource NMT through Relevance Based Linguistic Features Incorporation<span class=acl-fixed-case>NMT</span> through Relevance Based Linguistic Features Incorporation</a></strong><br><a href=/people/a/abhisek-chakrabarty/>Abhisek Chakrabarty</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--376><div class="card-body p-3 small">In this study, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a> at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> of the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. Experiments are conducted on translation tasks from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to eight Asian languages, with no more than twenty thousand sentences for training. The proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> improve translation quality for all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> by up to 3.09 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a>. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.383/>Filtering Back-Translated Data in Unsupervised Neural Machine Translation</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--383><div class="card-body p-3 small">Unsupervised neural machine translation (NMT) utilizes only <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> for training. The quality of back-translated data plays an important role in the performance of <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. In <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, all generated pseudo parallel sentence pairs are not of the same quality. Taking inspiration from domain adaptation where in-domain sentences are given more weight in training, in this paper we propose an approach to filter back-translated data as part of the training process of unsupervised NMT. Our approach gives more weight to good pseudo parallel sentence pairs in the back-translation phase. We calculate the weight of each pseudo parallel sentence pair using sentence-wise round-trip BLEU score which is normalized batch-wise. We compare our approach with the current state of the art approaches for unsupervised NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.384/>Lost in Back-Translation : Emotion Preservation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--384><div class="card-body p-3 small">Machine translation provides powerful methods to convert text between languages, and is therefore a technology enabling a <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual world</a>. An important part of <a href=https://en.wikipedia.org/wiki/Communication>communication</a>, however, takes place at the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>non-propositional level</a> (e.g., <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>, <a href=https://en.wikipedia.org/wiki/Formality>formality</a>, emotions), and it is far from clear whether current MT methods properly translate this information. This paper investigates the specific hypothesis that the non-propositional level of emotions is at least partially lost in MT. We carry out a number of experiments in a back-translation setup and establish that (1) <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are indeed partially lost during translation ; (2) this tendency can be reversed almost completely with a simple re-ranking approach informed by an emotion classifier, taking advantage of diversity in the n-best list ; (3) the re-ranking approach can also be applied to change <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>, obtaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are toned down or amplified, such as change of modality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.389/>Context-Aware Cross-Attention for Non-Autoregressive Translation</a></strong><br><a href=/people/l/liang-ding/>Liang Ding</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/d/di-wu/>Di Wu</a>
|
<a href=/people/d/dacheng-tao/>Dacheng Tao</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--389><div class="card-body p-3 small">Non-autoregressive translation (NAT) significantly accelerates the inference process by predicting the entire target sequence. However, due to the lack of target dependency modelling in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, the conditional generation process heavily depends on the cross-attention. In this paper, we reveal a localness perception problem in NAT cross-attention, for which it is difficult to adequately capture source context. To alleviate this problem, we propose to enhance signals of neighbour source tokens into conventional cross-attention. Experimental results on several representative datasets show that our approach can consistently improve translation quality over strong NAT baselines. Extensive analyses demonstrate that the enhanced cross-attention achieves better exploitation of source contexts by leveraging both local and global information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.390/>Does Gender Matter? Towards Fairness in Dialogue Systems</a></strong><br><a href=/people/h/haochen-liu/>Haochen Liu</a>
|
<a href=/people/j/jamell-dacon/>Jamell Dacon</a>
|
<a href=/people/w/wenqi-fan/>Wenqi Fan</a>
|
<a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/z/zitao-liu/>Zitao Liu</a>
|
<a href=/people/j/jiliang-tang/>Jiliang Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--390><div class="card-body p-3 small">Recently there are increasing concerns about the <a href=https://en.wikipedia.org/wiki/Fairness>fairness</a> of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence (AI)</a> in real-world applications such as <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendations</a>. For example, <a href=https://en.wikipedia.org/wiki/Computer_vision>recognition algorithms</a> in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> are unfair to black people such as poorly detecting their faces and inappropriately identifying them as <a href=https://en.wikipedia.org/wiki/Gorilla>gorillas</a>. As one crucial application of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>, <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> have been extensively applied in our society. They are usually built with real human conversational data ; thus they could inherit some fairness issues which are held in the real world. However, the <a href=https://en.wikipedia.org/wiki/Equity_(economics)>fairness</a> of <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> has not been well investigated. In this paper, we perform a pioneering study about the <a href=https://en.wikipedia.org/wiki/Fair_division>fairness issues</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue systems</a>. In particular, we construct a benchmark dataset and propose quantitative measures to understand <a href=https://en.wikipedia.org/wiki/Equity_(economics)>fairness</a> in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>, we propose two simple but effective debiasing methods. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can reduce the bias in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue systems</a> significantly. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the implementation are released to foster fairness research in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--392 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.392/>Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer</a></strong><br><a href=/people/d/duzhen-zhang/>Duzhen Zhang</a>
|
<a href=/people/x/xiuyi-chen/>Xiuyi Chen</a>
|
<a href=/people/s/shuang-xu/>Shuang Xu</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--392><div class="card-body p-3 small">Emotion recognition in textual conversations (ERTC) plays an important role in a wide range of applications, such as <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>, <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a>, and so on. ERTC, however, is a challenging task. For one thing, speakers often rely on the context and commonsense knowledge to express emotions ; for another, most utterances contain neutral emotion in conversations, as a result, the confusion between a few non-neutral utterances and much more neutral ones restrains the <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> performance. In this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly, we devise a dual-level graph attention mechanism to leverage commonsense knowledge, which augments the semantic information of the utterance. Then we apply the Incremental Transformer to encode multi-turn contextual utterances. Moreover, we are the first to introduce <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to alleviate the aforementioned confusion and thus further improve the <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> performance. Extensive experimental results show that our KAITML model outperforms the state-of-the-art models across five benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.395/>Leveraging Discourse Rewards for Document-Level Neural Machine Translation</a></strong><br><a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/n/nazanin-esmaili/>Nazanin Esmaili</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--395><div class="card-body p-3 small">Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of the individual sentences in the document needs to retain aspects of the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion and <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F-BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--396 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.396/>Effective Use of Target-side Context for Neural Machine Translation</a></strong><br><a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/h/hitoshi-ito/>Hitoshi Ito</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/i/ichiro-yamada/>Ichiro Yamada</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--396><div class="card-body p-3 small">In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a>. Neural machine translation (NMT) systems suffer degraded performance when trained with <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy data</a>. Because there is no clean Japanese-English parallel data for <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, such as <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> and clean data. Though the existing methods try to overcome this problem by using <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.398/>Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation<span class=acl-fixed-case>MAP</span> Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation</a></strong><br><a href=/people/b/bryan-eikema/>Bryan Eikema</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--398><div class="card-body p-3 small">Recent studies have revealed a number of pathologies of neural machine translation (NMT) systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> or its training algorithm, <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Most of this evidence was gathered using maximum a posteriori (MAP) decoding, a <a href=https://en.wikipedia.org/wiki/Decision_rule>decision rule</a> aimed at identifying the highest-scoring translation, i.e. the <a href=https://en.wikipedia.org/wiki/Mode_(user_interface)>mode</a>. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT&#8217;s statistical assumptions nor MLE. In particular, we show that the most likely <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translations</a> under the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> accumulate so little <a href=https://en.wikipedia.org/wiki/Probability_mass_function>probability mass</a> that the mode can be considered essentially arbitrary. We therefore advocate for the use of <a href=https://en.wikipedia.org/wiki/Decision_rule>decision rules</a> that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.399.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--399 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.399 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.399" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.399/>Domain Transfer based Data Augmentation for Neural Query Translation</a></strong><br><a href=/people/l/liang-yao/>Liang Yao</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/h/haibo-zhang/>Haibo Zhang</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--399><div class="card-body p-3 small">Query translation (QT) serves as a critical factor in successful <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>cross-lingual information retrieval (CLIR)</a>. Due to the lack of parallel query samples, neural-based QT models are usually optimized with synthetic data which are derived from large-scale monolingual queries. Nevertheless, such kind of pseudo corpus is mostly produced by a general-domain translation model, making it be insufficient to guide the learning of QT model. In this paper, we extend the <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> with a domain transfer procedure, thus to revise synthetic candidates to search-aware examples. Specifically, the domain transfer model is built upon advanced Transformer, in which layer coordination and mixed attention are exploited to speed up the refining process and leverage parameters from a pre-trained cross-lingual language model. In order to examine the effectiveness of the proposed method, we collected French-to-English and Spanish-to-English QT test sets, each of which consists of 10,000 parallel query pairs with careful manual-checking. Qualitative and quantitative analyses reveal that our model significantly outperforms strong baselines and the related domain transfer methods on both translation quality and retrieval accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--401 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.401/>Aspectuality Across Genre : A Distributional Semantics Approach</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/m/matthew-stone/>Matthew Stone</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--401><div class="card-body p-3 small">The interpretation of the <a href=https://en.wikipedia.org/wiki/Lexical_aspect>lexical aspect</a> of verbs in <a href=https://en.wikipedia.org/wiki/English_language>English</a> plays a crucial role in tasks such as recognizing textual entailment and learning discourse-level inferences. We show that two elementary dimensions of aspectual class, states vs. events, and telic vs. atelic events, can be modelled effectively with <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. We find that a verb&#8217;s local context is most indicative of its <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspectual class</a>, and we demonstrate that closed class words tend to be stronger discriminating contexts than <a href=https://en.wikipedia.org/wiki/Content_word>content words</a>. Our approach outperforms previous work on three datasets. Further, we present a new dataset of human-human conversations annotated with lexical aspects and present experiments that show the correlation of <a href=https://en.wikipedia.org/wiki/Telicity>telicity</a> with genre and discourse goals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.406/>Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner Recognition Using BERT<span class=acl-fixed-case>P</span>ersian Word Segmentation Correction and Zero-Width Non-Joiner Recognition Using <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/e/ehsan-doostmohammadi/>Ehsan Doostmohammadi</a>
|
<a href=/people/m/minoo-nassajian/>Minoo Nassajian</a>
|
<a href=/people/a/adel-rahimi/>Adel Rahimi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--406><div class="card-body p-3 small">Words are properly segmented in the <a href=https://en.wikipedia.org/wiki/Persian_alphabet>Persian writing system</a> ; in practice, however, these writing rules are often neglected, resulting in single words being written disjointedly and multiple words written without any white spaces between them. This paper addresses the problems of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and zero-width non-joiner (ZWNJ) recognition in <a href=https://en.wikipedia.org/wiki/Persian_language>Persian</a>, which we approach jointly as a sequence labeling problem. We achieved a macro-averaged F1-score of 92.40 % on a carefully collected corpus of 500 sentences with a high level of difficulty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.407/>Syllable-based Neural Thai Word Segmentation<span class=acl-fixed-case>T</span>hai Word Segmentation</a></strong><br><a href=/people/p/pattarawat-chormai/>Pattarawat Chormai</a>
|
<a href=/people/p/ponrawee-prasertsom/>Ponrawee Prasertsom</a>
|
<a href=/people/j/jin-cheevaprawatdomrong/>Jin Cheevaprawatdomrong</a>
|
<a href=/people/a/attapol-rutherford/>Attapol Rutherford</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--407><div class="card-body p-3 small">Word segmentation is a challenging pre-processing step for Thai Natural Language Processing due to the lack of explicit word boundaries. The previous systems rely on powerful neural network architecture alone and ignore linguistic substructures of Thai words. We utilize the linguistic observation that Thai strings can be segmented into syllables, which should narrow down the search space for the word boundaries and provide helpful features. Here, we propose a neural Thai Word Segmenter that uses syllable embeddings to capture linguistic constraints and uses dilated CNN filters to capture the environment of each character. Within this goal, we develop the first ML-based Thai orthographical syllable segmenter, which yields syllable embeddings to be used as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> by the word segmenter. Our word segmentation system outperforms the previous state-of-the-art system in both <a href=https://en.wikipedia.org/wiki/Speed>speed</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on both in-domain and out-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.409/>Morphological disambiguation from stemming data</a></strong><br><a href=/people/a/antoine-nzeyimana/>Antoine Nzeyimana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--409><div class="card-body p-3 small">Morphological analysis and disambiguation is an important task and a crucial preprocessing step in natural language processing of morphologically rich languages. Kinyarwanda, a morphologically rich language, currently lacks tools for automated morphological analysis. While linguistically curated finite state tools can be easily developed for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>, the morphological richness of the language allows many ambiguous analyses to be produced, requiring effective disambiguation. In this paper, we propose learning to morphologically disambiguate Kinyarwanda verbal forms from a new stemming dataset collected through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a>. Using <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and a feed-forward neural network based classifier, we achieve about 89 % non-contextualized disambiguation accuracy. Our experiments reveal that inflectional properties of <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> and morpheme association rules are the most discriminative features for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.415/>Cross-lingual Transfer Learning for Grammatical Error Correction</a></strong><br><a href=/people/i/ikumi-yamashita/>Ikumi Yamashita</a>
|
<a href=/people/s/satoru-katsumata/>Satoru Katsumata</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--415><div class="card-body p-3 small">In this study, we explore cross-lingual transfer learning in grammatical error correction (GEC) tasks. Many languages lack the resources required to train <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GEC models</a>. Cross-lingual transfer learning from high-resource languages (the source models) is effective for training models of low-resource languages (the target models) for various tasks. However, in GEC tasks, the possibility of transferring grammatical knowledge (e.g., grammatical functions) across languages is not evident. Therefore, we investigate cross-lingual transfer learning methods for GEC. Our results demonstrate that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from other languages can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GEC</a>. We also demonstrate that proximity to source languages has a significant impact on the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of correcting certain types of errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.417/>ContraCAT : Contrastive Coreference Analytical Templates for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>C</span>ontra<span class=acl-fixed-case>CAT</span>: Contrastive Coreference Analytical Templates for Machine Translation</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/b/benno-krojer/>Benno Krojer</a>
|
<a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--417><div class="card-body p-3 small">Recent high scores on pronoun translation using context-aware neural machine translation have suggested that current approaches work well. ContraPro is a notable example of a contrastive challenge set for EnglishGerman pronoun translation. The high scores achieved by transformer models may suggest that they are able to effectively model the complicated set of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inferences</a> required to carry out pronoun translation. This entails the ability to determine which entities could be referred to, identify which entity a source-language pronoun refers to (if any), and access the target-language grammatical gender for that entity. We first show through a series of targeted adversarial attacks that in fact current approaches are not able to model all of this information well. Inserting small amounts of <a href=https://en.wikipedia.org/wiki/Distraction>distracting information</a> is enough to strongly reduce scores, which should not be the case. We then create a new template test set ContraCAT, designed to individually assess the ability to handle the specific steps necessary for successful pronoun translation. Our analyses show that current approaches to context-aware NMT rely on a set of <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>surface heuristics</a>, which break down when translations require real reasoning. We also propose an approach for augmenting the training data, with some improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.420/>A Human Evaluation of AMR-to-English Generation Systems<span class=acl-fixed-case>AMR</span>-to-<span class=acl-fixed-case>E</span>nglish Generation Systems</a></strong><br><a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/s/shira-wein/>Shira Wein</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--420><div class="card-body p-3 small">Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, which are known to be problematic for <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these <a href=https://en.wikipedia.org/wiki/System>systems</a> and how our results compare to those of automatic metrics, finding that while the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>common errors</a> made by these <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--423 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.423" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.423/>Manual Clustering and Spatial Arrangement of Verbs for Multilingual Evaluation and Typology Analysis</a></strong><br><a href=/people/o/olga-majewska/>Olga Majewska</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/diana-mccarthy/>Diana McCarthy</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--423><div class="card-body p-3 small">We present the first evaluation of the applicability of a spatial arrangement method (SpAM) to a typologically diverse language sample, and its potential to produce semantic evaluation resources to support multilingual NLP, with a focus on verb semantics. We demonstrate <a href=https://en.wikipedia.org/wiki/Spamming>SpAM</a>&#8217;s utility in allowing for quick bottom-up creation of large-scale evaluation datasets that balance cross-lingual alignment with language specificity. Starting from a shared sample of 825 English verbs, translated into <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, we apply a two-phase annotation process which produces (i) semantic verb classes and (ii) fine-grained similarity scores for nearly 130 thousand verb pairs. We use the two types of verb data to (a) examine cross-lingual similarities and variation, and (b) evaluate the capacity of static and contextualised representation models to accurately reflect verb semantics, contrasting the performance of large language specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.427" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.427/>Measuring Correlation-to-Causation Exaggeration in Press Releases</a></strong><br><a href=/people/b/bei-yu/>Bei Yu</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a>
|
<a href=/people/l/lu-guo/>Lu Guo</a>
|
<a href=/people/y/yingya-li/>Yingya Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--427><div class="card-body p-3 small">Press releases have an increasingly strong influence on media coverage of health research ; however, they have been found to contain seriously exaggerated claims that can misinform the public and undermine public trust in science. In this study we propose an NLP approach to identify exaggerated causal claims made in health press releases that report on <a href=https://en.wikipedia.org/wiki/Observational_study>observational studies</a>, which are designed to establish <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlational findings</a>, but are often exaggerated as causal. We developed a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that can identify causal claims in the main statements in a <a href=https://en.wikipedia.org/wiki/Press_release>press release</a>. By comparing the claims made in a press release with the corresponding claims in the original research paper, we found that 22 % of <a href=https://en.wikipedia.org/wiki/Press_release>press releases</a> made exaggerated causal claims from <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlational findings</a> in <a href=https://en.wikipedia.org/wiki/Observational_study>observational studies</a>. Furthermore, universities exaggerated more often than <a href=https://en.wikipedia.org/wiki/Academic_journal>journal publishers</a> by a ratio of 1.5 to 1. Encouragingly, the <a href=https://en.wikipedia.org/wiki/Exaggeration>exaggeration rate</a> has slightly decreased over the past 10 years, despite the increase of the total number of press releases. More research is needed to understand the cause of the decreasing pattern.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.428/>Inflating Topic Relevance with Ideology : A Case Study of Political Ideology Bias in Social Topic Detection Models</a></strong><br><a href=/people/m/meiqi-guo/>Meiqi Guo</a>
|
<a href=/people/r/rebecca-hwa/>Rebecca Hwa</a>
|
<a href=/people/y/yu-ru-lin/>Yu-Ru Lin</a>
|
<a href=/people/w/wen-ting-chung/>Wen-Ting Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--428><div class="card-body p-3 small">We investigate the impact of political ideology biases in training data. Through a set of comparison studies, we examine the propagation of biases in several widely-used NLP models and its effect on the overall retrieval accuracy. Our work highlights the susceptibility of large, complex models to propagating the biases from human-selected input, which may lead to a deterioration of retrieval accuracy, and the importance of controlling for these <a href=https://en.wikipedia.org/wiki/Bias>biases</a>. Finally, as a way to mitigate the bias, we propose to learn a text representation that is invariant to <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a> while still judging topic relevance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.432/>Balanced Joint Adversarial Training for Robust Intent Detection and Slot Filling</a></strong><br><a href=/people/x/xu-cao/>Xu Cao</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/c/chongyang-shi/>Chongyang Shi</a>
|
<a href=/people/c/chao-wang/>Chao Wang</a>
|
<a href=/people/y/yao-meng/>Yao Meng</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--432><div class="card-body p-3 small">Joint intent detection and slot filling has recently achieved tremendous success in advancing the performance of utterance understanding. However, many joint models still suffer from the robustness problem, especially on <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy inputs</a> or <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>rare / unseen events</a>. To address this issue, we propose a Joint Adversarial Training (JAT) model to improve the robustness of joint intent detection and slot filling, which consists of two parts : (1) automatically generating joint adversarial examples to attack the joint model, and (2) training the model to defend against the joint adversarial examples so as to robustify the model on small perturbations. As the generated joint adversarial examples have different impacts on the intent detection and slot filling loss, we further propose a Balanced Joint Adversarial Training (BJAT) model that applies a balance factor as a regularization term to the final loss function, which yields a stable training procedure. Extensive experiments and analyses on the lightweight models show that our proposed methods achieve significantly higher scores and substantially improve the robustness of both intent detection and slot filling. In addition, the combination of our BJAT with BERT-large achieves state-of-the-art results on two datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.434/>Understanding Unnatural Questions Improves Reasoning over Text</a></strong><br><a href=/people/x/xiaoyu-guo/>Xiaoyu Guo</a>
|
<a href=/people/y/yuan-fang-li/>Yuan-Fang Li</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--434><div class="card-body p-3 small">Complex question answering (CQA) over raw text is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. A prominent approach to this task is based on the programmer-interpreter framework, where the programmer maps the question into a sequence of reasoning actions and the <a href=https://en.wikipedia.org/wiki/Interpreter_(computing)>interpreter</a> then executes these actions on the raw text. Learning an effective CQA model requires large amounts of human-annotated data, consisting of the ground-truth sequence of reasoning actions, which is time-consuming and expensive to collect at scale. In this paper, we address the challenge of learning a high-quality programmer (parser) by projecting natural human-generated questions into unnatural machine-generated questions which are more convenient to parse. We firstly generate synthetic (question, action sequence) pairs by a data generator, and train a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that associates synthetic questions with their corresponding action sequences. To capture the diversity when applied to natural questions, we learn a <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection model</a> to map natural questions into their most similar unnatural questions for which the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> can work well. Without any natural training data, our projection model provides high-quality action sequences for the CQA task. Experimental results show that the QA model trained exclusively with synthetic data outperforms its state-of-the-art counterpart trained on human-labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--449 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.449" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.449/>A Mixture-of-Experts Model for Learning Multi-Facet Entity Embeddings</a></strong><br><a href=/people/r/rana-alshaikh/>Rana Alshaikh</a>
|
<a href=/people/z/zied-bouraoui/>Zied Bouraoui</a>
|
<a href=/people/s/shelan-jeawak/>Shelan Jeawak</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--449><div class="card-body p-3 small">Various <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> have already been proposed for learning entity embeddings from text descriptions. Such embeddings are commonly used for inferring properties of entities, for <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a> and entity-oriented search, and for injecting background knowledge into neural architectures, among others. Entity embeddings essentially serve as a compact encoding of a <a href=https://en.wikipedia.org/wiki/Similarity_(geometry)>similarity relation</a>, but <a href=https://en.wikipedia.org/wiki/Similarity_(geometry)>similarity</a> is an inherently multi-faceted notion. By representing entities as single vectors, existing methods leave it to downstream applications to identify these different facets, and to select the most relevant ones. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that instead learns several vectors for each entity, each of which intuitively captures a different aspect of the considered domain. We use a mixture-of-experts formulation to jointly learn these facet-specific embeddings. The individual entity embeddings are learned using a variant of the GloVe model, which has the advantage that we can easily identify which properties are modelled well in which of the learned embeddings. This is exploited by an associated gating network, which uses pre-trained word vectors to encourage the properties that are modelled by a given embedding to be semantically coherent, i.e. to encourage each of the individual embeddings to capture a meaningful facet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--450 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.450" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.450/>Classifier Probes May Just Learn from Linear Context Features</a></strong><br><a href=/people/j/jenny-kunz/>Jenny Kunz</a>
|
<a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--450><div class="card-body p-3 small">Classifiers trained on auxiliary probing tasks are a popular tool to analyze the representations learned by neural sentence encoders such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and ELMo. While many authors are aware of the difficulty to distinguish between extracting the linguistic structure encoded in the representations and learning the probing task, the validity of probing methods calls for further research. Using a neighboring word identity prediction task, we show that the token embeddings learned by neural sentence encoders contain a significant amount of information about the exact linear context of the token, and hypothesize that, with such information, learning standard probing tasks may be feasible even without additional linguistic structure. We develop this hypothesis into a framework in which analysis efforts can be scrutinized and argue that, with current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and baselines, conclusions that <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> contain linguistic structure are not well-founded. Current probing methodology, such as restricting the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier&#8217;s expressiveness</a> or using strong baselines, can help to better estimate the complexity of <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.451/>Priorless Recurrent Networks Learn Curiously</a></strong><br><a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/j/jeffrey-bowers/>Jeffrey Bowers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--451><div class="card-body p-3 small">Recently, domain-general recurrent neural networks, without explicit linguistic inductive biases, have been shown to successfully reproduce a range of human language behaviours, such as accurately predicting number agreement between nouns and verbs. We show that such <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> will also learn <a href=https://en.wikipedia.org/wiki/Number_agreement>number agreement</a> within <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>unnatural sentence structures</a>, i.e. structures that are not found within any <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> and which humans struggle to process. These results suggest that the models are learning from their input in a manner that is substantially different from human language acquisition, and we undertake an analysis of how the learned knowledge is stored in the weights of the network. We find that while the model has an effective understanding of singular versus plural for individual sentences, there is a lack of a unified concept of <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> connecting these processes across the full range of inputs. Moreover, the <a href=https://en.wikipedia.org/wiki/Weighting>weights</a> handling natural and unnatural structures overlap substantially, in a way that underlines the non-human-like nature of the knowledge learned by the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.460/>Identifying Motion Entities in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a> and A Case Study for Named Entity Recognition</a></strong><br><a href=/people/n/ngoc-phuoc-an-vo/>Ngoc Phuoc An Vo</a>
|
<a href=/people/i/irene-manotas/>Irene Manotas</a>
|
<a href=/people/v/vadim-sheinin/>Vadim Sheinin</a>
|
<a href=/people/o/octavian-popescu/>Octavian Popescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--460><div class="card-body p-3 small">Motion recognition is one of the basic cognitive capabilities of many life forms, however, detecting and understanding motion in text is not a trivial task. In addition, identifying motion entities in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is not only challenging but also beneficial for a better <a href=https://en.wikipedia.org/wiki/Natural_language>natural language understanding</a>. In this paper, we present a Motion Entity Tagging (MET) model to identify entities in motion in a text using the Literal-Motion-in-Text (LiMiT) dataset for training and evaluating the model. Then we propose a new method to split clauses and phrases from complex and long motion sentences to improve the performance of our MET model. We also present results showing that motion features, in particular, entity in motion benefits the Named-Entity Recognition (NER) task. Finally, we present an analysis for the special co-occurrence relation between the person category in NER and animate entities in motion, which significantly improves the classification performance for the person category in NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.463/>User Memory Reasoning for Conversational Recommendation</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--463><div class="card-body p-3 small">We study an end-to-end approach for conversational recommendation that dynamically manages and reasons over users&#8217; past (offline) preferences and current (online) requests through a structured and cumulative user memory knowledge graph. This formulation extends existing state tracking beyond the boundary of a single dialog to user state tracking (UST). For this study, we create a new Memory Graph (MG)-Conversational Recommendation parallel corpus called MGConvRex with 7K+ human-to-human role-playing dialogs, grounded on a large-scale user memory bootstrapped from real-world user scenarios. MGConvRex captures human-level reasoning over user memory and has disjoint training / testing sets of users for zero-shot (cold-start) reasoning for recommendation. We propose a simple yet expandable formulation for constructing and updating the MG, and an end-to-end graph-based reasoning model that updates MG from unstructured utterances and predicts optimal dialog policies (eg recommendation) based on updated MG. The prediction of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inherits the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, providing a natural way to explain policies. Experiments are conducted for both <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>offline metrics</a> and <a href=https://en.wikipedia.org/wiki/Online_simulation>online simulation</a>, showing competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.464/>Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs<span class=acl-fixed-case>QA</span> based on <span class=acl-fixed-case>DPP</span>s</a></strong><br><a href=/people/s/shogo-fujita/>Shogo Fujita</a>
|
<a href=/people/t/tomohide-shibata/>Tomohide Shibata</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--464><div class="card-body p-3 small">In community-based question answering (CQA) platforms, it takes time for a user to get useful information from among many answers. Although one solution is an answer ranking method, the user still needs to read through the top-ranked answers carefully. This paper proposes a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of selecting a diverse and non-redundant answer set rather than ranking the answers. Our method is based on determinantal point processes (DPPs), and it calculates the answer importance and similarity between answers by using BERT. We built a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> focusing on a Japanese CQA site, and the experiments on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrated that the proposed method outperformed several baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--465 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.465" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.465/>An empirical analysis of existing <a href=https://en.wikipedia.org/wiki/System>systems</a> and datasets toward general simple question answering</a></strong><br><a href=/people/n/namgi-han/>Namgi Han</a>
|
<a href=/people/g/goran-topic/>Goran Topic</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--465><div class="card-body p-3 small">In this paper, we evaluate the progress of our field toward solving simple factoid questions over a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, a practically important problem in natural language interface to database. As in other natural language understanding tasks, a common practice for this task is to train and evaluate a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a single dataset, and recent studies suggest that SimpleQuestions, the most popular and largest dataset, is nearly solved under this setting. However, this common setting does not evaluate the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the systems outside of the distribution of the used training data. We rigorously evaluate such <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of existing <a href=https://en.wikipedia.org/wiki/System>systems</a> using different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Our analysis, including shifting of training and test datasets and training on a union of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--472 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.472/>Scientific Keyphrase Identification and Classification by Pre-Trained Language Models Intermediate Task Transfer Learning</a></strong><br><a href=/people/s/seoyeon-park/>Seoyeon Park</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--472><div class="card-body p-3 small">Scientific keyphrase identification and classification is the task of detecting and classifying keyphrases from scholarly text with their types from a set of predefined classes. This task has a wide range of benefits, but it is still challenging in performance due to the lack of large amounts of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> required for training <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a>. In order to overcome this challenge, we explore pre-trained language models BERT and SciBERT with intermediate task transfer learning, using 42 data-rich related intermediate-target task combinations. We reveal that intermediate task transfer learning on SciBERT induces a better starting point for target task fine-tuning compared with BERT and achieves competitive performance in scientific keyphrase identification and classification compared to both previous works and strong baselines. Interestingly, we observe that BERT with intermediate task transfer learning fails to improve the performance of scientific keyphrase identification and classification potentially due to significant catastrophic forgetting. This result highlights that <a href=https://en.wikipedia.org/wiki/Scientific_knowledge>scientific knowledge</a> achieved during the pre-training of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> on large scientific collections plays an important role in the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We also observe that sequence tagging related intermediate tasks, especially syntactic structure learning tasks such as POS Tagging, tend to work best for scientific keyphrase identification and classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--473 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.473/>Exploiting Microblog Conversation Structures to Detect Rumors</a></strong><br><a href=/people/j/jiawen-li/>Jiawen Li</a>
|
<a href=/people/y/yudianto-sujana/>Yudianto Sujana</a>
|
<a href=/people/h/hung-yu-kao/>Hung-Yu Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--473><div class="card-body p-3 small">As one of the most popular <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has become a primary source of information for many people. Unfortunately, both valid information and <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> are propagated on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> due to the lack of an automatic information verification system. Twitter users communicate by replying to other users&#8217; messages, forming a conversation structure. Using this <a href=https://en.wikipedia.org/wiki/Structure>structure</a>, users can decide whether the information in the source tweet is a <a href=https://en.wikipedia.org/wiki/Rumor>rumor</a> by reading the tweet&#8217;s replies, which voice other users&#8217; stances on the tweet. The majority of rumor detection researchers process such tweets based on time, ignoring the conversation structure. To reap the benefits of the Twitter conversation structure, we developed a model to detect <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> by modeling conversation structure as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Thus, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s improved representation of the conversation structure enhances its rumor detection accuracy. The experimental results on two rumor datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms several baseline models, including a state-of-the-art model</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.477/>Words are the Window to the Soul : Language-based User Representations for Fake News Detection</a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--477><div class="card-body p-3 small">Cognitive and social traits of individuals are reflected in <a href=https://en.wikipedia.org/wiki/Usage_(language)>language use</a>. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that creates representations of individuals on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> based only on the language they produce, and use them to detect <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. We show that language-based user representations are beneficial for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the <a href=https://en.wikipedia.org/wiki/Social_graph>social graph</a> to assess the presence of the Echo Chamber effect in our data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--481 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.481/>Go Simple and Pre-Train on Domain-Specific Corpora : On the Role of Training Data for Text Classification</a></strong><br><a href=/people/a/aleksandra-edwards/>Aleksandra Edwards</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/h/helene-de-ribaupierre/>Hélène De Ribaupierre</a>
|
<a href=/people/a/alun-preece/>Alun Preece</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--481><div class="card-body p-3 small">Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, i.e., <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> or <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. As for the comparison, BERT outperforms all baselines in standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with large training sets. However, in settings with small training datasets a simple method like <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--483 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.483/>Exploiting Narrative Context and A Priori Knowledge of Categories in Textual Emotion Classification</a></strong><br><a href=/people/h/hikari-tanabe/>Hikari Tanabe</a>
|
<a href=/people/t/tetsuji-ogawa/>Tetsuji Ogawa</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--483><div class="card-body p-3 small">Recognition of the mental state of a human character in text is a major challenge in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this study, we investigate the efficacy of the <a href=https://en.wikipedia.org/wiki/Narrative>narrative context</a> in recognizing the emotional states of human characters in text and discuss an approach to make use of a priori knowledge regarding the employed emotion category system. Specifically, we experimentally show that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a> is substantially increased by encoding the preceding context of the target sentence using a BERT-based text encoder. We also compare ways to incorporate a priori knowledge of emotion categories by altering the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> used in training, in which our proposal of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> that jointly learns to classify positive / negative polarity of emotions is included. The experimental results suggest that, when using Plutchik&#8217;s Wheel of Emotions, it is better to jointly classify the basic emotion categories with positive / negative polarity rather than directly exploiting its characteristic structure in which eight basic categories are arranged in a wheel.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--485 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.485/>Few-Shot Text Classification with Edge-Labeling Graph Neural Network-Based Prototypical Network</a></strong><br><a href=/people/c/chen-lyu/>Chen Lyu</a>
|
<a href=/people/w/weijie-liu/>Weijie Liu</a>
|
<a href=/people/p/ping-wang/>Ping Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--485><div class="card-body p-3 small">In this paper, we propose a new few-shot text classification method. Compared with supervised learning methods which require a large corpus of labeled documents, our method aims to make it possible to classify unlabeled text with few labeled data. To achieve this goal, we take advantage of advanced pre-trained language model to extract the <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> of each document. Furthermore, we utilize an edge-labeling graph neural network to implicitly models the intra-cluster similarity and the inter-cluster dissimilarity of the documents. Finally, we take the results of the graph neural network as the input of a prototypical network to classify the unlabeled texts. We verify the effectiveness of our method on a sentiment analysis dataset and a relation classification dataset and achieve the state-of-the-art performance on both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--488 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.488" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.488/>Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification</a></strong><br><a href=/people/t/timo-schick/>Timo Schick</a>
|
<a href=/people/h/helmut-schmid/>Helmut Schmid</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--488><div class="card-body p-3 small">A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s abilities. To mitigate this issue, we devise an approach that automatically finds such a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> given small amounts of training data. For a number of tasks, the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> found by our approach performs almost as well as hand-crafted label-to-word mappings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--490 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.490" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.490/>IntKB : A Verifiable Interactive Framework for Knowledge Base Completion<span class=acl-fixed-case>I</span>nt<span class=acl-fixed-case>KB</span>: A Verifiable Interactive Framework for Knowledge Base Completion</a></strong><br><a href=/people/b/bernhard-kratzwald/>Bernhard Kratzwald</a>
|
<a href=/people/g/guo-kunpeng/>Guo Kunpeng</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a>
|
<a href=/people/d/dennis-diefenbach/>Dennis Diefenbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--490><div class="card-body p-3 small">Knowledge bases (KBs) are essential for many downstream NLP tasks, yet their prime shortcoming is that they are often incomplete. State-of-the-art <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a> for KB completion often lack sufficient accuracy to work fully automated without <a href=https://en.wikipedia.org/wiki/Supervisor>human supervision</a>. As a remedy, we propose : a novel interactive framework for KB completion from text based on a question answering pipeline. Our framework is tailored to the specific needs of a human-in-the-loop paradigm : (i) We generate facts that are aligned with text snippets and are thus immediately verifiable by humans. (ii) Our system is designed such that it continuously learns during the KB completion task and, therefore, significantly improves its performance upon initial zero- and few-shot relations over time. (iii) We only trigger <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human interactions</a> when there is enough information for a correct prediction. Therefore, we train our <a href=https://en.wikipedia.org/wiki/System>system</a> with negative examples and a fold-option if there is no answer. Our framework yields a favorable performance : <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves a hit@1 ratio of 29.7 % for initially unseen relations, upon which <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> gradually improves to 46.2 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.496/>Multimodal Sentence Summarization via Multimodal Selective Encoding</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--496><div class="card-body p-3 small">This paper studies the problem of generating a summary for a given sentence-image pair. Existing multimodal sequence-to-sequence approaches mainly focus on enhancing the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> by visual signals, while ignoring that the <a href=https://en.wikipedia.org/wiki/Image>image</a> can improve the ability of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to identify highlights of a news event or a document. Thus, we propose a multimodal selective gate network that considers reciprocal relationships between textual and multi-level visual features, including global image descriptor, activation grids, and object proposals, to select highlights of the event when encoding the source sentence. In addition, we introduce a modality regularization to encourage the summary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--499 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.499" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.499/>How Domain Terminology Affects Meeting Summarization Performance</a></strong><br><a href=/people/j/jia-jin-koay/>Jia Jin Koay</a>
|
<a href=/people/a/alexander-roustai/>Alexander Roustai</a>
|
<a href=/people/x/xiaojin-dai/>Xiaojin Dai</a>
|
<a href=/people/d/dillon-burns/>Dillon Burns</a>
|
<a href=/people/a/alec-kerrigan/>Alec Kerrigan</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--499><div class="card-body p-3 small">Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus ; they are known as <a href=https://en.wikipedia.org/wiki/Jargon>jargon terms</a>. We then analyze the performance of a meeting summarization system with and without <a href=https://en.wikipedia.org/wiki/Jargon>jargon terms</a>. Our findings reveal that domain terminology can have a substantial impact on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> performance. We publicly release all domain terminology to advance research in meeting summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--502 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.502/>On the Faithfulness for E-commerce Product Summarization<span class=acl-fixed-case>E</span>-commerce Product Summarization</a></strong><br><a href=/people/p/peng-yuan/>Peng Yuan</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/s/song-xu/>Song Xu</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--502><div class="card-body p-3 small">In this work, we present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate e-commerce product summaries. The consistency between the generated summary and the product attributes is an essential criterion for the ecommerce product summarization task. To enhance the consistency, first, we encode the product attribute table to guide the process of summary generation. Second, we identify the <a href=https://en.wikipedia.org/wiki/Attribute_(grammar)>attribute words</a> from the vocabulary, and we constrain these <a href=https://en.wikipedia.org/wiki/Attribute_(grammar)>attribute words</a> can be presented in the summaries only through copying from the source, i.e., the <a href=https://en.wikipedia.org/wiki/Attribute_(grammar)>attribute words</a> not in the source can not be generated. We construct a Chinese e-commerce product summarization dataset, and the experimental results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrate that our models significantly improve the <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--508 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.508" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.508/>Variation in Coreference Strategies across Genres and <a href=https://en.wikipedia.org/wiki/Mass_media>Production Media</a></a></strong><br><a href=/people/b/berfin-aktas/>Berfin Aktaş</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--508><div class="card-body p-3 small">In response to (i) inconclusive results in the literature as to the properties of coreference chains in written versus spoken language, and (ii) a general lack of work on automatic coreference resolution on both spoken language and social media, we undertake a corpus study involving the various genre sections of Ontonotes, the Switchboard corpus, and a corpus of Twitter conversations. Using a set of <a href=https://en.wikipedia.org/wiki/Measurement>measures</a> that previously have been applied individually to different <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>, we find fairly clear patterns of behavior for the different genres / media. Besides their role for psycholinguistic investigation (why do we employ different coreference strategies when we write or speak) and for the placement of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> in the spokenwritten continuum, we see our results as a contribution to approaching genre-/media-specific coreference resolution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--512 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.512/>Using Eye-tracking Data to Predict the Readability of Brazilian Portuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches</a></strong><br><a href=/people/s/sidney-evaldo-leal/>Sidney Evaldo Leal</a>
|
<a href=/people/j/joao-marcos-munguba-vieira/>João Marcos Munguba Vieira</a>
|
<a href=/people/e/erica-dos-santos-rodrigues/>Erica dos Santos Rodrigues</a>
|
<a href=/people/e/elisangela-nogueira-teixeira/>Elisângela Nogueira Teixeira</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--512><div class="card-body p-3 small">Sentence complexity assessment is a relatively new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. One of its aims is to highlight in a text which sentences are more complex to support the simplification of contents for a target audience (e.g., children, cognitively impaired users, <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native speakers</a> and <a href=https://en.wikipedia.org/wiki/Literacy>low-literacy readers</a> (Scarton and Specia, 2018)). This task is evaluated using <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of pairs of aligned sentences including the complex and simple version of the same sentence. For <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> was addressed by (Leal et al., 2018), who set up the first <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to evaluate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in this <a href=https://en.wikipedia.org/wiki/Language>language</a>, reaching 87.8 % of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. The present work advances these results, using models inspired by (Gonzalez-Garduno and Sgaard, 2018), which hold the state-of-the-art for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, with multi-task learning and eye-tracking measures. First-Pass Duration, Total Regression Duration and Total Fixation Duration were used in two moments ; first to select a subset of linguistic features and then as an auxiliary task in the multi-task and sequential learning models. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> proposed here reaches the new state-of-the-art for <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> with 97.5 % accuracy 1, an increase of almost 10 points compared to the best previous results, in addition to proposing improvements in the public dataset after analysing the errors of our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--513 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.513" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.513/>Retrieving Skills from Job Descriptions : A Language Model Based Extreme Multi-label Classification Framework</a></strong><br><a href=/people/a/akshay-bhola/>Akshay Bhola</a>
|
<a href=/people/k/kishaloy-halder/>Kishaloy Halder</a>
|
<a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--513><div class="card-body p-3 small">We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65 % of <a href=https://en.wikipedia.org/wiki/Job_description>job descriptions</a> miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9 % and 7 % absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.515/>An Analysis of Dataset Overlap on Winograd-Style Tasks<span class=acl-fixed-case>W</span>inograd-Style Tasks</a></strong><br><a href=/people/a/ali-emami/>Ali Emami</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--515><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge (WSC)</a> and variants inspired by <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> have become important benchmarks for common-sense reasoning (CSR). Model performance on the WSC has quickly progressed from chance-level to near-human using neural language models trained on massive corpora. In this paper, we analyze the effects of varying degrees of overlaps that occur between these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> and the test instances in WSC-style tasks. We find that a large number of test instances overlap considerably with the pretraining corpora on which state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained, and that a significant drop in classification accuracy occurs when <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are evaluated on instances with minimal overlap. Based on these results, we provide the WSC-Web dataset, consisting of over 60k pronoun disambiguation problems scraped from web data, being both the largest corpus to date, and having a significantly lower proportion of overlaps with current pretraining corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--518 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.518/>Do n’t Patronize Me ! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities</a></strong><br><a href=/people/c/carla-perez-almendros/>Carla Perez Almendros</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--518><div class="card-body p-3 small">In this paper, we introduce a new annotated dataset which is aimed at supporting the development of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such <a href=https://en.wikipedia.org/wiki/Language>language</a> in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--523 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.523/>WikiUMLS : Aligning UMLS to Wikipedia via Cross-lingual Neural Ranking<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>UMLS</span>: Aligning <span class=acl-fixed-case>UMLS</span> to <span class=acl-fixed-case>W</span>ikipedia via Cross-lingual Neural Ranking</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--523><div class="card-body p-3 small">We present our work on aligning the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System (UMLS)</a> to <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, to facilitate manual alignment of the two resources. We propose a cross-lingual neural reranking model to match a UMLS concept with a Wikipedia page, which achieves a recall@1of 72 %, a substantial improvement of 20 % over word- and char-level BM25, enabling manual alignment with minimal effort. We release our resources, including ranked Wikipedia pages for 700k UMLSconcepts, and WikiUMLS, a dataset for training and evaluation of alignment models between <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> collected from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. This will provide easier access to <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> for <a href=https://en.wikipedia.org/wiki/Health_professional>health professionals</a>, patients, and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>, including in multilingual settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--524 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.524/>The Transference Architecture for Automatic Post-Editing</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--524><div class="card-body p-3 small">In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-encoder based neural APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt information and its integration with pe decisions. In this paper we present an efficient multi-encoder based APE model, called <a href=https://en.wikipedia.org/wiki/Transference>transference</a>. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second encoder combining src mt, and (iii) feeds this representation into a final decoder block generating pe. Our model outperforms the best performing systems by 1 BLEU point on the WMT 2016, 2017, and 2018 EnglishGerman APE shared tasks (PBSMT and NMT). Furthermore, the results of our model on the WMT 2019 APE task using NMT data shows a comparable performance to the state-of-the-art system. The <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a> of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is similar to the vanilla transformer-based NMT system although our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> deals with two separate encoders. We further investigate the importance of our newly introduced second encoder and find that a too small amount of <a href=https://en.wikipedia.org/wiki/Abstraction_layer>layers</a> does hurt the performance, while reducing the number of layers of the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> does not matter much.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.526/>A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction</a></strong><br><a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yingfeng-luo/>Yingfeng Luo</a>
|
<a href=/people/y/ye-lin/>Ye Lin</a>
|
<a href=/people/q/quan-du/>Quan Du</a>
|
<a href=/people/h/huizhen-wang/>Huizhen Wang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--526><div class="card-body p-3 small">Unsupervised Bilingual Dictionary Induction methods based on the initialization and the <a href=https://en.wikipedia.org/wiki/Autodidacticism>self-learning</a> have achieved great success in similar language pairs, e.g., <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-Spanish</a>. But they still fail and have an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0 % in many distant language pairs, e.g., English-Japanese. In this work, we show that this failure results from the gap between the actual initialization performance and the minimum initialization performance for the self-learning to succeed. We propose Iterative Dimension Reduction to bridge this gap. Our experiments show that this simple method does not hamper the performance of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similar language pairs</a> and achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 13.64 55.53 % between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and four distant languages, i.e., <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.527/>Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora</a></strong><br><a href=/people/m/martin-laville/>Martin Laville</a>
|
<a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--527><div class="card-body p-3 small">Narrow specialized comparable corpora are often small in size. This particularity makes it difficult to build efficient <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to acquire translation equivalents, especially for less frequent and rare words. One way to overcome this issue is to enrich the specialized corpora with out-of-domain resources. Although some recent studies have shown improvements using <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, the enrichment method was roughly conducted by adding out-of-domain data with no particular attention given to how to enrich words and how to do it optimally. In this paper, we contrast several data selection techniques to improve bilingual lexicon induction from specialized comparable corpora. We first apply two well-established data selection techniques often used in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> that is : Tf-Idf and <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a>. Then, we propose to exploit BERT for data selection. Overall, all the proposed techniques improve the quality of the extracted bilingual lexicons by a large margin. The best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is the <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a>, obtaining a gain of about 4 points in MAP while decreasing <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> by a factor of 10.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--528 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.528/>A Locally Linear Procedure for Word Translation</a></strong><br><a href=/people/s/soham-dan/>Soham Dan</a>
|
<a href=/people/h/hagai-taitelbaum/>Hagai Taitelbaum</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--528><div class="card-body p-3 small">Learning a mapping between word embeddings of two languages given a dictionary is an important problem with several applications. A common mapping approach is using an <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a>. The Orthogonal Procrustes Analysis (PA) algorithm can be applied to find the optimal <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a>. This <a href=https://en.wikipedia.org/wiki/Solution>solution</a> restricts the expressiveness of the translation model which may result in sub-optimal translations. We propose a natural extension of the PA algorithm that uses multiple orthogonal translation matrices to model the mapping and derive an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to learn these multiple <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrices</a>. We achieve better performance in a bilingual word translation task and a cross-lingual word similarity task compared to the single matrix baseline. We also show how multiple matrices can model multiple senses of a word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--530 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.530/>The SADID Evaluation Datasets for Low-Resource Spoken Language Machine Translation of Arabic Dialects<span class=acl-fixed-case>SADID</span> Evaluation Datasets for Low-Resource Spoken Language Machine Translation of <span class=acl-fixed-case>A</span>rabic Dialects</a></strong><br><a href=/people/w/wael-abid/>Wael Abid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--530><div class="card-body p-3 small">Low-resource Machine Translation recently gained a lot of popularity, and for certain languages, it has made great strides. However, it is still difficult to track progress in other languages for which there is no publicly available evaluation data. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and its dialects. We describe our design process and motivations and analyze the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to understand their resulting properties. Numerous successful attempts use large monolingual corpora to augment low-resource pairs. We try to approach augmentation differently and investigate whether it is possible to improve MT models without any external sources of data. We accomplish this by bootstrapping existing <a href=https://en.wikipedia.org/wiki/Parallelism_(grammar)>parallel sentences</a> and complement this with multilingual training to achieve strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--532 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.532/>Understanding Translationese in Multi-view Embedding Spaces</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--532><div class="card-body p-3 small">Recent studies use a combination of lexical and syntactic features to show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In this paper, we focus on embedding-based semantic spaces, exploiting departures from isomorphism between spaces built from original target language and translations into this target language to predict relations between languages in an unsupervised way. We use different views of the data words, <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts of speech</a>, semantic tags and synsets to track translationese. Our analysis shows that (i) semantic distances between original target language and translations into this target language can be detected using the notion of <a href=https://en.wikipedia.org/wiki/Isomorphism>isomorphism</a>, (ii) language family ties with characteristics similar to linguistically motivated phylogenetic trees can be inferred from the distances and (iii) with delexicalised embeddings exhibiting source-language interference most significantly, other levels of abstraction display the same tendency, indicating the lexicalised results to be not just due to possible topic differences between original and translated texts. To the best of our knowledge, this is the first time departures from isomorphism between embedding spaces are used to track <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translationese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--533 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.533/>Building The First English-Brazilian Portuguese Corpus for Automatic Post-Editing<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Corpus for Automatic Post-Editing</a></strong><br><a href=/people/f/felipe-almeida-costa/>Felipe Almeida Costa</a>
|
<a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/a/adriana-pagano/>Adriana Pagano</a>
|
<a href=/people/w/wagner-meira/>Wagner Meira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--533><div class="card-body p-3 small">This paper introduces the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for Automatic Post-Editing of English and a low-resource language, Brazilian Portuguese. The source English texts were extracted from the WebNLG corpus and automatically translated into <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> using a state-of-the-art industrial neural machine translator. Post-edits were then obtained in an experiment with native speakers of <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>. To assess the quality of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we performed error analysis and computed complexity indicators measuring how difficult the APE task would be. We report preliminary results of Phrase-Based and Neural Machine Translation Models on this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Data and code publicly available in our repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--534 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.534/>Analysing cross-lingual transfer in <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a><span class=acl-fixed-case>I</span>ndian languages</a></strong><br><a href=/people/k/kumar-saurav/>Kumar Saurav</a>
|
<a href=/people/k/kumar-saunack/>Kumar Saunack</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--534><div class="card-body p-3 small">Lemmatization aims to reduce the sparse data problem by relating the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> of a word to its <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary form</a>. However, most of the prior work on this topic has focused on high resource languages. In this paper, we evaluate cross-lingual approaches for low resource languages, especially in the context of morphologically rich Indian languages. We test our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on six languages from two different families and develop linguistic insights into each <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--535 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.535/>Neural Automated Essay Scoring Incorporating Handcrafted Features</a></strong><br><a href=/people/m/masaki-uto/>Masaki Uto</a>
|
<a href=/people/y/yikuan-xie/>Yikuan Xie</a>
|
<a href=/people/m/maomi-ueno/>Maomi Ueno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--535><div class="card-body p-3 small">Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to <a href=https://en.wikipedia.org/wiki/Grading_in_education>grading</a> by human raters. Conventional <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES</a> typically relies on handcrafted features, whereas recent studies have proposed <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES models</a> based on deep neural networks (DNNs) to obviate the need for <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Furthermore, hybrid methods that integrate handcrafted features in a DNN-AES model have been recently developed and have achieved state-of-the-art accuracy. One of the most popular hybrid methods is formulated as a DNN-AES model with an additional <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> that processes a sequence of handcrafted sentence-level features. However, this method has the following problems : 1) It can not incorporate effective essay-level features developed in previous <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES</a> research. 2) It greatly increases the numbers of <a href=https://en.wikipedia.org/wiki/Statistical_model>model parameters</a> and tuning parameters, increasing the difficulty of model training. 3) It has an additional <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNN</a> to process sentence-level features, enabling extension to various DNN-AES models complex. To resolve these problems, we propose a new hybrid method that integrates handcrafted essay-level features into a DNN-AES model. Specifically, our method concatenates handcrafted essay-level features to a distributed essay representation vector, which is obtained from an intermediate layer of a DNN-AES model. Our method is a simple DNN-AES extension, but significantly improves scoring accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--536 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.536/>A Straightforward Approach to Narratologically Grounded Character Identification</a></strong><br><a href=/people/l/labiba-jahan/>Labiba Jahan</a>
|
<a href=/people/r/rahul-mittal/>Rahul Mittal</a>
|
<a href=/people/w/w-victor-yarlott/>W. Victor Yarlott</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--536><div class="card-body p-3 small">One of the most fundamental elements of <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a> is <a href=https://en.wikipedia.org/wiki/Character_(arts)>character</a> : if we are to understand a narrative, we must be able to identify the characters of that narrative. Therefore, character identification is a critical task in narrative natural language understanding. Most prior work has lacked a narratologically grounded definition of character, instead relying on simplified or implicit definitions that do not capture essential distinctions between <a href=https://en.wikipedia.org/wiki/Character_(arts)>characters</a> and other referents in narratives. In prior work we proposed a preliminary definition of <a href=https://en.wikipedia.org/wiki/Character_(arts)>character</a> that was based in clear narratological principles : a <a href=https://en.wikipedia.org/wiki/Character_(arts)>character</a> is an animate entity that is important to the plot. Here we flesh out this concept, demonstrate that it can be reliably annotated (0.78 Cohen&#8217;s), and provide annotations of 170 narrative texts, drawn from 3 different corpora, containing 1,347 <a href=https://en.wikipedia.org/wiki/Co-reference>character co-reference chains</a> and 21,999 <a href=https://en.wikipedia.org/wiki/Co-reference>non-character chains</a> that include 3,937 <a href=https://en.wikipedia.org/wiki/Co-reference>animate chains</a>. Furthermore, we have shown that a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> using a simple set of easily computable <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> can effectively identify these <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> (overall <a href=https://en.wikipedia.org/wiki/F-number>F1</a> of 0.90). A detailed error analysis shows that character identification is first and foremost affected by co-reference quality, and further, that the shorter a chain is the harder it is to effectively identify as a character. We release our code and data for the benefit of other researchers</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--537 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.537" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.537/>Fine-grained Information Status Classification Using Discourse Context-Aware BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--537><div class="card-body p-3 small">Previous work on bridging anaphora recognition (Hou et al., 2013) casts the problem as a subtask of learning fine-grained information status (IS). However, these <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> heavily depend on many <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>hand-crafted linguistic features</a>. In this paper, we propose a simple discourse context-aware BERT model for fine-grained IS classification. On the ISNotes corpus (Markert et al., 2012), our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.8 absolute overall accuracy improvement compared to Hou et al. More importantly, we also show an improvement of 10.5 <a href=https://en.wikipedia.org/wiki/F-number>F1 points</a> for bridging anaphora recognition without using any complex hand-crafted semantic features designed for capturing the bridging phenomenon. We further analyze the trained model and find that the most attended signals for each IS category correspond well to linguistic notions of information status.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--542 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.542/>Text Classification by Contrastive Learning and Cross-lingual Data Augmentation for Alzheimer’s Disease Detection<span class=acl-fixed-case>A</span>lzheimer’s Disease Detection</a></strong><br><a href=/people/z/zhiqiang-guo/>Zhiqiang Guo</a>
|
<a href=/people/z/zhaoci-liu/>Zhaoci Liu</a>
|
<a href=/people/z/zhenhua-ling/>Zhenhua Ling</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/l/lingjing-jin/>Lingjing Jin</a>
|
<a href=/people/y/yunxia-li/>Yunxia Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--542><div class="card-body p-3 small">Data scarcity is always a constraint on analyzing speech transcriptions for automatic Alzheimer&#8217;s disease (AD) detection, especially when the subjects are non-English speakers. To deal with this issue, this paper first proposes a contrastive learning method to obtain effective representations for text classification based on monolingual embeddings of BERT. Furthermore, a cross-lingual data augmentation method is designed by building autoencoders to learn the text representations shared by both languages. Experiments on a Mandarin AD corpus show that the contrastive learning method can achieve better detection accuracy than conventional CNN-based and BERTbased methods. Our cross-lingual data augmentation method also outperforms other compared methods when using another English AD corpus for augmentation. Finally, a best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>detection accuracy</a> of 81.6 % is obtained by our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on the Mandarin AD corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.549" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.549/>Hierarchical Text Segmentation for Medieval Manuscripts</a></strong><br><a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/b/beatrice-daille/>Beatrice Daille</a>
|
<a href=/people/d/dominique-stutzmann/>Dominique Stutzmann</a>
|
<a href=/people/c/christopher-kermorvant/>Christopher Kermorvant</a>
|
<a href=/people/l/louis-chevalier/>Louis Chevalier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--549><div class="card-body p-3 small">In this paper, we address the segmentation of books of hours, Latin devotional manuscripts of the late Middle Ages, that exhibit challenging issues : a complex hierarchical entangled structure, variable content, noisy transcriptions with no sentence markers, and strong correlations between sections for which topical information is no longer sufficient to draw segmentation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for <a href=https://en.wikipedia.org/wiki/Books_of_hours>books of hours</a> and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--550 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.550/>Are We Ready for this Disaster? Towards Location Mention Recognition from Crisis Tweets</a></strong><br><a href=/people/r/reem-suwaileh/>Reem Suwaileh</a>
|
<a href=/people/m/muhammad-imran/>Muhammad Imran</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--550><div class="card-body p-3 small">The widespread usage of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> during emergencies has provided a new opportunity and timely resource to crisis responders for various disaster management tasks. Geolocation information of pertinent tweets is crucial for gaining <a href=https://en.wikipedia.org/wiki/Situation_awareness>situational awareness</a> and delivering aid. However, the majority of tweets do not come with <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geoinformation</a>. In this work, we focus on the task of location mention recognition from crisis-related tweets. Specifically, we investigate the influence of different types of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>labeled training data</a> on the performance of a BERT-based classification model. We explore several <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training settings</a> such as combing in- and out-domain data from <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> and <a href=https://en.wikipedia.org/wiki/Twitter>general-purpose and crisis-related tweets</a>. Furthermore, we investigate the effect of geospatial proximity while training on near or far-away events from the target event. Using five different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our extensive experiments provide answers to several critical research questions that are useful for the research community to foster research in this important direction. For example, results show that, for training a location mention recognition model, Twitter-based data is preferred over general-purpose data ; and crisis-related data is preferred over general-purpose Twitter data. Furthermore, training on <a href=https://en.wikipedia.org/wiki/Data>data</a> from geographically-nearby disaster events to the target event boosts the performance compared to training on distant events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.562/>Regularized Attentive Capsule Network for Overlapped Relation Extraction</a></strong><br><a href=/people/t/tianyi-liu/>Tianyi Liu</a>
|
<a href=/people/x/xiangyu-lin/>Xiangyu Lin</a>
|
<a href=/people/w/weijia-jia/>Weijia Jia</a>
|
<a href=/people/m/mingliang-zhou/>Mingliang Zhou</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--562><div class="card-body p-3 small">Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.565.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--565 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.565 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.565/>Graph Convolution over Multiple Dependency Sub-graphs for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/a/angrosh-mandya/>Angrosh Mandya</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/f/frans-coenen/>Frans Coenen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--565><div class="card-body p-3 small">We propose a contextualised graph convolution network over multiple dependency-based sub-graphs for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. A novel method to construct multiple sub-graphs using words in shortest dependency path and words linked to entities in the dependency parse is proposed. Graph convolution operation is performed over the resulting multiple sub-graphs to obtain more informative features useful for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our experimental results show that the proposed method achieves superior performance over the existing GCN-based models achieving state-of-the-art performance on cross-sentence n-ary relation extraction dataset and SemEval 2010 Task 8 sentence-level relation extraction dataset. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also achieves a comparable performance to the SoTA on the TACRED dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.572.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--572 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.572 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.572" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.572/>NYTWIT : A Dataset of Novel Words in the New York Times<span class=acl-fixed-case>NYTWIT</span>: A Dataset of Novel Words in the <span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>Y</span>ork <span class=acl-fixed-case>T</span>imes</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra L. Jacobs</a>
|
<a href=/people/m/max-bittker/>Max Bittker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--572><div class="card-body p-3 small">We present the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times Word Innovation Types dataset</a>, or NYTWIT, a collection of over 2,500 novel English words published in the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times</a> between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). We present baseline results for both uncontextual and contextual prediction of novelty class, showing that there is room for improvement even for state-of-the-art NLP systems. We hope this resource will prove useful for linguists and NLP practitioners by providing a real-world environment of novel word appearance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.575.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--575 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.575 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.575" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.575/>XED : A Multilingual Dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and Emotion Detection<span class=acl-fixed-case>XED</span>: A Multilingual Dataset for Sentiment Analysis and Emotion Detection</a></strong><br><a href=/people/e/emily-ohman/>Emily Öhman</a>
|
<a href=/people/m/marc-pamies/>Marc Pàmies</a>
|
<a href=/people/k/kaisla-kajava/>Kaisla Kajava</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--575><div class="card-body p-3 small">We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik&#8217;s core emotions to annotate the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.576/>Human or Neural Translation?</a></strong><br><a href=/people/s/shivendra-bhardwaj/>Shivendra Bhardwaj</a>
|
<a href=/people/d/david-alfonso-hermelo/>David Alfonso Hermelo</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a>
|
<a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--576><div class="card-body p-3 small">Deep neural models tremendously improved <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In this context, we investigate whether distinguishing machine from human translations is still feasible. We trained and applied 18 classifiers under two settings : a monolingual task, in which the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> only looks at the translation ; and a bilingual task, in which the source text is also taken into consideration. We report on extensive experiments involving 4 neural MT systems (Google Translate, DeepL, as well as two systems we trained) and varying the domain of texts. We show that the bilingual task is the easiest one and that transfer-based deep-learning classifiers perform best, with mean accuracies around 85 % in-domain and 75 % out-of-domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.580" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.580/>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps<span class=acl-fixed-case>QA</span> Dataset for Comprehensive Evaluation of Reasoning Steps</a></strong><br><a href=/people/x/xanh-ho/>Xanh Ho</a>
|
<a href=/people/a/anh-khoa-duong-nguyen/>Anh-Khoa Duong Nguyen</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--580><div class="card-body p-3 small">A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to read multiple paragraphs to answer a given question. However, current <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> do not provide a complete explanation for the <a href=https://en.wikipedia.org/wiki/Reason>reasoning process</a> from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we introduce the <a href=https://en.wikipedia.org/wiki/Evidence-based_practice>evidence information</a> containing a reasoning path for multi-hop questions. The evidence information has two benefits : (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We carefully design a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> and a set of <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is challenging for multi-hop models and it ensures that multi-hop reasoning is required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.582.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--582 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.582 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.582/>Exploring the Language of Data</a></strong><br><a href=/people/g/gabor-bella/>Gábor Bella</a>
|
<a href=/people/l/linda-gremes/>Linda Gremes</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--582><div class="card-body p-3 small">We set out to uncover the unique grammatical properties of an important yet so far under-researched type of natural language text : that of short labels typically found within structured datasets. We show that such labels obey a specific type of abbreviated grammar that we call the Language of Data, with properties significantly different from the kinds of text typically addressed in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, such as &#8216;standard&#8217; written language or social media messages. We analyse <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>, <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts of speech</a>, and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> over a large, bilingual, hand-annotated corpus of data labels collected from a variety of domains. We perform experiments on <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenisation</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> over real-world structured data, demonstrating that models adapted to the Language of Data outperform those trained on standard text. These observations point in a new direction to be explored as future research, in order to develop new NLP tools and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> dedicated to the Language of Data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--587 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.587/>Creation of Corpus and analysis in Code-Mixed Kannada-English Twitter data for Emotion Prediction<span class=acl-fixed-case>K</span>annada-<span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>T</span>witter data for Emotion Prediction</a></strong><br><a href=/people/a/abhinav-reddy-appidi/>Abhinav Reddy Appidi</a>
|
<a href=/people/v/vamshi-krishna-srirangam/>Vamshi Krishna Srirangam</a>
|
<a href=/people/d/darsi-suhas/>Darsi Suhas</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--587><div class="card-body p-3 small">Emotion prediction is a critical task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. There has been a significant amount of work done in emotion prediction for resource-rich languages. There has been work done on code-mixed social media corpus but not on emotion prediction of Kannada-English code-mixed Twitter data. In this paper, we analyze the problem of emotion prediction on <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> obtained from code-mixed Kannada-English extracted from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> annotated with their respective &#8216;Emotion&#8217; for each tweet. We experimented with machine learning prediction models using features like Character N-Grams, Word N-Grams, Repetitive characters, and others on SVM and LSTM on our corpus, which resulted in an accuracy of 30 % and 32 % respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.588.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--588 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.588 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.588" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.588/>Fair Evaluation in Concept Normalization : a Large-scale Comparative Analysis for BERT-based Models<span class=acl-fixed-case>BERT</span>-based Models</a></strong><br><a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/a/artur-kadurin/>Artur Kadurin</a>
|
<a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--588><div class="card-body p-3 small">Linking of biomedical entity mentions to various terminologies of chemicals, <a href=https://en.wikipedia.org/wiki/Disease>diseases</a>, <a href=https://en.wikipedia.org/wiki/Gene>genes</a>, adverse drug reactions is a challenging task, often requiring non-syntactic interpretation. A large number of biomedical corpora and state-of-the-art models have been introduced in the past five years. However, there are no general guidelines regarding the evaluation of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> in single- and cross-terminology settings. In this work, we perform a comparative evaluation of various benchmarks and study the efficiency of state-of-the-art neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) for linking of three entity types across three domains : research abstracts, drug labels, and user-generated texts on drug therapy in English. We have made the source code and results available at https://github.com/insilicomedicine/Fair-Evaluation-BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.591.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--591 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.591 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.591" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.591/>Multilingual Neural RST Discourse Parsing<span class=acl-fixed-case>RST</span> Discourse Parsing</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/k/ke-shi/>Ke Shi</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--591><div class="card-body p-3 small">Text discourse parsing plays an important role in understanding <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> and argumentative structure in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Previous research under the Rhetorical Structure Theory (RST) has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via : (1) utilizing multilingual vector representations ; and (2) adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.593.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--593 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.593 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.593/>Tree Representations in Transition System for RST Parsing<span class=acl-fixed-case>RST</span> Parsing</a></strong><br><a href=/people/j/jinfen-li/>Jinfen Li</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--593><div class="card-body p-3 small">The transition-based systems in the past studies propose a series of actions, to build a right-heavy binarized tree for the RST parsing. However, the nodes of the binary-nuclear relations (e.g., Contrast) have the same nuclear type with those of the multi-nuclear relations (e.g., Joint) in the <a href=https://en.wikipedia.org/wiki/Binary_tree>binary tree structure</a>. In addition, the reduce action only construct <a href=https://en.wikipedia.org/wiki/Binary_tree>binary trees</a> instead of multi-branch trees, which is the original RST tree structure. In our paper, we design a new nuclear type for the multi-nuclear relations, and a new action to construct a multi-branch tree. We enrich the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature set</a> by extracting additional refined dependency feature of texts from the Bi-Affine model. We also compare the performance of two approaches for RST parsing in the transition-based system : a joint action of reduce-shift and nuclear type (i.e., Reduce-SN) vs a separate one that applies Reduce action first and then assigns nuclear type. We find that the new devised nuclear type and <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>action</a> are more capable of capturing the multi-nuclear relation and the joint action is more suitable than the separate one. Our multi-branch tree structure obtains the state-of-the-art performance for all the 18 coarse relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.597.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--597 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.597 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.597/>Resource Constrained Dialog Policy Learning Via Differentiable Inductive Logic Programming</a></strong><br><a href=/people/z/zhenpeng-zhou/>Zhenpeng Zhou</a>
|
<a href=/people/a/ahmad-beirami/>Ahmad Beirami</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/a/alborz-geramifard/>Alborz Geramifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--597><div class="card-body p-3 small">Motivated by the needs of resource constrained dialog policy learning, we introduce dialog policy via differentiable inductive logic (DILOG). We explore the tasks of one-shot learning and zero-shot domain transfer with DILOG on SimDial and MultiWoZ. Using a single representative dialog from the restaurant domain, we train DILOG on the SimDial dataset and obtain 99+% in-domain test accuracy. We also show that the trained DILOG zero-shot transfers to all other domains with 99+% accuracy, proving the suitability of DILOG to slot-filling dialogs. We further extend our study to the MultiWoZ dataset achieving 90+% inform and success metrics. We also observe that these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are not capturing some of the shortcomings of DILOG in terms of <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a>, prompting us to measure an auxiliary Action F1 score. We show that DILOG is 100x more data efficient than state-of-the-art neural approaches on MultiWoZ while achieving similar performance metrics. We conclude with a discussion on the strengths and weaknesses of DILOG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.598" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.598/>German’s Next Language Model<span class=acl-fixed-case>G</span>erman’s Next Language Model</a></strong><br><a href=/people/b/branden-chan/>Branden Chan</a>
|
<a href=/people/s/stefan-schweter/>Stefan Schweter</a>
|
<a href=/people/t/timo-moller/>Timo Möller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--598><div class="card-body p-3 small">In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and our results indicate that both adding more data and utilizing WWM improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. By benchmarking against existing German models, we show that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are the best German models to date. All trained <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> will be made publicly available to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.602/>Do n’t Invite BERT to Drink a Bottle : Modeling the Interpretation of Metonymies Using BERT and Distributional Representations<span class=acl-fixed-case>BERT</span> to Drink a Bottle: Modeling the Interpretation of Metonymies Using <span class=acl-fixed-case>BERT</span> and Distributional Representations</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--602><div class="card-body p-3 small">In this work, we carry out two experiments in order to assess the ability of BERT to capture the meaning shift associated with <a href=https://en.wikipedia.org/wiki/Metonymy>metonymic expressions</a>. We test the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that is representative of the most common types of <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a>. We compare BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge. The results reveal that, while BERT ability to deal with <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a> is quite limited, SDM is good at predicting the meaning of metonymic expressions, providing support for an account of <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a> based on event knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--606 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.606/>Interpretable Multi-headed Attention for Abstractive Summarization at Controllable Lengths</a></strong><br><a href=/people/r/ritesh-sarkhel/>Ritesh Sarkhel</a>
|
<a href=/people/m/moniba-keymanesh/>Moniba Keymanesh</a>
|
<a href=/people/a/arnab-nandi/>Arnab Nandi</a>
|
<a href=/people/s/srinivasan-parthasarathy/>Srinivasan Parthasarathy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--606><div class="card-body p-3 small">Abstractive summarization at controllable lengths is a challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer (MLS), a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in English show that MLS outperforms strong baselines by up to 14.70 % in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--609 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.609" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.609/>CharacterBERT : Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters<span class=acl-fixed-case>C</span>haracter<span class=acl-fixed-case>BERT</span>: Reconciling <span class=acl-fixed-case>ELM</span>o and <span class=acl-fixed-case>BERT</span> for Word-Level Open-Vocabulary Representations From Characters</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--609><div class="card-body p-3 small">Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a>. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>general domain</a> is not always suitable, especially when building <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> for <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>specialized domains</a> (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--610 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.610" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.610/>Autoregressive Reasoning over Chains of Facts with Transformers</a></strong><br><a href=/people/r/ruben-cartuyvels/>Ruben Cartuyvels</a>
|
<a href=/people/g/graham-spinks/>Graham Spinks</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--610><div class="card-body p-3 small">This paper proposes an iterative inference algorithm for multi-hop explanation regeneration, that retrieves relevant factual evidence in the form of text snippets, given a natural language question and its answer. Combining multiple sources of evidence or facts for multi-hop reasoning becomes increasingly hard when the number of sources needed to make an <a href=https://en.wikipedia.org/wiki/Inference>inference</a> grows. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> copes with this by decomposing the selection of facts from a corpus autoregressively, conditioning the next iteration on previously selected facts. This allows us to use a pairwise learning-to-rank loss. We validate our method on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of the TextGraphs 2019 and 2020 Shared Tasks for explanation regeneration. Existing work on this task either evaluates facts in isolation or artificially limits the possible chains of facts, thus limiting multi-hop inference. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, when used with a pre-trained transformer model, outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in terms of <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a>, <a href=https://en.wikipedia.org/wiki/Time_complexity>training time</a> and inference efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.611/>Augmenting NLP models using Latent Feature Interpolations<span class=acl-fixed-case>NLP</span> models using Latent Feature Interpolations</a></strong><br><a href=/people/a/amit-jindal/>Amit Jindal</a>
|
<a href=/people/a/arijit-ghosh-chowdhury/>Arijit Ghosh Chowdhury</a>
|
<a href=/people/a/aniket-didolkar/>Aniket Didolkar</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--611><div class="card-body p-3 small">Models with a large number of parameters are prone to <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a> and often fail to capture the underlying input distribution. We introduce Emix, a data augmentation method that uses interpolations of word embeddings and hidden layer representations to construct virtual examples. We show that Emix shows significant improvements over previously used <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>interpolation based regularizers</a> and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation techniques</a>. We also demonstrate how our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is more robust to sparsification. We highlight the merits of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> by performing thorough quantitative and qualitative assessments.</div></div></div><hr><div id=2020coling-demos><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.coling-demos/>Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.0/>Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/m/michal-ptaszynski/>Michal Ptaszynski</a>
|
<a href=/people/b/bartosz-ziolko/>Bartosz Ziolko</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-demos--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-demos.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.5/>Fast Word Predictor for On-Device Application</a></strong><br><a href=/people/h/huy-tien-nguyen/>Huy Tien Nguyen</a>
|
<a href=/people/k/khoi-tuan-nguyen/>Khoi Tuan Nguyen</a>
|
<a href=/people/a/anh-tuan-nguyen/>Anh Tuan Nguyen</a>
|
<a href=/people/t/thanh-lac-thi-tran/>Thanh Lac Thi Tran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-demos--5><div class="card-body p-3 small">Learning on large text corpora, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> achieve promising results in the next word prediction task. However, deploying these huge <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on devices has to deal with constraints of <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>low latency</a> and a small binary size. To address these challenges, we propose a fast word predictor performing efficiently on <a href=https://en.wikipedia.org/wiki/Mobile_device>mobile devices</a>. Compared with a standard <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> which has a similar word prediction rate, the proposed model obtains 60 % reduction in memory size and 100X faster inference time on a middle-end mobile device. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is developed as a <a href=https://en.wikipedia.org/wiki/Software_feature>feature</a> for a <a href=https://en.wikipedia.org/wiki/Online_chat>chat application</a> which serves more than 100 million users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-demos--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-demos.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.10/>Discussion Tracker : Supporting Teacher Learning about Students’ Collaborative Argumentation in High School Classrooms</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/c/christopher-olshefski/>Christopher Olshefski</a>
|
<a href=/people/r/ravneet-singh/>Ravneet Singh</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/a/amanda-godley/>Amanda Godley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-demos--10><div class="card-body p-3 small">Teaching collaborative argumentation is an advanced skill that many K-12 teachers struggle to develop. To address this, we have developed Discussion Tracker, a classroom discussion analytics system based on novel algorithms for classifying argument moves, specificity, and collaboration. Results from a classroom deployment indicate that teachers found the <a href=https://en.wikipedia.org/wiki/Analytics>analytics</a> useful, and that the underlying <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> perform with moderate to substantial agreement with humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-demos--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-demos.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.11/>An Online Readability Leveled Arabic Thesaurus<span class=acl-fixed-case>A</span>rabic Thesaurus</a></strong><br><a href=/people/z/zhengyang-jiang/>Zhengyang Jiang</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/m/muhamed-al-khalil/>Muhamed Al Khalil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-demos--11><div class="card-body p-3 small">This demo paper introduces the online Readability Leveled Arabic Thesaurus interface. For a given user input word, this interface provides the word&#8217;s possible lemmas, <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>roots</a>, English glosses, related Arabic words and phrases, and <a href=https://en.wikipedia.org/wiki/Readability>readability</a> on a five-level readability scale. This <a href=https://en.wikipedia.org/wiki/Interface_(computing)>interface</a> builds on and connects multiple existing Arabic resources and processing tools. This one-of-a-kind system enables <a href=https://en.wikipedia.org/wiki/Arabic>Arabic speakers</a> and learners to benefit from advances in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Arabic computational linguistics technologies</a>. Feedback from users of the <a href=https://en.wikipedia.org/wiki/System>system</a> will help the developers to identify lexical coverage gaps and errors. A live link to the demo is available at : http://samer.camel-lab.com/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-demos--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-demos.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.12/>TrainX Named Entity Linking with Active Sampling and Bi-Encoders<span class=acl-fixed-case>T</span>rain<span class=acl-fixed-case>X</span> – Named Entity Linking with Active Sampling and Bi-Encoders</a></strong><br><a href=/people/t/tom-oberhauser/>Tom Oberhauser</a>
|
<a href=/people/t/tim-bischoff/>Tim Bischoff</a>
|
<a href=/people/k/karl-brendel/>Karl Brendel</a>
|
<a href=/people/m/maluna-menke/>Maluna Menke</a>
|
<a href=/people/t/tobias-klatt/>Tobias Klatt</a>
|
<a href=/people/a/amy-siu/>Amy Siu</a>
|
<a href=/people/f/felix-alexander-gers/>Felix Alexander Gers</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-demos--12><div class="card-body p-3 small">We demonstrate TrainX, a <a href=https://en.wikipedia.org/wiki/System>system</a> for Named Entity Linking for medical experts. It combines state-of-the-art entity recognition and linking architectures, such as Flair and fine-tuned Bi-Encoders based on BERT, with an easy-to-use interface for healthcare professionals. We support medical experts in annotating training data by using active sampling strategies to forward informative samples to the annotator. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is capable of linking against large knowledge bases, such as <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a> (3.6 million entities), and supporting zero-shot cases, where the linker has never seen the entity before. Those zero-shot capabilities help to mitigate the problem of rare and expensive training data that is a common issue in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-demos.16/>Epistolary Education in 21st Century : A System to Support Composition of E-mails by Students to Superiors in Japanese<span class=acl-fixed-case>E</span>-mails by Students to Superiors in <span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/k/kenji-ryu/>Kenji Ryu</a>
|
<a href=/people/m/michal-ptaszynski/>Michal Ptaszynski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-demos--16><div class="card-body p-3 small">E-mail is a <a href=https://en.wikipedia.org/wiki/Communication>communication tool</a> widely used by people of all ages on the Internet today, often in business and formal situations, especially in <a href=https://en.wikipedia.org/wiki/Japan>Japan</a>. Moreover, Japanese E-mail communication has a set of specific rules taught using specialized guidebooks. E-mail literacy education for many Japanese students is typically provided in a traditional, yet inefficient lecture-based way. We propose a system to support Japanese students in writing E-mails to superiors (teachers, job hunting representatives, etc.). We firstly make an investigation into the importance of formal E-mails in <a href=https://en.wikipedia.org/wiki/Japan>Japan</a>, and what is needed to successfully write a formal E-mail. Next, we develop the <a href=https://en.wikipedia.org/wiki/System>system</a> with accordance to those rules. Finally, we evaluated the <a href=https://en.wikipedia.org/wiki/System>system</a> twofold. The results, although performed on a small number of samples, were generally positive, and clearly indicated additional ways to improve the <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div></div><hr><div id=2020coling-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.coling-tutorials/>Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-tutorials.0/>Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/d/daniel-beck/>Daniel Beck</a></span></p></div><hr><div id=2020coling-industry><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.coling-industry/>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.0/>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a></strong><br><a href=/people/a/ann-clifton/>Ann Clifton</a>
|
<a href=/people/c/courtney-napoles/>Courtney Napoles</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.4/>Query Distillation : BERT-based Distillation for Ensemble Ranking<span class=acl-fixed-case>BERT</span>-based Distillation for Ensemble Ranking</a></strong><br><a href=/people/w/wangshu-zhang/>Wangshu Zhang</a>
|
<a href=/people/j/junhong-liu/>Junhong Liu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--4><div class="card-body p-3 small">Recent years have witnessed substantial progress in the development of neural ranking networks, but also an increasingly heavy computational burden due to growing numbers of parameters and the adoption of model ensembles. Knowledge Distillation (KD) is a common solution to balance the effectiveness and efficiency. However, it is not straightforward to apply KD to ranking problems. Ranking Distillation (RD) has been proposed to address this issue, but only shows effectiveness on recommendation tasks. We present a novel two-stage distillation method for ranking problems that allows a smaller student model to be trained while benefitting from the better performance of the teacher model, providing better control of the inference latency and computational burden. We design a novel BERT-based ranking model structure for list-wise ranking to serve as our student model. All ranking candidates are fed to the BERT model simultaneously, such that the self-attention mechanism can enable joint inference to rank the document list. Our experiments confirm the advantages of our method, not just with regard to the inference latency but also in terms of higher-quality rankings compared to the original teacher model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.8/>Interactive Question Clarification in Dialogue via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiang-hu/>Xiang Hu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--8><div class="card-body p-3 small">Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human interaction</a>, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement model</a> to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep policy network</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on real-world user clicks and demonstrate significant improvements across several different experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.9/>Towards building a Robust Industry-scale Question Answering System</a></strong><br><a href=/people/r/rishav-chakravarti/>Rishav Chakravarti</a>
|
<a href=/people/a/anthony-ferritto/>Anthony Ferritto</a>
|
<a href=/people/b/bhavani-iyer/>Bhavani Iyer</a>
|
<a href=/people/l/lin-pan/>Lin Pan</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--9><div class="card-body p-3 small">Industry-scale NLP systems necessitate two <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Robustness : zero-shot transfer learning (ZSTL) performance has to be commendable and 2. Efficiency : systems have to train efficiently and respond instantaneously. In this paper, we introduce the development of a production model called GAAMA (Go Ahead Ask Me Anything) which possess the above two characteristics. For <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> trains on the recently introduced Natural Questions (NQ) dataset. NQ poses additional challenges over older datasets like SQuAD : (a) QA systems need to read and comprehend an entire Wikipedia article rather than a small passage, and (b) NQ does not suffer from <a href=https://en.wikipedia.org/wiki/Observation_bias>observation bias</a> during construction, resulting in less lexical overlap between the question and the article. GAAMA consists of Attention-over-Attention, diversity among attention heads, hierarchical transfer learning, and synthetic data augmentation while being computationally inexpensive. Building on top of the powerful BERTQA model, GAAMA provides a 2.0 % absolute boost in F1 over the industry-scale state-of-the-art (SOTA) system on NQ. Further, we show that GAAMA transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks : the BioASQ and the newly introduced CovidQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.18/>Learning Domain Terms-Empirical Methods to Enhance Enterprise Text Analytics Performance</a></strong><br><a href=/people/g/gargi-roy/>Gargi Roy</a>
|
<a href=/people/l/lipika-dey/>Lipika Dey</a>
|
<a href=/people/m/mohammad-shakir/>Mohammad Shakir</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--18><div class="card-body p-3 small">Performance of standard text analytics algorithms are known to be substantially degraded on consumer generated data, which are often very noisy. These algorithms also do not work well on enterprise data which has a very different nature from <a href=https://en.wikipedia.org/wiki/News_aggregator>News repositories</a>, storybooks or Wikipedia data. Text cleaning is a mandatory step which aims at <a href=https://en.wikipedia.org/wiki/Noise_reduction>noise removal</a> and <a href=https://en.wikipedia.org/wiki/Noise_reduction>correction</a> to improve performance. However, enterprise data need special cleaning methods since it contains many domain terms which appear to be noise against a standard <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, but in reality are not so. In this work we present detailed analysis of characteristics of enterprise data and suggest <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> for cleaning these repositories after domain terms have been automatically segregated from true noise terms. Noise terms are thereafter corrected in a contextual fashion. The effectiveness of the method is established through careful manual evaluation of error corrections over several standard <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>, including those available for hate speech detection, where there is deliberate distortion to avoid detection. We also share results to show enhancement in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> after noise correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.20/>ScopeIt : Scoping Task Relevant Sentences in Documents<span class=acl-fixed-case>S</span>cope<span class=acl-fixed-case>I</span>t: Scoping Task Relevant Sentences in Documents</a></strong><br><a href=/people/b/barun-patra/>Barun Patra</a>
|
<a href=/people/v/vishwas-suryanarayanan/>Vishwas Suryanarayanan</a>
|
<a href=/people/c/chala-fufa/>Chala Fufa</a>
|
<a href=/people/p/pamela-bhattacharya/>Pamela Bhattacharya</a>
|
<a href=/people/c/charles-c-lee/>Charles Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--20><div class="card-body p-3 small">A prominent problem faced by conversational agents working with large documents (Eg : email-based assistants) is the frequent presence of information in the document that is irrelevant to the assistant. This in turn makes it harder for the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> to accurately detect intents, extract entities relevant to those intents and perform the desired action. To address this issue we present a neural model for scoping relevant information for the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> from a large document. We show that when used as the first step in a popularly used email-based assistant for helping users schedule meetings, our proposed model helps improve the performance of the intent detection and entity extraction tasks required by the agent for correctly scheduling meetings : across a suite of 6 downstream tasks, by using our proposed method, we observe an average gain of 35 % in precision without any drop in recall. Additionally, we demonstrate that the same approach can be used for component level analysis in large documents, such as signature block identification.</div></div></div><hr><div id=2020argmining-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.argmining-1/>Proceedings of the 7th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.0/>Proceedings of the 7th Workshop on Argument Mining</a></strong><br><a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.argmining-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.1/>DebateSum : A large-scale argument mining and summarization dataset<span class=acl-fixed-case>D</span>ebate<span class=acl-fixed-case>S</span>um: A large-scale argument mining and summarization dataset</a></strong><br><a href=/people/a/allen-roush/>Allen Roush</a>
|
<a href=/people/a/arvind-balaji/>Arvind Balaji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--1><div class="card-body p-3 small">Prior work in <a href=https://en.wikipedia.org/wiki/Argument_mining>Argument Mining</a> frequently alludes to its potential applications in automatic debating systems. Despite this focus, almost no <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> or <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> exist which apply <a href=https://en.wikipedia.org/wiki/Natural-language_processing>natural language processing techniques</a> to problems found within competitive formal debate. To remedy this, we present the DebateSum dataset. DebateSum consists of 187,386 unique pieces of evidence with corresponding <a href=https://en.wikipedia.org/wiki/Argument>argument</a> and extractive summaries. DebateSum was made using <a href=https://en.wikipedia.org/wiki/Data>data</a> compiled by competitors within the <a href=https://en.wikipedia.org/wiki/National_Speech_and_Debate_Association>National Speech and Debate Association</a> over a 7year period. We train several transformer summarization models to benchmark <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> performance on DebateSum. We also introduce a set of fasttext word-vectors trained on DebateSum called debate2vec. Finally, we present a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> for this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> which is utilized extensively by members of the <a href=https://en.wikipedia.org/wiki/National_Speech_and_Debate_Association>National Speech and Debate Association</a> today. The DebateSum search engine is available to the public here : http://www.debate.cards</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.2/>Annotating Topics, Stance, Argumentativeness and Claims in Dutch Social Media Comments : A Pilot Study<span class=acl-fixed-case>D</span>utch Social Media Comments: A Pilot Study</a></strong><br><a href=/people/n/nina-bauwelinck/>Nina Bauwelinck</a>
|
<a href=/people/e/els-lefever/>Els Lefever</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--2><div class="card-body p-3 small">One of the major challenges currently facing the field of argumentation mining is the lack of consensus on how to analyse argumentative user-generated texts such as online comments. The theoretical motivations underlying the annotation guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. This pilot study reports on the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 100 Dutch user comments made in response to politically-themed news articles on <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>. The annotation covers topic and aspect labelling, stance labelling, argumentativeness detection and claim identification. Our IAA study reports substantial agreement scores for argumentativeness detection (0.76 Fleiss&#8217; kappa) and moderate agreement for claim labelling (0.45 Fleiss&#8217; kappa). We provide a clear justification of the theories and definitions underlying the design of our <a href=https://en.wikipedia.org/wiki/Guideline>guidelines</a>. Our analysis of the annotations signal the importance of adjusting our guidelines to include allowances for missing context information and defining the concept of argumentativeness in connection with <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>. Our annotated corpus and associated guidelines are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.argmining-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.5/>Aspect-Based Argument Mining</a></strong><br><a href=/people/d/dietrich-trautmann/>Dietrich Trautmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--5><div class="card-body p-3 small">Computational Argumentation in general and <a href=https://en.wikipedia.org/wiki/Argument_mining>Argument Mining</a> in particular are important research fields. In previous works, many of the challenges to automatically extract and to some degree reason over natural language arguments were addressed. The tools to extract <a href=https://en.wikipedia.org/wiki/Argument_of_a_function>argument units</a> are increasingly available and further open problems can be addressed. In this work, we are presenting the task of Aspect-Based Argument Mining (ABAM), with the essential subtasks of Aspect Term Extraction (ATE) and Nested Segmentation (NS). At the first instance, we create and release an annotated corpus with aspect information on the token-level. We consider <a href=https://en.wikipedia.org/wiki/Element_(mathematics)>aspects</a> as the main point(s) argument units are addressing. This information is important for further downstream tasks such as argument ranking, argument summarization and generation, as well as the search for counter-arguments on the aspect-level. We present several experiments using state-of-the-art supervised architectures and demonstrate their performance for both of the subtasks. The annotated benchmark is available at https://github.com/trtm/ABAM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.argmining-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.6/>Annotation and Detection of Arguments in Tweets</a></strong><br><a href=/people/r/robin-schaefer/>Robin Schaefer</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--6><div class="card-body p-3 small">Notwithstanding the increasing role <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> plays in modern political and social discourse, resources built for conducting <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> on <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> remain limited. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of <a href=https://en.wikipedia.org/wiki/Twitter>German tweets</a> annotated for <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument components</a>. To the best of our knowledge, this is the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using <a href=https://en.wikipedia.org/wiki/Supervised_classification>supervised classification</a> (F1 : 0.82) and sequence labeling (F1 : 0.72) approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.8/>ECHR : <a href=https://en.wikipedia.org/wiki/Corpus_Juris_Civilis>Legal Corpus</a> for Argument Mining<span class=acl-fixed-case>ECHR</span>: Legal Corpus for Argument Mining</a></strong><br><a href=/people/p/prakash-poudyal/>Prakash Poudyal</a>
|
<a href=/people/j/jaromir-savelka/>Jaromir Savelka</a>
|
<a href=/people/a/aagje-ieven/>Aagje Ieven</a>
|
<a href=/people/m/marie-francine-moens/>Marie Francine Moens</a>
|
<a href=/people/t/teresa-goncalves/>Teresa Goncalves</a>
|
<a href=/people/p/paulo-quaresma/>Paulo Quaresma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--8><div class="card-body p-3 small">In this paper, we publicly release an annotated corpus of 42 decisions of the <a href=https://en.wikipedia.org/wiki/European_Court_of_Human_Rights>European Court of Human Rights (ECHR)</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is annotated in terms of three types of <a href=https://en.wikipedia.org/wiki/Clause_(logic)>clauses</a> useful in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> : premise, conclusion, and non-argument parts of the text. Furthermore, relationships among the premises and conclusions are mapped. We present baselines for three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that lead from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured texts</a> to structured arguments. The tasks are argument clause recognition, clause relation prediction, and premise / conclusion recognition. Despite a straightforward application of the bidirectional encoders from Transformers (BERT), we obtained very promising results F1 0.765 on argument recognition, 0.511 on relation prediction, and 0.859/0.628 on premise / conclusion recognition). The results suggest the usefulness of pre-trained language models based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network architectures</a> in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Because of the simplicity of the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, there is ample space for improvement in future work based on the released corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.argmining-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--argmining-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.argmining-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.argmining-1.11/>Annotating argumentation in Swedish social media<span class=acl-fixed-case>S</span>wedish social media</a></strong><br><a href=/people/a/anna-lindahl/>Anna Lindahl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--argmining-1--11><div class="card-body p-3 small">This paper presents a small study of annotating <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> in Swedish social media. Annotators were asked to annotate spans of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> in 9 threads from two <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. At the post level, Cohen&#8217;s k and Krippendorff&#8217;s alpha 0.48 was achieved. When manually inspecting the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> the annotators seemed to agree when conditions in the guidelines were explicitly met, but implicit argumentation and opinions, resulting in annotators having to interpret what&#8217;s missing in the text, caused disagreements.</div></div></div><hr><div id=2020cogalex-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cogalex-1/>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.0/>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</a></strong><br><a href=/people/m/michael-zock/>Michael Zock</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.1/>Individual corpora predict fast memory retrieval during reading</a></strong><br><a href=/people/m/markus-j-hofmann/>Markus J. Hofmann</a>
|
<a href=/people/l/lara-muller/>Lara Müller</a>
|
<a href=/people/a/andre-rolke/>Andre Rölke</a>
|
<a href=/people/r/ralph-radach/>Ralph Radach</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--1><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, from which a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive language model</a> is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500 K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression analyses</a> revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual&#8217;s <a href=https://en.wikipedia.org/wiki/Long-term_memory>long-term memory structure</a> can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.cogalex-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.4/>Less is Better : A cognitively inspired unsupervised model for language segmentation</a></strong><br><a href=/people/j/jinbiao-yang/>Jinbiao Yang</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--4><div class="card-body p-3 small">Language users process utterances by segmenting them into many cognitive units, which vary in their sizes and <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic levels</a>. Although we can do such unitization / segmentation easily, its <a href=https://en.wikipedia.org/wiki/Cognition>cognitive mechanism</a> is still not clear. This paper proposes an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a>, Less-is-Better (LiB), to simulate the human cognitive process with respect to language unitization / segmentation. LiB follows the principle of least effort and aims to build a lexicon which minimizes the number of unit tokens (alleviating the effort of analysis) and number of unit types (alleviating the effort of storage) at the same time on any given corpus. LiB&#8217;s workflow is inspired by empirical cognitive phenomena. The design makes the mechanism of LiB cognitively plausible and the computational requirement light-weight. The lexicon generated by LiB performs the best among different types of lexicons (e.g. ground-truth words) both from an information-theoretical view and a cognitive view, which suggests that the LiB lexicon may be a plausible proxy of the <a href=https://en.wikipedia.org/wiki/Mental_lexicon>mental lexicon</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.5/>The CogALex Shared Task on Monolingual and Multilingual Identification of Semantic Relations<span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>AL</span>ex Shared Task on Monolingual and Multilingual Identification of Semantic Relations</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/l/luca-iacoponi/>Luca Iacoponi</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--5><div class="card-body p-3 small">The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>, <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a>, <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hypernymy</a> and no relation at all. Two test sets were released for evaluating the participating <a href=https://en.wikipedia.org/wiki/System>systems</a>. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.cogalex-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.7/>CogALex-VI Shared Task : Transrelation-A Robust Multilingual Language Model for Multilingual Relation Identification<span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>AL</span>ex-<span class=acl-fixed-case>VI</span> Shared Task: Transrelation - A Robust Multilingual Language Model for Multilingual Relation Identification</a></strong><br><a href=/people/l/lennart-wachowiak/>Lennart Wachowiak</a>
|
<a href=/people/c/christian-lang/>Christian Lang</a>
|
<a href=/people/b/barbara-heinisch/>Barbara Heinisch</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--7><div class="card-body p-3 small">We describe our submission to the CogALex-VI shared task on the identification of multilingual paradigmatic relations building on XLM-RoBERTa (XLM-R), a robustly optimized and multilingual BERT model. In spite of several experiments with data augmentation, data addition and ensemble methods with a Siamese Triple Net, Translrelation, the XLM-R model with a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> adapted to this specific task, performed best in testing and achieved the best results in the final evaluation of the shared task, even for a previously unseen language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.14/>Translating Collocations : The Need for Task-driven Word Associations</a></strong><br><a href=/people/o/olivia-o-y-kwong/>Oi Yee Kwong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--14><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> may help collocation translation by suggesting associated words in the form of collocations, <a href=https://en.wikipedia.org/wiki/Thesaurus>thesaurus</a>, and example sentences. We propose to enhance them with task-driven word associations, illustrating the need by a few scenarios and outlining a possible approach based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. An example is given, using pre-trained word embedding, while more extensive investigation with more refined methods and resources is underway.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.15/>Characterizing Dynamic Word Meaning Representations in the Brain</a></strong><br><a href=/people/n/nora-aguirre-celis/>Nora Aguirre-Celis</a>
|
<a href=/people/r/risto-miikkulainen/>Risto Miikkulainen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--15><div class="card-body p-3 small">During <a href=https://en.wikipedia.org/wiki/Sentence_comprehension>sentence comprehension</a>, humans adjust <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meanings</a> according to the combination of the concepts that occur in the sentence. This paper presents a neural network model called CEREBRA (Context-dEpendent meaning REpresentation in the BRAin) that demonstrates this process based on fMRI sentence patterns and the Concept Attribute Rep-resentation (CAR) theory. In several experiments, CEREBRA is used to quantify conceptual combination effect and demonstrate that it matters to humans. Such context-based representations could be used in future <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> allowing them to mirror human performance more accurately.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cogalex-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cogalex-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cogalex-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cogalex-1.16/>Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge</a></strong><br><a href=/people/s/sathvik-nair/>Sathvik Nair</a>
|
<a href=/people/m/mahesh-srinivasan/>Mahesh Srinivasan</a>
|
<a href=/people/s/stephan-meylan/>Stephan Meylan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cogalex-1--16><div class="card-body p-3 small">Understanding context-dependent variation in word meanings is a key aspect of <a href=https://en.wikipedia.org/wiki/Sentence_processing>human language comprehension</a> supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation ; for example, they often do not encode how closely senses, or <a href=https://en.wikipedia.org/wiki/Semantic_change>discretized word meanings</a>, are related to one another. Our work investigates whether recent advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> and <a href=https://en.wikipedia.org/wiki/Homonym>homonymy</a>. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants&#8217; judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Specifically, homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.</div></div></div><hr><div id=2020crac-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.crac-1/>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.0/>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.2/>It’s absolutely divine ! Can fine-grained sentiment analysis benefit from <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>?</a></strong><br><a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--2><div class="card-body p-3 small">While it has been claimed that anaphora or <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> plays an important role in <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>, it is not clear to what extent <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> actually boosts performance, if at all. In this paper, we investigate the potential added value of <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> for the aspect-based sentiment analysis of restaurant reviews in two languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. We focus on the task of aspect category classification and investigate whether including coreference information prior to <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> to resolve implicit aspect mentions is beneficial. Because <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is not a solved task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on a combination of lexical and semantic features, we show that resolving the coreferential relations prior to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> is beneficial in a joint optimization setup. However, this is only the case when relying on <a href=https://en.wikipedia.org/wiki/Gold_standard>gold-standard relations</a> and the result is more outspoken for <a href=https://en.wikipedia.org/wiki/English_language>English</a> than for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. When validating the optimal models, however, we found that only the Dutch pipeline is able to achieve a satisfying performance on a held-out test set and does so regardless of whether coreference information was included.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.3/>Anaphoric Zero Pronoun Identification : A Multilingual Approach</a></strong><br><a href=/people/a/abdulrahman-aloraini/>Abdulrahman Aloraini</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--3><div class="card-body p-3 small">Pro-drop languages such as <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> or <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> allow morphologically null but referential arguments in certain syntactic positions, called anaphoric zero-pronouns. Much NLP work on anaphoric zero-pronouns (AZP) is based on gold mentions, but models for their identification are a fundamental prerequisite for their resolution in real-life applications. Such <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>identification</a> requires <a href=https://en.wikipedia.org/wiki/Complex_system>complex language understanding</a> and knowledge of real-world entities. Transfer learning models, such as BERT, have recently shown to learn surface, syntactic, and semantic information, which can be very useful in recognizing AZPs. We propose a BERT-based multilingual model for AZP identification from predicted zero pronoun positions, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, this is the first neural network model of AZP identification for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> ; and our approach outperforms the stateof-the-art for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Experiment results suggest that BERT implicitly encode information about AZPs through their surrounding context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.4/>Predicting Coreference in Abstract Meaning Representations<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentations</a></strong><br><a href=/people/t/tatiana-anikina/>Tatiana Anikina</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--4><div class="card-body p-3 small">This work addresses <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in Abstract Meaning Representation (AMR) graphs, a popular formalism for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a state-of-the-art <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.crac-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.6/>TwiConv : A Coreference-annotated Corpus of Twitter Conversations<span class=acl-fixed-case>T</span>wi<span class=acl-fixed-case>C</span>onv: A Coreference-annotated Corpus of <span class=acl-fixed-case>T</span>witter Conversations</a></strong><br><a href=/people/b/berfin-aktas/>Berfin Aktaş</a>
|
<a href=/people/a/annalena-kohnert/>Annalena Kohnert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--6><div class="card-body p-3 small">This article introduces TwiConv, an English coreference-annotated corpus of microblog conversations from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. We describe the corpus compilation process and the annotation scheme, and release the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> publicly, along with this paper. We manually annotated nominal coreference in 1756 tweets arranged in 185 conversation threads. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> achieves satisfactory <a href=https://en.wikipedia.org/wiki/Annotation>annotation agreement</a> results. We also present a new method for mapping the tweet contents with distributed stand-off annotations, which can easily be adapted to different annotation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.8/>Reference to Discourse Topics : Introducing Global Shell Nouns</a></strong><br><a href=/people/f/fabian-simonjetz/>Fabian Simonjetz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--8><div class="card-body p-3 small">Shell nouns (SNs) are abstract nouns like fact, issue, and decision, which are capable of refer- ring to non-nominal antecedents, much like <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric pronouns</a>. As an extension of classical anaphora resolution, the automatic detection of SNs alongside their respective antecedents has received a growing research interest in recent years but proved to be a challenging task. This paper critically examines the assumption prevalent in previous research that SNs are typically accompanied by a specific antecedent, arguing that SNs like issue and decision are frequently used to refer, not to specific antecedents, but to global discourse topics, in which case they are out of reach of previously proposed resolution strategies that are tailored to SNs with explicit antecedents. The contribution of this work is three-fold. First, the notion of global SNs is defined ; second, their qualitative and quantitative impact on previous SN research is investigated ; and third, implications for previous and future approaches to SN resolution are discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.10/>Partially-supervised Mention Detection</a></strong><br><a href=/people/l/lesly-miculicich-werlen/>Lesly Miculicich</a>
|
<a href=/people/j/james-henderson/>James Henderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--10><div class="card-body p-3 small">Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Here, we investigate two approaches to deal with partial annotation of mentions : weighted loss and soft-target classification. We also propose two neural mention detection approaches : a sequence tagging, and an <a href=https://en.wikipedia.org/wiki/Brute-force_search>exhaustive search</a>. We evaluate our methods with <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> as a downstream task, using <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>. The results show that the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> improve for all <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.12/>Enhanced Labelling in Active Learning for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/v/vebjorn-espeland/>Vebjørn Espeland</a>
|
<a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/b/benjamin-bach/>Benjamin Bach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--12><div class="card-body p-3 small">In this paper we describe our attempt to increase the amount of information that can be retrieved through active learning sessions compared to previous approaches. We optimise the annotator&#8217;s labelling process using active learning in the context of <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. Using simulated active learning experiments, we suggest three adjustments to ensure the labelling time is spent as efficiently as possible. All three adjustments provide more information to the machine learner than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, though a large impact on the F1 score over time is not observed. Compared to previous <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, we report a marginal F1 improvement on the final coreference models trained using for two out of the three approaches tested when applied to the English OntoNotes 2012 Coreference Resolution data. Our best-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 58.01 <a href=https://en.wikipedia.org/wiki/F-number>F1</a>, an increase of 0.93 F1 over the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.13/>Reference in Team Communication for Robot-Assisted Disaster Response : An Initial Analysis</a></strong><br><a href=/people/n/natalia-skachkova/>Natalia Skachkova</a>
|
<a href=/people/i/ivana-kruijff-korbayova/>Ivana Kruijff-Korbayova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--13><div class="card-body p-3 small">We analyze reference phenomena in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of robot-assisted disaster response team communication. The annotation scheme we designed for this purpose distinguishes different types of <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a>, <a href=https://en.wikipedia.org/wiki/Role>roles</a>, reference units and <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. We focus particularly on mission-relevant objects, locations and actors and also annotate a rich set of reference links, including <a href=https://en.wikipedia.org/wiki/Co-reference>co-reference</a> and various other kinds of relations. We explain the categories used in our <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, present their distribution in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and discuss challenging cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.14/>Resolving Pronouns in Twitter Streams : Context can Help !<span class=acl-fixed-case>T</span>witter Streams: Context can Help!</a></strong><br><a href=/people/a/anietie-andy/>Anietie Andy</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/derry-tanti-wijaya/>Derry Tanti Wijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--14><div class="card-body p-3 small">Many people live-tweet televised events like <a href=https://en.wikipedia.org/wiki/United_States_presidential_debates>Presidential debates</a> and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people / characters. We propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for resolving <a href=https://en.wikipedia.org/wiki/Personal_pronoun>personal pronouns</a> that make reference to people involved in an event, in tweet streams collected during the event.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.crac-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--crac-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.crac-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.crac-1.15/>Coreference Strategies in English-German Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>G</span>erman Translation</a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/m/marie-pauline-krielke/>Marie-Pauline Krielke</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--crac-1--15><div class="card-body p-3 small">We present a study focusing on variation of <a href=https://en.wikipedia.org/wiki/Coreference>coreferential devices</a> in English original <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> and <a href=https://en.wikipedia.org/wiki/News>news texts</a> and their <a href=https://en.wikipedia.org/wiki/German_language>German translations</a>. Using exploratory techniques we contemplate a diverse set of coreference devices as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> which we assume indicate language-specific and register-based variation as well as potential translation strategies. Our findings reflect differences on both dimensions with stronger variation along the lines of <a href=https://en.wikipedia.org/wiki/Register_(sociolinguistics)>register</a> than between languages. By exposing interactions between text type and cross-linguistic variation, they can also inform multilingual NLP applications, especially <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div></div><hr><div id=2020dmr-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.dmr-1/>Proceedings of the Second International Workshop on Designing Meaning Representations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.dmr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.dmr-1.0/>Proceedings of the Second International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.dmr-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--dmr-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.dmr-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.dmr-1.4/>Cross-lingual annotation : a road map for low- and no-resource languages</a></strong><br><a href=/people/m/meagan-vigus/>Meagan Vigus</a>
|
<a href=/people/j/jens-e-l-van-gysel/>Jens E. L. Van Gysel</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/a/andrew-cowell/>Andrew Cowell</a>
|
<a href=/people/r/rosa-vallejos/>Rosa Vallejos</a>
|
<a href=/people/w/william-croft/>William Croft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--dmr-1--4><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Road_map>road map</a> for the annotation of semantic categories in typologically diverse languages, with potentially few linguistic resources, and often no existing <a href=https://en.wikipedia.org/wiki/Computational_resource>computational resources</a>. Past <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotation</a> efforts have focused largely on high-resource languages, or relatively low-resource languages with a large number of native speakers. However, there are certain typological traits, namely the synthesis of multiple concepts into a single word, that are more common in languages with a smaller speech community. For example, what is expressed as a sentence in a more <a href=https://en.wikipedia.org/wiki/Analytic_language>analytic language</a> like <a href=https://en.wikipedia.org/wiki/English_language>English</a>, may be expressed as a single word in a more <a href=https://en.wikipedia.org/wiki/Synthetic_language>synthetic language</a> like <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho</a>. This paper proposes solutions for annotating analytic and synthetic languages in a comparable way based on existing typological research, and introduces a road map for the annotation of languages with a dearth of resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.dmr-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--dmr-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.dmr-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.dmr-1.6/>K-SNACS : Annotating Korean Adposition Semantics<span class=acl-fixed-case>SNACS</span>: Annotating <span class=acl-fixed-case>K</span>orean Adposition Semantics</a></strong><br><a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/h/hanwool-choe/>Hanwool Choe</a>
|
<a href=/people/n/na-rae-han/>Na-Rae Han</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--dmr-1--6><div class="card-body p-3 small">While many languages use adpositions to encode <a href=https://en.wikipedia.org/wiki/Semantics>semantic relationships</a> between content words in a sentence (e.g., <a href=https://en.wikipedia.org/wiki/Agent_(grammar)>agentivity</a> or temporality), the details of how adpositions work vary widely across languages with respect to both form and meaning. In this paper, we empirically adapt the SNACS framework (Schneider et al., 2018) to <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, a language that is typologically distant from Englishthe language SNACS was based on. We apply the SNACS framework to annotate the highly popular novellaThe Little Prince with semantic supersense labels over allKorean postpositions. Thus, we introduce the first broad-coverage corpus annotated with <a href=https://en.wikipedia.org/wiki/Korean_language>Korean postposition semantics</a> and provide a detailed analysis of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with an apples-to-apples comparison between <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> and English annotations</div></div></div><hr><div id=2020ecomnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.ecomnlp-1/>Proceedings of Workshop on Natural Language Processing in E-Commerce</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ecomnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ecomnlp-1.0/>Proceedings of Workshop on Natural Language Processing in E-Commerce</a></strong><br><a href=/people/h/huasha-zhao/>Huasha Zhao</a>
|
<a href=/people/p/parikshit-sondhi/>Parikshit Sondhi</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/s/sanjika-hewavitharana/>Sanjika Hewavitharana</a>
|
<a href=/people/y/yifan-he/>Yifan He</a>
|
<a href=/people/l/luo-si/>Luo Si</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ecomnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ecomnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ecomnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ecomnlp-1.7/>BERT-based similarity learning for product matching<span class=acl-fixed-case>BERT</span>-based similarity learning for product matching</a></strong><br><a href=/people/j/janusz-tracz/>Janusz Tracz</a>
|
<a href=/people/p/piotr-iwo-wojcik/>Piotr Iwo Wójcik</a>
|
<a href=/people/k/kalina-jasinska-kobus/>Kalina Jasinska-Kobus</a>
|
<a href=/people/r/riccardo-belluzzo/>Riccardo Belluzzo</a>
|
<a href=/people/r/robert-mroczkowski/>Robert Mroczkowski</a>
|
<a href=/people/i/ireneusz-gawlik/>Ireneusz Gawlik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ecomnlp-1--7><div class="card-body p-3 small">Product matching, i.e., being able to infer the product being sold for a merchant-created offer, is crucial for any <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce marketplace</a>, enabling product-based navigation, <a href=https://en.wikipedia.org/wiki/Comparison_shopping_website>price comparisons</a>, <a href=https://en.wikipedia.org/wiki/Review_site>product reviews</a>, etc. This problem proves a challenging task, mostly due to the extent of product catalog, data heterogeneity, missing product representants, and varying levels of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a>. Moreover, new products are being introduced every day, making it difficult to cast the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a classification task. In this work, we apply BERT-based models in a similarity learning setup to solve the product matching problem. We provide a thorough ablation study, showing the impact of <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> and training objective choices. Application of transformer-based architectures and proper <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>sampling techniques</a> significantly boosts performance for a range of e-commerce domains, allowing for <a href=https://en.wikipedia.org/wiki/Software_deployment>production deployment</a>.</div></div></div><hr><div id=2020fnp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.fnp-1/>Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.0/>Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation</a></strong><br><a href=/people/d/dr-mahmoud-el-haj/>Dr Mahmoud El-Haj</a>
|
<a href=/people/d/dr-vasiliki-athanasakou/>Dr Vasiliki Athanasakou</a>
|
<a href=/people/d/dr-sira-ferradans/>Dr Sira Ferradans</a>
|
<a href=/people/d/dr-catherine-salzedo/>Dr Catherine Salzedo</a>
|
<a href=/people/d/dr-ans-elhag/>Dr Ans Elhag</a>
|
<a href=/people/d/dr-houda-bouamor/>Dr Houda Bouamor</a>
|
<a href=/people/d/dr-marina-litvak/>Dr Marina Litvak</a>
|
<a href=/people/d/dr-paul-rayson/>Dr Paul Rayson</a>
|
<a href=/people/d/dr-george-giannakopoulos/>Dr George Giannakopoulos</a>
|
<a href=/people/n/nikiforos-pittaras/>Nikiforos Pittaras</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.3/>The Financial Document Causality Detection Shared Task (FinCausal 2020)<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020)</a></strong><br><a href=/people/d/dominique-mariko/>Dominique Mariko</a>
|
<a href=/people/h/hanna-abi-akl/>Hanna Abi-Akl</a>
|
<a href=/people/e/estelle-labidurie/>Estelle Labidurie</a>
|
<a href=/people/s/stephane-durfort/>Stephane Durfort</a>
|
<a href=/people/h/hugues-de-mazancourt/>Hugues De Mazancourt</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--3><div class="card-body p-3 small">We present the FinCausal 2020 Shared Task on Causality Detection in Financial Documents and the associated FinCausal dataset, and discuss the participating systems and results. Two sub-tasks are proposed : a binary classification task (Task 1) and a relation extraction task (Task 2). A total of 16 teams submitted runs across the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>Tasks</a> and 13 of them contributed with a system description paper. This workshop is associated to the Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020), held at The 28th International Conference on Computational Linguistics (COLING&#8217;2020), Barcelona, Spain on September 12, 2020.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.7/>JDD @ FinCausal 2020, Task 2 : Financial Document Causality Detection<span class=acl-fixed-case>JDD</span> @ <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Task 2: Financial Document Causality Detection</a></strong><br><a href=/people/t/toshiya-imoto/>Toshiya Imoto</a>
|
<a href=/people/t/tomoki-ito/>Tomoki Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--7><div class="card-body p-3 small">This paper describes the approach we built for the Financial Document Causality Detection Shared Task (FinCausal-2020) Task 2 : Cause and Effect Detection. Our approach is based on a multi-class classifier using BiLSTM with Graph Convolutional Neural Network (GCN) trained by minimizing the binary cross entropy loss. In our approach, we have not used any extra data source apart from combining the trial and practice dataset. We achieve <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1 score</a> to 75.61 percent and are ranked at 7-th place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.9/>NITK NLP at FinCausal-2020 Task 1 Using BERT and Linear models.<span class=acl-fixed-case>NITK</span> <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal-2020 Task 1 Using <span class=acl-fixed-case>BERT</span> and Linear models.</a></strong><br><a href=/people/h/hariharan-r-l/>Hariharan R L</a>
|
<a href=/people/a/anand-kumar-m/>Anand Kumar M</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--9><div class="card-body p-3 small">FinCausal-2020 is the shared task which focuses on the causality detection of factual data for <a href=https://en.wikipedia.org/wiki/Financial_analysis>financial analysis</a>. The financial data facts do n&#8217;t provide much explanation on the variability of these <a href=https://en.wikipedia.org/wiki/Data>data</a>. This paper aims to propose an efficient method to classify the data into one which is having any financial cause or not. Many models were used to classify the data, out of which <a href=https://en.wikipedia.org/wiki/Statistical_model>SVM model</a> gave an <a href=https://en.wikipedia.org/wiki/F-score>F-Score</a> of 0.9435, BERT with specific <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> achieved best results with <a href=https://en.wikipedia.org/wiki/F-score>F-Score</a> of 0.9677.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.10/>Fraunhofer IAIS at FinCausal 2020, Tasks 1 & 2 : Using Ensemble Methods and Sequence Tagging to Detect Causality in Financial Documents<span class=acl-fixed-case>IAIS</span> at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Tasks 1 & 2: Using Ensemble Methods and Sequence Tagging to Detect Causality in Financial Documents</a></strong><br><a href=/people/m/maren-pielka/>Maren Pielka</a>
|
<a href=/people/r/rajkumar-ramamurthy/>Rajkumar Ramamurthy</a>
|
<a href=/people/a/anna-ladi/>Anna Ladi</a>
|
<a href=/people/e/eduardo-brito/>Eduardo Brito</a>
|
<a href=/people/c/clayton-chapman/>Clayton Chapman</a>
|
<a href=/people/p/paul-mayer/>Paul Mayer</a>
|
<a href=/people/r/rafet-sifa/>Rafet Sifa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--10><div class="card-body p-3 small">The FinCausal 2020 shared task aims to detect <a href=https://en.wikipedia.org/wiki/Causality>causality</a> on financial news and identify those parts of the causal sentences related to the underlying cause and effect. We apply ensemble-based and sequence tagging methods for identifying <a href=https://en.wikipedia.org/wiki/Causality>causality</a>, and extracting causal subsequences. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield promising results on both sub-tasks, with the prospect of further improvement given more time and computing resources. With respect to task 1, we achieved an F1 score of 0.9429 on the evaluation data, and a corresponding ranking of 12/14. For task 2, we were ranked 6/10, with an F1 score of 0.76 and an ExactMatch score of 0.1912.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.11/>NTUNLPL at FinCausal 2020, Task 2 : Improving Causality Detection Using Viterbi Decoder<span class=acl-fixed-case>NTUNLPL</span> at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Task 2:Improving Causality Detection Using <span class=acl-fixed-case>V</span>iterbi Decoder</a></strong><br><a href=/people/p/pei-wei-kao/>Pei-Wei Kao</a>
|
<a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--11><div class="card-body p-3 small">In order to provide an explanation of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the <a href=https://en.wikipedia.org/wiki/Viterbi_decoder>Viterbi decoder</a> for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our <a href=https://en.wikipedia.org/wiki/Source_code>code</a> for academic usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.12/>FiNLP at FinCausal 2020 Task 1 : Mixture of BERTs for Causal Sentence Identification in Financial Texts<span class=acl-fixed-case>F</span>i<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020 Task 1: Mixture of <span class=acl-fixed-case>BERT</span>s for Causal Sentence Identification in Financial Texts</a></strong><br><a href=/people/s/sarthak-gupta/>Sarthak Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--12><div class="card-body p-3 small">This paper describes our system developed for the sub-task 1 of the FinCausal shared task in the FNP-FNS workshop held in conjunction with COLING-2020. The <a href=https://en.wikipedia.org/wiki/System>system</a> classifies whether a financial news text segment contains causality or not. To address this task, we fine-tune and ensemble the generic and domain-specific BERT language models pre-trained on financial text corpora. The task data is highly imbalanced with the majority non-causal class ; therefore, we train the models using strategies such as under-sampling, cost-sensitive learning, and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a weighted F1-score of 96.98 securing 4th position on the evaluation leaderboard. The code is available at https://github.com/sarthakTUM/fincausal</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.15/>Domino at FinCausal 2020, Task 1 and 2 : Causal Extraction System<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Task 1 and 2: Causal Extraction System</a></strong><br><a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/t/tushar-kanakagiri/>Tushar Kanakagiri</a>
|
<a href=/people/k/karthik-radhakrishnan/>Karthik Radhakrishnan</a>
|
<a href=/people/a/anjana-umapathy/>Anjana Umapathy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--15><div class="card-body p-3 small">Automatic identification of cause-effect relationships from <a href=https://en.wikipedia.org/wiki/Data>data</a> is a challenging but important problem in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. Identifying semantic relationships has become increasingly important for multiple downstream applications like <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> and Event Prediction. In this work, we tackle the problem of <a href=https://en.wikipedia.org/wiki/Causal_inference>causal relationship extraction</a> from financial news using the FinCausal 2020 dataset. We tackle two tasks-1) Detecting the presence of <a href=https://en.wikipedia.org/wiki/Causality>causal relationships</a> and 2) Extracting segments corresponding to cause and effect from news snippets. We propose Transformer based sequence and token classification models with post-processing rules which achieve an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 96.12 and 79.60 on Tasks 1 and 2 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.16/>IITkgp at FinCausal 2020, Shared Task 1 : Causality Detection using Sentence Embeddings in Financial Reports<span class=acl-fixed-case>IIT</span>kgp at <span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>C</span>ausal 2020, Shared Task 1: Causality Detection using Sentence Embeddings in Financial Reports</a></strong><br><a href=/people/a/arka-mitra/>Arka Mitra</a>
|
<a href=/people/h/harshvardhan-srivastava/>Harshvardhan Srivastava</a>
|
<a href=/people/y/yugam-tiwari/>Yugam Tiwari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--16><div class="card-body p-3 small">The paper describes the work that the team submitted to FinCausal 2020 Shared Task. This work is associated with the first sub-task of identifying causality in sentences. The various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used in the experiments tried to obtain a latent space representation for each of the sentences. Linear regression was performed on these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> to classify whether the sentence is causal or not. The experiments have shown BERT (Large) performed the best, giving a F1 score of 0.958, in the task of detecting the causality of sentences in <a href=https://en.wikipedia.org/wiki/Financial_statement>financial texts</a> and <a href=https://en.wikipedia.org/wiki/Financial_statement>reports</a>. The class imbalance was dealt with a modified <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> to give a better metric score for the evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.17/>Extractive Financial Narrative Summarisation based on DPPs<span class=acl-fixed-case>DPP</span>s</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yafei-jiang/>Yafei Jiang</a>
|
<a href=/people/y/yinan-liu/>Yinan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--17><div class="card-body p-3 small">We participate in the FNS-Summarisation 2020 shared task to be held at FNP 2020 workshop at COLING 2020. Based on Determinantal Point Processes (DPPs), we build an extractive automatic financial summarisation system for the specific task. In this <a href=https://en.wikipedia.org/wiki/System>system</a>, we first analyze the long report data to select the important narrative parts and generate an intermediate document. Next, we build the kernel Matrix L for the intermediate document, which represents the quality of its sentences. On the basis of <a href=https://en.wikipedia.org/wiki/L_(complexity)>L</a>, we then can use the DPPs sampling algorithm to choose those sentences with high quality and diversity as the final summary sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.20/>End-to-end Training For Financial Report Summarization</a></strong><br><a href=/people/m/moreno-la-quatra/>Moreno La Quatra</a>
|
<a href=/people/l/luca-cagliero/>Luca Cagliero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--20><div class="card-body p-3 small">Quoted companies are requested to periodically publish <a href=https://en.wikipedia.org/wiki/Financial_statement>financial reports</a> in textual form. The annual financial reports typically include detailed financial and business information, thus giving relevant insights into company outlooks. However, a manual exploration of these financial reports could be very time consuming since most of the available information can be deemed as non-informative or redundant by expert readers. Hence, an increasing research interest has been devoted to automatically extracting domain-specific summaries, which include only the most relevant information. This paper describes the SumTO system architecture, which addresses the Shared Task of the Financial Narrative Summarisation (FNS) 2020 contest. The main task objective is to automatically extract the most informative, domain-specific textual content from financial, English-written documents. The aim is to create a summary of each company report covering all the business-relevant key points. To address the above-mentioned goal, we propose an end-to-end training method relying on Deep NLP techniques. The idea behind the system is to exploit the syntactic overlap between input sentences and ground-truth summaries to fine-tune pre-trained BERT embedding models, thus making such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> tailored to the specific context. The achieved results confirm the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, especially when the goal is to select relatively long text snippets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.23/>AMEX AI-Labs : An Investigative Study on Extractive Summarization of Financial Documents<span class=acl-fixed-case>AMEX</span> <span class=acl-fixed-case>AI</span>-Labs: An Investigative Study on Extractive Summarization of Financial Documents</a></strong><br><a href=/people/p/piyush-arora/>Piyush Arora</a>
|
<a href=/people/p/priya-radhakrishnan/>Priya Radhakrishnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--23><div class="card-body p-3 small">We describe the work carried out by AMEX AI-LABS on an extractive summarization benchmark task focused on Financial Narratives Summarization (FNS). This task focuses on summarizing annual financial reports which poses two main challenges as compared to typical news document summarization tasks : i) <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a> are more lengthier (average length about 80 pages) as compared to typical news documents, and ii) <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a> are more loosely structured e.g. comprising of tables, charts, <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a> and <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, which makes it challenging to effectively summarize. To address this summarization task we investigate a range of unsupervised, supervised and ensemble based techniques. We find that ensemble based techniques perform relatively better as compared to using only the unsupervised and supervised based techniques. Our ensemble based model achieved the highest rank of 9 out of 31 systems submitted for the benchmark task based on Rouge-L evaluation metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.28/>Taxy.io@FinTOC-2020 : Multilingual Document Structure Extraction using Transfer Learning<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>TOC</span>-2020: Multilingual Document Structure Extraction using Transfer Learning</a></strong><br><a href=/people/f/frederic-haase/>Frederic Haase</a>
|
<a href=/people/s/steffen-kirchhoff/>Steffen Kirchhoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--28><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to the FinTOC-2020 shared task on financial doc- ument structure extraction. We propose a two-step approach to identify titles in financial docu- ments and to extract their table of contents (TOC). First, we identify text blocks as candidates for titles using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> based on character-level information of each document. Then, we apply <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> on a self-constructed regression task to predict the depth of each text block in the document structure hierarchy using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> combined with document features and layout features. It is noteworthy that our single multilingual model performs well on both tasks and on different languages, which indicates the usefulness of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for title detection and TOC generation. Moreover, our approach is independent of the presence of actual <a href=https://en.wikipedia.org/wiki/Table_of_contents>TOC pages</a> in the documents. It is also one of the few submissions to the FinTOC-2020 shared task addressing both subtasks in both languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, with one single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.fnp-1.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.31/>A Computational Analysis of Financial and Environmental Narratives within <a href=https://en.wikipedia.org/wiki/Financial_statement>Financial Reports</a> and its Value for Investors</a></strong><br><a href=/people/f/felix-armbrust/>Felix Armbrust</a>
|
<a href=/people/h/henry-schafer/>Henry Schäfer</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--31><div class="card-body p-3 small">Public companies are obliged to include financial and non-financial information within their cor- porate filings under Regulation S-K, in the United States (SEC, 2010). However, the requirements still allow for manager&#8217;s discretion. This raises the question to which extent the information is actually included and if this information is at all relevant for investors. We answer this question by training and evaluating an end-to-end deep learning approach (based on BERT and GloVe embeddings) to predict the financial and environmental performance of the company from the Management&#8217;s Discussion and Analysis of Financial Conditions and Results of Operations (MD&A) section of 10-K (yearly) and 10-Q (quarterly) filings. We further analyse the mediating effect of the environmental performance on the relationship between the company&#8217;s disclosures and financial performance. Hereby, we address the results of previous studies regarding environ- mental performance. We find that the textual information contained within the MD&A section does not allow for conclusions about the future (corporate) financial performance. However, there is evidence that the environmental performance can be extracted by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.fnp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--fnp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.fnp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.fnp-1.33/>Mitigating Silence in Compliance Terminology during Parsing of Utterances</a></strong><br><a href=/people/e/esmeralda-manandise/>Esme Manandise</a>
|
<a href=/people/c/conrad-de-peuter/>Conrad de Peuter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--fnp-1--33><div class="card-body p-3 small">This paper reports on an approach to increase multi-token-term recall in a parsing task. We use a compliance-domain parser to extract, during the process of parsing raw text, terms that are unlisted in the terminology. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> uses a similarity measure (Generalized Dice Coefficient) between listed terms and unlisted term candidates to (i) determine term status, (ii) serve putative terms to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, (iii) decrease parsing complexity by glomming multi-tokens as lexical singletons, and (iv) automatically augment the terminology after parsing of an utterance completes. We illustrate a small experiment with examples from the tax-and-regulations domain. Bootstrapping the parsing process to detect out- of-vocabulary terms at runtime increases parsing accuracy in addition to producing other benefits to a natural-language-processing pipeline, which translates arithmetic calculations written in English into computer-executable operations.</div></div></div><hr><div id=2020gebnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.gebnlp-1/>Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gebnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gebnlp-1.0/>Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gebnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gebnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gebnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gebnlp-1.2/>Interdependencies of Gender and Race in Contextualized Word Embeddings</a></strong><br><a href=/people/m/may-jiang/>May Jiang</a>
|
<a href=/people/c/christiane-fellbaum/>Christiane Fellbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gebnlp-1--2><div class="card-body p-3 small">Recent years have seen a surge in research on the biases in word embeddings with respect to <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and, to a lesser extent, <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>. Few of these studies, however, have given attention to the critical intersection of <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a>. In this case study, we analyze the dimensions of <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a> in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of <a href=https://en.wikipedia.org/wiki/Intersectionality>intersectionality</a> for future studies on <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and debiasing in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gebnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gebnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gebnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gebnlp-1.3/>Fine-tuning Neural Machine Translation on Gender-Balanced Datasets</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/a/adria-de-jorge/>Adrià de Jorge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gebnlp-1--3><div class="card-body p-3 small">Misrepresentation of certain communities in datasets is causing big disruptions in <a href=https://en.wikipedia.org/wiki/List_of_applications_of_artificial_intelligence>artificial intelligence applications</a>. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. This <a href=https://en.wikipedia.org/wiki/Balanced_set>balanced set</a> is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gebnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gebnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gebnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gebnlp-1.7/>Conversational Assistants and <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>Gender Stereotypes</a> : Public Perceptions and Desiderata for Voice Personas</a></strong><br><a href=/people/a/amanda-cercas-curry/>Amanda Cercas Curry</a>
|
<a href=/people/j/judy-robertson/>Judy Robertson</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gebnlp-1--7><div class="card-body p-3 small">Conversational voice assistants are rapidly developing from purely transactional systems to social companions with personality. UNESCO recently stated that the female and submissive personality of current <a href=https://en.wikipedia.org/wiki/Digital_assistant>digital assistants</a> gives rise for concern as it reinforces <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a>. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure / personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants.</div></div></div><hr><div id=2020lantern-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lantern-1/>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lantern-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lantern-1.0/>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></strong><br><a href=/people/a/aditya-mogadala/>Aditya Mogadala</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/z/zeynep-akata/>Zeynep Akata</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lantern-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lantern-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lantern-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lantern-1.2/>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a></strong><br><a href=/people/s/stanislav-frolov/>Stanislav Frolov</a>
|
<a href=/people/s/shailza-jolly/>Shailza Jolly</a>
|
<a href=/people/j/jorn-hees/>Jörn Hees</a>
|
<a href=/people/a/andreas-dengel/>Andreas Dengel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lantern-1--2><div class="card-body p-3 small">Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as <a href=https://en.wikipedia.org/wiki/Bird>birds</a> and <a href=https://en.wikipedia.org/wiki/Face>human faces</a>, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the <a href=https://en.wikipedia.org/wiki/Image_quality>image quality</a> and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82 % to 84.79 % when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.</div></div></div><hr><div id=2020latechclfl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.latechclfl-1/>Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.0/>Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/s/stefania-degaetano/>Stefania DeGaetano</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.latechclfl-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.2/>Automatic Topological Field Identification in (Historical) German Texts<span class=acl-fixed-case>G</span>erman Texts</a></strong><br><a href=/people/k/katrin-ortmann/>Katrin Ortmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--2><div class="card-body p-3 small">For the study of certain <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic phenomena</a> and their development over time, large amounts of textual data must be enriched with relevant <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. Since the manual creation of such <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> requires a lot of effort, automating the process with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP methods</a> would be convenient. But the required amounts of training data are usually not available for non-standard or historical language. The present study investigates whether <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on modern newspaper text can be used to automatically identify <a href=https://en.wikipedia.org/wiki/Topology>topological fields</a>, i.e. syntactic structures, in different modern and historical German texts. The evaluation shows that, in general, it is possible to transfer a parser model to other registers or time periods with overall F1-scores 92 %. However, an error analysis makes clear that additional rules and domain-specific training data would be beneficial if <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structures</a> differ significantly from the training data, e.g. in the case of <a href=https://en.wikipedia.org/wiki/Early_New_High_German>Early New High German</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.5/>Neural Machine Translation of Artwork Titles Using Iconclass Codes</a></strong><br><a href=/people/n/nikolay-banar/>Nikolay Banar</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--5><div class="card-body p-3 small">We investigate the use of <a href=https://en.wikipedia.org/wiki/Iconclass>Iconclass</a> in the context of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> for NL-EN artwork titles. Iconclass is a widely used <a href=https://en.wikipedia.org/wiki/Iconography>iconographic classification system</a> used in the <a href=https://en.wikipedia.org/wiki/Cultural_heritage>cultural heritage domain</a> to describe and retrieve subjects represented in the <a href=https://en.wikipedia.org/wiki/Visual_arts>visual arts</a>. The resource contains keywords and definitions to encode the presence of objects, people, events and ideas depicted in <a href=https://en.wikipedia.org/wiki/Work_of_art>artworks</a>, such as <a href=https://en.wikipedia.org/wiki/Painting>paintings</a>. We propose a simple concatenation approach that improves the quality of automatically generated title translations for <a href=https://en.wikipedia.org/wiki/Work_of_art>artworks</a>, by leveraging textual information extracted from <a href=https://en.wikipedia.org/wiki/Iconclass>Iconclass</a>. Our results demonstrate that a neural machine translation system is able to exploit this <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> to boost the translation performance of artwork titles. This <a href=https://en.wikipedia.org/wiki/Technology>technology</a> enables interesting applications of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> in resource-scarce domains in the <a href=https://en.wikipedia.org/wiki/Culture>cultural sector</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.8/>Vital Records : Uncover the past from historical handwritten records</a></strong><br><a href=/people/h/herve-dejean/>Herve Dejean</a>
|
<a href=/people/j/jean-luc-meunier/>Jean-Luc Meunier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--8><div class="card-body p-3 small">We present Vital Records, a demonstrator based on deep-learning approaches to handwritten-text recognition, table processing and information extraction, which enables data from century-old documents to be parsed and analysed, making it possible to explore death records in space and time. This demonstrator provides a <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> for browsing and visualising data extracted from 80,000 handwritten pages of tabular data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.11/>Life still goes on : Analysing Australian WW1 Diaries through Distant Reading<span class=acl-fixed-case>A</span>ustralian <span class=acl-fixed-case>WW</span>1 Diaries through Distant Reading</a></strong><br><a href=/people/a/ashley-dennis-henderson/>Ashley Dennis-Henderson</a>
|
<a href=/people/m/matthew-roughan/>Matthew Roughan</a>
|
<a href=/people/l/lewis-mitchell/>Lewis Mitchell</a>
|
<a href=/people/j/jonathan-tuke/>Jonathan Tuke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--11><div class="card-body p-3 small">An increasing amount of historic data is now available in digital (text) formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional <a href=https://en.wikipedia.org/wiki/Close_reading>close reading</a>, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> that has recently been transcribed is a collection of over 500 Australian World War I (WW1) diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that <a href=https://en.wikipedia.org/wiki/Data>data</a>, topic modelling and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.latechclfl-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.14/>Results of a Single Blind Literary Taste Test with Short Anonymized Novel Fragments</a></strong><br><a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/c/corina-koolen/>Corina Koolen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--14><div class="card-body p-3 small">It is an open question to what extent perceptions of literary quality are derived from text-intrinsic versus social factors. While supervised models can predict literary quality ratings from textual factors quite successfully, as shown in the Riddle of Literary Quality project (Koolen et al., 2020), this does not prove that social factors are not important, nor can we assume that readers make judgments on literary quality in the same way and based on the same information as machine learning models. We report the results of a pilot study to gauge the effect of textual features on literary ratings of Dutch-language novels by participants in a controlled experiment with 48 participants. In an exploratory analysis, we compare the <a href=https://en.wikipedia.org/wiki/Audience_measurement>ratings</a> to those from the large reader survey of the Riddle in which social factors were not excluded, and to machine learning predictions of those literary ratings. We find moderate to strong correlations of questionnaire ratings with the <a href=https://en.wikipedia.org/wiki/Survey_methodology>survey ratings</a>, but the predictions are closer to the <a href=https://en.wikipedia.org/wiki/Survey_methodology>survey ratings</a>. Code and data : https://github.com/andreasvc/litquest</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.17/>Interpretation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> in Aeschylus’s Greek Tragedy<span class=acl-fixed-case>G</span>reek Tragedy</a></strong><br><a href=/people/v/vijaya-kumari-yeruva/>Vijaya Kumari Yeruva</a>
|
<a href=/people/m/mayanka-chandrashekar/>Mayanka ChandraShekar</a>
|
<a href=/people/y/yugyung-lee/>Yugyung Lee</a>
|
<a href=/people/j/jeff-rydberg-cox/>Jeff Rydberg-Cox</a>
|
<a href=/people/v/virginia-blanton/>Virginia Blanton</a>
|
<a href=/people/n/nathan-a-oyler/>Nathan A Oyler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--17><div class="card-body p-3 small">Recent advancements in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> have created unique challenges and opportunities for digital humanities research. In particular, there are ample opportunities for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and machine learning researchers to analyze data from <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a> and to broaden our understanding of human sentiment in <a href=https://en.wikipedia.org/wiki/Greek_tragedy>classical Greek tragedy</a>. In this paper, we will explore the challenges and benefits from the human and machine collaboration for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/Greek_tragedy>Greek tragedy</a> and address some open questions related to the collaborative annotation for the sentiments in literary texts. We focus primarily on (i) an analysis of the challenges in sentiment analysis tasks for humans and machines, and (ii) whether consistent annotation results are generated from the multiple <a href=https://en.wikipedia.org/wiki/Annotation>human annotators</a> and multiple <a href=https://en.wikipedia.org/wiki/Annotation>machine annotators</a>. For human annotators, we have used a survey-based approach with about 60 college students. We have selected three popular sentiment analysis tools for machine annotators, including VADER, CoreNLP&#8217;s sentiment annotator, and TextBlob. We have conducted a qualitative and quantitative evaluation and confirmed our observations on sentiments in <a href=https://en.wikipedia.org/wiki/Greek_tragedy>Greek tragedy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.latechclfl-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--latechclfl-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.latechclfl-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.latechclfl-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.latechclfl-1.19/>Finding and Generating a Missing Part for Story Completion</a></strong><br><a href=/people/y/yusuke-mori/>Yusuke Mori</a>
|
<a href=/people/h/hiroaki-yamane/>Hiroaki Yamane</a>
|
<a href=/people/y/yusuke-mukuta/>Yusuke Mukuta</a>
|
<a href=/people/t/tatsuya-harada/>Tatsuya Harada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--latechclfl-1--19><div class="card-body p-3 small">Creating a story is difficult. Professional writers often experience a writer&#8217;s block. Thus, providing automatic support to writers is crucial but also challenging. Recently, in the field of generating and understanding stories, story completion (SC) has been proposed as a method for generating missing parts of an incomplete story. Despite this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>&#8217;s usefulness in providing creative support, its applicability is currently limited because it requires the user to have prior knowledge of the missing part of a story. Writers do not always know which part of their writing is flawed. To overcome this problem, we propose a novel approach called missing position prediction (MPP). Given an <a href=https://en.wikipedia.org/wiki/Unfinished_creative_work>incomplete story</a>, we aim to predict the position of the missing part. We also propose a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> for <a href=https://en.wikipedia.org/wiki/Mean_time_between_failures>MPP</a> and <a href=https://en.wikipedia.org/wiki/Mean_time_between_failures>SC</a>. We first conduct an experiment focusing on MPP, and our analysis shows that highly accurate predictions can be obtained when the missing part of a story is the beginning or the end. This suggests that if a story has a specific beginning or end, they play significant roles. We conduct an experiment on <a href=https://en.wikipedia.org/wiki/Small_interfering_RNA>SC</a> using MPP, and our proposed method demonstrates promising results.</div></div></div><hr><div id=2020law-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.law-1/>Proceedings of the 14th Linguistic Annotation Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.0/>Proceedings of the 14th Linguistic Annotation Workshop</a></strong><br><a href=/people/s/stefanie-dipper/>Stefanie Dipper</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--law-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.law-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.8/>Cookpad Parsed Corpus : Linguistic Annotations of Japanese Recipes<span class=acl-fixed-case>J</span>apanese Recipes</a></strong><br><a href=/people/j/jun-harashima/>Jun Harashima</a>
|
<a href=/people/m/makoto-hiramatsu/>Makoto Hiramatsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--law-1--8><div class="card-body p-3 small">It has become increasingly common for people to share <a href=https://en.wikipedia.org/wiki/Recipe>cooking recipes</a> on the Internet. Along with the increase in the number of shared recipes, there have been corresponding increases in recipe-related studies and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. However, there are still few <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that provide linguistic annotations for the recipe-related studies even though such <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> should form the basis of the studies. This paper introduces a novel recipe-related dataset, named Cookpad Parsed Corpus, which contains linguistic annotations for Japanese recipes. We randomly extracted 500 recipes from the largest recipe-related dataset, the Cookpad Recipe Dataset, and annotated 4 ; 738 sentences in the <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> with morphemes, named entities, and dependency relations. This paper also reports benchmark results on our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for Japanese morphological analysis, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and dependency parsing. We show that there is still room for improvement in the analyses of recipes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--law-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.law-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.law-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.10/>PASTRIE : A Corpus of Prepositions Annotated with Supersense Tags in Reddit International English<span class=acl-fixed-case>PASTRIE</span>: A Corpus of Prepositions Annotated with Supersense Tags in <span class=acl-fixed-case>R</span>eddit International <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/michael-kranzlein/>Michael Kranzlein</a>
|
<a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/s/siyao-peng/>Siyao Peng</a>
|
<a href=/people/s/shira-wein/>Shira Wein</a>
|
<a href=/people/a/aryaman-arora/>Aryaman Arora</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--law-1--10><div class="card-body p-3 small">We present the Prepositions Annotated with Supsersense Tags in Reddit International English (PASTRIE) corpus, a new dataset containing manually annotated preposition supersenses of English data from presumed speakers of four L1s : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are comprehensive, covering all preposition types and tokens in the sample. Along with the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we provide analysis of distributional patterns across the included <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>L1s</a> and a discussion of the influence of <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>L1s</a> on L2 preposition choice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--law-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.law-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.law-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.13/>Querent Intent in Multi-Sentence Questions</a></strong><br><a href=/people/l/laurie-burchell/>Laurie Burchell</a>
|
<a href=/people/j/jie-chi/>Jie Chi</a>
|
<a href=/people/t/tom-hosking/>Tom Hosking</a>
|
<a href=/people/n/nina-markl/>Nina Markl</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--law-1--13><div class="card-body p-3 small">Multi-sentence questions (MSQs) are sequences of questions connected by relations which, unlike sequences of standalone questions, need to be answered as a unit. Following Rhetorical Structure Theory (RST), we recognise that different question discourse relations between the subparts of MSQs reflect different speaker intents, and consequently elicit different answering strategies. Correctly identifying these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> is therefore a crucial step in automatically answering MSQs. We identify five different types of MSQs in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and define five novel <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> to describe them. We extract over 162,000 MSQs from <a href=https://en.wikipedia.org/wiki/Stack_Exchange>Stack Exchange</a> to enable future research. Finally, we implement a high-precision baseline classifier based on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>surface features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.law-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--law-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.law-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.law-1.17/>Annotating Coherence Relations for Studying Topic Transitions in Social Talk</a></strong><br><a href=/people/a/alex-luu/>Alex Luu</a>
|
<a href=/people/s/sophia-a-malamud/>Sophia A. Malamud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--law-1--17><div class="card-body p-3 small">This study develops the strand of research on topic transitions in social talk which aims to gain a better understanding of interlocutors&#8217; conversational goals. Lu and Malamud (2020) proposed that one way to identify such transitions is to annotate coherence relations, and then to identify utterances potentially expressing new topics as those that fail to participate in these relations. This work validates and refines their suggested annotation methodology, focusing on annotating most prominent <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence relations</a> in <a href=https://en.wikipedia.org/wiki/Face-to-face_interaction>face-to-face social dialogue</a>. The result is a publicly accessible gold standard corpus with efficient and reliable <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, whose broad coverage provides a foundation for future steps of identifying and classifying new topic utterances.</div></div></div><hr><div id=2020msr-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.msr-1/>Proceedings of the Third Workshop on Multilingual Surface Realisation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.msr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.msr-1.0/>Proceedings of the Third Workshop on Multilingual Surface Realisation</a></strong><br><a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.msr-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--msr-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.msr-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.msr-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.msr-1.1/>The Third Multilingual Surface Realisation Shared Task (SR’20): Overview and Evaluation Results<span class=acl-fixed-case>SR</span>’20): Overview and Evaluation Results</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--msr-1--1><div class="card-body p-3 small">This paper presents results from the Third Shared Task on Multilingual Surface Realisation (SR&#8217;20) which was organised as part of the COLING&#8217;20 Workshop on Multilingual Surface Realisation. As in SR&#8217;18 and SR&#8217;19, the shared task comprised two tracks : (1) a Shallow Track where the inputs were full UD structures with word order information removed and tokens lemmatised ; and (2) a Deep Track where additionally, functional words and morphological information were removed. Moreover, each <a href=https://en.wikipedia.org/wiki/Track_(navigation)>track</a> had two subtracks : (a) restricted-resource, where only the data provided or approved as part of a track could be used for training models, and (b) open-resource, where any data could be used. The Shallow Track was offered in 11 languages, whereas the Deep Track in 3 ones. Systems were evaluated using both automatic metrics and direct assessment by human evaluators in terms of <a href=https://en.wikipedia.org/wiki/Readability>Readability</a> and Meaning Similarity to reference outputs. We present the evaluation results, along with descriptions of the SR&#8217;19 tracks, data and evaluation methods, as well as brief summaries of the participating systems. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.msr-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--msr-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.msr-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.msr-1.2/>BME-TUW at SR’20 : Lexical grammar induction for surface realization<span class=acl-fixed-case>BME</span>-<span class=acl-fixed-case>TUW</span> at <span class=acl-fixed-case>SR</span>’20: Lexical grammar induction for surface realization</a></strong><br><a href=/people/g/gabor-recski/>Gábor Recski</a>
|
<a href=/people/a/adam-kovacs/>Ádám Kovács</a>
|
<a href=/people/k/kinga-gemes/>Kinga Gémes</a>
|
<a href=/people/j/judit-acs/>Judit Ács</a>
|
<a href=/people/a/andras-kornai/>Andras Kornai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--msr-1--2><div class="card-body p-3 small">We present a system for mapping Universal Dependency structures to raw text which learns to restore word order by training an Interpreted Regular Tree Grammar (IRTG) that establishes a mapping between string and graph operations. The reinflection step is handled by a standard sequence-to-sequence architecture with a biLSTM encoder and an LSTM decoder with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. We modify our 2019 system (Kovcs et al., 2019) with a new grammar induction mechanism that allows IRTG rules to operate on <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemmata</a> in addition to part-of-speech tags and ensures that each word and its dependents are reordered using the most specific set of learned patterns. We also introduce a hierarchical approach to <a href=https://en.wikipedia.org/wiki/Word_order>word order restoration</a> that independently determines the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> of each clause in a sentence before arranging them with respect to the main clause, thereby improving overall readability and also making the IRTG parsing task tractable. We participated in the 2020 Surface Realization Shared task, subtrack T1a (shallow, closed). Human evaluation shows we achieve significant improvements on two of the three out-of-domain datasets compared to the 2019 system we modified. Both <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> of our <a href=https://en.wikipedia.org/wiki/System>system</a> are available on GitHub under an MIT license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.msr-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--msr-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.msr-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.msr-1.6/>NILC at SR’20 : Exploring Pre-Trained Models in Surface Realisation<span class=acl-fixed-case>NILC</span> at <span class=acl-fixed-case>SR</span>’20: Exploring Pre-Trained Models in Surface Realisation</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--msr-1--6><div class="card-body p-3 small">This paper describes the submission by the NILC Computational Linguistics research group of the University of S ao Paulo / Brazil to the English Track 2 (closed sub-track) at the Surface Realisation Shared Task 2020. The success of the current pre-trained models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> or GPT-2 in several tasks is well-known, however, this is not the case for data-to-text generation tasks and just recently some initiatives focused on it. This way, we explore how a pre-trained model (GPT-2) performs on the UD-to-text generation task. In general, the achieved results were poor, but there are some interesting ideas to explore. Among the learned lessons we may note that it is necessary to study strategies to represent UD inputs and to introduce structural knowledge into these pre-trained models.</div></div></div><hr><div id=2020mwe-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.mwe-1/>Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.0/>Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/j/john-philip-mccrae/>John McCrae</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/c/carole-tiberius/>Carole Tiberius</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/a/ashwini-vaidya/>Ashwini Vaidya</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.mwe-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.1/>CollFrEn : Rich Bilingual EnglishFrench Collocation Resource<span class=acl-fixed-case>C</span>oll<span class=acl-fixed-case>F</span>r<span class=acl-fixed-case>E</span>n: Rich Bilingual <span class=acl-fixed-case>E</span>nglish–<span class=acl-fixed-case>F</span>rench Collocation Resource</a></strong><br><a href=/people/b/beatriz-fisas/>Beatriz Fisas</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/j/joan-codina-filba/>Joan Codina-Filbá</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--1><div class="card-body p-3 small">Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words traditionally pose a challenge to language learners and many Natural Language Processing (NLP) applications alike. Reliable ground truth (i.e., ideally manually compiled) resources are thus of high value. We present a manually compiled bilingual EnglishFrench collocation resource with 7,480 collocations in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and 6,733 in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. Each <a href=https://en.wikipedia.org/wiki/Collocation>collocation</a> is enriched with information that facilitates its downstream exploitation in NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, relation classification, and so forth. Our proposed enrichment covers : the semantic category of the collocation (its lexical function), its vector space representation (for each individual word as well as their joint collocation embedding), a subcategorization pattern of both its elements, as well as their corresponding BabelNet i d, and finally, indices of their occurrences in large scale reference corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.3/>Hierarchy-aware Learning of Sequential Tool Usage via Semi-automatically Constructed Taxonomies</a></strong><br><a href=/people/n/nima-nabizadeh/>Nima Nabizadeh</a>
|
<a href=/people/m/martin-heckmann/>Martin Heckmann</a>
|
<a href=/people/d/dorothea-kolossa/>Dorothea Kolossa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--3><div class="card-body p-3 small">When repairing a device, humans employ a series of tools that corresponds to the arrangement of the device components. Such sequences of tool usage can be learned from repair manuals, so that at each step, having observed the previously applied tools, a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> can predict the next required tool. In this paper, we improve the tool prediction performance of such methods by additionally taking the hierarchical relationships among the tools into account. To this aim, we build a taxonomy of tools with <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy</a> and hypernymy relations from the data by decomposing all multi-word expressions of tool names. We then develop a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> that performs a binary prediction for each node in the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a>. The evaluation of the method on a dataset of repair manuals shows that encoding the tools with the constructed <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> and using a top-down beam search for decoding increases the prediction accuracy and yields an interpretable <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> as a potentially valuable byproduct.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.mwe-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.6/>AlphaMWE : Construction of Multilingual Parallel Corpora with MWE Annotations<span class=acl-fixed-case>A</span>lpha<span class=acl-fixed-case>MWE</span>: Construction of Multilingual Parallel Corpora with <span class=acl-fixed-case>MWE</span> Annotations</a></strong><br><a href=/people/l/lifeng-han/>Lifeng Han</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a>
|
<a href=/people/a/alan-smeaton/>Alan Smeaton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--6><div class="card-body p-3 small">In this work, we present the construction of multilingual parallel corpora with annotation of multiword expressions (MWEs). MWEs include verbal MWEs (vMWEs) defined in the PARSEME shared task that have a verb as the head of the studied terms. The annotated vMWEs are also bilingually and multilingually aligned manually. The languages covered include <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Our original <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a> is taken from the PARSEME shared task in 2018. We performed <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> of this source corpus followed by human post editing and annotation of target MWEs. Strict quality control was applied for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error limitation</a>, i.e., each MT output sentence received first manual post editing and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> plus second manual quality rechecking. One of our findings during corpora preparation is that accurate <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> of MWEs presents challenges to MT systems. To facilitate further MT research, we present a categorisation of the error types encountered by MT systems in performing MWE related translation. To acquire a broader view of MT issues, we selected four popular state-of-the-art MT models for comparisons namely : Microsoft Bing Translator, GoogleMT, Baidu Fanyi and DeepL MT. Because of the noise removal, translation post editing and MWE annotation by human professionals, we believe our AlphaMWE dataset will be an asset for cross-lingual and multilingual research, such as <a href=https://en.wikipedia.org/wiki/Multilingualism>MT</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Our multilingual corpora are available as open access at github.com/poethan/AlphaMWE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.7/>Annotating Verbal MWEs in <a href=https://en.wikipedia.org/wiki/Irish_language>Irish</a> for the PARSEME Shared Task 1.2<span class=acl-fixed-case>MWE</span>s in <span class=acl-fixed-case>I</span>rish for the <span class=acl-fixed-case>PARSEME</span> Shared Task 1.2</a></strong><br><a href=/people/a/abigail-walsh/>Abigail Walsh</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--7><div class="card-body p-3 small">This paper describes the creation of two <a href=https://en.wikipedia.org/wiki/Irish_language>Irish corpora</a> (labelled and unlabelled) for verbal MWEs for inclusion in the PARSEME Shared Task 1.2 on automatic identification of verbal MWEs, and the process of developing verbal MWE categories for <a href=https://en.wikipedia.org/wiki/Irish_language>Irish</a>. A qualitative analysis on the two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> is presented, along with discussion of Irish verbal MWEs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.10/>Multi-word Expressions for Abusive Speech Detection in Serbian<span class=acl-fixed-case>S</span>erbian</a></strong><br><a href=/people/r/ranka-stankovic/>Ranka Stanković</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/d/danka-jokic/>Danka Jokić</a>
|
<a href=/people/c/cvetana-krstev/>Cvetana Krstev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--10><div class="card-body p-3 small">This paper presents our work on the refinement and improvement of the Serbian language part of Hurtlex, a multilingual lexicon of words to hurt. We pay special attention to adding Multi-word expressions that can be seen as abusive, as such lexical entries are very important in obtaining good results in a plethora of abusive language detection tasks. We use Serbian morphological dictionaries as a basis for <a href=https://en.wikipedia.org/wiki/Data_cleaning>data cleaning</a> and MWE dictionary creation. A connection to other lexical and semantic resources in <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a> is outlined and building of abusive language detection systems based on that connection is foreseen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.12/>Comparing <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe for Automatic Measurement of MWE Compositionality<span class=acl-fixed-case>G</span>lo<span class=acl-fixed-case>V</span>e for Automatic Measurement of <span class=acl-fixed-case>MWE</span> Compositionality</a></strong><br><a href=/people/t/thomas-pickard/>Thomas Pickard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--12><div class="card-body p-3 small">This paper explores the use of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe embeddings for unsupervised measurement of the semantic compositionality of MWE candidates. Through comparison with several human-annotated reference sets, we find <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> to be substantively superior to <a href=https://en.wikipedia.org/wiki/GloVe>GloVe</a> for this task. We also find Simple <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> to be a poor-quality resource for compositionality assessment, but demonstrate that a sample of 10 % of sentences in the <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> can provide a conveniently tractable corpus with only moderate reduction in the quality of outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mwe-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mwe-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mwe-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mwe-1.20/>MultiVitaminBooster at PARSEME Shared Task 2020 : Combining Window- and Dependency-Based Features with Multilingual Contextualised Word Embeddings for VMWE Detection<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>V</span>itamin<span class=acl-fixed-case>B</span>ooster at <span class=acl-fixed-case>PARSEME</span> Shared Task 2020: Combining Window- and Dependency-Based Features with Multilingual Contextualised Word Embeddings for <span class=acl-fixed-case>VMWE</span> Detection</a></strong><br><a href=/people/s/sebastian-gombert/>Sebastian Gombert</a>
|
<a href=/people/s/sabine-bartsch/>Sabine Bartsch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mwe-1--20><div class="card-body p-3 small">In this paper, we present MultiVitaminBooster, a system implemented for the PARSEME shared task on semi-supervised identification of verbal multiword expressions-edition 1.2. For our approach, we interpret detecting verbal multiword expressions as a token classification task aiming to decide whether a token is part of a verbal multiword expression or not. For this purpose, we train <a href=https://en.wikipedia.org/wiki/Gradient_boosting>gradient boosting-based models</a>. We encode tokens as feature vectors combining multilingual contextualized word embeddings provided by the XLM-RoBERTa language model with a more traditional linguistic feature set relying on context windows and dependency relations. Our system was ranked 7th in the official open track ranking of the shared task evaluations with an encoding-related bug distorting the results. For this reason we carry out further unofficial evaluations. Unofficial versions of our <a href=https://en.wikipedia.org/wiki/System>systems</a> would have achieved higher ranks.</div></div></div><hr><div id=2020nlp4if-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlp4if-1/>Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlp4if-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlp4if-1.0/>Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></strong><br><a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/c/chris-brew/>Chris Brew</a>
|
<a href=/people/g/giovanni-luca-ciampaglia/>Giovanni Luca Ciampaglia</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlp4if-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlp4if-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlp4if-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlp4if-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.nlp4if-1.1/>Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking<span class=acl-fixed-case>COVID</span>-19 Fake News Detection and Fact Checking</a></strong><br><a href=/people/r/rutvik-vijjali/>Rutvik Vijjali</a>
|
<a href=/people/p/prathyush-potluri/>Prathyush Potluri</a>
|
<a href=/people/s/siddharth-kumar/>Siddharth Kumar</a>
|
<a href=/people/s/sundeep-teki/>Sundeep Teki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlp4if-1--1><div class="card-body p-3 small">The rapid advancement of technology in <a href=https://en.wikipedia.org/wiki/Online_communication>online communication</a> via <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> has led to a prolific rise in the spread of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> and <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The first <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> leverages a novel <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking algorithm</a> that retrieves the most relevant facts concerning user queries about particular COVID-19 claims. The second model verifies the level of truth in the queried claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlp4if-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlp4if-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlp4if-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlp4if-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.nlp4if-1.2/>Measuring Alignment to Authoritarian State Media as Framing Bias</a></strong><br><a href=/people/t/timothy-niven/>Timothy Niven</a>
|
<a href=/people/h/hung-yu-kao/>Hung-Yu Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlp4if-1--2><div class="card-body p-3 small">We introduce what is to the best of our knowledge a new task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> : measuring alignment to authoritarian state media. We operationalize alignment in terms of sociological definitions of <a href=https://en.wikipedia.org/wiki/Media_bias>media bias</a>. We take as a case study the alignment of four <a href=https://en.wikipedia.org/wiki/Media_of_Taiwan>Taiwanese media outlets</a> to the <a href=https://en.wikipedia.org/wiki/State_media>Chinese Communist Party state media</a>. We present the results of an initial investigation using the frequency of words in psychologically meaningful categories. Our findings suggest that the chosen word categories correlate with <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing choices</a>. We develop a calculation method that yields reasonable results for measuring <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a>, agreeing well with the known labels. We confirm that our method does capture event selection bias, but whether it captures <a href=https://en.wikipedia.org/wiki/Framing_bias>framing bias</a> requires further investigation.</div></div></div><hr><div id=2020peoples-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.peoples-1/>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.0/>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--peoples-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.peoples-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.4/>Persuasiveness of News Editorials depending on Ideology and Personality</a></strong><br><a href=/people/r/roxanne-el-baff/>Roxanne El Baff</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--peoples-1--4><div class="card-body p-3 small">News editorials aim to shape the opinions of their readership and the general public on timely controversial issues. The impact of an editorial on the reader&#8217;s opinion does not only depend on its content and style, but also on the reader&#8217;s profile. Previous work has studied the effect of editorial style depending on <a href=https://en.wikipedia.org/wiki/Ideology>general political ideologies</a> (liberals vs.conservatives). In our work, we dig deeper into the persuasiveness of both content and style, exploring the role of the intensity of an <a href=https://en.wikipedia.org/wiki/Ideology>ideology</a> (lean vs.extreme) and the reader&#8217;s personality traits (agreeableness, <a href=https://en.wikipedia.org/wiki/Conscientiousness>conscientiousness</a>, <a href=https://en.wikipedia.org/wiki/Extraversion_and_introversion>extraversion</a>, <a href=https://en.wikipedia.org/wiki/Neuroticism>neuroticism</a>, and openness). Concretely, we train content- and style-based models on New York Times editorials for different ideology- and personality-specific groups. Our results suggest that particularly readers with extreme ideology and non role model personalities are impacted by <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a>. We further analyze the importance of various text features with respect to the editorials&#8217; impact, the readers&#8217; profile, and the editorials&#8217; geographical scope.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--peoples-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.peoples-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.6/>KanCMD : Kannada CodeMixed Dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and Offensive Language Detection<span class=acl-fixed-case>K</span>an<span class=acl-fixed-case>CMD</span>: <span class=acl-fixed-case>K</span>annada <span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>M</span>ixed Dataset for Sentiment Analysis and Offensive Language Detection</a></strong><br><a href=/people/a/adeep-hande/>Adeep Hande</a>
|
<a href=/people/r/ruba-priyadharshini/>Ruba Priyadharshini</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--peoples-1--6><div class="card-body p-3 small">We introduce Kannada CodeMixed Dataset (KanCMD), a multi-task learning dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and offensive language identification. The KanCMD dataset highlights two real-world issues from the <a href=https://en.wikipedia.org/wiki/Social_media>social media text</a>. First, it contains actual comments in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Second, it has been annotated for two tasks, namely <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and offensive language detection for under-resourced Kannada language. Hence, KanCMD is meant to stimulate research in under-resourced Kannada language on real-world code-mixed social media text and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. KanCMD was obtained by crawling the <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>, and a minimum of three annotators annotates each comment. We release KanCMD 7,671 comments for multitask learning research purpose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--peoples-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.peoples-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.7/>Contextual Augmentation of Pretrained Language Models for <a href=https://en.wikipedia.org/wiki/Emotion_recognition>Emotion Recognition</a> in Conversations</a></strong><br><a href=/people/j/jonggu-kim/>Jonggu Kim</a>
|
<a href=/people/h/hyeonmok-ko/>Hyeonmok Ko</a>
|
<a href=/people/s/seoha-song/>Seoha Song</a>
|
<a href=/people/s/saebom-jang/>Saebom Jang</a>
|
<a href=/people/j/jiyeon-hong/>Jiyeon Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--peoples-1--7><div class="card-body p-3 small">Since language model pretraining to learn contextualized word representations has been proposed, pretrained language models have made success in many natural language processing tasks. That is because it is helpful to use individual contextualized representations of self-attention layers as to initialize parameters for downstream tasks. Yet, unfortunately, use of pretrained language models for <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> in conversations has not been studied enough. We firstly use ELECTRA which is a state-of-the-art pretrained language model and validate the performance on <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> in conversations. Furthermore, we propose contextual augmentation of pretrained language models for emotion recognition in conversations, which is to consider not only previous utterances, but also conversation-related information such as speakers, speech acts and topics. We classify information based on what the information is related to, and propose position of words corresponding to the information in the entire input sequence. To validate the proposed method, we conduct experiments on the DailyDialog dataset which contains abundant annotated information of conversations. The experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves state-of-the-art F1 scores on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and significantly improves the performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--peoples-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.peoples-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.peoples-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.11/>Multilingual Emoticon Prediction of Tweets about COVID-19<span class=acl-fixed-case>COVID</span>-19</a></strong><br><a href=/people/s/stefanos-stoikos/>Stefanos Stoikos</a>
|
<a href=/people/m/mike-izbicki/>Mike Izbicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--peoples-1--11><div class="card-body p-3 small">Emojis are a widely used tool for encoding emotional content in informal messages such as <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, and predicting which <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> corresponds to a piece of text can be used as a proxy for measuring the emotional content in the text. This paper presents the first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for predicting emojis in highly multilingual text. Our BERTmoticon model is a fine-tuned version of the BERT model, and it can predict emojis for text written in 102 different languages. We trained our BERTmoticon model on 54.2 million geolocated tweets sent in the first 6 months of 2020,and we apply the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to a case study analyzing the emotional reaction of Twitter users to news about the <a href=https://en.wikipedia.org/wiki/Coronavirus>coronavirus</a>. Example findings include a spike in sadness when the World Health Organization (WHO) declared that coronavirus was a global pandemic, and a spike in anger and disgust when the number of COVID-19 related deaths in the United States surpassed one hundred thousand. We provide an easy-to-use and open source python library for predicting emojis with BERTmoticon so that the model can easily be applied to other data mining tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.peoples-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--peoples-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.peoples-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.peoples-1.12/>Experiencers, Stimuli, or Targets : Which Semantic Roles Enable <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> to Infer the Emotions?</a></strong><br><a href=/people/l/laura-ana-maria-oberlander/>Laura Ana Maria Oberländer</a>
|
<a href=/people/k/kevin-reich/>Kevin Reich</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--peoples-1--12><div class="card-body p-3 small">Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a>, <a href=https://en.wikipedia.org/wiki/Sadness>sadness</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Trust_(social_science)>trust</a>, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like : who is described to feel the emotion? (experiencer), what causes this emotion? (stimulus), and at which entity is it directed? (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> about the position of the role improves the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification decision</a>. Particularly on <a href=https://en.wikipedia.org/wiki/Text_corpus>literature corpora</a> we find that the role information improves the <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>.</div></div></div><hr><div id=2020rdsm-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.rdsm-1/>Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rdsm-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rdsm-1.0/>Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM)</a></strong><br><a href=/people/a/ahmet-aker/>Ahmet Aker</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rdsm-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rdsm-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rdsm-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rdsm-1.3/>Covid or not Covid? Topic Shift in Information Cascades on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/l/liana-ermakova/>Liana Ermakova</a>
|
<a href=/people/d/diana-nurbakova/>Diana Nurbakova</a>
|
<a href=/people/i/irina-ovchinnikova/>Irina Ovchinnikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rdsm-1--3><div class="card-body p-3 small">Social media have become a valuable source of information. However, its power to shape <a href=https://en.wikipedia.org/wiki/Public_opinion>public opinion</a> can be dangerous, especially in the case of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>. The existing studies on misinformation detection hypothesise that the initial message is fake. In contrast, we focus on information distortion occurring in <a href=https://en.wikipedia.org/wiki/Information_cascade>cascades</a> as the initial message is quoted or receives a reply. We show a significant topic shift in <a href=https://en.wikipedia.org/wiki/Information_cascade>information cascades</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> during the Covid-19 pandemic providing valuable insights for the automatic analysis of information distortion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rdsm-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rdsm-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rdsm-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rdsm-1.6/>Automatic Detection of Hungarian Clickbait and Entertaining Fake News<span class=acl-fixed-case>H</span>ungarian Clickbait and Entertaining Fake News</a></strong><br><a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/m/martina-katalin-szabo/>Martina Katalin Szabó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rdsm-1--6><div class="card-body p-3 small">Online news do not always come from reliable sources and they are not always even realistic. The constantly growing number of online textual data has raised the need for detecting deception and bias in texts from different domains recently. In this paper, we identify different types of unrealistic news (clickbait and fake news written for entertainment purposes) written in Hungarian on the basis of a rich feature set and with the help of machine learning methods. Our tool achieves competitive scores : it is able to classify <a href=https://en.wikipedia.org/wiki/Clickbait>clickbait</a>, <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> written for entertainment purposes and <a href=https://en.wikipedia.org/wiki/News>real news</a> with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of over 80 %. It is also highlighted that <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphological features</a> perform the best in this <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rdsm-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rdsm-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rdsm-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.rdsm-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.rdsm-1.7/>Fake or Real? A Study of Arabic Satirical Fake News<span class=acl-fixed-case>A</span>rabic Satirical Fake News</a></strong><br><a href=/people/h/hadeel-saadany/>Hadeel Saadany</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a>
|
<a href=/people/e/emad-mohamed/>Emad Mohamed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rdsm-1--7><div class="card-body p-3 small">One very common type of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> is <a href=https://en.wikipedia.org/wiki/Satire>satire</a> which comes in a form of a <a href=https://en.wikipedia.org/wiki/Online_newspaper>news website</a> or an <a href=https://en.wikipedia.org/wiki/Online_newspaper>online platform</a> that parodies reputable real news agencies to create a sarcastic version of reality. This type of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> is often disseminated by individuals on their online platforms as it has a much stronger effect in delivering criticism than through a straightforward message. However, when the satirical text is disseminated via <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> without mention of its source, it can be mistaken for real news. This study conducts several exploratory analyses to identify the <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic properties</a> of <a href=https://en.wikipedia.org/wiki/Fake_news>Arabic fake news</a> with <a href=https://en.wikipedia.org/wiki/Satire>satirical content</a>. It shows that although it parodies <a href=https://en.wikipedia.org/wiki/News>real news</a>, Arabic satirical news has distinguishing features on the lexico-grammatical level. We exploit these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to build a number of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> capable of identifying satirical fake news with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of up to 98.6 %. The study introduces a new dataset (3185 articles) scraped from two Arabic satirical news websites (&#8216;Al-Hudood&#8217; and &#8216;Al-Ahram Al-Mexici&#8217;) which consists of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. The real news dataset consists of 3710 articles collected from three official news sites : the &#8216;BBC-Arabic&#8217;, the &#8216;CNN-Arabic&#8217; and &#8216;Al-Jazeera news&#8217;. Both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are concerned with political issues related to the Middle East.</div></div></div><hr><div id=2020semeval-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.semeval-1/>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.0/>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></strong><br><a href=/people/a/aurelie-herbelot/>Aurelie Herbelot</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.6/>Discovery Team at SemEval-2020 Task 1 : Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection</a></strong><br><a href=/people/m/matej-martinc/>Matej Martinc</a>
|
<a href=/people/s/syrielle-montariol/>Syrielle Montariol</a>
|
<a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--6><div class="card-body p-3 small">This paper describes the approaches used by the Discovery Team to solve SemEval-2020 Task 1-Unsupervised Lexical Semantic Change Detection. The proposed method is based on clustering of BERT contextual embeddings, followed by a comparison of cluster distributions across time. The best results were obtained by an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of this method and static Word2Vec embeddings. According to the official results, our approach proved the best for <a href=https://en.wikipedia.org/wiki/Latin>Latin</a> in Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.7/>GM-CTSC at SemEval-2020 Task 1 : Gaussian Mixtures Cross Temporal Similarity Clustering<span class=acl-fixed-case>GM</span>-<span class=acl-fixed-case>CTSC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: <span class=acl-fixed-case>G</span>aussian Mixtures Cross Temporal Similarity Clustering</a></strong><br><a href=/people/p/pierluigi-cassotti/>Pierluigi Cassotti</a>
|
<a href=/people/a/annalina-caputo/>Annalina Caputo</a>
|
<a href=/people/m/marco-polignano/>Marco Polignano</a>
|
<a href=/people/p/pierpaolo-basile/>Pierpaolo Basile</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--7><div class="card-body p-3 small">This paper describes the system proposed by the Random team for SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection. We focus our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> on the <a href=https://en.wikipedia.org/wiki/Detection_theory>detection problem</a>. Given the semantics of words captured by temporal word embeddings in different time periods, we investigate the use of <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to detect when the target word has gained or lost senses. To this end, we define a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on Gaussian Mixture Models to cluster the target similarities computed over the two periods. We compare the proposed approach with a number of <a href=https://en.wikipedia.org/wiki/Similarity_(geometry)>similarity-based thresholds</a>. We found that, although the performance of the detection methods varies across the word embedding algorithms, the combination of Gaussian Mixture with Temporal Referencing resulted in our best system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.10/>RIJP at SemEval-2020 Task 1 : Gaussian-based Embeddings for Semantic Change Detection<span class=acl-fixed-case>RIJP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: <span class=acl-fixed-case>G</span>aussian-based Embeddings for Semantic Change Detection</a></strong><br><a href=/people/r/ran-iwamoto/>Ran Iwamoto</a>
|
<a href=/people/m/masahiro-yukawa/>Masahiro Yukawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--10><div class="card-body p-3 small">This paper describes the model proposed and submitted by our RIJP team to SemEval 2020 Task1 : Unsupervised Lexical Semantic Change Detection. In the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, words are represented by <a href=https://en.wikipedia.org/wiki/Normal_distribution>Gaussian distributions</a>. For Subtask 1, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved average scores of 0.51 and 0.70 in the evaluation and post-evaluation processes, respectively. The higher score in the post-evaluation process than that in the evaluation process was achieved owing to appropriate parameter tuning. The results indicate that the proposed Gaussian-based embedding model is able to express <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shifts</a> while having a low computational</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.14/>UiO-UvA at SemEval-2020 Task 1 : Contextualised Embeddings for Lexical Semantic Change Detection<span class=acl-fixed-case>U</span>i<span class=acl-fixed-case>O</span>-<span class=acl-fixed-case>U</span>v<span class=acl-fixed-case>A</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--14><div class="card-body p-3 small">We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> depends on the distribution of gold scores in the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.15/>BMEAUT at SemEval-2020 Task 2 : <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Entailment</a> with Semantic Graphs<span class=acl-fixed-case>BMEAUT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 2: Lexical Entailment with Semantic Graphs</a></strong><br><a href=/people/a/adam-kovacs/>Ádám Kovács</a>
|
<a href=/people/k/kinga-gemes/>Kinga Gémes</a>
|
<a href=/people/a/andras-kornai/>Andras Kornai</a>
|
<a href=/people/g/gabor-recski/>Gábor Recski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--15><div class="card-body p-3 small">In this paper we present a novel rule-based, language independent method for determining lexical entailment relations using semantic representations built from Wiktionary definitions. Combined with a simple WordNet-based method our system achieves top scores on the English and Italian datasets of the Semeval-2020 task Predicting Multilingual and Cross-lingual (graded) Lexical Entailment (Glava et al., 2020). A detailed error analysis of our output uncovers future di- rections for improving both the semantic parsing method and the inference process on semantic graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.16/>BRUMS at SemEval-2020 Task 3 : Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity<span class=acl-fixed-case>BRUMS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity</a></strong><br><a href=/people/h/hansi-hettiarachchi/>Hansi Hettiarachchi</a>
|
<a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--16><div class="card-body p-3 small">This paper presents the team BRUMS submission to SemEval-2020 Task 3 : Graded Word Similarity in Context. The system utilises state-of-the-art contextualised word embeddings, which have some task-specific adaptations, including stacked embeddings and average embeddings. Overall, the <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> achieves good evaluation scores across all the languages, while maintaining simplicity. Following the final rankings, our approach is ranked within the top 5 solutions of each language while preserving the 1st position of Finnish subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.19/>UZH at SemEval-2020 Task 3 : Combining BERT with WordNet Sense Embeddings to Predict Graded Word Similarity Changes<span class=acl-fixed-case>UZH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Combining <span class=acl-fixed-case>BERT</span> with <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Sense Embeddings to Predict Graded Word Similarity Changes</a></strong><br><a href=/people/l/li-tang/>Li Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--19><div class="card-body p-3 small">CoSimLex is a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that can be used to evaluate the ability of context-dependent word embed- dings for modeling subtle, graded changes of meaning, as perceived by humans during reading. At SemEval-2020, task 3, subtask 1 is about predicting the (graded) effect of context in word similarity, using CoSimLex to quantify such a change of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> for a pair of words, from one context to another. Here, a meaning shift is composed of two aspects, a) discrete changes observed between different word senses, and b) more subtle changes of meaning representation that are not captured in those discrete changes. Therefore, this SemEval task was designed to allow the evaluation of <a href=https://en.wikipedia.org/wiki/System>systems</a> that can deal with a mix of both situations of <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a>, as they occur in the human perception of meaning. The described system was developed to improve the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BERT baseline</a> provided with the task, by reducing distortions in the BERT semantic space, compared to the human semantic space. To this end, complementarity between 768- and 1024-dimensional BERT embeddings, and average word sense vectors were used. With this <a href=https://en.wikipedia.org/wiki/System>system</a>, after some fine-tuning, the baseline performance of 0.705 (uncentered Pearson correlation with human semantic shift data from 27 annotators) was enhanced by more than 6 %, to 0.7645. We hope that this work can make a contribution to further our understanding of the semantic vector space of human perception, as it can be modeled with context-dependent word embeddings in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.23/>DCC-Uchile at SemEval-2020 Task 1 : Temporal Referencing Word Embeddings<span class=acl-fixed-case>DCC</span>-Uchile at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Temporal Referencing Word Embeddings</a></strong><br><a href=/people/f/frank-d-zamora-reina/>Frank D. Zamora-Reina</a>
|
<a href=/people/f/felipe-bravo-marquez/>Felipe Bravo-Marquez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--23><div class="card-body p-3 small">We present a system for the task of unsupervised lexical change detection : given a target word and two corpora spanning different periods of time, automatically detects whether the word has lost or gained senses from one corpus to another. Our system employs the temporal referencing method to obtain compatible representations of target words in different periods of time. This is done by concatenating corpora of different periods and performing a temporal referencing of target words i.e., treating occurrences of target words in different periods as two independent tokens. Afterwards, we train <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on the joint corpus and compare the referenced vectors of each target word using <a href=https://en.wikipedia.org/wiki/Cosine>cosine similarity</a>. Our submission was ranked 7th among 34 teams for subtask 1, obtaining an average accuracy of 0.637, only 0.050 points behind the first ranked system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.26/>SST-BERT at SemEval-2020 Task 1 : Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces<span class=acl-fixed-case>SST</span>-<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Semantic Shift Tracing by Clustering in <span class=acl-fixed-case>BERT</span>-based Embedding Spaces</a></strong><br><a href=/people/v/vani-kanjirangat/>Vani Kanjirangat</a>
|
<a href=/people/s/sandra-mitrovic/>Sandra Mitrovic</a>
|
<a href=/people/a/alessandro-antonucci/>Alessandro Antonucci</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--26><div class="card-body p-3 small">Lexical semantic change detection (also known as semantic shift tracing) is a task of identifying words that have changed their meaning over time. Unsupervised semantic shift tracing, focal point of SemEval2020, is particularly challenging. Given the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setup</a>, in this work, we propose to identify <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> among different occurrences of each target word, considering these as representatives of different word meanings. As such, disagreements in obtained clusters naturally allow to quantify the level of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic shift</a> per each target word in four target languages. To leverage this idea, <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> is performed on contextualized (BERT-based) embeddings of word occurrences. The obtained results show that our approach performs well both measured separately (per language) and overall, where we surpass all provided SemEval baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.27/>TemporalTeller at SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection with Temporal Referencing<span class=acl-fixed-case>T</span>emporal<span class=acl-fixed-case>T</span>eller at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</a></strong><br><a href=/people/j/jinan-zhou/>Jinan Zhou</a>
|
<a href=/people/j/jiaxin-li/>Jiaxin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--27><div class="card-body p-3 small">This paper describes our TemporalTeller system for SemEval Task 1 : Unsupervised Lexical Semantic Change Detection. We develop a unified framework for the common semantic change detection pipelines including preprocessing, learning word embeddings, calculating vector distances and determining threshold. We also propose Gamma Quantile Threshold to distinguish between changed and stable words. Based on our system, we conduct a comprehensive comparison among BERT, <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a>, Temporal Referencing and alignment-based methods. Evaluation results show that <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a> with Temporal Referencing achieves the best performance of 66.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>classification accuracy</a> and 51.8 % Spearman&#8217;s Ranking Correlation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.35/>Ferryman at SemEval-2020 Task 3 : Bert with TFIDF-Weighting for Predicting the Effect of Context in Word Similarity<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Bert with <span class=acl-fixed-case>TFIDF</span>-Weighting for Predicting the Effect of Context in Word Similarity</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/x/xin-yuan/>Xin Yuan</a>
|
<a href=/people/s/sai-zhang/>Sai Zhang</a>
|
<a href=/people/j/jiehui-wu/>Jiehui Wu</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--35><div class="card-body p-3 small">Word similarity is widely used in machine learning applications like <a href=https://en.wikipedia.org/wiki/Web_search_engine>searching engine</a> and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>. Measuring the changing meaning of the same word between two different sentences is not only a way to handle complex features in word usage (such as sentence syntax and semantics), but also an important method for different word polysemy modeling. In this paper, we present the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> proposed by team Ferryman. Our system is based on the Bidirectional Encoder Representations from Transformers (BERT) model combined with term frequency-inverse document frequency (TF-IDF), applying the method on the provided datasets called CoSimLex, which covers four different languages including <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>. Our team Ferryman wins the the first position for English task and the second position for <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> in the subtask 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.37/>JUSTMasters at SemEval-2020 Task 3 : Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity<span class=acl-fixed-case>JUSTM</span>asters at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity</a></strong><br><a href=/people/n/nour-al-khdour/>Nour Al-khdour</a>
|
<a href=/people/m/mutaz-bni-younes/>Mutaz Bni Younes</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--37><div class="card-body p-3 small">There is a growing research interest in studying word similarity. Without a doubt, two similar words in a context may considered different in another context. Therefore, this paper investigates the effect of the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in word similarity. The SemEval-2020 workshop has provided a shared task (Task 3 : Predicting the (Graded) Effect of Context in Word Similarity). In this task, the organizers provided unlabeled datasets for four languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> and <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a>. Our team, JUSTMasters, has participated in this competition in the two subtasks : A and B. Our approach has used a weighted average ensembling method for different pretrained embeddings techniques for each of the four languages. Our proposed model outperformed the baseline models in both subtasks and acheived the best result for subtask 2 in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, with score 0.725 and 0.68 respectively. We have been ranked the sixth for subtask 1, with scores for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a> as follows : 0.738, 0.44, 0.546, 0.512.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.38/>Will_Go at SemEval-2020 Task 3 : An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on BERT<span class=acl-fixed-case>W</span>ill_<span class=acl-fixed-case>G</span>o at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/w/wei-bao/>Wei Bao</a>
|
<a href=/people/h/hongshu-che/>Hongshu Che</a>
|
<a href=/people/j/jiandong-zhang/>Jiandong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--38><div class="card-body p-3 small">Natural Language Processing (NLP) has been widely used in the <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> in recent years. Our paper mainly discusses a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to analyze the effect that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer (BERT). Our team will go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.41/>SemEval-2020 Task 6 : Definition Extraction from Free Text with the DEFT Corpus<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Definition Extraction from Free Text with the <span class=acl-fixed-case>DEFT</span> Corpus</a></strong><br><a href=/people/s/sasha-spala/>Sasha Spala</a>
|
<a href=/people/n/nicholas-miller/>Nicholas Miller</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/c/carl-dockhorn/>Carl Dockhorn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--41><div class="card-body p-3 small">Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered. In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks : 1) Sentence classification, 2) <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, and 3) relation extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.42/>IIE-NLP-NUT at SemEval-2020 Task 4 : Guiding PLM with Prompt Template Reconstruction Strategy for ComVE<span class=acl-fixed-case>IIE</span>-<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>NUT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Guiding <span class=acl-fixed-case>PLM</span> with Prompt Template Reconstruction Strategy for <span class=acl-fixed-case>C</span>om<span class=acl-fixed-case>VE</span></a></strong><br><a href=/people/l/luxi-xing/>Luxi Xing</a>
|
<a href=/people/y/yuqiang-xie/>Yuqiang Xie</a>
|
<a href=/people/y/yue-hu/>Yue Hu</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--42><div class="card-body p-3 small">This paper introduces our systems for the first two subtasks of SemEval Task4 : Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the <a href=https://en.wikipedia.org/wiki/Question_answering>subtasks</a> into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a>. Our approaches secure the third rank on both <a href=https://en.wikipedia.org/wiki/Standard_score>official test sets</a> of the first two subtasks with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 96.4 and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.3 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.46/>BUT-FIT at SemEval-2020 Task 4 : Multilingual Commonsense<span class=acl-fixed-case>BUT</span>-<span class=acl-fixed-case>FIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Multilingual Commonsense</a></strong><br><a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/martin-fajcik/>Martin Fajcik</a>
|
<a href=/people/m/martin-docekal/>Martin Docekal</a>
|
<a href=/people/p/pavel-smrz/>Pavel Smrz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--46><div class="card-body p-3 small">We participated in all three subtasks. In subtasks A and B, our submissions are based on pretrained language representation models (namely ALBERT) and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We experimented with solving the task for another language, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, by means of multilingual models and machine translated dataset, or translated model inputs. We show that with a strong machine translation system, our <a href=https://en.wikipedia.org/wiki/System>system</a> can be used in another language with a small <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy loss</a>. In subtask C, our submission, which is based on pretrained sequence-to-sequence model (BART), ranked 1st in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score ranking</a>, however, we show that the correlation between <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluation, in which our submission ended up 4th, is low. We analyse the metrics used in the evaluation and we propose an additional score based on model from subtask B, which correlates well with our manual ranking, as well as reranking method based on the same principle. We performed an error and dataset analysis for all subtasks and we present our findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.49/>Masked Reasoner at SemEval-2020 Task 4 : Fine-Tuning RoBERTa for Commonsense Reasoning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Fine-Tuning <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a for Commonsense Reasoning</a></strong><br><a href=/people/d/daming-lu/>Daming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--49><div class="card-body p-3 small">This paper describes the masked reasoner system that participated in SemEval-2020 Task 4 : Commonsense Validation and Explanation. The <a href=https://en.wikipedia.org/wiki/System>system</a> participated in the subtask B.We proposes a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to fine-tune RoBERTa by masking the most important word in the statement. We believe that the confidence of the system in recovering that word is positively correlated to the score the masked language model gives to the current statement-explanation pair. We evaluate the importance of each word using InferSent and do the masked fine-tuning on RoBERTa. Then we use the fine-tuned model to predict the most plausible explanation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is fast in training and achieved 73.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.52/>UoR at SemEval-2020 Task 4 : Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation</a></strong><br><a href=/people/t/thanet-markchom/>Thanet Markchom</a>
|
<a href=/people/b/bhuvana-dhruva/>Bhuvana Dhruva</a>
|
<a href=/people/c/chandresh-pravin/>Chandresh Pravin</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--52><div class="card-body p-3 small">SemEval Task 4 Commonsense Validation and Explanation Challenge is to validate whether a system can differentiate natural language statements that make sense from those that do not make sense. Two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>, A and B, are focused in this work, i.e., detecting against-common-sense statements and selecting explanations of why they are false from the given options. Intuitively, commonsense validation requires additional knowledge beyond the given statements. Therefore, we propose a system utilising pre-trained sentence transformer models based on BERT, RoBERTa and DistillBERT architectures to embed the statements before classification. According to the results, these embeddings can improve the performance of the typical MLP and LSTM classifiers as downstream models of both subtasks compared to regular tokenised statements. These embedded statements are shown to comprise additional information from external resources which help validate common sense in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.53/>BUT-FIT at SemEval-2020 Task 5 : Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models<span class=acl-fixed-case>BUT</span>-<span class=acl-fixed-case>FIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models</a></strong><br><a href=/people/m/martin-fajcik/>Martin Fajcik</a>
|
<a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/martin-docekal/>Martin Docekal</a>
|
<a href=/people/p/pavel-smrz/>Pavel Smrz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--53><div class="card-body p-3 small">This paper describes BUT-FIT&#8217;s submission at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual (Subtask 1) and extracting both antecedent and consequent parts of the counterfactual from the text (Subtask 2). We experimented with various state-of-the-art language representation models (LRMs). We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.58/>ACNLP at SemEval-2020 Task 6 : A Supervised Approach for Definition Extraction<span class=acl-fixed-case>ACNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: A Supervised Approach for Definition Extraction</a></strong><br><a href=/people/f/fabien-caspani/>Fabien Caspani</a>
|
<a href=/people/p/pirashanth-ratnamogan/>Pirashanth Ratnamogan</a>
|
<a href=/people/m/mathis-linger/>Mathis Linger</a>
|
<a href=/people/m/mhamed-hajaiej/>Mhamed Hajaiej</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--58><div class="card-body p-3 small">We describe our contribution to two of the subtasks of SemEval 2020 Task 6, DeftEval : Extracting term-definition pairs in free text. The system for Subtask 1 : Sentence Classification is based on a transformer architecture where we use <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to fine-tune a pretrained model on the downstream task, and the one for Subtask 3 : Relation Classification uses a Random Forest classifier with handcrafted dedicated features. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> respectively achieve 0.830 and 0.994 <a href=https://en.wikipedia.org/wiki/Standard_score>F1-scores</a> on the <a href=https://en.wikipedia.org/wiki/Standard_score>official test set</a>, and we believe that the insights derived from our study are potentially relevant to help advance the research on definition extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.60/>CN-HIT-IT.NLP at SemEval-2020 Task 4 : Enhanced Language Representation with Multiple Knowledge Triples<span class=acl-fixed-case>CN</span>-<span class=acl-fixed-case>HIT</span>-<span class=acl-fixed-case>IT</span>.<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Enhanced Language Representation with Multiple Knowledge Triples</a></strong><br><a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/j/jiaxuan-lin/>Jiaxuan Lin</a>
|
<a href=/people/y/yang-fan/>Yang Fan</a>
|
<a href=/people/p/peng-jin/>Peng Jin</a>
|
<a href=/people/y/yuanchao-liu/>Yuanchao Liu</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--60><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in the SemEval-2020 task 4 : Commonsense Validation and Explanation. For this task, it is obvious that external knowledge, such as <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge graph</a>, can help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> understand commonsense in natural language statements. But how to select the right triples for statements remains unsolved, so how to reduce the interference of irrelevant triples on <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance is a research focus. This paper adopt a modified K-BERT as the language encoder, to enhance language representation through triples from <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is better than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without external knowledge, and is slightly better than the original K-BERT. We got an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.97 in subtaskA, ranking 1/45, and got an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.948, ranking 2/35.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.62/>CS-NLP Team at SemEval-2020 Task 4 : Evaluation of State-of-the-art NLP Deep Learning Architectures on Commonsense Reasoning Task<span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>NLP</span> Team at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Evaluation of State-of-the-art <span class=acl-fixed-case>NLP</span> Deep Learning Architectures on Commonsense Reasoning Task</a></strong><br><a href=/people/s/sirwe-saeedi/>Sirwe Saeedi</a>
|
<a href=/people/a/aliakbar-panahi/>Aliakbar Panahi</a>
|
<a href=/people/s/seyran-saeedi/>Seyran Saeedi</a>
|
<a href=/people/a/alvis-c-fong/>Alvis C Fong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--62><div class="card-body p-3 small">In this paper, we investigate a commonsense inference task that unifies <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We describe our attempt at SemEval-2020 Task 4 competition : Commonsense Validation and Explanation (ComVE) challenge. We discuss several state-of-the-art deep learning architectures for this challenge. Our system uses prepared labeled textual datasets that were manually curated for three different natural language inference subtasks. The goal of the first subtask is to test whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can distinguish between natural language statements that make sense and those that do not make sense. We compare the performance of several <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and fine-tuned classifiers. Then, we propose a method inspired by <a href=https://en.wikipedia.org/wiki/Question_answering>question / answering tasks</a> to treat a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification problem</a> as a <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice question task</a> to boost the performance of our experimental results (96.06 %), which is significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. For the second subtask, which is to select the reason why a statement does not make sense, we stand within the first six teams (93.7 %) among 27 participants with very competitive results. Our result for last subtask of generating reason against the nonsense statement shows many potentials for future researches as we applied the most powerful generative model of language (GPT-2) with 6.1732 BLEU score among first four teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.65/>JBNU at SemEval-2020 Task 4 : BERT and UniLM for Commonsense Validation and Explanation<span class=acl-fixed-case>JBNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>LM</span> for Commonsense Validation and Explanation</a></strong><br><a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a>
|
<a href=/people/j/jong-hyeon-lee/>Jong-Hyeon Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--65><div class="card-body p-3 small">This paper presents our contributions to the SemEval-2020 Task 4 Commonsense Validation and Explanation (ComVE) and includes the experimental results of the two Subtasks B and C of the SemEval-2020 Task 4. Our systems rely on pre-trained language models, i.e., BERT (including its variants) and UniLM, and rank 10th and 7th among 27 and 17 systems on Subtasks B and C, respectively. We analyze the commonsense ability of the existing pretrained language models by testing them on the SemEval-2020 Task 4 ComVE dataset, specifically for Subtasks B and C, the explanation subtasks with multi-choice and sentence generation, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.67" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.67/>KaLM at SemEval-2020 Task 4 : Knowledge-aware Language Models for Comprehension and Generation<span class=acl-fixed-case>K</span>a<span class=acl-fixed-case>LM</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation</a></strong><br><a href=/people/j/jiajing-wan/>Jiajing Wan</a>
|
<a href=/people/x/xinting-huang/>Xinting Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--67><div class="card-body p-3 small">This paper presents our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in SemEval 2020 Task 4 : Commonsense Validation and <a href=https://en.wikipedia.org/wiki/Explanation>Explanation</a>. We propose a novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks. The results show that our evidence-searching approach improves <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on commonsense explanation task. Our <a href=https://en.wikipedia.org/wiki/Team>team</a> ranks 2nd in subtask C according to human evaluation score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.70/>LMVE at SemEval-2020 Task 4 : Commonsense Validation and Explanation Using Pretraining Language Model<span class=acl-fixed-case>LMVE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Commonsense Validation and Explanation Using Pretraining Language Model</a></strong><br><a href=/people/s/shilei-liu/>Shilei Liu</a>
|
<a href=/people/y/yu-guo/>Yu Guo</a>
|
<a href=/people/b/bochao-li/>BoChao Li</a>
|
<a href=/people/f/feiliang-ren/>Feiliang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--70><div class="card-body p-3 small">This paper introduces our <a href=https://en.wikipedia.org/wiki/System>system</a> for commonsense validation and <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. For Sen-Making task, we use a novel pretraining language model based architecture to pick out one of the two given statements that is againstcommon sense. For Explanation task, we use a hint sentence mechanism to improve the performance greatly. In addition, we propose a subtask level transfer learning to share information between subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.73/>SSN-NLP at SemEval-2020 Task 4 : Text Classification and Generation on Common Sense Context Using Neural Networks<span class=acl-fixed-case>SSN</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Text Classification and Generation on Common Sense Context Using Neural Networks</a></strong><br><a href=/people/r/rishivardhan-k/>Rishivardhan K.</a>
|
<a href=/people/k/kayalvizhi-s/>Kayalvizhi S</a>
|
<a href=/people/t/thenmozhi-d/>Thenmozhi D.</a>
|
<a href=/people/r/raghav-r/>Raghav R.</a>
|
<a href=/people/k/kshitij-sharma/>Kshitij Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--73><div class="card-body p-3 small">Common sense validation deals with testing whether a <a href=https://en.wikipedia.org/wiki/System>system</a> can differentiate natural language statements that make sense from those that do not make sense. This paper describes the our approach to solve this challenge. For common sense validation with <a href=https://en.wikipedia.org/wiki/Multiple_choice>multi choice</a>, we propose a stacking based approach to classify sentences that are more favourable in terms of common sense to the particular statement. We have used majority voting classifier methodology amongst three models such as Bidirectional Encoder Representations from Transformers (BERT), Micro Text Classification (Micro TC) and XLNet. For sentence generation, we used Neural Machine Translation (NMT) model to generate explanatory sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.77/>UAICS at SemEval-2020 Task 4 : Using a Bidirectional Transformer for Task a<span class=acl-fixed-case>UAICS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Using a Bidirectional Transformer for Task a</a></strong><br><a href=/people/c/ciprian-gabriel-cusmuliuc/>Ciprian-Gabriel Cusmuliuc</a>
|
<a href=/people/l/lucia-georgiana-coca/>Lucia-Georgiana Coca</a>
|
<a href=/people/a/adrian-iftene/>Adrian Iftene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--77><div class="card-body p-3 small">Commonsense Validation and Explanation has been a difficult task for machines since the dawn of <a href=https://en.wikipedia.org/wiki/Computing>computing</a>. Although very trivial to humans it poses a high <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> for machines due to the necessity of <a href=https://en.wikipedia.org/wiki/Inference>inference</a> over a pre-existing knowledge base. In order to try and solve this problem the SemEval 2020 Task 4-Commonsense Validation and Explanation (ComVE) aims to evaluate systems capable of multiple stages of ComVE. The challenge includes 3 <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (A, B and C), each with it&#8217;s own requirements. Our team participated only in task A which required selecting the statement that made the least sense. We choose to use a bidirectional transformer in order to solve the challenge, this paper presents the details of our method, runs and result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.79/>Warren at SemEval-2020 Task 4 : ALBERT and Multi-Task Learning for Commonsense Validation<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: <span class=acl-fixed-case>ALBERT</span> and Multi-Task Learning for Commonsense Validation</a></strong><br><a href=/people/y/yuhang-wu/>Yuhang Wu</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--79><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> in subtask A of SemEval 2020 Shared Task 4. We propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning model</a> based on MTL(Multi-Task Learning) to enhance the prediction ability of commonsense validation. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the single-task text classification model. We combine MTL and ALBERT pretrain model to achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.904 and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is ranked 16th on the final leader board of the competition among the 45 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.83/>ETHAN at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing<span class=acl-fixed-case>ETHAN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing</a></strong><br><a href=/people/l/len-yabloko/>Len Yabloko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--83><div class="card-body p-3 small">I present ETHAN : Experimental Testing of Hybrid AI Node implemented entirely on free cloud computing infrastructure. The ultimate goal of this research is to create modular reusable hybrid neuro-symbolic architecture for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a>. As a test case I model natural language comprehension of causal relations from open domain text corpus that combines semi-supervised language model (Huggingface Transformers) with constituency and dependency parsers (Allen Institute for Artificial Intelligence.)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.84/>Ferryman as SemEval-2020 Task 5 : Optimized BERT for Detecting Counterfactuals<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Optimized <span class=acl-fixed-case>BERT</span> for Detecting Counterfactuals</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/y/yan-zhuang/>Yan Zhuang</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/f/feng-hong/>Feng Hong</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--84><div class="card-body p-3 small">The main purpose of this article is to state the effect of using different <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> and <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> for <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual determination</a> and detection of causal knowledge. Nowadays, counterfactual reasoning has been widely used in various fields. In the realm of natural language process(NLP), counterfactual reasoning has huge potential to improve the correctness of a sentence. In the shared Task 5 of detecting counterfactual in SemEval 2020, we pre-process the officially given dataset according to case conversion, extract stem and abbreviation replacement. We use last-5 bidirectional encoder representation from bidirectional encoder representation from transformer (BERT)and term frequencyinverse document frequency (TF-IDF) vectorizer for counterfactual detection. Meanwhile, multi-sample dropout and <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross validation</a> are used to improve versatility and prevent problems such as poor generosity caused by <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. Finally, our team Ferryman ranked the 8th place in the sub-task 1 of this competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.86/>Lee at SemEval-2020 Task 5 : ALBERT Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: <span class=acl-fixed-case>ALBERT</span> Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/y/yuhang-wu/>Yuhang Wu</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/h/haiyan-ding/>Haiyan Ding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--86><div class="card-body p-3 small">This article describes the system submitted to SemEval 2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. In this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we only participate in the subtask A which is detecting counterfactual statements. In order to solve this sub-task, first of all, because of the problem of data balance, we use the undersampling and oversampling methods to process the data set. Second, we used the ALBERT model and the maximum ensemble method based on the ALBERT model. Our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> achieved a F1 score of 0.85 in subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.87/>NLU-Co at SemEval-2020 Task 5 : NLU / SVM Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data<span class=acl-fixed-case>NLU</span>-Co at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: <span class=acl-fixed-case>NLU</span>/<span class=acl-fixed-case>SVM</span> Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data</a></strong><br><a href=/people/e/elvis-mboning-tchiaze/>Elvis Mboning Tchiaze</a>
|
<a href=/people/d/damien-nouvel/>Damien Nouvel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--87><div class="card-body p-3 small">In this article, we try to solve the problem of classification of counterfactual statements and extraction of antecedents / consequences in raw data, by mobilizing on one hand Support vector machine (SVMs) and on the other hand Natural Language Understanding (NLU) infrastructures available on the market for conversational agents. Our experiments allowed us to test different <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipelines</a> of two known <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> (Snips NLU and Rasa NLU). The results obtained show that a Rasa NLU pipeline, built with a well-preprocessed dataset and tuned algorithms, allows to model accurately the structure of a <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual event</a>, in order to facilitate the identification and the extraction of its components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.89/>YNU-oxz at SemEval-2020 Task 5 : Detecting Counterfactuals Based on Ordered Neurons LSTM and Hierarchical Attention Network<span class=acl-fixed-case>YNU</span>-oxz at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Detecting Counterfactuals Based on Ordered Neurons <span class=acl-fixed-case>LSTM</span> and Hierarchical Attention Network</a></strong><br><a href=/people/x/xiaozhi-ou/>Xiaozhi Ou</a>
|
<a href=/people/s/shengyan-liu/>Shengyan Liu</a>
|
<a href=/people/h/hongling-li/>Hongling Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--89><div class="card-body p-3 small">This paper describes the system and results of our team&#8217;s participation in SemEval-2020 Task5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals, which aims to simulate counterfactual semantics and reasoning in natural language. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> contains two subtasks : Subtask1Detecting counterfactual statements and Subtask2Detecting antecedent and consequence. We only participated in Subtask1, aiming to determine whether a given sentence is counterfactual. In order to solve this task, we proposed a system based on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and used Pooling operation for dimensionality reduction. Finally, we used the K-fold approach as the ensemble method. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.7040 in Subtask1 (Ranked 16/27).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.90/>BERTatDE at SemEval-2020 Task 6 : Extracting Term-definition Pairs in Free Text Using Pre-trained Model<span class=acl-fixed-case>BERT</span>at<span class=acl-fixed-case>DE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Extracting Term-definition Pairs in Free Text Using Pre-trained Model</a></strong><br><a href=/people/h/huihui-zhang/>Huihui Zhang</a>
|
<a href=/people/f/feiliang-ren/>Feiliang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--90><div class="card-body p-3 small">Definition extraction is an important task in Nature Language Processing, and it is used to identify the terms and definitions related to terms. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> contains sentence classification task (i.e., classify whether it contains definition) and sequence labeling task (i.e., find the boundary of terms and definitions). The paper describes our system BERTatDE1 in sentence classification task (subtask 1) and sequence labeling task (subtask 2) in the definition extraction (SemEval-2020 Task 6). We use BERT to solve the multi-domain problems including the uncertainty of term boundary that is, different areas have different ways to definite the domain related terms. We use <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, BiLSTM and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> in subtask 1 and our best result achieved 79.71 % in F1 and the eighteenth place in subtask 1. For the subtask 2, we use BERT, BiLSTM and CRF to <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, and achieve 40.73 % in Macro-averaged F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.92/>Defx at SemEval-2020 Task 6 : Joint Extraction of Concepts and Relations for Definition Extraction<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Joint Extraction of Concepts and Relations for Definition Extraction</a></strong><br><a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a>
|
<a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--92><div class="card-body p-3 small">Definition Extraction systems are a valuable knowledge source for both humans and algorithms. In this paper we describe our submissions to the DeftEval shared task (SemEval-2020 Task 6), which is evaluated on an English textbook corpus. We provide a detailed explanation of our <a href=https://en.wikipedia.org/wiki/System>system</a> for the joint extraction of definition concepts and the relations among them. Furthermore we provide an ablation study of our model variations and describe the results of an error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.97" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.97/>UPB at SemEval-2020 Task 6 : Pretrained Language Models for Definition Extraction<span class=acl-fixed-case>UPB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Pretrained Language Models for Definition Extraction</a></strong><br><a href=/people/a/andrei-marius-avram/>Andrei-Marius Avram</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/c/costin-chiru/>Costin Chiru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--97><div class="card-body p-3 small">This work presents our contribution in the context of the 6th task of SemEval-2020 : Extracting Definitions from Free Text in Textbooks (DeftEval). This competition consists of three subtasks with different levels of granularity : (1) classification of sentences as definitional or non-definitional, (2) labeling of definitional sentences, and (3) relation classification. We use various pretrained language models (i.e., BERT, XLNet, RoBERTa, SciBERT, and ALBERT) to solve each of the three subtasks of the competition. Specifically, for each language model variant, we experiment by both freezing its weights and fine-tuning them. We also explore a multi-task architecture that was trained to jointly predict the outputs for the second and the third subtasks. Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> evaluated on the DeftEval dataset obtains the 32nd place for the first subtask and the 37th place for the second subtask. The code is available for further research at :<url>https://github.com/avramandrei/DeftEval</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.104/>Buhscitu at SemEval-2020 Task 7 : Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases</a></strong><br><a href=/people/k/kristian-norgaard-jensen/>Kristian Nørgaard Jensen</a>
|
<a href=/people/n/nicolaj-filrup-rasmussen/>Nicolaj Filrup Rasmussen</a>
|
<a href=/people/t/thai-wang/>Thai Wang</a>
|
<a href=/people/m/marco-placenti/>Marco Placenti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--104><div class="card-body p-3 small">This paper describes a system that aims at assessing humour intensity in edited news headlines as part of the 7th task of SemEval-2020 on Humor, <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>Emphasis</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>Sentiment</a>. Various factors need to be accounted for in order to assess the <a href=https://en.wikipedia.org/wiki/Funniness>funniness</a> of an edited headline. We propose an architecture that uses hand-crafted features, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to understand <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, and combines them in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. In general, automatic humour assessment remains a difficult task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.105/>Hasyarasa at SemEval-2020 Task 7 : Quantifying Humor as Departure from Expectedness<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Quantifying Humor as Departure from Expectedness</a></strong><br><a href=/people/r/ravi-theja-desetty/>Ravi Theja Desetty</a>
|
<a href=/people/r/ranit-chatterjee/>Ranit Chatterjee</a>
|
<a href=/people/s/smita-ghaisas/>Smita Ghaisas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--105><div class="card-body p-3 small">This paper describes our system submission Hasyarasa for the SemEval-2020 Task-7 : Assessing Humor in Edited News Headlines. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> has two subtasks. The goal of Subtask 1 is to predict the mean funniness of the edited headline given the original and the edited headline. In Subtask 2, given two edits on the original headline, the goal is to predict the funnier of the two. We observed that the departure from expected state/ actions of situations/ individuals is the cause of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in the edited headlines. We propose two novel features : Contextual Semantic Distance and Contextual Neighborhood Distance to estimate this departure and thus capture the contextual absurdity and hence the <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in the edited headlines. We have used these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> together with a Bi-LSTM Attention based model and have achieved 0.53310 RMSE for Subtask 1 and 60.19 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.110/>YNU-HPCC at SemEval-2020 Task 7 : Using an Ensemble BiGRU Model to Evaluate the Humor of Edited News Titles<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Using an Ensemble <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>GRU</span> Model to Evaluate the Humor of Edited News Titles</a></strong><br><a href=/people/j/joseph-tomasulo/>Joseph Tomasulo</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--110><div class="card-body p-3 small">This paper describes an ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>, Elmo, and Word2Vec to encode these titles then pass them to a bidirectional gated recurrent unit (BiGRU) with attention. Finally, we used <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a> on the concatenation of the results of the different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to make predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.113/>NLP_UIOWA at SemEval-2020 Task 8 : You’re Not the Only One Cursed with Knowledge-Multi Branch Model Memotion Analysis<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>UIOWA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: You’re Not the Only One Cursed with Knowledge - Multi Branch Model Memotion Analysis</a></strong><br><a href=/people/i/ingroj-shrestha/>Ingroj Shrestha</a>
|
<a href=/people/j/jonathan-rusert/>Jonathan Rusert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--113><div class="card-body p-3 small">We propose hybrid models (HybridE and HybridW) for meme analysis (SemEval 2020 Task 8), which involves sentiment classification (Subtask A), humor classification (Subtask B), and scale of semantic classes (Subtask C). The hybrid model consists of BLSTM and CNN for text and image processing respectively. HybridE provides equal weight to BLSTM and CNN performance, while HybridW provides weightage based on the performance of BLSTM and CNN on a validation set. The performances (macro F1) of our hybrid model on Subtask A are 0.329 (HybridE), 0.328 (HybridW), on Subtask B are 0.507 (HybridE), 0.512 (HybridW), and on Subtask C are 0.309 (HybridE), 0.311 (HybridW).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.117/>CS-Embed at SemEval-2020 Task 9 : The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis<span class=acl-fixed-case>CS</span>-Embed at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis</a></strong><br><a href=/people/f/frances-adriana-laureano-de-leon/>Frances Adriana Laureano De Leon</a>
|
<a href=/people/f/florimond-gueniat/>Florimond Guéniat</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--117><div class="card-body p-3 small">The growing popularity and applications of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of social media posts has naturally led to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis of posts</a> written in multiple languages, a practice known as <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. While recent research into code-switched posts has focused on the use of multilingual word embeddings, these embeddings were not trained on code-switched data. In this work, we present <a href=https://en.wikipedia.org/wiki/Word_embedding>word-embeddings</a> trained on <a href=https://en.wikipedia.org/wiki/Code-switching>code-switched tweets</a>, specifically those that make use of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and English, known as <a href=https://en.wikipedia.org/wiki/Spanglish>Spanglish</a>. We explore the embedding space to discover how they capture the meanings of words in both languages. We test the effectiveness of these embeddings by participating in SemEval 2020 Task 9 : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> on Code-Mixed Social Media Text. We utilised them to train a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classifier</a> that achieves an F-1 score of 0.722. This is higher than the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baseline</a> for the <a href=https://en.wikipedia.org/wiki/Surveying>competition</a> of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baseline</a>.<i>Sentiment Analysis on Code-Mixed Social Media Text</i>. We utilised them to train a sentiment classifier that achieves an F-1 score of 0.722. This is higher than the baseline for the competition of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.118/>FII-UAIC at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using CNN<span class=acl-fixed-case>FII</span>-<span class=acl-fixed-case>UAIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using <span class=acl-fixed-case>CNN</span></a></strong><br><a href=/people/l/lavinia-aparaschivei/>Lavinia Aparaschivei</a>
|
<a href=/people/a/andrei-palihovici/>Andrei Palihovici</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gîfu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--118><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> for Code-Mixed Social Media Text task at the SemEval 2020 competition focuses on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in code-mixed social media text, specifically, on the combination of English with Spanish (Spanglish) and Hindi (Hinglish). In this paper, we present a system able to classify <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, from Spanish and English languages, into positive, negative and neutral. Firstly, we built a classifier able to provide corresponding <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment labels</a>. Besides the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment labels</a>, we provide the <a href=https://en.wikipedia.org/wiki/Linguistic_description>language labels</a> at the <a href=https://en.wikipedia.org/wiki/Linguistic_description>word level</a>. Secondly, we generate a word-level representation, using Convolutional Neural Network (CNN) architecture. Our solution indicates promising results for the Sentimix Spanglish-English task (0.744), the team, Lavinia_Ap, occupied the 9th place. However, for the Sentimix Hindi-English task (0.324) the results have to be improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.123/>NLP-CIC at SemEval-2020 Task 9 : Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>CIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier</a></strong><br><a href=/people/j/jason-angel/>Jason Angel</a>
|
<a href=/people/s/segun-taofeek-aroyehun/>Segun Taofeek Aroyehun</a>
|
<a href=/people/a/antonio-tamayo/>Antonio Tamayo</a>
|
<a href=/people/a/alexander-gelbukh/>Alexander Gelbukh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--123><div class="card-body p-3 small">Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This phenomenon presents a challenge for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In this paper, we use a standard <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network model</a> to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of 0:71 on test set on the competition. We analyze our best model capabilities and perform <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> to expose important difficulties for classifying sentiment in a code-switching setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.124/>Palomino-Ochoa at SemEval-2020 Task 9 : Robust System Based on Transformer for Code-Mixed Sentiment Classification<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Robust System Based on Transformer for Code-Mixed Sentiment Classification</a></strong><br><a href=/people/d/daniel-palomino/>Daniel Palomino</a>
|
<a href=/people/j/jose-ochoa-luna/>José Ochoa-Luna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--124><div class="card-body p-3 small">We present a transfer learning system to perform a mixed Spanish-English sentiment classification task. Our proposal uses the state-of-the-art language model BERT and embed it within a ULMFiT transfer learning pipeline. This combination allows us to predict the polarity detection of code-mixed (English-Spanish) tweets. Thus, among 29 submitted systems, our approach (referred to as dplominop) is ranked 4th on the Sentimix Spanglish test set of SemEval 2020 Task 9. In fact, our <a href=https://en.wikipedia.org/wiki/System>system</a> yields the weighted-F1 score value of 0.755 which can be easily reproduced the source code and implementation details are made available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.125/>ULD@NUIG at SemEval-2020 Task 9 : Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text<span class=acl-fixed-case>ULD</span>@<span class=acl-fixed-case>NUIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text</a></strong><br><a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--125><div class="card-body p-3 small">Code mixing is a common phenomena in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual societies</a> where people switch from one language to another for various reasons. Recent advances in public communication over different <a href=https://en.wikipedia.org/wiki/Social_media>social media sites</a> have led to an increase in the frequency of code-mixed usage in <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. In this paper, we present the Generative Morphemes with Attention (GenMA) Model sentiment analysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to predict the sentiments of the given English-Hindi code-mixed tweets without using word-level language tags instead inferring this automatically using a <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological model</a>. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name koustava on the Sentimix Hindi English page.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.129/>ECNU at SemEval-2020 Task 7 : Assessing Humor in Edited News Headlines Using BiLSTM with Attention<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Humor in Edited News Headlines Using <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> with Attention</a></strong><br><a href=/people/t/tiantian-zhang/>Tiantian Zhang</a>
|
<a href=/people/z/zhixuan-chen/>Zhixuan Chen</a>
|
<a href=/people/m/man-lan/>Man Lan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--129><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2020 Task 7 : Assessing Humor in Edited News Headlines. We participated in all subtasks, in which the main goal is to predict the mean funniness of the edited headline given the original and the edited headline. Our system involves two similar sub-networks, which generate <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.130/>ELMo-NB at SemEval-2020 Task 7 : Assessing Sense of Humor in EditedNews Headlines Using ELMo and NB<span class=acl-fixed-case>ELM</span>o-<span class=acl-fixed-case>NB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Sense of Humor in <span class=acl-fixed-case>E</span>dited<span class=acl-fixed-case>N</span>ews Headlines Using <span class=acl-fixed-case>ELM</span>o and <span class=acl-fixed-case>NB</span></a></strong><br><a href=/people/e/enas-khwaileh/>Enas Khwaileh</a>
|
<a href=/people/m/muntaha-a-al-asad/>Muntaha A. Al-As’ad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--130><div class="card-body p-3 small">Our approach is constructed to improve on a couple of aspects ; preprocessing with an emphasis on humor sense detection, using embeddings from state-of-the-art language model(Elmo), and ensembling the results came up with using machine learning model Na ve Bayes(NB) with a deep learning pre-trained models. Elmo-NB participation has scored (0.5642) on the competition leader board, where results were measured by Root Mean Squared Error (RMSE).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.131/>Ferryman at SemEval-2020 Task 7 : Ensemble Model for Assessing Humor in Edited News Headlines<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Ensemble Model for Assessing Humor in Edited News Headlines</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/j/jipeng-li/>Jipeng Li</a>
|
<a href=/people/c/chenghao-huang/>Chenghao Huang</a>
|
<a href=/people/w/wei-bai/>Wei Bai</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--131><div class="card-body p-3 small">Natural language processing (NLP) has been applied to various <a href=https://en.wikipedia.org/wiki/Field_(computer_science)>fields</a> including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the shared task of assessing the funniness of edited news headlines, which is a part of the SemEval 2020 competition, we preprocess datasets by replacing abbreviation, stemming words, then merge three models including Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bidirectional Encoder Representation from Transformer (BERT) by taking the average to perform the best. Our team Ferryman wins the 9th place in Sub-task 1 of Task 7-Regression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.132/>Funny3 at SemEval-2020 Task 7 : Humor Detection of Edited Headlines with LSTM and TFIDF Neural Network System<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Humor Detection of Edited Headlines with <span class=acl-fixed-case>LSTM</span> and <span class=acl-fixed-case>TFIDF</span> Neural Network System</a></strong><br><a href=/people/x/xuefeng-luo/>Xuefeng Luo</a>
|
<a href=/people/k/kuan-tang/>Kuan Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--132><div class="card-body p-3 small">This paper presents a neural network system where we participate in the first task of SemEval-2020 shared task 7 Assessing the Funniness of Edited News Headlines. Our target is to create to <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that can predict the funniness of edited headlines. We build our model using a combination of LSTM and TF-IDF, then a feed-forward neural network. The <a href=https://en.wikipedia.org/wiki/System>system</a> manages to slightly improve RSME scores regarding our mean score baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.133/>HumorAAC at SemEval-2020 Task 7 : Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions<span class=acl-fixed-case>H</span>umor<span class=acl-fixed-case>AAC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions</a></strong><br><a href=/people/a/anna-katharina-dick/>Anna-Katharina Dick</a>
|
<a href=/people/c/charlotte-weirich/>Charlotte Weirich</a>
|
<a href=/people/a/alla-kutkina/>Alla Kutkina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--133><div class="card-body p-3 small">In this paper we describe our contribution to the Semeval-2020 Humor Assessment task. We essentially use three different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are passed into a <a href=https://en.wikipedia.org/wiki/Ridge_regression>ridge regression</a> to determine a funniness score for an edited news headline : statistical, count-based features, semantic features and contextual information. For deciding which one of two given edited headlines is funnier, we additionally use <a href=https://en.wikipedia.org/wiki/Score_(statistics)>scoring information</a> and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. Our work was mostly concentrated on investigating <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, rather than improving <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> based on pre-trained language models. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> is task-specific, lightweight and performs above the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>majority baseline</a>. Our experiments indicate that <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> related to socio-cultural context, in our case mentions of Donald Trump, generally perform better than context-independent features like headline length.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.136/>MLEngineer at SemEval-2020 Task 7 : BERT-Flair Based Humor Detection Model (BFHumor)<span class=acl-fixed-case>MLE</span>ngineer at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: <span class=acl-fixed-case>BERT</span>-Flair Based Humor Detection Model (<span class=acl-fixed-case>BFH</span>umor)</a></strong><br><a href=/people/f/fara-shatnawi/>Fara Shatnawi</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/m/mahmoud-hammad/>Mahmoud Hammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--136><div class="card-body p-3 small">Task 7, Assessing the Funniness of Edited News Headlines, in the International Workshop SemEval2020 introduces two sub-tasks to predict the funniness values of edited news headlines from the Reddit website. This paper proposes the BFHumor model of the MLEngineer team that participates in both sub-tasks in this competition. The BFHumor&#8217;s model is defined as a BERT-Flair based humor detection model that is a combination of different pre-trained models with various Natural Language Processing (NLP) techniques. The Bidirectional Encoder Representations from Transformers (BERT) regressor is considered the primary pre-trained model in our approach, whereas Flair is the main NLP library. It is worth mentioning that the BFHumor model has been ranked 4th in sub-task1 with a root mean square error (RMSE) value of 0.51966, and it is 0.02 away from the first ranked model. Also, the team is ranked 12th in the sub-task2 with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.62291, which is 0.05 away from the top-ranked model. Our results indicate that the BFHumor model is one of the top models for detecting humor in the text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.140/>UTFPR at SemEval-2020 Task 7 : Using Co-occurrence Frequencies to Capture Unexpectedness<span class=acl-fixed-case>UTFPR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Using Co-occurrence Frequencies to Capture Unexpectedness</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Henrique Paetzold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--140><div class="card-body p-3 small">We describe the UTFPR system for SemEval-2020&#8217;s Task 7 : Assessing Humor in Edited News Headlines. Ours is a minimalist unsupervised system that uses word co-occurrence frequencies from large corpora to capture unexpectedness as a mean to capture funniness. Our <a href=https://en.wikipedia.org/wiki/System>system</a> placed 22nd on the shared task&#8217;s Task 2. We found that our approach requires more text than we used to perform reliably, and that unexpectedness alone is not sufficient to gauge funniness for humorous content that targets a diverse target audience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.141/>WUY at SemEval-2020 Task 7 : Combining BERT and Naive Bayes-SVM for Humor Assessment in Edited News Headlines<span class=acl-fixed-case>WUY</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Combining <span class=acl-fixed-case>BERT</span> and Naive <span class=acl-fixed-case>B</span>ayes-<span class=acl-fixed-case>SVM</span> for Humor Assessment in Edited News Headlines</a></strong><br><a href=/people/c/cheng-zhang/>Cheng Zhang</a>
|
<a href=/people/h/hayato-yamana/>Hayato Yamana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--141><div class="card-body p-3 small">This paper describes our participation in SemEval 2020 Task 7 on assessment of humor in edited news headlines, which includes two subtasks, estimating the humor of micro-editd news headlines (subtask A) and predicting the more humorous of the two edited headlines (subtask B). To address these tasks, we propose two <a href=https://en.wikipedia.org/wiki/System>systems</a>. The first system adopts a regression-based fine-tuned single-sequence bidirectional encoder representations from transformers (BERT) model with easy data augmentation (EDA), called BERT+EDA. The second system adopts a hybrid of a regression-based fine-tuned sequence-pair BERT model and a combined <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a> and support vector machine (SVM) model estimated on term frequencyinverse document frequency (TFIDF) features, called BERT+NB-SVM. In this case, no additional training datasets were used, and the BERT+NB-SVM model outperformed BERT+EDA. The official root-mean-square deviation (RMSE) score for subtask A is 0.57369 and ranks 31st out of 48, whereas the best RMSE of BERT+NB-SVM is 0.52429, ranking 7th. For subtask B, we simply use a sequence-pair BERT model, the official accuracy of which is 0.53196 and ranks 25th out of 32.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.144/>BERT at SemEval-2020 Task 8 : Using BERT to Analyse Meme Emotions<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Using <span class=acl-fixed-case>BERT</span> to Analyse Meme Emotions</a></strong><br><a href=/people/a/adithya-avvaru/>Adithya Avvaru</a>
|
<a href=/people/s/sanath-vobilisetty/>Sanath Vobilisetty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--144><div class="card-body p-3 small">Sentiment analysis, being one of the most sought after research problems within Natural Language Processing (NLP) researchers. The range of problems being addressed by <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> is increasing. Till now, most of the research focuses on predicting sentiment, or sentiment categories like <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Humour>humor</a>, offense and motivation on text data. But, there is very limited research that is focusing on predicting or analyzing the sentiment of <a href=https://en.wikipedia.org/wiki/Internet_meme>internet memes</a>. We try to address this problem as part of Task 8 of SemEval 2020 : Memotion Analysis. We have participated in all the three tasks under Memotion Analysis. Our system built using state-of-the-art Transformer-based pre-trained Bidirectional Encoder Representations from Transformers (BERT) performed better compared to baseline models for the two tasks A and C and performed close to the baseline model for task B. In this paper, we present the data used, steps used by us for data cleaning and preparation, the fine-tuning process for BERT based model and finally predict the sentiment or sentiment categories. We found that the sequence models like Long Short Term Memory(LSTM) and its variants performed below par in predicting the sentiments. We also performed a comparative analysis with other Transformer based models like DistilBERT and XLNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.146/>CSECU_KDE_MA at SemEval-2020 Task 8 : A Neural Attention Model for Memotion Analysis<span class=acl-fixed-case>CSECU</span>_<span class=acl-fixed-case>KDE</span>_<span class=acl-fixed-case>MA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: A Neural Attention Model for Memotion Analysis</a></strong><br><a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a>
|
<a href=/people/u/umme-aymun-siddiqua/>Umme Aymun Siddiqua</a>
|
<a href=/people/m/masaki-aono/>Masaki Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--146><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Meme>meme</a> is a pictorial representation of an idea or theme. In the age of emerging volume of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are spreading rapidly from person to person and becoming a trending ways of opinion expression. However, due to the multimodal characteristics of meme contents, detecting and analyzing the underlying emotion of a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> is a formidable task. In this paper, we present our approach for detecting the emotion of a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> defined in the SemEval-2020 Task 8. Our team CSECU_KDE_MA employs an attention-based neural network model to tackle the problem. Upon extracting the text contents from a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> using an optical character reader (OCR), we represent it using the distributed representation of words. Next, we perform the <a href=https://en.wikipedia.org/wiki/Convolution>convolution</a> based on multiple kernel sizes to obtain the higher-level feature sequences. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sequences</a> are then fed into the attentive time-distributed bidirectional LSTM model to learn the long-term dependencies effectively. Experimental results show that our proposed neural model obtained competitive performance among the participants&#8217; systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.149/>Hitachi at SemEval-2020 Task 8 : Simple but Effective Modality Ensemble for Meme Emotion Recognition<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition</a></strong><br><a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/s/shota-horiguchi/>Shota Horiguchi</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--149><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking services</a> often share their emotions via multi-modal content, usually <a href=https://en.wikipedia.org/wiki/Image>images</a> paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> of so-called <a href=https://en.wikipedia.org/wiki/Internet_meme>internet memes</a>. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained <a href=https://en.wikipedia.org/wiki/Visual_system>visual models</a> (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble methods</a> to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.154/>Memebusters at SemEval-2020 Task 8 : Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/i/ilanthenral-kandasamy/>Ilanthenral Kandasamy</a>
|
<a href=/people/w/w-b-vasantha/>W.b. Vasantha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--154><div class="card-body p-3 small">In this paper, we describe our deep learning system used for SemEval 2020 Task 8 : Memotion analysis. We participated in all the subtasks i.e Subtask A : Sentiment classification, Subtask B : Humor classification, and Subtask C : Scales of semantic classes. Similar multimodal architecture was used for each subtask. The proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> makes use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for images and text feature extraction. The extracted <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are then fused together using stacked bidirectional Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) model with attention mechanism for final predictions. We also propose a single model for predicting semantic classes (Subtask B) as well as their scales (Subtask C) by branching the final output of the post LSTM dense layers. Our model was ranked 5 in Subtask B and ranked 8 in Subtask C and performed nicely in Subtask A on the leader board. Our system makes use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and fusion of image and text features for predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.157/>SIS@IIITH at SemEval-2020 Task 8 : An Overview of Simple Text Classification Methods for Meme Analysis<span class=acl-fixed-case>SIS</span>@<span class=acl-fixed-case>IIITH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: An Overview of Simple Text Classification Methods for Meme Analysis</a></strong><br><a href=/people/s/sravani-boinepelli/>Sravani Boinepelli</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--157><div class="card-body p-3 small">Memes are steadily taking over the feeds of the public on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. There is always the threat of malicious users on the internet posting offensive content, even through <a href=https://en.wikipedia.org/wiki/Meme>memes</a>. Hence, the automatic detection of offensive images / memes is imperative along with detection of offensive text. However, this is a much more complex task as it involves both <a href=https://en.wikipedia.org/wiki/Sensory_cue>visual cues</a> as well as <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>cultural / context knowledge</a>. This paper describes our approach to the task of SemEval-2020 Task 8 : Memotion Analysis. We chose to participate only in Task A which dealt with Sentiment Classification, which we formulated as a text classification problem. Through our experiments, we explored multiple training models to evaluate the performance of simple text classification algorithms on the raw text obtained after running <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> on meme images. Our submitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.69 % and exceeded the existing baseline&#8217;s Macro F1 score by 8 % on the official test dataset. Apart from describing our official submission, we shall elucidate how different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a> respond to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.159/>UoR at SemEval-2020 Task 8 : Gaussian Mixture Modelling (GMM) Based Sampling Approach for Multi-modal Memotion Analysis<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: <span class=acl-fixed-case>G</span>aussian Mixture Modelling (<span class=acl-fixed-case>GMM</span>) Based Sampling Approach for Multi-modal Memotion Analysis</a></strong><br><a href=/people/z/zehao-liu/>Zehao Liu</a>
|
<a href=/people/e/emmanuel-osei-brefo/>Emmanuel Osei-Brefo</a>
|
<a href=/people/s/siyuan-chen/>Siyuan Chen</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--159><div class="card-body p-3 small">Memes are widely used on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. They usually contain multi-modal information such as <a href=https://en.wikipedia.org/wiki/Image>images</a> and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a>, serving as valuable data sources to analyse opinions and sentiment orientations of online communities. The provided memes data often face an imbalanced data problem, that is, some classes or labelled sentiment categories significantly outnumber other classes. This often results in difficulty in applying machine learning techniques where balanced labelled input data are required. In this paper, a Gaussian Mixture Model sampling method is proposed to tackle the problem of <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance</a> for the memes sentiment classification task. To utilise both text and image data, a multi-modal CNN-LSTM model is proposed to jointly learn latent features for positive, negative and neutral category predictions. The experiments show that the re-sampling model can slightly improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the trial data of sub-task A of Task 8. The multi-modal CNN-LSTM model can achieve macro F1 score 0.329 on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.162/>BAKSA at SemEval-2020 Task 9 : Bolstering CNN with Self-Attention for Sentiment Analysis of Code Mixed Text<span class=acl-fixed-case>BAKSA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Bolstering <span class=acl-fixed-case>CNN</span> with Self-Attention for Sentiment Analysis of Code Mixed Text</a></strong><br><a href=/people/a/ayush-kumar/>Ayush Kumar</a>
|
<a href=/people/h/harsh-agarwal/>Harsh Agarwal</a>
|
<a href=/people/k/keshav-bansal/>Keshav Bansal</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--162><div class="card-body p-3 small">Sentiment Analysis of code-mixed text has diversified applications in <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> among multiple <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment bearing units</a>. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.164/>Deep Learning Brasil-NLP at SemEval-2020 Task 9 : Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models</a></strong><br><a href=/people/m/manoel-verissimo-dos-santos-neto/>Manoel Veríssimo dos Santos Neto</a>
|
<a href=/people/a/ayrton-amaral/>Ayrton Amaral</a>
|
<a href=/people/n/nadia-silva/>Nádia Silva</a>
|
<a href=/people/a/anderson-da-silva-soares/>Anderson da Silva Soares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--164><div class="card-body p-3 small">In this paper, we describe a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to predict <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> in code-mixed tweets (hindi-english). Our team called verissimo.manoel in CodaLab developed an approach based on an ensemble of four models (MultiFiT, BERT, ALBERT, and XLNET). The final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification algorithm</a> was an ensemble of some predictions of all softmax values from these four <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. This <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> was used and evaluated in the context of the SemEval 2020 challenge (task 9), and our <a href=https://en.wikipedia.org/wiki/System>system</a> got 72.7 % on the F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.170/>IUST at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines<span class=acl-fixed-case>IUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines</a></strong><br><a href=/people/s/soroush-javdan/>Soroush Javdan</a>
|
<a href=/people/t/taha-shangipour-ataei/>Taha Shangipour ataei</a>
|
<a href=/people/b/behrouz-minaei-bidgoli/>Behrouz Minaei-Bidgoli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--170><div class="card-body p-3 small">Sentiment Analysis is a well-studied field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. However, the rapid growth of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and noisy content within them poses significant challenges in addressing this problem with well-established methods and tools. One of these challenges is <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>, which means using different languages to convey thoughts in social media texts. Our group, with the name of IUST(username : TAHA), participated at the SemEval-2020 shared task 9 on Sentiment Analysis for Code-Mixed Social Media Text, and we have attempted to develop a system to predict the sentiment of a given code-mixed tweet. We used different preprocessing techniques and proposed to use different methods that vary from NBSVM to more complicated deep neural network models. Our best performing method obtains an F1 score of 0.751 for the Spanish-English sub-task and 0.706 over the Hindi-English sub-task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.174/>MeisterMorxrc at SemEval-2020 Task 9 : Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets<span class=acl-fixed-case>M</span>eister<span class=acl-fixed-case>M</span>orxrc at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets</a></strong><br><a href=/people/q/qi-wu/>Qi Wu</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/c/chenghao-huang/>Chenghao Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--174><div class="card-body p-3 small">Natural language processing (NLP) has been applied to various <a href=https://en.wikipedia.org/wiki/Field_(computer_science)>fields</a> including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the shared task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of code-mixed tweets, which is a part of the SemEval-2020 competition, we preprocess datasets by replacing <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.181/>WESSA at SemEval-2020 Task 9 : Code-Mixed Sentiment Analysis Using Transformers<span class=acl-fixed-case>WESSA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Code-Mixed Sentiment Analysis Using Transformers</a></strong><br><a href=/people/a/ahmed-sultan/>Ahmed Sultan</a>
|
<a href=/people/m/mahmoud-salim/>Mahmoud Salim</a>
|
<a href=/people/a/amina-gaber/>Amina Gaber</a>
|
<a href=/people/i/islam-el-hosary/>Islam El Hosary</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--181><div class="card-body p-3 small">In this paper, we describe our system submitted for SemEval 2020 Task 9, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> for Code-Mixed Social Media Text alongside other experiments. Our best performing system is a Transfer Learning-based model that fine-tunes XLM-RoBERTa, a transformer-based multilingual masked language model, on monolingual English and Spanish data and Spanish-English code-mixed data. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the official task baseline by achieving a 70.1 % average F1-Score on the official leaderboard using the test set. For later submissions, our <a href=https://en.wikipedia.org/wiki/System>system</a> manages to achieve a 75.9 % average F1-Score on the test set using CodaLab username ahmed0sultan.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.183/>Zyy1510 Team at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations</a></strong><br><a href=/people/y/yueying-zhu/>Yueying Zhu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/h/hongling-li/>Hongling Li</a>
|
<a href=/people/k/kunjie-dong/>Kunjie Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--183><div class="card-body p-3 small">This paper reports the zyy1510 team&#8217;s work in the International Workshop on Semantic Evaluation (SemEval-2020) shared task on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment analysis</a> for Code-Mixed (Hindi-English, English-Spanish) Social Media Text. The purpose of this task is to determine the polarity of the text, dividing it into one of the three labels positive, negative and neutral. To achieve this goal, we propose an ensemble model of word n-grams-based Multinomial Naive Bayes (MNB) and sub-word level representations in LSTM (Sub-word LSTM) to identify the sentiments of code-mixed data of Hindi-English and English-Spanish. This <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> combines the advantage of rich sequential patterns and the intermediate features after convolution from the LSTM model, and the polarity of keywords from the MNB model to obtain the final sentiment score. We have tested our system on Hindi-English and English-Spanish code-mixed social media data sets released for the task. Our model achieves the F1 score of 0.647 in the Hindi-English task and 0.682 in the English-Spanish task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.188/>SemEval-2020 Task 12 : Multilingual Offensive Language Identification in Social Media (OffensEval 2020)<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Multilingual Offensive Language Identification in Social Media (<span class=acl-fixed-case>O</span>ffens<span class=acl-fixed-case>E</span>val 2020)</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/s/sara-rosenthal/>Sara Rosenthal</a>
|
<a href=/people/p/pepa-atanasova/>Pepa Atanasova</a>
|
<a href=/people/g/georgi-karadzhov/>Georgi Karadzhov</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/z/zeses-pitenis/>Zeses Pitenis</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--188><div class="card-body p-3 small">We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages : a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.189/>Galileo at SemEval-2020 Task 12 : Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models</a></strong><br><a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/j/jiaxiang-liu/>Jiaxiang Liu</a>
|
<a href=/people/x/xuan-ouyang/>Xuan Ouyang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--189><div class="card-body p-3 small">This paper describes <a href=https://en.wikipedia.org/wiki/Galileo_(spacecraft)>Galileo</a>&#8217;s performance in SemEval-2020 Task 12 on detecting and categorizing <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A-Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B-Automatic Categorization of Offense Types and Sub-task C-Offence Target Identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.191/>Aschern at SemEval-2020 Task 11 : It Takes Three to Tango : RoBERTa, CRF, and Transfer Learning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: It Takes Three to Tango: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a, <span class=acl-fixed-case>CRF</span>, and Transfer Learning</a></strong><br><a href=/people/a/anton-chernyavskiy/>Anton Chernyavskiy</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--191><div class="card-body p-3 small">We describe our <a href=https://en.wikipedia.org/wiki/System>system</a> for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd (almost tied with the 2nd) out of 36 teams on the span identification subtask with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams on the technique classification subtask with an F1 score of 0.62.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.198/>AdelaideCyC at SemEval-2020 Task 12 : Ensemble of Classifiers for Offensive Language Detection in Social Media<span class=acl-fixed-case>A</span>delaide<span class=acl-fixed-case>C</span>y<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media</a></strong><br><a href=/people/m/mahen-herath/>Mahen Herath</a>
|
<a href=/people/t/thushari-atapattu/>Thushari Atapattu</a>
|
<a href=/people/h/hoang-anh-dung/>Hoang Anh Dung</a>
|
<a href=/people/c/christoph-treude/>Christoph Treude</a>
|
<a href=/people/k/katrina-falkner/>Katrina Falkner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--198><div class="card-body p-3 small">This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The challenge focuses on three subtasks offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.202/>GruPaTo at SemEval-2020 Task 12 : Retraining mBERT on Social Media and Fine-tuned Offensive Language Models<span class=acl-fixed-case>G</span>ru<span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>T</span>o at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Retraining m<span class=acl-fixed-case>BERT</span> on Social Media and Fine-tuned Offensive Language Models</a></strong><br><a href=/people/d/davide-colla/>Davide Colla</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/m/michael-granitzer/>Michael Granitzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--202><div class="card-body p-3 small">We introduce an approach to multilingual Offensive Language Detection based on the mBERT transformer model. We download extra training data from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, and use it to re-train the model. We then fine-tuned the model on the provided training data and, in some configurations, implement transfer learning approach exploiting the typological relatedness between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. Our systems obtained good results across the three languages (.9036 for EN,.7619 for DA, and.7789 for TR).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.203/>GUIR at SemEval-2020 Task 12 : Domain-Tuned Contextualized Models for Offensive Language Detection<span class=acl-fixed-case>GUIR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Domain-Tuned Contextualized Models for Offensive Language Detection</a></strong><br><a href=/people/s/sajad-sotudeh/>Sajad Sotudeh</a>
|
<a href=/people/t/tong-xiang/>Tong Xiang</a>
|
<a href=/people/h/hao-ren-yao/>Hao-Ren Yao</a>
|
<a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/e/eugene-yang/>Eugene Yang</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a>
|
<a href=/people/o/ophir-frieder/>Ophir Frieder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--203><div class="card-body p-3 small">Offensive language detection is an important and challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We present our submissions to the OffensEval 2020 shared task, which includes three English sub-tasks : identifying the presence of offensive language (Sub-task A), identifying the presence of target in offensive language (Sub-task B), and identifying the categories of the target (Sub-task C). Our experiments explore using a domain-tuned contextualized language model (namely, BERT) for this task. We also experiment with different <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> and configurations (e.g., a multi-view SVM) stacked upon BERT models for specific <a href=https://en.wikipedia.org/wiki/Subroutine>sub-tasks</a>. Our submissions achieve <a href=https://en.wikipedia.org/wiki/F-number>F1 scores</a> of 91.7 % in <a href=https://en.wikipedia.org/wiki/Task_(computing)>Sub-task</a> A, 66.5 % in Sub-task B, and 63.2 % in Sub-task C. We perform an ablation study which reveals that domain tuning considerably improves the classification performance. Furthermore, error analysis shows common misclassification errors made by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and outlines research directions for future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.204/>IIITG-ADBU at SemEval-2020 Task 12 : Comparison of BERT and BiLSTM in Detecting Offensive Language<span class=acl-fixed-case>IIITG</span>-<span class=acl-fixed-case>ADBU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Comparison of <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> in Detecting Offensive Language</a></strong><br><a href=/people/a/arup-baruah/>Arup Baruah</a>
|
<a href=/people/k/kaushik-das/>Kaushik Das</a>
|
<a href=/people/f/ferdous-barbhuiya/>Ferdous Barbhuiya</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--204><div class="card-body p-3 small">Task 12 of SemEval 2020 consisted of 3 subtasks, namely offensive language identification (Subtask A), categorization of offense type (Subtask B), and offense target identification (Subtask C). This paper presents the results our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> obtained for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a> in the 3 subtasks. The <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> used by us were BERT and BiLSTM. On the test set, our BERT classifier obtained macro F1 score of 0.90707 for subtask A, and 0.65279 for subtask B. The BiLSTM classifier obtained macro F1 score of 0.57565 for subtask C. The paper also performs an analysis of the errors made by our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We conjecture that the presence of few misleading instances in the dataset is affecting the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Our analysis also discusses the need of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>temporal context</a> and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> to determine the offensiveness of few comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.208/>NUIG at SemEval-2020 Task 12 : Pseudo Labelling for Offensive Content Classification<span class=acl-fixed-case>NUIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Pseudo Labelling for Offensive Content Classification</a></strong><br><a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/m/mihael-arcan/>Mihael Arcan</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--208><div class="card-body p-3 small">This work addresses the classification problem defined by sub-task A (English only) of the OffensEval 2020 challenge. We used a semi-supervised approach to classify given tweets into an offensive (OFF) or not-offensive (NOT) class. As the OffensEval 2020 dataset is loosely labelled with confidence scores given by <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a>, we used last year&#8217;s offensive language identification dataset (OLID) to label the OffensEval 2020 dataset. Our approach uses a pseudo-labelling method to annotate the current <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We trained four text classifiers on the OLID dataset and the classifier with the highest macro-averaged F1-score has been used to pseudo label the OffensEval 2020 dataset. The same model which performed best amongst four text classifiers on <a href=https://en.wikipedia.org/wiki/Online_analytical_processing>OLID dataset</a> has been trained on the combined dataset of <a href=https://en.wikipedia.org/wiki/Online_analytical_processing>OLID</a> and pseudo labelled OffensEval 2020. We evaluated the classifiers with precision, recall and macro-averaged F1-score as the primary evaluation metric on the OLID and OffensEval 2020 datasets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details :.<url>http://creativecommons.org/licenses/by/4.0/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.213/>UHH-LT at SemEval-2020 Task 12 : Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--213><div class="card-body p-3 small">Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is performed on <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>task-specific training datasets</a> in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised MLM</a> resembling the actual classification target dataset allows for domain adaptation of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Further experiments with the ALBERT model even surpass this result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.214/>EL-BERT at SemEval-2020 Task 10 : A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media<span class=acl-fixed-case>EL</span>-<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media</a></strong><br><a href=/people/c/chandresh-kanani/>Chandresh Kanani</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--214><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Media_(communication)>visual media</a>, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text ; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>emphasis</a>, it helps in automatically designing <a href=https://en.wikipedia.org/wiki/Poster>posters</a> and other media contents with <a href=https://en.wikipedia.org/wiki/Writing>written text</a>. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensembles</a> for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.218/>LAST at SemEval-2020 Task 10 : Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM<span class=acl-fixed-case>LAST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and <span class=acl-fixed-case>L</span>ight<span class=acl-fixed-case>GBM</span></a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--218><div class="card-body p-3 small">To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.220/>Randomseed19 at SemEval-2020 Task 10 : Emphasis Selection for Written Text in Visual Media<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Emphasis Selection for Written Text in Visual Media</a></strong><br><a href=/people/a/aleksandr-shatilov/>Aleksandr Shatilov</a>
|
<a href=/people/d/denis-gordeev/>Denis Gordeev</a>
|
<a href=/people/a/alexey-rey/>Alexey Rey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--220><div class="card-body p-3 small">This paper describes our approach to emphasis selection for <a href=https://en.wikipedia.org/wiki/Writing>written text</a> in <a href=https://en.wikipedia.org/wiki/Visual_media>visual media</a> as a solution for SemEval 2020 Task 10. We used an ensemble of several different Transformer-based models and cast the task as a sequence labeling problem with two tags : &#8216;I&#8217; as &#8216;emphasized&#8217; and &#8216;O&#8217; as &#8216;non-emphasized&#8217; for each token in the text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.224/>YNU-HPCC at SemEval-2020 Task 10 : Using a Multi-granularity Ordinal Classification of the BiLSTM Model for Emphasis Selection<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Using a Multi-granularity Ordinal Classification of the <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Model for Emphasis Selection</a></strong><br><a href=/people/d/dawei-liao/>Dawei Liao</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--224><div class="card-body p-3 small">In this study, we propose a multi-granularity ordinal classification method to address the problem of emphasis selection. In detail, the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> is learned from Embeddings from Language Model (ELMO) to extract feature vector representation. Then, the ordinal classifica-tions are implemented on four different multi-granularities to approximate the continuous em-phasize values. Comparative experiments were conducted to compare the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with baseline in which the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is transformed to label distribution problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.229/>JUST at SemEval-2020 Task 11 : Detecting Propaganda Techniques Using BERT Pre-trained Model<span class=acl-fixed-case>JUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detecting Propaganda Techniques Using <span class=acl-fixed-case>BERT</span> Pre-trained Model</a></strong><br><a href=/people/o/ola-altiti/>Ola Altiti</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/r/rasha-obiedat/>Rasha Obiedat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--229><div class="card-body p-3 small">This paper presents the submission to semeval-2020 task 11, Detection of Propaganda Techniques in News Articles. Knowing that there are two subtasks in this competition, we have participated in the Technique Classification subtask (TC), which aims to identify the propaganda techniques used in a specific propaganda span. We have used and implemented various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on BERT uncased pre-trained language model as it has achieved state-of-the-art performance on multiple NLP benchmarks. The performance results of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> have scored 0.55307 F1-Score, which outperforms the baseline model provided by the organizers with 0.2519 F1-Score, and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is 0.07 away from the best performing team. Compared to other participating systems, our submission is ranked 15th out of 31 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.232/>NLFIIT at SemEval-2020 Task 11 : Neural Network Architectures for Detection of Propaganda Techniques in News Articles<span class=acl-fixed-case>NLFIIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Neural Network Architectures for Detection of Propaganda Techniques in News Articles</a></strong><br><a href=/people/m/matej-martinkovic/>Matej Martinkovic</a>
|
<a href=/people/s/samuel-pecar/>Samuel Pecar</a>
|
<a href=/people/m/marian-simko/>Marian Simko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--232><div class="card-body p-3 small">Since <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> became more common technique in <a href=https://en.wikipedia.org/wiki/News>news</a>, it is very important to look for possibilities of its automatic detection. In this paper, we present neural model architecture submitted to the SemEval-2020 Task 11 competition : Detection of Propaganda Techniques in News Articles. We participated in both subtasks, propaganda span identification and propaganda technique classification. Our model utilizes recurrent Bi-LSTM layers with pre-trained word representations and also takes advantage of self-attention mechanism. Our model managed to achieve score 0.405 F1 for subtask 1 and 0.553 F1 for subtask 2 on test set resulting in 17th and 16th place in subtask 1 and subtask 2, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.233/>PsuedoProp at SemEval-2020 Task 11 : Propaganda Span Detection Using BERT-CRF and Ensemble Sentence Level Classifier<span class=acl-fixed-case>P</span>suedo<span class=acl-fixed-case>P</span>rop at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Span Detection Using <span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>CRF</span> and Ensemble Sentence Level Classifier</a></strong><br><a href=/people/a/aniruddha-chauhan/>Aniruddha Chauhan</a>
|
<a href=/people/h/harshita-diddee/>Harshita Diddee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--233><div class="card-body p-3 small">This paper explains our teams&#8217; submission to the Shared Task of Fine-Grained Propaganda Detection in which we propose a sequential BERT-CRF based Span Identification model where the fine-grained detection is carried out only on the articles that are flagged as containing propaganda by an ensemble SLC model. We propose this setup bearing in mind the practicality of this approach in identifying propaganda spans in the exponentially increasing content base where the fine-tuned analysis of the entire data repository may not be the optimal choice due to its massive computational resource requirements. We present our analysis on different <a href=https://en.wikipedia.org/wiki/Electoral_system>voting ensembles</a> for the SLC model. Our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 14th on the test set and 22nd on the development set and with an F1 score of 0.41 and 0.39 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.234/>SkoltechNLP at SemEval-2020 Task 11 : Exploring Unsupervised Text Augmentation for Propaganda Detection<span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Exploring Unsupervised Text Augmentation for Propaganda Detection</a></strong><br><a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/i/igor-markov/>Igor Markov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--234><div class="card-body p-3 small">This paper presents a solution for the Span Identification (SI) task in the Detection of Propaganda Techniques in News Articles competition at SemEval-2020. The goal of the SI task is to identify specific fragments of each article which contain the use of at least one propaganda technique. This is a binary sequence tagging task. We tested several approaches finally selecting a fine-tuned BERT model as our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Our main contribution is an investigation of several unsupervised data augmentation techniques based on distributional semantics expanding the original small training dataset as applied to this BERT-based sequence tagger. We explore various expansion strategies and show that they can substantially shift the balance between <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, while maintaining comparable levels of the F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.237/>syrapropa at SemEval-2020 Task 11 : BERT-based Models Design for Propagandistic Technique and Span Detection<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: <span class=acl-fixed-case>BERT</span>-based Models Design for Propagandistic Technique and Span Detection</a></strong><br><a href=/people/j/jinfen-li/>Jinfen Li</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--237><div class="card-body p-3 small">This paper describes the BERT-based models proposed for two subtasks in SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles. We first build the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for Span Identification (SI) based on SpanBERT, and facilitate the detection by a deeper <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and a sentence-level representation. We then develop a hybrid model for the Technique Classification (TC). The hybrid model is composed of three submodels including two BERT models with different training methods, and a feature-based Logistic Regression model. We endeavor to deal with imbalanced dataset by adjusting <a href=https://en.wikipedia.org/wiki/Cost_function>cost function</a>. We are in the seventh place in SI subtask (0.4711 of F1-measure), and in the third place in TC subtask (0.6783 of F1-measure) on the development set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.238/>Team DiSaster at SemEval-2020 Task 11 : Combining BERT and Hand-crafted Features for Identifying Propaganda Techniques in News<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>S</span>aster at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Combining <span class=acl-fixed-case>BERT</span> and Hand-crafted Features for Identifying Propaganda Techniques in News</a></strong><br><a href=/people/a/anders-kaas/>Anders Kaas</a>
|
<a href=/people/v/viktor-torp-thomsen/>Viktor Torp Thomsen</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--238><div class="card-body p-3 small">The identification of communication techniques in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> such as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> is important, as such techniques can influence the opinions of large numbers of people. Most work so far focused on the <a href=https://en.wikipedia.org/wiki/Identification_(biology)>identification</a> at the news article level. Recently, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and shared task has been proposed for the identification of propaganda techniques at the finer-grained span level. This paper describes our system submission to the subtask of technique classification (TC) for the SemEval 2020 shared task on detection of propaganda techniques in news articles. We propose a method of combining neural BERT representations with hand-crafted features via stacked generalization. Our model has the added advantage that it combines the power of contextual representations from BERT with simple span-based and article-based global features. We present an ablation study which shows that even though BERT representations are very powerful also for this task, BERT still benefits from being combined with carefully designed task-specific features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.240/>TTUI at SemEval-2020 Task 11 : Propaganda Detection with Transfer Learning and Ensembles<span class=acl-fixed-case>TTUI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles</a></strong><br><a href=/people/m/moonsung-kim/>Moonsung Kim</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--240><div class="card-body p-3 small">In this paper, we describe our approaches and <a href=https://en.wikipedia.org/wiki/System>systems</a> for the SemEval-2020 Task 11 on propaganda technique detection. We fine-tuned BERT and RoBERTa pre-trained models then merged them with an average ensemble. We conducted several experiments for input representations dealing with long texts and preserving context as well as for the imbalanced class problem. Our system ranked 20th out of 36 teams with 0.398 F1 in the SI task and 14th out of 31 teams with 0.556 F1 in the TC task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.241/>UAIC1860 at SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles<span class=acl-fixed-case>UAIC</span>1860 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detection of Propaganda Techniques in News Articles</a></strong><br><a href=/people/v/vlad-ermurachi/>Vlad Ermurachi</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gifu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--241><div class="card-body p-3 small">The Detection of Propaganda Techniques in News Articles task at the SemEval 2020 competition focuses on detecting and classifying <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, pervasive in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a>. In this paper, we present a system able to evaluate on sentence level, three traditional text representation techniques for these study goals, using : tf*idf, word and character n-grams. Firstly, we built a binary classifier able to provide corresponding propaganda labels, propaganda or non-propaganda. Secondly, we build a multilabel multiclass model to identify applied <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.242/>UMSIForeseer at SemEval-2020 Task 11 : Propaganda Detection by Fine-Tuning BERT with Resampling and Ensemble Learning<span class=acl-fixed-case>UMSIF</span>oreseer at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Detection by Fine-Tuning <span class=acl-fixed-case>BERT</span> with Resampling and Ensemble Learning</a></strong><br><a href=/people/y/yunzhe-jiang/>Yunzhe Jiang</a>
|
<a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/q/qiaozhu-mei/>Qiaozhu Mei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--242><div class="card-body p-3 small">We describe our participation at the SemEval 2020 Detection of Propaganda Techniques in News Articles-Techniques Classification (TC) task, designed to categorize textual fragments into one of the 14 given propaganda techniques. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> leverages pre-trained BERT models. We present our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> implementations, evaluation results and analysis of these results. We also investigate the potential of combining <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> with <a href=https://en.wikipedia.org/wiki/Sample-rate_conversion>resampling</a> and ensemble learning methods to deal with data imbalance and improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.243/>UNTLing at SemEval-2020 Task 11 : Detection of Propaganda Techniques in English News Articles<span class=acl-fixed-case>UNTL</span>ing at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detection of Propaganda Techniques in <span class=acl-fixed-case>E</span>nglish News Articles</a></strong><br><a href=/people/m/maia-petee/>Maia Petee</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--243><div class="card-body p-3 small">Our system for the PropEval task explores the ability of semantic features to detect and label propagandistic rhetorical techniques in English news articles. For Subtask 2, labeling identified propagandistic fragments with one of fourteen technique labels, our system attains a micro-averaged F1 of 0.40 ; in this paper, we take a detailed look at the fourteen labels and how well our semantically-focused model detects each of them. We also propose strategies to fill the gaps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.250/>Amsqr at SemEval-2020 Task 12 : Offensive Language Detection Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and Anti-adversarial Features<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Detection Using Neural Networks and Anti-adversarial Features</a></strong><br><a href=/people/a/alejandro-mosquera/>Alejandro Mosquera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--250><div class="card-body p-3 small">This paper describes a method and <a href=https://en.wikipedia.org/wiki/System>system</a> to solve the problem of detecting offensive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> using anti-adversarial features. Our submission to the SemEval-2020 task 12 challenge was generated by an stacked ensemble of neural networks fine-tuned on the OLID dataset and additional external sources. For Task-A (English), text normalisation filters were applied at both graphical and lexical level. The normalisation step effectively mitigates not only the natural presence of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>lexical variants</a> but also intentional attempts to bypass moderation by introducing out of vocabulary words. Our approach provides strong F1 scores for both 2020 (0.9134) and 2019 (0.8258) challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.258/>Hitachi at SemEval-2020 Task 12 : Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing</a></strong><br><a href=/people/m/manikandan-ravikiran/>Manikandan Ravikiran</a>
|
<a href=/people/a/amin-ekant-muljibhai/>Amin Ekant Muljibhai</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/s/sakata-masayuki/>Sakata Masayuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--258><div class="card-body p-3 small">In this paper, we present our participation in SemEval-2020 Task-12 Subtask-A (English Language) which focuses on offensive language identification from noisy labels. To this end, we developed a hybrid system with the BERT classifier trained with tweets selected using Statistical Sampling Algorithm (SA) and Post-Processed (PP) using an offensive wordlist. Our developed system achieved 34th position with Macro-averaged F1-score (Macro-F1) of 0.90913 over both offensive and non-offensive classes. We further show comprehensive results and <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> to assist future research in offensive language identification with noisy labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.263/>IR3218-UI at SemEval-2020 Task 12 : Emoji Effects on Offensive Language IdentifiCation<span class=acl-fixed-case>IR</span>3218-<span class=acl-fixed-case>UI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Emoji Effects on Offensive Language <span class=acl-fixed-case>I</span>dentifi<span class=acl-fixed-case>C</span>ation</a></strong><br><a href=/people/s/sandy-kurniawan/>Sandy Kurniawan</a>
|
<a href=/people/i/indra-budi/>Indra Budi</a>
|
<a href=/people/m/muhammad-okky-ibrohim/>Muhammad Okky Ibrohim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--263><div class="card-body p-3 small">In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub-tasks in OffensEval 2020 namely offensive language identification (sub-task A), automatic categorization of offense types (sub-task B), and offense target identification (sub-task C). We participated in sub-task A of English OffensEval 2020. Our approach emphasizes on how the <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> affects offensive language identification. Our model used LSTM combined with GloVe pre-trained word vectors to identify offensive language on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtained macro F1-score of 0.88428.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.266/>JCT at SemEval-2020 Task 12 : Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams<span class=acl-fixed-case>JCT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams</a></strong><br><a href=/people/m/moshe-uzan/>Moshe Uzan</a>
|
<a href=/people/y/yaakov-hacohen-kerner/>Yaakov HaCohen-Kerner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--266><div class="card-body p-3 small">In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12-Multilingual Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. We developed different models for four languages : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> using <a href=https://en.wikipedia.org/wiki/Random_Forest>Random Forest</a>. This <a href=https://en.wikipedia.org/wiki/Model_(person)>model</a> was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are more characterized by <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> than by words, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are short, and contain various special sequences of characters, e.g., <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, <a href=https://en.wikipedia.org/wiki/Shortcut_(computing)>shortcuts</a>, slang words, and typos.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.273/>Lee at SemEval-2020 Task 12 : A BERT Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: A <span class=acl-fixed-case>BERT</span> Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/z/zichen-zhang/>Zichen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--273><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2020 Task 12 : OffensEval 2020. This task aims to identify and classify offensive languages in different languages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We only participate in the English part of subtask A, which aims to identify offensive languages in English. To solve this task, we propose a BERT model system based on the transform mechanism, and use the maximum self-ensemble to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved a macro F1 score of 0.913(ranked 13/82) in subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.274/>LIIR at SemEval-2020 Task 12 : A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification<span class=acl-fixed-case>LIIR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification</a></strong><br><a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--274><div class="card-body p-3 small">This paper presents our system entitled &#8216;LIIR&#8217; for SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2). We have participated in sub-task A for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish languages</a>. We adapt and fine-tune the <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and Multilingual Bert models made available by Google AI for English and non-English languages respectively. For the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, we use a combination of two fine-tuned BERT models. For other languages we propose a cross-lingual augmentation approach in order to enrich training data and we use Multilingual BERT to obtain sentence representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.283/>SalamNET at SemEval-2020 Task 12 : Deep Learning Approach for Arabic Offensive Language Detection<span class=acl-fixed-case>S</span>alam<span class=acl-fixed-case>NET</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Deep Learning Approach for <span class=acl-fixed-case>A</span>rabic Offensive Language Detection</a></strong><br><a href=/people/f/fatemah-husain/>Fatemah Husain</a>
|
<a href=/people/j/jooyeon-lee/>Jooyeon Lee</a>
|
<a href=/people/s/sam-henry/>Sam Henry</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--283><div class="card-body p-3 small">This paper describes SalamNET, an Arabic offensive language detection system that has been submitted to SemEval 2020 shared task 12 : Multilingual Offensive Language Identification in Social Media. Our approach focuses on applying multiple deep learning models and conducting in depth error analysis of results to provide system implications for future development considerations. To pursue our goal, a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and Long-Short Term Memory (LSTM) models with different design architectures have been developed and evaluated. The SalamNET, a Bi-directional Gated Recurrent Unit (Bi-GRU) based model, reports a macro-F1 score of 0.83 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.285/>Sonal.kumari at SemEval-2020 Task 12 : Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models</a></strong><br><a href=/people/s/sonal-kumari/>Sonal Kumari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--285><div class="card-body p-3 small">In this paper, we present our approaches and results for SemEval-2020 Task 12, Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The OffensEval 2020 had three subtasks : A) Identifying the tweets to be offensive (OFF) or non-offensive (NOT) for Arabic, Danish, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Greek, and Turkish languages, B) Detecting if the offensive tweet is targeted (TIN) or untargeted (UNT) for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, and C) Categorizing the offensive targeted tweets into three classes, namely : individual (IND), Group (GRP), or Other (OTH) for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. We participate in all the subtasks A, B, and C. In our solution, first we use the pre-trained BERT model for all <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>subtasks</a>, A, B, and C and then we apply the BiLSTM model with attention mechanism (Attn-BiLSTM) for the same. Our result demonstrates that the pre-trained model is not giving good results for all types of languages and is compute and memory intensive whereas the Attn-BiLSTM model is fast and gives good accuracy with fewer resources. The Attn-BiLSTM model is giving better accuracy for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> where the pre-trained model is not able to capture the complete context of these languages due to lower vocab-size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.287/>SSN_NLP_MLRG at SemEval-2020 Task 12 : Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> Using BERT and Machine Learning Approach<span class=acl-fixed-case>SSN</span>_<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>MLRG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Identification in <span class=acl-fixed-case>E</span>nglish, <span class=acl-fixed-case>D</span>anish, <span class=acl-fixed-case>G</span>reek Using <span class=acl-fixed-case>BERT</span> and Machine Learning Approach</a></strong><br><a href=/people/a/a-kalaivani/>A Kalaivani</a>
|
<a href=/people/t/thenmozhi-d/>Thenmozhi D.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--287><div class="card-body p-3 small">Offensive language identification is to detect the hurtful tweets, <a href=https://en.wikipedia.org/wiki/Pejorative>derogatory comments</a>, swear words on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. As an emerging growth of social media communication, offensive language detection has received more attention in the last years ; we focus to perform the task on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We have investigated which can be effect more on pre-trained models BERT (Bidirectional Encoder Representation from Transformer) and Machine Learning Approaches. Our investigation shows the difference performance between the three languages and to identify the best performance is evaluated by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification algorithms</a>. In the shared task SemEval-2020, our team SSN_NLP_MLRG submitted for three languages that are Subtasks A, B, C in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Subtask A in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and Subtask A in <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. Our team SSN_NLP_MLRG obtained the <a href=https://en.wikipedia.org/wiki/F1_score>F1 Scores</a> as 0.90, 0.61, 0.52 for the Subtasks A, B, C in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 0.56 for the Subtask A in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and 0.67 for the Subtask A in Greek respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.289" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.289/>TAC at SemEval-2020 Task 12 : Ensembling Approach for Multilingual Offensive Language Identification in Social Media<span class=acl-fixed-case>TAC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Ensembling Approach for Multilingual Offensive Language Identification in Social Media</a></strong><br><a href=/people/t/talha-anwar/>Talha Anwar</a>
|
<a href=/people/o/omer-baig/>Omer Baig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--289><div class="card-body p-3 small">Usage of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is getting more common these days, and there is a need of a mechanism to detect it and control it. This paper deals with offensive language detection in five different languages ; <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. We presented an almost similar <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble pipeline</a> comprised of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning and deep learning models</a> for all five languages. Three <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and four <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> were used in the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. In the OffensEval-2020 competition our model achieved <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.85, 0.74, 0.68, 0.81, and 0.9 for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, Greek and English language tasks respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.295/>UoB at SemEval-2020 Task 12 : Boosting BERT with Corpus Level Information<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>B</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Boosting <span class=acl-fixed-case>BERT</span> with Corpus Level Information</a></strong><br><a href=/people/w/wah-meng-lim/>Wah Meng Lim</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--295><div class="card-body p-3 small">Pre-trained language model word representation, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, have been extremely successful in several Natural Language Processing tasks significantly improving on the state-of-the-art. This can largely be attributed to their ability to better capture <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> contained within a sentence. Several tasks, however, can benefit from information available at a corpus level, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work we test the effectiveness of integrating this <a href=https://en.wikipedia.org/wiki/Information>information</a> with BERT on the task of identifying abuse on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and show that integrating this <a href=https://en.wikipedia.org/wiki/Information>information</a> with BERT does indeed significantly improve performance. We participate in Sub-Task A (abuse detection) wherein we achieve a score within two points of the top performing team and in Sub-Task B (target detection) wherein we are ranked 4 of the 44 participating teams.</div></div></div><hr><div id=2020smm4h-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.smm4h-1/>Proceedings of the Fifth Social Media Mining for Health Applications Workshop & Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.0/>Proceedings of the Fifth Social Media Mining for Health Applications Workshop & Shared Task</a></strong><br><a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a>
|
<a href=/people/a/ari-z-klein/>Ari Z. Klein</a>
|
<a href=/people/i/ivan-flores/>Ivan Flores</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/a/arjun-magge/>Arjun Magge</a>
|
<a href=/people/k/karen-o-connor/>Karen O'Connor</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a>
|
<a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.2/>Conversation-Aware Filtering of Online Patient Forum Messages</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a>
|
<a href=/people/s/suzan-verberne/>Suzan Verberne</a>
|
<a href=/people/w/wessel-kraaij/>Wessel Kraaij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--2><div class="card-body p-3 small">Previous approaches to NLP tasks on online patient forums have been limited to single posts as units, thereby neglecting the overarching conversational structure. In this paper we explore the benefit of exploiting <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a> for filtering posts relevant to a specific <a href=https://en.wikipedia.org/wiki/Medicine>medical topic</a>. We experiment with two approaches to add conversational context to a BERT model : a sequential CRF layer and manually engineered features. Although neither approach can outperform the F1 score of the BERT baseline, we find that adding a sequential layer improves <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> for all target classes whereas adding a non-sequential layer with manually engineered features leads to a higher <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> for two out of three target classes. Thus, depending on the end goal, conversation-aware modelling may be beneficial for identifying relevant messages. We hope our findings encourage other researchers in this domain to move beyond studying messages in isolation towards more discourse-based data collection and classification. We release our code for the purpose of follow-up research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.4/>Overview of the Fifth <a href=https://en.wikipedia.org/wiki/Social_media_mining>Social Media Mining</a> for Health Applications (# SMM4H) Shared Tasks at COLING 2020<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>) Shared Tasks at <span class=acl-fixed-case>COLING</span> 2020</a></strong><br><a href=/people/a/ari-klein/>Ari Klein</a>
|
<a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/i/ivan-flores/>Ivan Flores</a>
|
<a href=/people/a/arjun-magge/>Arjun Magge</a>
|
<a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/k/karen-oconnor/>Karen O’Connor</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--4><div class="card-body p-3 small">The vast amount of data on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> presents significant opportunities and challenges for utilizing <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> as a resource for <a href=https://en.wikipedia.org/wiki/Health_informatics>health informatics</a>. The fifth iteration of the Social Media Mining for Health Applications (# SMM4H) shared tasks sought to advance the use of Twitter data (tweets) for <a href=https://en.wikipedia.org/wiki/Pharmacovigilance>pharmacovigilance</a>, <a href=https://en.wikipedia.org/wiki/Toxicovigilance>toxicovigilance</a>, and epidemiology of birth defects. In addition to re-runs of three tasks, # SMM4H 2020 included new tasks for detecting adverse effects of medications in French and Russian tweets, characterizing chatter related to prescription medication abuse, and detecting self reports of birth defect pregnancy outcomes. The five tasks required methods for <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>, multi-class classification, and named entity recognition (NER). With 29 teams and a total of 130 system submissions, participation in the # SMM4H shared tasks continues to grow.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.5/>Ensemble BERT for Classifying Medication-mentioning Tweets<span class=acl-fixed-case>BERT</span> for Classifying Medication-mentioning Tweets</a></strong><br><a href=/people/h/huong-dang/>Huong Dang</a>
|
<a href=/people/k/kahyun-lee/>Kahyun Lee</a>
|
<a href=/people/s/sam-henry/>Sam Henry</a>
|
<a href=/people/o/ozlem-uzuner/>Özlem Uzuner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--5><div class="card-body p-3 small">Twitter is a valuable source of patient-generated data that has been used in various <a href=https://en.wikipedia.org/wiki/Population_study>population health studies</a>. The first step in many of these studies is to identify and capture Twitter messages (tweets) containing medication mentions. In this article, we describe our submission to Task 1 of the Social Media Mining for Health Applications (SMM4H) Shared Task 2020. This task challenged participants to detect tweets that mention <a href=https://en.wikipedia.org/wiki/Medication>medications</a> or <a href=https://en.wikipedia.org/wiki/Dietary_supplement>dietary supplements</a> in a natural, highly imbalance dataset. Our system combined a handcrafted preprocessing step with an ensemble of 20 BERT-based classifiers generated by dividing the training dataset into subsets using 10-fold cross validation and exploiting two BERT embedding models. Our system ranked first in this task, and improved the average F1 score across all participating teams by 19.07 % with a precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, and <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on the test set of 83.75 %, 87.01 %, and 85.35 % respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.9/>SMM4H Shared Task 2020-A Hybrid Pipeline for Identifying Prescription Drug Abuse from Twitter : <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a>, <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a>, and Post-Processing<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> Shared Task 2020 - A Hybrid Pipeline for Identifying Prescription Drug Abuse from <span class=acl-fixed-case>T</span>witter: Machine Learning, Deep Learning, and Post-Processing</a></strong><br><a href=/people/i/isabel-metzger/>Isabel Metzger</a>
|
<a href=/people/e/emir-y-haskovic/>Emir Y. Haskovic</a>
|
<a href=/people/a/allison-black/>Allison Black</a>
|
<a href=/people/w/whitley-m-yi/>Whitley M. Yi</a>
|
<a href=/people/r/rajat-s-chandra/>Rajat S. Chandra</a>
|
<a href=/people/m/mark-t-rutledge/>Mark T. Rutledge</a>
|
<a href=/people/w/william-mcmahon/>William McMahon</a>
|
<a href=/people/y/yindalon-aphinyanaphongs/>Yindalon Aphinyanaphongs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--9><div class="card-body p-3 small">This paper presents our approach to multi-class text categorization of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> mentioning prescription medications as being indicative of potential abuse / misuse (A), consumption / non-abuse (C), mention-only (M), or an unrelated reference (U) using natural language processing techniques. Data augmentation increased our <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and validation corpora</a> from 13,172 tweets to 28,094 tweets. We also created word-embeddings on domain-specific social media and medical corpora. Our hybrid pipeline of an attention-based CNN with <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> was the best performing system in task 4 of SMM4H 2020, with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.51 for class A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.14/>How Far Can We Go with Just Out-of-the-box BERT Models?<span class=acl-fixed-case>BERT</span> Models?</a></strong><br><a href=/people/l/lucie-gattepaille/>Lucie Gattepaille</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--14><div class="card-body p-3 small">Social media have been seen as a promising data source for <a href=https://en.wikipedia.org/wiki/Pharmacovigilance>pharmacovigilance</a>. Howev-er, methods for automatic extraction of Adverse Drug Reactions from social media plat-forms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> still need further development before they can be included reliably in routine pharmacovigilance practices. As the Bidirectional Encoder Representations from Transformer (BERT) models have shown great performance in many major NLP tasks recently, we decided to test its performance on the SMM4H Shared Tasks 1 to 3, by submitting results of pretrained and fine-tuned BERT models without more added knowledge than the one carried in the training datasets and additional datasets. Our three submissions all ended up above average over all teams&#8217; submissions : 0.766 F1 for task 1 (15 % above the average of 0.665), 0.47 F1 for task 2 (2 % above the average of 0.46) and 0.380 F1 score for task 3 (30 % above the average of 0.292). Used in many of the high-ranking submission in the 2019 edition of the SMM4H Shared Task, BERT contin-ues to be state-of-the-art in ADR extraction for Twitter data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.15/>FBK@SMM4H2020 : RoBERTa for Detecting Medications on Twitter<span class=acl-fixed-case>FBK</span>@<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>2020: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a for Detecting Medications on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/silvia-casola/>Silvia Casola</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--15><div class="card-body p-3 small">This paper describes a classifier for tweets that mention medications or supplements, based on a pretrained transformer. We developed such a system for our participation in Subtask 1 of the Social Media Mining for Health Application workshop, which featured an extremely unbalanced dataset. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> showed promising results, with an F1 of 0.8 (task mean : 0.66).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.16/>Autobots Ensemble : Identifying and Extracting Adverse Drug Reaction from Tweets Using Transformer Based Pipelines</a></strong><br><a href=/people/s/sougata-saha/>Sougata Saha</a>
|
<a href=/people/s/souvik-das/>Souvik Das</a>
|
<a href=/people/p/prashi-khurana/>Prashi Khurana</a>
|
<a href=/people/r/rohini-k-srihari/>Rohini Srihari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--16><div class="card-body p-3 small">This paper details a <a href=https://en.wikipedia.org/wiki/System>system</a> designed for Social Media Mining for Health Applications (SMM4H) Shared Task 2020. We specifically describe the systems designed to solve task 2 : Automatic classification of multilingual tweets that report adverse effects, and task 3 : Automatic extraction and normalization of adverse effects in English tweets. Fine tuning RoBERTa large for classifying English tweets enables us to achieve a F1 score of 56 %, which is an increase of +10 % compared to the average F1 score for all the submissions. Using BERT based NER and question answering, we are able to achieve a F1 score of 57.6 % for extracting adverse reaction mentions from tweets, which is an increase of +1.2 % compared to the average F1 score for all the submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.19/>SpeechTrans@SMM4H’20 : Impact of Preprocessing and N-grams on Automatic Classification of Tweets That Mention Medications<span class=acl-fixed-case>S</span>peech<span class=acl-fixed-case>T</span>rans@<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>’20: Impact of Preprocessing and N-grams on Automatic Classification of Tweets That Mention Medications</a></strong><br><a href=/people/m/mohamed-lichouri/>Mohamed Lichouri</a>
|
<a href=/people/m/mourad-abbas/>Mourad Abbas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--19><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> developed for automatically classifying tweets that mention medications. We used the <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Tree classifier</a> for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. We have shown that using some elementary preprocessing steps and TF-IDF n-grams led to acceptable <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance. Indeed, the F1-score recorded was 74.58 % in the development phase and 63.70 % in the test phase.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.20/>Want to Identify, Extract and Normalize Adverse Drug Reactions in Tweets? Use RoBERTa<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a</a></strong><br><a href=/people/k/katikapalli-subramanyam-kalyan/>Katikapalli Subramanyam Kalyan</a>
|
<a href=/people/s/sivanesan-sangeetha/>Sivanesan Sangeetha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--20><div class="card-body p-3 small">This paper presents our approach for task 2 and task 3 of Social Media Mining for Health (SMM4H) 2020 shared tasks. In task 2, we have to differentiate adverse drug reaction (ADR) tweets from nonADR tweets and is treated as <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>. Task 3 involves extracting ADR mentions and then mapping them to MedDRA codes. Extracting ADR mentions is treated as sequence labeling and normalizing ADR mentions is treated as multi-class classification. Our system is based on pre-trained language model RoBERTa and it achieves a) F1-score of 58 % in task 2 which is 12 % more than the average score b) relaxed F1-score of 70.1 % in ADR extraction of task 3 which is 13.7 % more than the average score and relaxed F1-score of 35 % in ADR extraction + normalization of task 3 which is 5.8 % more than the average score. Overall, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve promising results in both the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> with significant improvements over average scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.21/>Detecting Tweets Reporting Birth Defect Pregnancy Outcome Using Two-View CNN RNN Based Architecture<span class=acl-fixed-case>CNN</span> <span class=acl-fixed-case>RNN</span> Based Architecture</a></strong><br><a href=/people/s/saichethan-reddy/>Saichethan Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--21><div class="card-body p-3 small">This research work addresses a new multi-class classification task (fifth task) provided at the fifth Social Media Mining for Health Applications (SMM4H) workshop. This automatic tweet classification task involves distinguishing three classes of tweets that mention <a href=https://en.wikipedia.org/wiki/Birth_defect>birth defects</a>. We propose a novel two view based CNN-BiGRU based architectures for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Experimental evaluation of our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> over the validation set gives encouraging results as it improves by approximately 7 % over our single view model for the fifth task. Code of our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is made available on Github</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.24/>LITL at SMM4H : An Old-school Feature-based Classifier for Identifying Adverse Effects in Tweets<span class=acl-fixed-case>LITL</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>: An Old-school Feature-based Classifier for Identifying Adverse Effects in Tweets</a></strong><br><a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a>
|
<a href=/people/l/lydia-mai-ho-dac/>Lydia-Mai Ho-Dac</a>
|
<a href=/people/c/cecile-fabre/>Cécile Fabre</a>
|
<a href=/people/r/roxane-bois/>Roxane Bois</a>
|
<a href=/people/t/touati-mohamed-yacine-haddad/>Touati Mohamed Yacine Haddad</a>
|
<a href=/people/c/claire-ibarboure/>Claire Ibarboure</a>
|
<a href=/people/m/marie-joyau/>Marie Joyau</a>
|
<a href=/people/f/francois-le-moal/>François Le moal</a>
|
<a href=/people/j/jade-moiilic/>Jade Moiilic</a>
|
<a href=/people/l/laura-roudaut/>Laura Roudaut</a>
|
<a href=/people/m/mathilde-simounet/>Mathilde Simounet</a>
|
<a href=/people/i/irena-stankovic/>Irena Stankovic</a>
|
<a href=/people/m/mickaela-vandewaetere/>Mickaela Vandewaetere</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--24><div class="card-body p-3 small">This paper describes our participation to the SMM4H shared task 2. We designed a rule-based classifier that estimates whether a tweet mentions an adverse effect associated to a medication. Our system addresses <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and is based on a number of specific word lists and features. These cues were mostly obtained through an extensive corpus analysis of the provided training data. Different <a href=https://en.wikipedia.org/wiki/Weighting>weighting schemes</a> were tested (manually tuned or based on a logistic regression), the best one achieving a <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.31 for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and 0.15 for <a href=https://en.wikipedia.org/wiki/French_language>French</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.25/>Sentence Classification with Imbalanced Data for Health Applications</a></strong><br><a href=/people/f/farhana-ferdousi-liza/>Farhana Ferdousi Liza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--25><div class="card-body p-3 small">Identifying and extracting reports of medications, their abuse or adverse effects from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is a challenging task. In <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, relevant reports are very infrequent, causes imbalanced class distribution for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>. Learning algorithms typically designed to optimize the overall accuracy without considering the relative distribution of each class. Thus, imbalanced class distribution is problematic as <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithms</a> have low predictive accuracy for the infrequent class. Moreover, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> represents natural linguistic variation in creative language expressions. In this paper, we have used a combination of data balancing and neural language representation techniques to address the challenges. Specifically, we participated the shared tasks 1, 2 (all languages), 4, and 3 (only the span detection, no normalization was attempted) in Social Media Mining for Health applications (SMM4H) 2020 (Klein et al., 2020). The results show that with the proposed methodology <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall scores</a> are better than the precision scores for the shared tasks. The <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall score</a> is also better compared to the mean score of the total submissions. However, the F1-score is worse than the mean score except for task 2 (French).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.smm4h-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--smm4h-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.smm4h-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.smm4h-1.31/>Sentence Contextual Encoder with BERT and BiLSTM for Automatic Classification with Imbalanced Medication Tweets<span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> for Automatic Classification with Imbalanced Medication Tweets</a></strong><br><a href=/people/o/olanrewaju-tahir-aduragba/>Olanrewaju Tahir Aduragba</a>
|
<a href=/people/j/jialin-yu/>Jialin Yu</a>
|
<a href=/people/g/gautham-senthilnathan/>Gautham Senthilnathan</a>
|
<a href=/people/a/alexandra-crsitea/>Alexandra Crsitea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--smm4h-1--31><div class="card-body p-3 small">This paper details the system description and approach used by our team for the SMM4H 2020 competition, Task 1. Task 1 targets the automatic classification of tweets that mention medication. We adapted the standard BERT pretrain-then-fine-tune approach to include an intermediate training stage with a biLSTM architecture neural network acting as a further fine-tuning stage. We were inspired by the effectiveness of within-task further pre-training and sentence encoders. We show that this approach works well for a highly imbalanced dataset. In this case, the positive class is only 0.2 % of the entire dataset. Our model performed better in both F1 and precision scores compared to the mean score for all participants in the competition and had a competitive recall score.</div></div></div><hr><div id=2020starsem-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.starsem-1/>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.0/>Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.starsem-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.6/>Semantic Structural Decomposition for Neural Machine Translation</a></strong><br><a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--6><div class="card-body p-3 small">Building on recent advances in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> on long sentences on an English-to- French setting with a training corpus of 5 M sentence pairs, while retaining comparable <a href=https://en.wikipedia.org/wiki/Adequality>adequacy</a>. We also perform a manual analysis which explores the tradeoff between adequacy and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> in the case where all sentence lengths are considered.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.starsem-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.10/>On the Systematicity of Probing Contextualized Word Representations : The Case of <a href=https://en.wikipedia.org/wiki/Hypernymy>Hypernymy</a> in BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--10><div class="card-body p-3 small">Contextualized word representations have become a driving force in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question : do probing studies shed light on systematic knowledge in BERT representations? As a case study, we examine <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy knowledge</a> encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary : even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT &#8216;understands&#8217; a concept, and it can not be expected to systematically generalize across applicable contexts.<i>do probing studies shed light on systematic knowledge in BERT representations?</i> As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT &#8216;understands&#8217; a concept, and it cannot be expected to systematically generalize across applicable contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.starsem-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.14/>PISA : A measure of Preference In Selection of Arguments to model verb argument recoverability<span class=acl-fixed-case>PISA</span>: A measure of Preference In Selection of Arguments to model verb argument recoverability</a></strong><br><a href=/people/g/giulia-cappelli/>Giulia Cappelli</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--14><div class="card-body p-3 small">Our paper offers a computational model of the semantic recoverability of verb arguments, tested in particular on direct objects and Instruments. Our fully distributional model is intended to improve on older taxonomy-based models, which require a lexicon in addition to the training corpus. We computed the selectional preferences of 99 transitive verbs and 173 Instrument verbs as the mean value of the pairwise cosines between their arguments (a weighted mean between all the arguments, or an unweighted mean with the topmost k arguments). Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can predict the recoverability of objects and Instruments, providing a similar result to that of taxonomy-based models but at a much cheaper computational cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.starsem-1.15/>Learning Negation Scope from <a href=https://en.wikipedia.org/wiki/Syntactic_structure>Syntactic Structure</a></a></strong><br><a href=/people/n/nick-mckenna/>Nick McKenna</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--starsem-1--15><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised model</a> which learns the semantics of negation purely through <a href=https://en.wikipedia.org/wiki/Syntactic_analysis>analysis of syntactic structure</a>. Linguistic theory posits that the semantics of negation can be understood purely syntactically, though recent research relies on combining a variety of features including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and semantic representations to achieve high task performance. Our simplified model returns to syntactic theory and achieves state-of-the-art performance on the task of Negation Scope Detection while demonstrating the tight relationship between the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Negation>semantics of negation</a>.</div></div></div><hr><div id=2020textgraphs-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.textgraphs-1/>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.0/>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fragkiskos-d-malliaros/>Fragkiskos D. Malliaros</a>
|
<a href=/people/i/ioana-hulpus/>Ioana Hulpuș</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/a/abhik-jana/>Abhik Jana</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--textgraphs-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.textgraphs-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.textgraphs-1.1.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.textgraphs-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.1/>A survey of embedding models of entities and relationships for knowledge graph completion</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--textgraphs-1--1><div class="card-body p-3 small">Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--textgraphs-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.textgraphs-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.5/>Contextual BERT : Conditioning the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> Using a Global State<span class=acl-fixed-case>BERT</span>: Conditioning the Language Model Using a Global State</a></strong><br><a href=/people/t/timo-i-denk/>Timo I. Denk</a>
|
<a href=/people/a/ana-peleteiro-ramallo/>Ana Peleteiro Ramallo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--textgraphs-1--5><div class="card-body p-3 small">BERT is a popular <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a <a href=https://en.wikipedia.org/wiki/State_(computer_science)>global state</a> for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> from the literature shows that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> improve <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a> significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.textgraphs-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--textgraphs-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.textgraphs-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.textgraphs-1.13.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.textgraphs-1.13/>Explanation Regeneration via Multi-Hop ILP Inference over Knowledge Base<span class=acl-fixed-case>ILP</span> Inference over Knowledge Base</a></strong><br><a href=/people/a/aayushee-gupta/>Aayushee Gupta</a>
|
<a href=/people/g/gopalakrishnan-srinivasaraghavan/>Gopalakrishnan Srinivasaraghavan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--textgraphs-1--13><div class="card-body p-3 small">Textgraphs 2020 Workshop organized a shared task on &#8216;Explanation Regeneration&#8217; that required reconstructing gold explanations for elementary science questions. This work describes our submission to the task which is based on multiple components : a BERT baseline ranking, an Integer Linear Program (ILP) based re-scoring and a regression model for re-ranking the explanation facts. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a Mean Average Precision score of 0.3659.</div></div></div><hr><div id=2020udw-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.udw-1/>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.0/>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.6/>Verification, Reproduction and Replication of NLP Experiments : a Case Study on Parsing Universal Dependencies<span class=acl-fixed-case>NLP</span> Experiments: a Case Study on Parsing <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--6><div class="card-body p-3 small">As in any field of inquiry that depends on experiments, the <a href=https://en.wikipedia.org/wiki/Verifiability>verifiability</a> of <a href=https://en.wikipedia.org/wiki/Experiment>experimental studies</a> is important in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. Despite increased attention to verification of empirical results, the practices in the field are unclear. Furthermore, we argue, certain traditions and practices that are seemingly useful for <a href=https://en.wikipedia.org/wiki/Verificationism>verification</a> may in fact be counterproductive. We demonstrate this through a set of multi-lingual experiments on parsing Universal Dependencies treebanks. In particular, we show that emphasis on exact replication leads to practices (some of which are now well established) that hide the variation in experimental results, effectively hindering verifiability with a false sense of certainty. The purpose of the present paper is to highlight the magnitude of the issues resulting from these common practices with the hope of instigating further discussion. Once we, as a community, are convinced about the importance of the problems, the solutions are rather obvious, although not necessarily easy to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.7/>From <a href=https://en.wikipedia.org/wiki/Land_mine>LFG</a> To <a href=https://en.wikipedia.org/wiki/Urban_exploration>UD</a> : A Combined Approach<span class=acl-fixed-case>LFG</span> To <span class=acl-fixed-case>UD</span>: A Combined Approach</a></strong><br><a href=/people/c/cheikh-m-bamba-dione/>Cheikh M. Bamba Dione</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--7><div class="card-body p-3 small">This paper reports on a systematic approach for deriving Universal Dependencies from LFG structures. The conversion starts with a step-wise transformation of the c-structure, combining part-of-speech (POS) information and the embedding path to determine the true head of dependency structures. The paper discusses several issues faced by existing <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> when applied on <a href=https://en.wikipedia.org/wiki/Wolof_language>Wolof</a> and presents the <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> used to account for these issues. An experimental evaluation indicated that our approach was able to generate the correct output in more than 90 % of the cases, leading to a substantial improvement in conversion accuracy compared to the previous models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.10/>Configurable Dependency Tree Extraction from CCG Derivations<span class=acl-fixed-case>CCG</span> Derivations</a></strong><br><a href=/people/k/kilian-evang/>Kilian Evang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--10><div class="card-body p-3 small">We revisit the problem of extracting <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency structures</a> from the <a href=https://en.wikipedia.org/wiki/Dependency_grammar>derivation structures</a> of Combinatory Categorial Grammar (CCG). Previous approaches are often restricted to a narrow subset of CCG or support only one flavor of dependency tree. Our approach is more general and easily configurable, so that multiple styles of dependency tree can be obtained. In an initial case study, we show promising results for converting English, German, Italian, and Dutch CCG derivations from the Parallel Meaning Bank into (unlabeled) UD-style dependency trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.21/>First Steps towards Universal Dependencies for Laz<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for <span class=acl-fixed-case>L</span>az</a></strong><br><a href=/people/u/utku-turk/>Utku Türk</a>
|
<a href=/people/k/kaan-bayar/>Kaan Bayar</a>
|
<a href=/people/a/aysegul-dilara-ozercan/>Ayşegül Dilara Özercan</a>
|
<a href=/people/g/gorkem-yigit-ozturk/>Görkem Yiğit Öztürk</a>
|
<a href=/people/s/saziye-betul-ozates/>Şaziye Betül Özateş</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--21><div class="card-body p-3 small">This paper presents the first <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> for the <a href=https://en.wikipedia.org/wiki/Laz_language>Laz language</a>, which is also the first Universal Dependencies Treebank for a <a href=https://en.wikipedia.org/wiki/Languages_of_the_Caucasus>South Caucasian language</a>. This <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> aims to create a syntactically and morphologically annotated resource for further research. We also aim to document an <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered language</a> in a systematic fashion within an inherently cross-linguistic framework : the Universal Dependencies Project (UD). As of now, our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> consists of 576 sentences and 2,306 tokens annotated in light with the UD guidelines. We evaluated the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> on the dependency parsing task using a pretrained multilingual parsing model, and the results are comparable with other low-resourced treebanks with no <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training set</a>. We aim to expand our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> in the near future to include 1,500 sentences. The bigger goal for our project is to create a set of <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> for minority languages in Anatolia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.22/>Dependency annotation of noun incorporation in <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a></a></strong><br><a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/k/karina-mishchenkova/>Karina Mishchenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--22><div class="card-body p-3 small">This paper describes an approach to annotating <a href=https://en.wikipedia.org/wiki/Incorporation_(linguistics)>noun incorporation</a> in Universal Dependencies. It motivates the need to annotate this particular morphosyntactic phenomenon and justifies it with respect to frequency of the construction. A case study is presented in which the proposed annotation scheme is applied to <a href=https://en.wikipedia.org/wiki/Chukchi_language>Chukchi</a>, a language that exhibits <a href=https://en.wikipedia.org/wiki/Incorporation_(linguistics)>noun incorporation</a>. We compare argument encoding in <a href=https://en.wikipedia.org/wiki/Chukchi_language>Chukchi</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and find that while in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Russian discourse elements are primarily tracked through noun phrases and pronouns, in <a href=https://en.wikipedia.org/wiki/Chukchi_language>Chukchi</a> they are tracked through agreement marking and incorporation, with a lesser role for noun phrases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--udw-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.udw-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.23/>Universal Dependency Treebank for Xibe<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebank for <span class=acl-fixed-case>X</span>ibe</a></strong><br><a href=/people/h/he-zhou/>He Zhou</a>
|
<a href=/people/j/juyeon-chung/>Juyeon Chung</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--udw-1--23><div class="card-body p-3 small">We present our work of constructing the first <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> for the <a href=https://en.wikipedia.org/wiki/Xibe>Xibe language</a> following the Universal Dependencies (UD) annotation scheme. Xibe is a low-resourced and severely endangered Tungusic language spoken by the Xibe minority living in the Xinjiang Uygur Autonomous Region of China. We collected 810 sentences so far, including 544 sentences from a grammar book on written Xibe and 266 sentences from Cabcal News. We annotated those sentences manually from scratch. In this paper, we report the procedure of building this <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> and analyze several important annotation issues of our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. Finally, we propose our plans for future work.</div></div></div><hr><div id=2020vardial-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.vardial-1/>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.0/>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.2/>ASR for Non-standardised Languages with Dialectal Variation : the case of <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss German</a><span class=acl-fixed-case>ASR</span> for Non-standardised Languages with Dialectal Variation: the case of <span class=acl-fixed-case>S</span>wiss <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/i/iuliia-nigmatulina/>Iuliia Nigmatulina</a>
|
<a href=/people/t/tannon-kew/>Tannon Kew</a>
|
<a href=/people/t/tanja-samardzic/>Tanja Samardzic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--2><div class="card-body p-3 small">Strong regional variation, together with the lack of standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>, makes Swiss German automatic speech recognition (ASR) particularly difficult in a multi-dialectal setting. This paper focuses on one of the many challenges, namely, the choice of the output text to represent non-standardised Swiss German. We investigate two potential options : a) dialectal writing approximate phonemic transcriptions that provide close correspondence between grapheme labels and the acoustic signal but are highly inconsistent and b) normalised writing transcriptions resembling standard <a href=https://en.wikipedia.org/wiki/German_language>German</a> that are relatively consistent but distant from the acoustic signal. To find out which <a href=https://en.wikipedia.org/wiki/Writing>writing</a> facilitates Swiss German ASR, we build several systems using the <a href=https://en.wikipedia.org/wiki/Kaldi_(software)>Kaldi toolkit</a> and a dataset covering 14 regional varieties. A formal comparison shows that the system trained on the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalised transcriptions</a> achieves better results in word error rate (WER) (29.39 %) but underperforms at the <a href=https://en.wikipedia.org/wiki/Character_(computing)>character level</a>, suggesting <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>dialectal transcriptions</a> offer a viable solution for downstream applications where <a href=https://en.wikipedia.org/wiki/Dialect>dialectal differences</a> are important. To better assess word-level performance for dialectal transcriptions, we use a flexible WER measure (FlexWER). When evaluated with this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, the system trained on <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>dialectal transcriptions</a> outperforms that trained on the normalised writing. Besides establishing a benchmark for Swiss German multi-dialectal ASR, our findings can be helpful in designing ASR systems for other languages without standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.4/>Machine-oriented NMT Adaptation for Zero-shot NLP tasks : Comparing the Usefulness of Close and Distant Languages<span class=acl-fixed-case>NMT</span> Adaptation for Zero-shot <span class=acl-fixed-case>NLP</span> tasks: Comparing the Usefulness of Close and Distant Languages</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--4><div class="card-body p-3 small">Neural Machine Translation (NMT) models are typically trained by considering humans as end-users and maximizing human-oriented objectives. However, in some scenarios, their output is consumed by automatic NLP components rather than by humans. In these scenarios, translations&#8217; quality is measured in terms of their fitness for purpose (i.e. maximizing performance of external NLP tools) rather than in terms of standard human fluency / adequacy criteria. Recently, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning techniques</a> exploiting the feedback from downstream NLP tools have been proposed for machine-oriented NMT adaptation. In this work, we tackle the problem in a multilingual setting where a single NMT model translates from multiple languages for downstream automatic processing in the target language. Knowledge sharing across close and distant languages allows to apply our machine-oriented approach in the zero-shot setting where no labeled data for the test language is seen at training time. Moreover, we incorporate multi-lingual BERT in the source side of our NMT system to benefit from the knowledge embedded in this model. Our experiments show coherent performance gains, for different language directions over both i) generic NMT models (trained for human consumption), and ii) fine-tuned multilingual BERT. This gain for zero-shot language directions (e.g. SpanishEnglish) is higher when the models are fine-tuned on a closely-related source language (Italian) than a distant one (German).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.5/>Character Alignment in Morphologically Complex Translation Sets for Related Languages</a></strong><br><a href=/people/m/michael-gasser/>Michael Gasser</a>
|
<a href=/people/b/binyam-ephrem-seyoum/>Binyam Ephrem Seyoum</a>
|
<a href=/people/n/nazareth-amlesom-kifle/>Nazareth Amlesom Kifle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--5><div class="card-body p-3 small">For languages with complex morphology, word-to-word translation is a task with various potential applications, for example, in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Language_education>language instruction</a>, and dictionary creation, as well as in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In this paper, we confine ourselves to the subtask of <a href=https://en.wikipedia.org/wiki/Character_alignment>character alignment</a> for the particular case of <a href=https://en.wikipedia.org/wiki/Language_family>families of related languages</a> with very few resources for most or all members. There are many such <a href=https://en.wikipedia.org/wiki/Language_family>families</a> ; we focus on the subgroup of Semitic languages spoken in <a href=https://en.wikipedia.org/wiki/Ethiopia>Ethiopia</a> and <a href=https://en.wikipedia.org/wiki/Eritrea>Eritrea</a>. We begin with an adaptation of the familiar <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment algorithms</a> behind <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>, modifying them as appropriate for our task. We show how <a href=https://en.wikipedia.org/wiki/Character_alignment>character alignment</a> can reveal morphological, phonological, and orthographic correspondences among related languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.6/>Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/n/navaneethan-rajasekaran/>Navaneethan Rajasekaran</a>
|
<a href=/people/m/mihael-arcan/>Mihael Arcan</a>
|
<a href=/people/k/kevin-mcguinness/>Kevin McGuinness</a>
|
<a href=/people/n/noel-e-oconnor/>Noel E. O’Connor</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--6><div class="card-body p-3 small">Bilingual lexicons are a vital tool for under-resourced languages and recent state-of-the-art approaches to this leverage pretrained monolingual word embeddings using supervised or semi-supervised approaches. However, these approaches require cross-lingual information such as seed dictionaries to train the model and find a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> between the word embedding spaces. Especially in the case of low-resourced languages, seed dictionaries are not readily available, and as such, these methods produce extremely weak results on these <a href=https://en.wikipedia.org/wiki/Programming_language>languages</a>. In this work, we focus on the <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>, namely <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, which are even more challenging as they are written in unique scripts. To take advantage of <a href=https://en.wikipedia.org/wiki/Orthography>orthographic information</a> and <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>, we bring the related languages into a single script. Previous approaches have used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these <a href=https://en.wikipedia.org/wiki/Language>languages</a> many times, making bilingual lexicon induction approaches feasible for such under-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.9/>Recycling and Comparing Morphological Annotation Models for Armenian Diachronic-Variational Corpus Processing<span class=acl-fixed-case>A</span>rmenian Diachronic-Variational Corpus Processing</a></strong><br><a href=/people/c/chahan-vidal-gorene/>Chahan Vidal-Gorène</a>
|
<a href=/people/v/victoria-khurshudyan/>Victoria Khurshudyan</a>
|
<a href=/people/a/anaid-donabedian-demopoulos/>Anaïd Donabédian-Demopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--9><div class="card-body p-3 small">Armenian is a language with significant variation and unevenly distributed NLP resources for different varieties. An attempt is made to process an <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN model</a> for morphological annotation on the basis of different Armenian data (provided or not with morphologically annotated corpora), and to compare the annotation results of <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a> and rule-based models. Different tests were carried out to evaluate the reuse of an unspecialized model of lemmatization and POS-tagging for under-resourced language varieties. The research focused on three dialects and further extended to <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a> with a <a href=https://en.wikipedia.org/wiki/Mean>mean accuracy</a> of 94,00 % in <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and 97,02 % in POS-tagging, as well as a possible reusability of models to cover different other <a href=https://en.wikipedia.org/wiki/Armenian_language>Armenian varieties</a>. Interestingly, the comparison of an RNN model trained on <a href=https://en.wikipedia.org/wiki/Eastern_Armenian>Eastern Armenian</a> with the <a href=https://en.wikipedia.org/wiki/Eastern_Armenian>Eastern Armenian National Corpus rule-based model</a> applied to <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a> showed an enhancement of 19 % in parsing. This <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> covers 88,79 % of a short heterogeneous dataset in <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a>, and could be a baseline for a massive corpus annotation in that standard. It is argued that an RNN-based model can be a valid alternative to a rule-based one giving consideration to such factors as time-consumption, <a href=https://en.wikipedia.org/wiki/Reusability>reusability</a> for different varieties of a target language and significant qualitative results in morphological annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.16/>Uralic Language Identification (ULI) 2020 shared task dataset and the Wanca 2017 corpora<span class=acl-fixed-case>ULI</span>) 2020 shared task dataset and the Wanca 2017 corpora</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--16><div class="card-body p-3 small">This article introduces the Wanca 2017 web corpora from which the sentences written in minor Uralic languages were collected for the test set of the Uralic Language Identification (ULI) 2020 shared task. We describe the ULI shared task and how the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> was constructed using the Wanca 2017 corpora and texts in different languages from the Leipzig corpora collection. We also provide the results of a baseline language identification experiment conducted using the ULI 2020 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.19/>HeLju@VarDial 2020 : Social Media Variety Geolocation with BERT Models<span class=acl-fixed-case>H</span>e<span class=acl-fixed-case>L</span>ju@<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2020: Social Media Variety Geolocation with <span class=acl-fixed-case>BERT</span> Models</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--19><div class="card-body p-3 small">This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> by far, and that improvements obtained by pre-training <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction : <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and <a href=https://en.wikipedia.org/wiki/Dialect>dialectal features</a>, both of which are handled well by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.21/>Experiments in Language Variety Geolocation and Dialect Identification</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--21><div class="card-body p-3 small">In this paper we describe the systems we used when participating in the VarDial Evaluation Campaign organized as part of the 7th workshop on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for similar languages, varieties and dialects. The shared tasks we participated in were the second edition of the Romanian Dialect Identification (RDI) and the first edition of the Social Media Variety Geolocation (SMG). The submissions of our SUKI team used generative language models based on <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a> and character n-grams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.22/>Exploring the Power of Romanian BERT for Dialect Identification<span class=acl-fixed-case>R</span>omanian <span class=acl-fixed-case>BERT</span> for Dialect Identification</a></strong><br><a href=/people/g/george-eduard-zaharia/>George-Eduard Zaharia</a>
|
<a href=/people/a/andrei-marius-avram/>Andrei-Marius Avram</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--22><div class="card-body p-3 small">Dialect identification represents a key aspect for improving a series of tasks, for example, <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>, considering that the location of the speaker can greatly influence the attitude towards a subject. In this work, we describe the systems developed by our team for VarDial 2020 : Romanian Dialect Identification, a task specifically created for challenging participants to solve the previously mentioned issue. More specifically, we introduce a series of neural systems based on Transformers, that combine a BERT model exclusively pre-trained on the <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian language</a> with techniques such as <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> or character-level embeddings. By using these approaches, we were able to obtain a 0.6475 macro F1 score on the test dataset, thus allowing us to be ranked 5th out of 8 participant teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.27/>Geolocation of Tweets with a BiLSTM Regression Model<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Regression Model</a></strong><br><a href=/people/p/piyush-mishra/>Piyush Mishra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--27><div class="card-body p-3 small">Identifying a user&#8217;s location can be useful for <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a>, <a href=https://en.wikipedia.org/wiki/Demography>demographic analyses</a>, and <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster outbreak monitoring</a>. Although <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> allows users to voluntarily reveal their location, such information is n&#8217;t universally available. Analyzing a tweet can provide a general estimation of a tweet location while giving insight into the dialect of the user and other <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a>. Such <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic attributes</a> can be used to provide a regional approximation of tweet origins. In this paper, we present a neural regression model that can identify the linguistic intricacies of a tweet to predict the location of the user. The final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies the <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> embedded in the tweet and predicts the location of the tweet.</div></div></div><hr><div id=2020wanlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wanlp-1/>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.0/>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/i/imed-zitouni/>Imed Zitouni</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.5/>A Semi-Supervised BERT Approach for Arabic Named Entity Recognition<span class=acl-fixed-case>BERT</span> Approach for <span class=acl-fixed-case>A</span>rabic Named Entity Recognition</a></strong><br><a href=/people/c/chadi-helwe/>Chadi Helwe</a>
|
<a href=/people/g/ghassan-dib/>Ghassan Dib</a>
|
<a href=/people/m/mohsen-shamas/>Mohsen Shamas</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--5><div class="card-body p-3 small">Named entity recognition (NER) plays a significant role in many applications such as <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and even <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Most of the work on NER using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> was done for non-Arabic languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and only few studies focused on <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets : AQMAR, NEWS, and TWEETS. We report a significant improvement in <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.14/>MANorm : A Normalization Dictionary for Moroccan Arabic Dialect Written in Latin Script<span class=acl-fixed-case>MAN</span>orm: A Normalization Dictionary for <span class=acl-fixed-case>M</span>oroccan <span class=acl-fixed-case>A</span>rabic Dialect Written in <span class=acl-fixed-case>L</span>atin Script</a></strong><br><a href=/people/r/randa-zarnoufi/>Randa Zarnoufi</a>
|
<a href=/people/h/hamid-jaafar/>Hamid Jaafar</a>
|
<a href=/people/w/walid-bachri/>Walid Bachri</a>
|
<a href=/people/m/mounia-abik/>Mounia Abik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--14><div class="card-body p-3 small">Social media user generated text is actually the main resource for many NLP tasks. This text, however, does not follow the standard <a href=https://en.wikipedia.org/wiki/Writing_system>rules of writing</a>. Moreover, the use of <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> such as <a href=https://en.wikipedia.org/wiki/Moroccan_Arabic>Moroccan Arabic</a> in <a href=https://en.wikipedia.org/wiki/Writing>written communications</a> increases further NLP tasks complexity. A <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> is a <a href=https://en.wikipedia.org/wiki/Spoken_language>verbal language</a> that does not have a standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>. The <a href=https://en.wikipedia.org/wiki/Dialect>written dialect</a> is based on the phonetic transliteration of spoken words which leads users to improvise spelling while writing. Thus, for the same word we can find multiple forms of <a href=https://en.wikipedia.org/wiki/Transliteration>transliterations</a>. Subsequently, it is mandatory to normalize these different <a href=https://en.wikipedia.org/wiki/Transliteration>transliterations</a> to one canonical word form. To reach this goal, we have exploited the powerfulness of word embedding models generated with a corpus of YouTube comments. Besides, using a Moroccan Arabic dialect dictionary that provides the canonical forms, we have built a normalization dictionary that we refer to as MANorm. We have conducted several experiments to demonstrate the efficiency of MANorm, which have shown its usefulness in dialect normalization. We made MANorm freely available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.17/>AraWEAT : Multidimensional Analysis of Biases in Arabic Word Embeddings<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>WEAT</span>: Multidimensional Analysis of Biases in <span class=acl-fixed-case>A</span>rabic Word Embeddings</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/r/rafik-takieddin/>Rafik Takieddin</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--17><div class="card-body p-3 small">Recent work has shown that distributional word vector spaces often encode <a href=https://en.wikipedia.org/wiki/Bias>human biases</a> like <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> or <a href=https://en.wikipedia.org/wiki/Racism>racism</a>. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely : embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.18/>Parallel resources for Tunisian Arabic Dialect Translation<span class=acl-fixed-case>T</span>unisian <span class=acl-fixed-case>A</span>rabic Dialect Translation</a></strong><br><a href=/people/s/sameh-kchaou/>Saméh Kchaou</a>
|
<a href=/people/r/rahma-boujelbane/>Rahma Boujelbane</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich-Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--18><div class="card-body p-3 small">The difficulty of processing <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> is clearly observed in the high cost of building <a href=https://en.wikipedia.org/wiki/Text_corpus>representative corpus</a>, in particular for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Indeed, all <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> require a huge amount and good management of training data, which represents a challenge in a low-resource setting such as the <a href=https://en.wikipedia.org/wiki/Tunisian_Arabic>Tunisian Arabic dialect</a>. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in order to build a Machine Translation (MT) model. The created <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> was used to build a sentence-based translation model. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reached a BLEU score of 15.03 % on a test set, while it was limited to 13.27 % utilizing the corpus without augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wanlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.21/>Improving Arabic Text Categorization Using Transformer Training Diversification<span class=acl-fixed-case>A</span>rabic Text Categorization Using Transformer Training Diversification</a></strong><br><a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/j/jung-soon-gyo/>Jung Soon-Gyo</a>
|
<a href=/people/j/joni-salminen/>Joni Salminen</a>
|
<a href=/people/b/bernard-j-jansen/>Bernard J. Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--21><div class="card-body p-3 small">Automatic categorization of short texts, such as <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>, has many applications ranging from <a href=https://en.wikipedia.org/wiki/Content_analysis>content analysis</a> to <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a>. In this paper, we use such <a href=https://en.wikipedia.org/wiki/Categorization>text categorization</a> i.e., labeling the social media posts to categories like &#8216;sports&#8217;, &#8216;politics&#8217;, &#8216;human-rights&#8217; among others, to showcase the efficacy of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> across different sources and varieties of <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. In doing so, we show that diversifying the training data, whether by using diverse training data for the specific task (an increase of 21 % macro F1) or using diverse data to pre-train a BERT model (26 % macro F1), leads to overall improvements in classification effectiveness. In our work, we also introduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, and <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>dialectal Arabic</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.22/>Team Alexa at NADI Shared Task<span class=acl-fixed-case>A</span>lexa at <span class=acl-fixed-case>NADI</span> Shared Task</a></strong><br><a href=/people/m/mutaz-younes/>Mutaz Younes</a>
|
<a href=/people/n/nour-al-khdour/>Nour Al-khdour</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--22><div class="card-body p-3 small">In this paper, we discuss our team&#8217;s work on the NADI Shared Task. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires classifying Arabic tweets among 21 dialects. We tested out different <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>, and the best <a href=https://en.wikipedia.org/wiki/One_(pronoun)>one</a> was the simplest one. Our best submission was using Multinational Naive Bayes (MNB) classifier (Small and Hsiao, 1985) with <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>. Despite its simplicity, this <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> shows better results than complicated <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as BERT. Our best submitted score was 17 % <a href=https://en.wikipedia.org/wiki/F1_score>F1-score</a> and 35 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.27/>Weighted combination of BERT and N-GRAM features for Nuanced Arabic Dialect Identification<span class=acl-fixed-case>BERT</span> and N-<span class=acl-fixed-case>GRAM</span> features for Nuanced <span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/a/ahmed-alami/>Ahmed Alami</a>
|
<a href=/people/h/hamza-alami/>Hamza Alami</a>
|
<a href=/people/a/ahmed-khoumsi/>Ahmed Khoumsi</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--27><div class="card-body p-3 small">Around the Arab world, different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> are spoken by more than 300 M persons, and are increasingly popular in social media texts. However, <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> are considered to be low-resource languages, limiting the development of machine-learning based systems for these <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a>. In this paper, we investigate the Arabic dialect identification task, from two perspectives : country-level dialect identification from 21 Arab countries, and province-level dialect identification from 100 provinces. We introduce an unified pipeline of state-of-the-art models, that can handle the two subtasks. Our experimental studies applied to the NADI shared task, show promising results both at the country-level (F1-score of 25.99 %) and the province-level (F1-score of 6.39 %), and thus allow us to be ranked 2nd for the country-level subtask, and 1st in the province-level subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.29/>Faheem at NADI shared task : Identifying the dialect of Arabic tweet<span class=acl-fixed-case>NADI</span> shared task: Identifying the dialect of <span class=acl-fixed-case>A</span>rabic tweet</a></strong><br><a href=/people/n/nouf-alshenaifi/>Nouf AlShenaifi</a>
|
<a href=/people/a/aqil-azmi/>Aqil Azmi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--29><div class="card-body p-3 small">This paper describes Faheem (adj. of understand), our submission to NADI (Nuanced Arabic Dialect Identification) shared task. With so many <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> being under-studied due to the scarcity of the resources, the objective is to identify the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialect</a> used in the tweet, country wise. We propose a machine learning approach where we utilize word-level n-gram (n = 1 to 3) and tf-idf features and feed them to six different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We train the <a href=https://en.wikipedia.org/wiki/System>system</a> using a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 21,000 tweetsprovided by the organizerscovering twenty-one Arab countries. Our top performing classifiers are : <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a>, <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a>, and Multinomial Na ve Bayes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.31/>The QMUL / HRBDT contribution to the NADI Arabic Dialect Identification Shared Task<span class=acl-fixed-case>QMUL</span>/<span class=acl-fixed-case>HRBDT</span> contribution to the <span class=acl-fixed-case>NADI</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/a/abdulrahman-aloraini/>Abdulrahman Aloraini</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/a/ayman-alhelbawy/>Ayman Alhelbawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--31><div class="card-body p-3 small">We present the Arabic dialect identification system that we used for the country-level subtask of the NADI challenge. Our model consists of three components : BiLSTM-CNN, character-level TF-IDF, and topic modeling features. We represent each tweet using these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and feed them into a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a>. We then add an effective <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic</a> that improves the overall performance. We achieved an F1-Macro score of 20.77 % and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 34.32 % on the test set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was also evaluated on the Arabic Online Commentary dataset, achieving results better than the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>