<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Subword and Character LEvel Models in NLP (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Subword and Character LEvel Models in NLP (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w17-41>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li></ul></div></div><div id=w17-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-41/>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4100/>Proceedings of the First Workshop on Subword and Character Level Models in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4101/>Character and Subword-Based Word Representation for Neural Language Modeling Prediction</a></strong><br><a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/a/alexandre-allauzen/>Alexandre Allauzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4101><div class="card-body p-3 small">Most of neural language models use different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a>. While <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> can be associated to each word in the vocabulary or derived from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> as well as factored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the context of prediction. This work investigates the effect of using subword units (character and factored morphological decomposition) to build output representations for neural language modeling. We present a case study on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, a morphologically-rich language, experimenting with different input and output representations. When working with the full training vocabulary, despite unstable training, our experiments show that augmenting the output word representations with character-based embeddings can significantly improve the performance of the model. Moreover, reducing the size of the output look-up table, to let the character-based embeddings represent rare words, brings further improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4102/>Learning variable length units for SMT between related languages via Byte Pair Encoding<span class=acl-fixed-case>SMT</span> between related languages via Byte Pair Encoding</a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4102><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segments</a> learnt using Byte Pair Encoding (referred to as BPE units) as basic units for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11 % increase in BLEU score. While orthographic syllables can be used only for languages whose <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4103/>Character Based Pattern Mining for Neology Detection</a></strong><br><a href=/people/g/gael-lejeune/>Gaël Lejeune</a>
|
<a href=/people/e/emmanuel-cartier/>Emmanuel Cartier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4103><div class="card-body p-3 small">Detecting neologisms is essential in real-time natural language processing applications. Not only can it enable to follow the lexical evolution of languages, but it is also essential for updating linguistic resources and <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. In this paper, neology detection is considered as a classification task where a system has to assess whether a given lexical item is an actual <a href=https://en.wikipedia.org/wiki/Neologism>neologism</a> or not. We propose a combination of an unsupervised data mining technique and a supervised machine learning approach. It is inspired by current researches in <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> and on token-level and character-level patterns. We train and evaluate our system on a manually designed reference dataset in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We show that this approach is able to largely outperform state-of-the-art neology detection systems. Furthermore, character-level patterns exhibit good properties for multilingual extensions of the <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4104/>Automated Word Stress Detection in Russian<span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/m/maria-ponomareva/>Maria Ponomareva</a>
|
<a href=/people/k/kirill-milintsevich/>Kirill Milintsevich</a>
|
<a href=/people/e/ekaterina-chernyak/>Ekaterina Chernyak</a>
|
<a href=/people/a/anatoli-starostin/>Anatoly Starostin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4104><div class="card-body p-3 small">In this study we address the problem of automated word stress detection in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> using character level models and no part-speech-taggers. We use a simple bidirectional RNN with LSTM nodes and achieve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 90 % or higher. We experiment with two training datasets and show that using the data from an annotated corpus is much more efficient than using only a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, since it allows to retain the context of the word and its <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4105/>A Syllable-based Technique for Word Embeddings of Korean Words<span class=acl-fixed-case>K</span>orean Words</a></strong><br><a href=/people/s/sanghyuk-choi/>Sanghyuk Choi</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a>
|
<a href=/people/j/jinseok-seol/>Jinseok Seol</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4105><div class="card-body p-3 small">Word embedding has become a fundamental component to many NLP tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, popular <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that learn such embeddings are unaware of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology of words</a>, so it is not directly applicable to highly <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a> such as <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. We propose a syllable-based learning model for <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, in which word representation is composed of trained syllable vectors. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4106/>Supersense Tagging with a Combination of Character, Subword, and Word-level Representations</a></strong><br><a href=/people/y/youhyun-shin/>Youhyun Shin</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4106><div class="card-body p-3 small">Recently, there has been increased interest in utilizing <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters or subwords</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tasks</a>. However, the effect of utilizing <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character</a>, subword, and word-level information simultaneously has not been examined so far. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to leverage various levels of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>input features</a> to improve on the performance of an supersense tagging task. Detailed analysis of experimental results show that different levels of input representation offer distinct characteristics that explain performance discrepancy among different tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4107/>Weakly supervised learning of allomorphy</a></strong><br><a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4107><div class="card-body p-3 small">Most NLP resources that offer annotations at the word segment level provide morphological annotation that includes features indicating tense, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_case>case</a>, and other inflectional information. Such <a href=https://en.wikipedia.org/wiki/Information>information</a> is rarely aligned to the relevant parts of the wordsi.e. the <a href=https://en.wikipedia.org/wiki/Allomorphism>allomorphs</a>, as such <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> would be very costly. These unaligned weak labelings are commonly provided by annotated NLP corpora such as <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> in various languages. Although they lack alignment information, the presence / absence of labels at the word level is also consistent with the amount of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> assumed to be provided to L1 and L2 learners. In this paper, we explore several methods to learn this latent alignment between parts of word forms and the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical information</a> provided. All the <a href=https://en.wikipedia.org/wiki/Linguistic_description>methods</a> under investigation favor hypotheses regarding allomorphs of morphemes that re-use a small inventory, i.e. implicitly minimize the number of <a href=https://en.wikipedia.org/wiki/Allomorphism>allomorphs</a> that a <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> can be realized as. We show that the provided <a href=https://en.wikipedia.org/wiki/Information>information</a> offers a significant advantage for both <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and the learning of allomorphy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4108/>Character-based recurrent neural networks for morphological relational reasoning</a></strong><br><a href=/people/o/olof-mogren/>Olof Mogren</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4108><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for predicting word forms based on morphological relational reasoning with <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a>. While previous work has explored tasks such as <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> and reinflection, these models rely on an explicit enumeration of morphological features, which may not be available in all cases. To address the task of predicting a word form given a demo relation (a pair of word forms) and a query word, we devise a character-based recurrent neural network architecture using three separate encoders and a decoder. We also investigate a multiclass learning setup, where the prediction of the relation type label is used as an auxiliary task. Our results show that the exact form can be predicted for <a href=https://en.wikipedia.org/wiki/English_language>English</a> with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.7 %. For <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, which has a more complex morphology with more <a href=https://en.wikipedia.org/wiki/Inflection>inflectional patterns</a> for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, the accuracy is 89.3 %. We also show that using the auxiliary task of learning the relation type speeds up convergence and improves the prediction accuracy for the word generation task.<i>morphological relational reasoning</i> with analogies. While previous work has explored\n tasks such as morphological inflection and reinflection, these models rely\n on an explicit enumeration of morphological features, which may not be\n available in all cases. To address the task of predicting a word form\n given a <i>demo relation</i> (a pair of word forms) and a <i>query word</i>, we devise a\n character-based recurrent neural network architecture using three separate\n encoders and a decoder. We also investigate a multiclass learning setup,\n where the prediction of the relation type label is used as an auxiliary\n task. Our results show that the exact form can be predicted for English\n with an accuracy of 94.7%. For Swedish, which has a more complex\n morphology with more inflectional patterns for nouns and verbs, the\n accuracy is 89.3%. We also show that using the auxiliary task of learning\n the relation type speeds up convergence and improves the prediction\n accuracy for the word generation task.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4110/>Exploring Cross-Lingual Transfer of Morphological Knowledge In Sequence-to-Sequence Models</a></strong><br><a href=/people/h/huiming-jin/>Huiming Jin</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4110><div class="card-body p-3 small">Multi-task training is an effective method to mitigate the data sparsity problem. It has recently been applied for cross-lingual transfer learning for paradigm completionthe task of producing inflected forms of lemmatawith sequence-to-sequence networks. However, it is still vague how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about <a href=https://en.wikipedia.org/wiki/Language>language</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> can be transferred.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4111/>Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4111><div class="card-body p-3 small">We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflectionthe task of generating one inflected wordform from another. This is achieved by using unlabeled tokens or random strings as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.92 % improvement over state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4112.Attachment.rar data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4112/>Vowel and Consonant Classification through <a href=https://en.wikipedia.org/wiki/Spectral_decomposition>Spectral Decomposition</a></a></strong><br><a href=/people/p/patricia-thaine/>Patricia Thaine</a>
|
<a href=/people/g/gerald-penn/>Gerald Penn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4112><div class="card-body p-3 small">We consider two related problems in this paper. Given an undeciphered alphabetic writing system or mono-alphabetic cipher, determine : (1) which of its letters are vowels and which are consonants ; and (2) whether the <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a> is a vocalic alphabet or an <a href=https://en.wikipedia.org/wiki/Abjad>abjad</a>. We are able to show that a very simple <a href=https://en.wikipedia.org/wiki/Spectral_decomposition>spectral decomposition</a> based on character co-occurrences provides nearly perfect performance with respect to answering both question types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4113.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4113/>Syllable-level Neural Language Model for Agglutinative Language</a></strong><br><a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/n/nilesh-kulkarni/>Nilesh Kulkarni</a>
|
<a href=/people/h/haejun-lee/>Haejun Lee</a>
|
<a href=/people/j/jihie-kim/>Jihie Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4113><div class="card-body p-3 small">We introduce a novel method to diminish the problem of out of vocabulary words by introducing an embedding method which leverages the agglutinative property of language. We propose additional embedding derived from <a href=https://en.wikipedia.org/wiki/Syllable>syllables</a> and <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> for the words to improve the performance of <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. We apply the above method to input prediction tasks and achieve state of the art performance in terms of Key Stroke Saving (KSS) w.r.t. to existing device input prediction methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4115/>Word Representation Models for Morphologically Rich Languages in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4115><div class="card-body p-3 small">Out-of-vocabulary words present a great challenge for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. Recently various character-level compositional models were proposed to address this issue. In current research we incorporate two most popular neural architectures, namely LSTM and CNN, into hard- and soft-attentional models of translation for character-level representation of the source. We propose semantic and morphological intrinsic evaluation of encoder-level representations. Our analysis of the learned representations reveals that character-based LSTM seems to be better at capturing morphological aspects compared to character-based CNN. We also show that hard-attentional model provides better character-level representations compared to vanilla one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4116/>Spell-Checking based on <a href=https://en.wikipedia.org/wiki/Syllabification>Syllabification</a> and Character-level Graphs for a Peruvian Agglutinative Language<span class=acl-fixed-case>P</span>eruvian Agglutinative Language</a></strong><br><a href=/people/c/carlo-alva/>Carlo Alva</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4116><div class="card-body p-3 small">There are several <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>native languages</a> in Peru which are mostly agglutinative. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> are transmitted from generation to generation mainly in <a href=https://en.wikipedia.org/wiki/Oral_tradition>oral form</a>, causing different forms of writing across different communities. For this reason, there are recent efforts to standardize the <a href=https://en.wikipedia.org/wiki/Spelling>spelling</a> in the written texts, and it would be beneficial to support these tasks with an automatic tool such as an <a href=https://en.wikipedia.org/wiki/Spell_checker>spell-checker</a>. In this way, this spelling corrector is being developed based on two steps : an automatic rule-based syllabification method and a character-level graph to detect the degree of error in a misspelled word. The experiments were realized on Shipibo-konibo, a highly agglutinative and amazonian language, and the results obtained have been promising in a dataset built for the purpose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4117/>What do we need to know about an unknown word when parsing German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4117><div class="card-body p-3 small">We propose a new type of subword embedding designed to provide more information about unknown compounds, a major source for OOV words in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We present an extrinsic evaluation where we use the compound embeddings as input to a neural dependency parser and compare the results to the ones obtained with other types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Our evaluation shows that adding compound embeddings yields a significant improvement of 2 % LAS over using word embeddings when no POS information is available. When adding POS embeddings to the input, however, the effect levels out. This suggests that it is not the missing information about the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the unknown words that causes problems for parsing <a href=https://en.wikipedia.org/wiki/German_language>German</a>, but the lack of morphological information for unknown words. To augment our evaluation, we also test the new embeddings in a language modelling task that requires both syntactic and semantic information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4118/>A General-Purpose Tagger with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4118><div class="card-body p-3 small">We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks : without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem ; it performs well on artificially unnormalized texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4119/>Reconstruction of Word Embeddings from Sub-Word Parameters</a></strong><br><a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4119><div class="card-body p-3 small">Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size. We propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Our approach is quite simple : before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures. We report interesting results on a variety of tasks : word similarity, word analogy, and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4120/>Inflection Generation for Spanish Verbs using <a href=https://en.wikipedia.org/wiki/Supervised_learning>Supervised Learning</a><span class=acl-fixed-case>S</span>panish Verbs using Supervised Learning</a></strong><br><a href=/people/c/cristina-barros/>Cristina Barros</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/e/elena-lloret/>Elena Lloret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4120><div class="card-body p-3 small">We present a novel supervised approach to inflection generation for <a href=https://en.wikipedia.org/wiki/Spanish_verbs>verbs in Spanish</a>. Our system takes as input the verb&#8217;s lemma form and the desired features such as <a href=https://en.wikipedia.org/wiki/Grammatical_person>person</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_number>number</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a>, and is able to predict the appropriate <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>grammatical conjugation</a>. Even though our approach learns from fewer examples comparing to previous work, it is able to deal with all the Spanish moods (indicative, subjunctive and imperative) in contrast to previous work which only focuses on indicative and subjunctive moods. We show that in an intrinsic evaluation, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 99 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, outperforming (although not significantly) two competitive state-of-art systems. The successful results obtained clearly indicate that our approach could be integrated into wider approaches related to <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4122/>Sub-character Neural Language Modelling in Japanese<span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/v/viet-nguyen/>Viet Nguyen</a>
|
<a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4122><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Languages_of_East_Asia>East Asian languages</a> such as <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a character are (somewhat) reflected in its sub-character elements. This paper examines the effect of using <a href=https://en.wikipedia.org/wiki/Character_(computing)>sub-characters</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. This is achieved by decomposing <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> according to a range of character decomposition datasets, and training a neural language model over variously decomposed character representations. Our results indicate that <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> can be improved through the inclusion of <a href=https://en.wikipedia.org/wiki/Character_(computing)>sub-characters</a>, though this result depends on a good choice of <a href=https://en.wikipedia.org/wiki/Data_set>decomposition dataset</a> and the appropriate granularity of decomposition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4123 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4123/>Byte-based Neural Machine Translation</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4123><div class="card-body p-3 small">This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. We compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4124 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4124/>Improving Opinion-Target Extraction with Character-Level Word Embeddings</a></strong><br><a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4124><div class="card-body p-3 small">Fine-grained sentiment analysis is receiving increasing attention in recent years. Extracting opinion target expressions (OTE) in reviews is often an important step in fine-grained, aspect-based sentiment analysis. Retrieving this information from <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a>, however, can be difficult. Customer reviews, for instance, are prone to contain misspelled words and are difficult to process due to their <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a>. In this work, we investigate whether character-level models can improve the performance for the identification of opinion target expressions. We integrate information about the character structure of a word into a sequence labeling system using character-level word embeddings and show their positive impact on the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance. Specifically, we obtain an increase by 3.3 points <a href=https://en.wikipedia.org/wiki/F-score>F1-score</a> with respect to our baseline model. In further experiments, we reveal encoded character patterns of the learned embeddings and give a nuanced view of the performance differences of both models.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>