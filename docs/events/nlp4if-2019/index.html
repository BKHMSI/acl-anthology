<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on NLP for Internet Freedom (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on NLP for Internet Freedom (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d19-50>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li></ul></div></div><div id=d19-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-50/>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5000/>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a></strong><br><a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/c/chris-brew/>Chris Brew</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5002/>Detecting context abusiveness using hierarchical deep learning</a></strong><br><a href=/people/j/ju-hyoung-lee/>Ju-Hyoung Lee</a>
|
<a href=/people/j/jun-u-park/>Jun-U Park</a>
|
<a href=/people/j/jeong-won-cha/>Jeong-Won Cha</a>
|
<a href=/people/y/yo-sub-han/>Yo-Sub Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5002><div class="card-body p-3 small">Abusive text is a serious problem in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and causes many issues among users as the number of users and the content volume increase. There are several attempts for detecting or preventing abusive text effectively. One simple yet effective approach is to use an abusive lexicon and determine the existence of an abusive word in text. This approach works well even when an abusive word is obfuscated. On the other hand, it is still a challenging problem to determine <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a> in a text having no explicit abusive words. Especially, it is hard to identify sarcasm or offensiveness in context without any <a href=https://en.wikipedia.org/wiki/Abuse>abusive words</a>. We tackle this problem using an ensemble deep learning model. Our model consists of two parts of extracting local features and global features, which are crucial for identifying implicit abusiveness in <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context level</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark data</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all the previous models for detecting <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a> in a <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text data</a> without abusive words. Furthermore, we combine our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and an abusive lexicon method. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has at least 4 % better performance compared with the previous approaches for identifying text abusiveness in case of with / without abusive words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5004/>Identifying Nuances in <a href=https://en.wikipedia.org/wiki/Fake_news>Fake News</a> vs. <a href=https://en.wikipedia.org/wiki/Satire>Satire</a> : Using Semantic and Linguistic Cues</a></strong><br><a href=/people/o/or-levi/>Or Levi</a>
|
<a href=/people/p/pedram-hosseini/>Pedram Hosseini</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5004><div class="card-body p-3 small">The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. Further to the efforts of reducing exposure to <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, purveyors of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. Previous work have studied whether <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a> can be distinguished based on <a href=https://en.wikipedia.org/wiki/Language>language differences</a>. Contrary to <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, <a href=https://en.wikipedia.org/wiki/Satire>satire stories</a> are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the <a href=https://en.wikipedia.org/wiki/Data>data</a> with current news events, to help identify a political or social message.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5007/>Generating Sentential Arguments from Diverse Perspectives on Controversial Topic</a></strong><br><a href=/people/c/chaehun-park/>ChaeHun Park</a>
|
<a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/j/jong-c-park/>Jong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5007><div class="card-body p-3 small">Considering diverse aspects of an argumentative issue is an essential step for mitigating a biased opinion and making reasonable decisions. A related generation model can produce flexible results that cover a wide range of topics, compared to the retrieval-based method that may show unstable performance for unseen data. In this paper, we study the problem of generating sentential arguments from multiple perspectives, and propose a neural method to address this problem. Our model, ArgDiver (Argument generation model from diverse perspectives), in a way a conversational system, successfully generates high-quality sentential arguments. At the same time, the automatically generated arguments by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> show a higher <a href=https://en.wikipedia.org/wiki/Diversity_index>diversity</a> than those generated by any other baseline models. We believe that our work provides evidence for the potential of a good generation model in providing diverse perspectives on a controversial topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5009/>Unraveling the Search Space of Abusive Language in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> with Dynamic Lexicon Acquisition<span class=acl-fixed-case>W</span>ikipedia with Dynamic Lexicon Acquisition</a></strong><br><a href=/people/w/wei-fan-chen/>Wei-Fan Chen</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5009><div class="card-body p-3 small">Many discussions on online platforms suffer from users offending others by using abusive terminology, threatening each other, or being sarcastic. Since an automatic detection of abusive language can support human moderators of <a href=https://en.wikipedia.org/wiki/Internet_forum>online discussion platforms</a>, detecting abusiveness has recently received increased attention. However, the existing approaches simply train one <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for the whole variety of <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a>. In contrast, our approach is to distinguish explicitly abusive cases from the more shadowed ones. By dynamically extending a lexicon of abusive terms (e.g., including new obfuscations of abusive terms), our approach can support a moderator with explicit unraveled explanations for why something was flagged as abusive : due to known explicitly abusive terms, due to newly detected (obfuscated) terms, or due to shadowed cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5013/>Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment levels</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5013><div class="card-body p-3 small">This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on Fine-Grained Propaganda Detection. Our system finished 5th out of 26 teams on the sentence-level classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5016/>JUSTDeep at NLP4IF 2019 Task 1 : Propaganda Detection using Ensemble Deep Learning Models<span class=acl-fixed-case>JUSTD</span>eep at <span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>IF</span> 2019 Task 1: Propaganda Detection using Ensemble Deep Learning Models</a></strong><br><a href=/people/h/hani-al-omari/>Hani Al-Omari</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/o/ola-altiti/>Ola AlTiti</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5016><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>internet</a> and the high use of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have enabled the modern-day journalism to publish, share and spread news that is difficult to distinguish if it is true or fake. Defining fake news is not well established yet, however, it can be categorized under several labels : false, biased, or framed to mislead the readers that are characterized as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Digital content production technologies with <a href=https://en.wikipedia.org/wiki/Fallacy>logical fallacies</a> and emotional language can be used as <a href=https://en.wikipedia.org/wiki/Propaganda_techniques>propaganda techniques</a> to gain more readers or mislead the audience. Recently, several researchers have proposed deep learning (DL) models to address this issue. This research paper provides an ensemble deep learning model using BiLSTM, <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a>, and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> to detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. The proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has been applied on the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence Level Classification (SLC) and it shows a significant performance over the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5017/>Detection of Propaganda Using Logistic Regression</a></strong><br><a href=/people/j/jinfen-li/>Jinfen Li</a>
|
<a href=/people/z/zhihao-ye/>Zhihao Ye</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5017><div class="card-body p-3 small">Various <a href=https://en.wikipedia.org/wiki/Propaganda_techniques>propaganda techniques</a> are used to manipulate peoples perspectives in order to foster a predetermined agenda such as by the use of <a href=https://en.wikipedia.org/wiki/Fallacy>logical fallacies</a> or appealing to the emotions of the audience. In this paper, we develop a Logistic Regression-based tool that automatically classifies whether a sentence is propagandistic or not. We utilize features like TF-IDF, BERT vector, sentence length, readability grade level, emotion feature, LIWC feature and emphatic content feature to help us differentiate these two categories. The linguistic and semantic features combination results in 66.16 % of F1 score, which outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> hugely.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5019/>Understanding BERT performance in propaganda analysis<span class=acl-fixed-case>BERT</span> performance in propaganda analysis</a></strong><br><a href=/people/y/yiqing-hua/>Yiqing Hua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5019><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used in the shared task for fine-grained propaganda analysis at <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. Despite the challenging nature of the task, our pretrained BERT model (team YMJA) fine tuned on the training dataset provided by the shared task scored 0.62 F1 on the test set and ranked third among 25 teams who participated in the contest. We present a set of illustrative experiments to better understand the performance of our BERT model on this shared task. Further, we explore beyond the given <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false-positive cases</a> that likely to be produced by our <a href=https://en.wikipedia.org/wiki/System>system</a>. We show that despite the high performance on the given testset, our system may have the tendency of classifying opinion pieces as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> and can not distinguish quotations of propaganda speech from actual usage of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda techniques</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5020/>Pretrained Ensemble Learning for Fine-Grained Propaganda Detection</a></strong><br><a href=/people/a/ali-fadel/>Ali Fadel</a>
|
<a href=/people/i/ibraheem-tuffaha/>Ibraheem Tuffaha</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5020><div class="card-body p-3 small">In this paper, we describe our team&#8217;s effort on the fine-grained propaganda detection on sentence level classification (SLC) task of NLP4IF 2019 workshop co-located with the EMNLP-IJCNLP 2019 conference. Our top performing <a href=https://en.wikipedia.org/wiki/System>system</a> results come from applying <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble average</a> on three pretrained models to make their predictions. The first two models use the uncased and cased versions of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) while the third model uses Universal Sentence Encoder (USE) (Cer et al. Out of 26 participating teams, our system is ranked in the first place with 68.8312 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the <a href=https://en.wikipedia.org/wiki/Software_development_process>development dataset</a> and in the sixth place with 61.3870 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the <a href=https://en.wikipedia.org/wiki/Software_testing>testing dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5022/>Sentence-Level Propaganda Detection in News Articles with Transfer Learning and BERT-BiLSTM-Capsule Model<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-Capsule Model</a></strong><br><a href=/people/g/george-alexandru-vlad/>George-Alexandru Vlad</a>
|
<a href=/people/m/mircea-adrian-tanase/>Mircea-Adrian Tanase</a>
|
<a href=/people/c/cristian-onose/>Cristian Onose</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5022><div class="card-body p-3 small">In recent years, the need for <a href=https://en.wikipedia.org/wiki/Communication>communication</a> increased in <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Propaganda is a mechanism which was used throughout history to influence public opinion and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF-2019 Shared Task SLC : Sentence-level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda labels</a>, <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> or non-propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre-training approach on a somewhat similar task (i.e., emotion classification) improving results against the cold-start model. Among the 26 participant teams in the NLP4IF-2019 Task SLC, our solution ranked 12th with an F1-score 0.5868 on the official test data. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> indicates promising results since our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly exceeds the baseline approach of the organizers by 0.1521 and is slightly lower than the winning <a href=https://en.wikipedia.org/wiki/System>system</a> by 0.0454.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5023/>Synthetic Propaganda Embeddings To Train A Linear Projection</a></strong><br><a href=/people/a/adam-ek/>Adam Ek</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5023><div class="card-body p-3 small">This paper presents a method of detecting fine-grained categories of propaganda in text. Given a sentence, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> aims to identify a span of words and predict the type of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> used. To detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, we explore a method for extracting features of propaganda from contextualized embeddings without fine-tuning the large parameters of the base model. We show that by generating synthetic embeddings we can train a <a href=https://en.wikipedia.org/wiki/Linear_function>linear function</a> with ReLU activation to extract useful labeled embeddings from an embedding space generated by a general-purpose language model. We also introduce an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference technique</a> to detect continuous spans in sequences of propaganda tokens in sentences. A result of the ensemble model is submitted to the first shared task in fine-grained propaganda detection at NLP4IF as Team Stalin. In this paper, we provide additional analysis regarding our method of detecting spans of propaganda with synthetically generated representations.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>