<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Annual Meeting of the Association for Computational Linguistics (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Annual Meeting of the Association for Computational Linguistics (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#p18-1>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a>
<span class="badge badge-info align-middle ml-1">156&nbsp;papers</span></li><li><a class=align-middle href=#p18-2>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a>
<span class="badge badge-info align-middle ml-1">75&nbsp;papers</span></li><li><a class=align-middle href=#p18-3>Proceedings of ACL 2018, Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#p18-4>Proceedings of ACL 2018, System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#p18-5>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w18-23>Proceedings of the BioNLP 2018 workshop</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-24>Proceedings of the Seventh Named Entities Workshop</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-25>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-26>Proceedings of the Workshop on Machine Reading for Question Answering</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-27>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-28>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-29>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-30>Proceedings of The Third Workshop on Representation Learning for NLP</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w18-31>Proceedings of the First Workshop on Economics and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-32>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-33>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-34>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w18-35>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w18-36>Proceedings of the First Workshop on Multilingual Surface Realisation</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-37>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li></ul></div></div><div id=p18-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P18-1/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1002.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807785 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1002/>A La Carte Embedding : Cheap but Effective Induction of Semantic Feature Vectors</a></strong><br><a href=/people/m/mikhail-khodak/>Mikhail Khodak</a>
|
<a href=/people/n/nikunj-saunshi/>Nikunj Saunshi</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a>
|
<a href=/people/t/tengyu-ma/>Tengyu Ma</a>
|
<a href=/people/b/brandon-m-stewart/>Brandon Stewart</a>
|
<a href=/people/s/sanjeev-arora/>Sanjeev Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1002><div class="card-body p-3 small">Motivations like <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, and <a href=https://en.wikipedia.org/wiki/Feature_learning>feature learning</a> have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> that is efficiently learnable using pretrained word vectors and <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression</a>. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807793 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1003/>Unsupervised Learning of Distributional Relation Vectors</a></strong><br><a href=/people/s/shoaib-jameel/>Shoaib Jameel</a>
|
<a href=/people/z/zied-bouraoui/>Zied Bouraoui</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1003><div class="card-body p-3 small">Word embedding models such as <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a> rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>relationships</a> are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation vectors</a> from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>word vectors</a> and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1004.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1004.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807800 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1004/>Explicit Retrofitting of Distributional Word Vectors</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1004><div class="card-body p-3 small">Semantic specialization of distributional word vectors, referred to as <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a>, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The <a href=https://en.wikipedia.org/wiki/ER_model>ER model</a> allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1006/>Triangular Architecture for Rare Language Translation</a></strong><br><a href=/people/s/shuo-ren/>Shuo Ren</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1006><div class="card-body p-3 small">Neural Machine Translation (NMT) performs poor on the low-resource language pair (X, Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y, Z) (may be small) and (X, Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1007/>Subword Regularization : Improving Neural Network Translation Models with Multiple Subword Candidates</a></strong><br><a href=/people/t/taku-kudo/>Taku Kudo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1007><div class="card-body p-3 small">Subword units are an effective way to alleviate the open vocabulary problems in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a <a href=https://en.wikipedia.org/wiki/Noise>noise</a> to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of <a href=https://en.wikipedia.org/wiki/Normal_mode>NMT</a>. We present a simple <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization method</a>, subword regularization, which trains the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1008.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1008.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807844 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1008/>The Best of Both Worlds : Combining Recent Advances in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mia-xu-chen/>Mia Xu Chen</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/a/ankur-bapna/>Ankur Bapna</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/l/llion-jones/>Llion Jones</a>
|
<a href=/people/m/mike-schuster/>Mike Schuster</a>
|
<a href=/people/n/noam-shazeer/>Noam Shazeer</a>
|
<a href=/people/n/niki-parmar/>Niki Parmar</a>
|
<a href=/people/a/ashish-vaswani/>Ashish Vaswani</a>
|
<a href=/people/j/jakob-uszkoreit/>Jakob Uszkoreit</a>
|
<a href=/people/l/lukasz-kaiser/>Lukasz Kaiser</a>
|
<a href=/people/z/zhifeng-chen/>Zhifeng Chen</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1008><div class="card-body p-3 small">The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT&#8217;14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1009.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807855 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1009/>Ultra-Fine Entity Typing</a></strong><br><a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1009><div class="card-body p-3 small">We introduce a new entity typing task : given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, <a href=https://en.wikipedia.org/wiki/Songwriter>songwriter</a>, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale : head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity ; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1012.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1012.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1012.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285807892 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1012/>Towards Understanding the Geometry of Knowledge Graph Embeddings</a></strong><br><a href=/people/c/chandrahas/>Chandrahas</a>
|
<a href=/people/a/aditya-sharma/>Aditya Sharma</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1012><div class="card-body p-3 small">Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>high-dimensional space</a>. Despite this popularity and effectiveness of KG embeddings in various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (e.g., link prediction), <a href=https://en.wikipedia.org/wiki/Geometry>geometric understanding</a> of such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> (i.e., arrangement of entity and relation vectors in vector space) is unexplored we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1013.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1013.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800568 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1013/>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</a></strong><br><a href=/people/w/wan-ting-hsu/>Wan-Ting Hsu</a>
|
<a href=/people/c/chieh-kai-lin/>Chieh-Kai Lin</a>
|
<a href=/people/m/ming-ying-lee/>Ming-Ying Lee</a>
|
<a href=/people/k/kerui-min/>Kerui Min</a>
|
<a href=/people/j/jing-tang/>Jing Tang</a>
|
<a href=/people/m/min-sun/>Min Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1013><div class="card-body p-3 small">We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN / Daily Mail dataset in a solid human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800574 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1014/>Extractive Summarization with SWAP-NET : Sentences and Words from Alternating Pointer Networks<span class=acl-fixed-case>SWAP</span>-<span class=acl-fixed-case>NET</span>: Sentences and Words from Alternating Pointer Networks</a></strong><br><a href=/people/a/aishwarya-jadhav/>Aishwarya Jadhav</a>
|
<a href=/people/v/vaibhav-rajan/>Vaibhav Rajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1014><div class="card-body p-3 small">We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1015.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800584 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1015/>Retrieve, Rerank and Rewrite : Soft Template Based Neural Summarization</a></strong><br><a href=/people/z/ziqiang-cao/>Ziqiang Cao</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1015><div class="card-body p-3 small">Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the <a href=https://en.wikipedia.org/wiki/BIBO_stability>stability</a> and readability of generated summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1016.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800596 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1016/>Simple and Effective Text Simplification Using Semantic and Neural Methods</a></strong><br><a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1016><div class="card-body p-3 small">Sentence splitting is a major <a href=https://en.wikipedia.org/wiki/Operator_(mathematics)>simplification operator</a>. Here we present a simple and efficient splitting algorithm based on an automatic semantic parser. After splitting, the text is amenable for further fine-tuned simplification operations. In particular, we show that <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural Machine Translation</a> can be effectively used in this situation. Previous application of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for simplification suffers from a considerable disadvantage in that they are over-conservative, often failing to modify the source in any way. Splitting based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in combined lexical and structural simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1019.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800638 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1019/>A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support <a href=https://en.wikipedia.org/wiki/Language_processing>Language Processing</a> for Medical Literature</a></strong><br><a href=/people/b/benjamin-nye/>Benjamin Nye</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/r/roma-patel/>Roma Patel</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/i/iain-marshall/>Iain Marshall</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1019><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the &#8216;PICO&#8217; elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1021.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1021.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800652 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1021/>Neural Argument Generation Augmented with Externally Retrieved Evidence</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1021><div class="card-body p-3 small">High quality arguments are essential elements for <a href=https://en.wikipedia.org/wiki/Reason>human reasoning</a> and <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making processes</a>. However, effective argument construction is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for both human and machines. In this work, we study a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1023.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800671 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1023/>Retrieval of the Best Counterargument without Prior Topic Knowledge</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1023><div class="card-body p-3 small">Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic can not be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments&#8217; premises and conclusions. A salient property of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments : For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1024.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1024.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1024/>LinkNBed : Multi-Graph Representation Learning with Entity Linkage<span class=acl-fixed-case>L</span>ink<span class=acl-fixed-case>NB</span>ed: Multi-Graph Representation Learning with Entity Linkage</a></strong><br><a href=/people/r/rakshit-trivedi/>Rakshit Trivedi</a>
|
<a href=/people/b/bunyamin-sisman/>Bunyamin Sisman</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a>
|
<a href=/people/c/christos-faloutsos/>Christos Faloutsos</a>
|
<a href=/people/j/jun-ma/>Jun Ma</a>
|
<a href=/people/h/hongyuan-zha/>Hongyuan Zha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1024><div class="card-body p-3 small">Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linkage</a> and build an efficient multi-task training procedure. Experiments on link prediction and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linkage</a> demonstrate substantial improvements over the state-of-the-art relational learning approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1025.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1025/>Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</a></strong><br><a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1025><div class="card-body p-3 small">Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> for both <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying <a href=https://en.wikipedia.org/wiki/Probability_measure>probability measure</a> to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1027.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1027.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1027" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1027/>Sharp Nearby, Fuzzy Far Away : How Neural Language Models Use Context</a></strong><br><a href=/people/u/urvashi-khandelwal/>Urvashi Khandelwal</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1027><div class="card-body p-3 small">We know very little about how neural language models (LM) use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>prior linguistic context</a>. In this paper, we investigate the role of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in an LSTM LM, through <a href=https://en.wikipedia.org/wiki/Ablation>ablation studies</a>. Specifically, we analyze the increase in <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> when prior context words are shuffled, replaced, or dropped. On two standard datasets, <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1028.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1028.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1028/>Bridging CNNs, <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a>, and <a href=https://en.wikipedia.org/wiki/Finite-state_machine>Weighted Finite-State Machines</a><span class=acl-fixed-case>CNN</span>s, <span class=acl-fixed-case>RNN</span>s, and Weighted Finite-State Machines</a></strong><br><a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1028><div class="card-body p-3 small">Recurrent and convolutional neural networks comprise two distinct families of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1029.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1029/>Zero-shot Learning of Classifiers from Natural Language Quantification</a></strong><br><a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1029><div class="card-body p-3 small">Humans can efficiently learn new <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> using <a href=https://en.wikipedia.org/wiki/Language>language</a>. We present a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> through which a set of explanations of a concept can be used to learn a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> without access to any labeled examples. We use <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., &#8216;usually&#8217; vs &#8216;always&#8217;) to drive <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. Experiments on three domains show that the learned <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1030.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1030/>Sentence-State LSTM for Text Representation<span class=acl-fixed-case>LSTM</span> for Text Representation</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1030><div class="card-body p-3 small">Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1033.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1033/>Improving Text-to-SQL Evaluation Methodology<span class=acl-fixed-case>SQL</span> Evaluation Methodology</a></strong><br><a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/k/karthik-ramanathan/>Karthik Ramanathan</a>
|
<a href=/people/s/sesh-sadasivam/>Sesh Sadasivam</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1033><div class="card-body p-3 small">To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries ; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> removes an important challenge of the task. Our observations highlight key difficulties, and our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> enables effective measurement of future development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1034/>Semantic Parsing with Syntax- and Table-Aware SQL Generation<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/g/guihong-cao/>Guihong Cao</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1034><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to map <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> into <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the <a href=https://en.wikipedia.org/wiki/Table_(database)>structure of table</a> and the <a href=https://en.wikipedia.org/wiki/SQL>syntax of SQL language</a>. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords ; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> significantly improves the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>execution accuracy</a> from 69.0 % to 74.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1035.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1035.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1035.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1035" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1035/>Multitask Parsing Across Semantic Representations</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1035><div class="card-body p-3 small">The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing <a href=https://en.wikipedia.org/wiki/Learning>learning</a> for one <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three <a href=https://en.wikipedia.org/wiki/Programming_language>languages</a>, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> significantly improves UCCA parsing in both in-domain and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1037.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1037.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1037/>AMR Parsing as Graph Prediction with Latent Alignment<span class=acl-fixed-case>AMR</span> Parsing as Graph Prediction with Latent Alignment</a></strong><br><a href=/people/c/chunchuan-lyu/>Chunchuan Lyu</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1037><div class="card-body p-3 small">Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> and words in the corresponding sentences. We introduce a neural parser which treats <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> within a joint probabilistic model of concepts, relations and <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a>. As <a href=https://en.wikipedia.org/wiki/Exact_inference>exact inference</a> requires marginalizing over <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves the best reported results on the standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> (74.4 % on LDC2016E25).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1040.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1040.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1040" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1040/>Discourse Representation Structure Parsing</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1040><div class="card-body p-3 small">We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT ; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages : basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1041.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1041.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1041/>Baseline Needs More Love : On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/m/martin-renqiang-min/>Martin Renqiang Min</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1041><div class="card-body p-3 small">Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>compositional functions</a>. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN / CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings : (i) a max-pooling operation for improved interpretability ; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : (i) (long) document classification ; (ii) text sequence matching ; and (iii) short text tasks, including classification and tagging.<i>compositionality</i> in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (<i>i</i>) a max-pooling operation for improved interpretability; and (<i>ii</i>) a hierarchical pooling operation, which preserves spatial (<tex-math>n</tex-math>-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (<i>i</i>) (long) document classification; (<i>ii</i>) text sequence matching; and (<i>iii</i>) short text tasks, including classification and tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1042.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1042/>ParaNMT-50 M : Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations<span class=acl-fixed-case>P</span>ara<span class=acl-fixed-case>NMT</span>-50<span class=acl-fixed-case>M</span>: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations</a></strong><br><a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1042><div class="card-body p-3 small">We describe ParaNMT-50 M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. Our hope is that ParaNMT-50 M can be a valuable resource for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50 M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1044.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1044/>Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-argument Structure Analysis</a></strong><br><a href=/people/s/shuhei-kurita/>Shuhei Kurita</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1044><div class="card-body p-3 small">Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PAS</a>. However, since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a <a href=https://en.wikipedia.org/wiki/Text_corpus>raw corpus</a>. In our experiments, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing state-of-the-art models for Japanese PAS analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1048.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1048/>Self-regulation : Employing a Generative Adversarial Network to Improve Event Detection</a></strong><br><a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/j/jingli-zhang/>Jingli Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1048><div class="card-body p-3 small">Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been successfully used for detecting events to a certain extent. However, such a <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a <a href=https://en.wikipedia.org/wiki/Generative_adversarial_network>generative adversarial network</a> to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1050.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1050/>Temporal Event Knowledge Acquisition via Identifying Narratives</a></strong><br><a href=/people/w/wenlin-yao/>Wenlin Yao</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1050><div class="card-body p-3 small">Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal before / after event knowledge across sentences in narrative stories. The double temporality states that a <a href=https://en.wikipedia.org/wiki/Narrative>narrative story</a> often describes a sequence of events following the <a href=https://en.wikipedia.org/wiki/Chronology>chronological order</a> and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1051.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1051/>Textual Deconvolution Saliency (TDS) : a deep tool box for <a href=https://en.wikipedia.org/wiki/Linguistic_analysis>linguistic analysis</a><span class=acl-fixed-case>TDS</span>) : a deep tool box for linguistic analysis</a></strong><br><a href=/people/l/laurent-vanni/>Laurent Vanni</a>
|
<a href=/people/m/melanie-ducoffe/>Melanie Ducoffe</a>
|
<a href=/people/c/carlos-aguilar/>Carlos Aguilar</a>
|
<a href=/people/f/frederic-precioso/>Frederic Precioso</a>
|
<a href=/people/d/damon-mayaffre/>Damon Mayaffre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1051><div class="card-body p-3 small">In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We extend Deconvolution Networks to <a href=https://en.wikipedia.org/wiki/Written_language>text</a> in order to present a new perspective on <a href=https://en.wikipedia.org/wiki/Written_language>text analysis</a> to the <a href=https://en.wikipedia.org/wiki/Linguistic_community>linguistic community</a>. We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1052.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1052.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1052/>Coherence Modeling of Asynchronous Conversations : A Neural Entity Grid Approach</a></strong><br><a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/m/muhammad-tasnim-mohiuddin/>Muhammad Tasnim Mohiuddin</a>
|
<a href=/people/d/dat-tien-nguyen/>Dat Tien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1052><div class="card-body p-3 small">We propose a novel coherence model for <a href=https://en.wikipedia.org/wiki/Asynchronous_communication>written asynchronous conversations</a> (e.g., <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to <a href=https://en.wikipedia.org/wiki/Asynchronous_communication>asynchronous conversations</a> by incorporating the underlying conversational structure in the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity grid representation</a> and feature computation. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We also demonstrate its effectiveness in reconstructing thread structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1053.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1053/>Deep Reinforcement Learning for Chinese Zero Pronoun Resolution<span class=acl-fixed-case>C</span>hinese Zero Pronoun Resolution</a></strong><br><a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1053><div class="card-body p-3 small">Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the <a href=https://en.wikipedia.org/wiki/Zero_pronoun>zero pronoun</a> and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1054.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1054/>Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Coreference Resolution and Predicate Argument Structure Analysis</a></strong><br><a href=/people/t/tomohide-shibata/>Tomohide Shibata</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1054><div class="card-body p-3 small">Predicate argument structure analysis is a task of identifying <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>structured events</a>. To improve this field, we need to identify a salient entity, which can not be identified without performing <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a>, and when the result of both analyses refers to an entity, the entity embedding is updated. The <a href=https://en.wikipedia.org/wiki/Analysis>analyses</a> take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1055.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1055/>Constraining MGbank : Agreement, L-Selection and Supertagging in Minimalist Grammars<span class=acl-fixed-case>MG</span>bank: Agreement, <span class=acl-fixed-case>L</span>-Selection and Supertagging in <span class=acl-fixed-case>M</span>inimalist <span class=acl-fixed-case>G</span>rammars</a></strong><br><a href=/people/j/john-torr/>John Torr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1055><div class="card-body p-3 small">This paper reports on two strategies that have been implemented for improving the <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of wide-coverage Minimalist Grammar (MG) parsing. The first extends the <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> presented in Torr and Stabler (2016) with a mechanism for enforcing fine-grained selectional restrictions and agreements. The second is a method for factoring computationally costly null heads out from bottom-up MG parsing ; this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers. These techniques aided in the task of generating MGbank, the first wide-coverage corpus of Minimalist Grammar derivation trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1058.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1058/>Give Me More Feedback : Annotating Argument Persuasiveness and Related Attributes in Student Essays</a></strong><br><a href=/people/w/winston-carlile/>Winston Carlile</a>
|
<a href=/people/n/nishant-gurrapadi/>Nishant Gurrapadi</a>
|
<a href=/people/z/zixuan-ke/>Zixuan Ke</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1058><div class="card-body p-3 small">While argument persuasiveness is one of the most important dimensions of argumentative essay quality, it is relatively little studied in automated essay scoring research. Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora. We present the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of essays</a> that are simultaneously annotated with argument components, argument persuasiveness scores, and attributes of argument components that impact an argument&#8217;s persuasiveness. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> could trigger the development of novel <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> concerning argument persuasiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1059.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1059.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1059" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1059/>Inherent Biases in Reference-based Evaluation for Grammatical Error Correction</a></strong><br><a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1059><div class="card-body p-3 small">The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation can not be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing <a href=https://en.wikipedia.org/wiki/System>systems</a> obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on <a href=https://en.wikipedia.org/wiki/Text_Simplification>Text Simplification</a> further support our claims.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1062.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1062.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1062" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1062/>Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</a></strong><br><a href=/people/g/guokan-shang/>Guokan Shang</a>
|
<a href=/people/w/wensi-ding/>Wensi Ding</a>
|
<a href=/people/z/zekun-zhang/>Zekun Zhang</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a>
|
<a href=/people/j/jean-pierre-lorre/>Jean-Pierre Lorré</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1062><div class="card-body p-3 small">We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> combines the strengths of multiple recent <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> while addressing their weaknesses. Moreover, we leverage recent advances in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Graph_degeneracy>graph degeneracy</a> applied to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. Code and data are publicly available, and our <a href=https://en.wikipedia.org/wiki/System>system</a> can be interactively tested.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1063.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1063/>Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</a></strong><br><a href=/people/y/yen-chun-chen/>Yen-Chun Chen</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1063><div class="card-body p-3 small">Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> in a hierarchical way, while maintaining <a href=https://en.wikipedia.org/wiki/Fluency>language fluency</a>. Empirically, we achieve the new state-of-the-art on all <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> (including human evaluation) on the CNN / Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1064.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1064/>Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation</a></strong><br><a href=/people/h/han-guo/>Han Guo</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1064><div class="card-body p-3 small">An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on both the CNN / DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s learned saliency and entailment skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1068.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800848 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1068/>Coarse-to-Fine Decoding for Neural Semantic Parsing</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1068><div class="card-body p-3 small">Semantic parsing aims at mapping <a href=https://en.wikipedia.org/wiki/Utterance>natural language utterances</a> into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as <a href=https://en.wikipedia.org/wiki/Variable_(computer_science)>variable names</a> and arguments) is glossed over. Then, we fill in missing details by taking into account the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language input</a> and the sketch itself. Experimental results on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple <a href=https://en.wikipedia.org/wiki/Code>decoders</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1069.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1069" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1069/>Confidence Modeling for Neural Semantic Parsing</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1069><div class="card-body p-3 small">In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to quantify these factors. These <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are then used to estimate <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence scores</a> that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probability</a>, and improves the quality of interpretation compared to simply relying on attention scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1072.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800939 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1072/>On the Limitations of Unsupervised Bilingual Dictionary Induction</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1072><div class="card-body p-3 small">Unsupervised machine translation-i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora-seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT : unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust <a href=https://en.wikipedia.org/wiki/Mathematical_induction>induction</a> and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1073.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800964 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1073" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1073/>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</a></strong><br><a href=/people/m/mikel-artetxe/>Mikel Artetxe</a>
|
<a href=/people/g/gorka-labaka/>Gorka Labaka</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1073><div class="card-body p-3 small">Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, even surpassing previous <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a>. Our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is released as an open source project at.<url>https://github.com/artetxem/vecmap</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1074.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285800977 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1074/>A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/s/shengqi-yang/>Shengqi Yang</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1074><div class="card-body p-3 small">We propose a multi-lingual multi-task architecture to develop <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> with a minimal amount of labeled data for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. In this new <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, we combine various <a href=https://en.wikipedia.org/wiki/Transfer_function>transfer models</a> using two layers of parameter sharing. On the first layer, we construct the basis of the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to provide universal word representation and feature extraction capability for all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Using Name Tagging as a target task, our <a href=https://en.wikipedia.org/wiki/Design_of_experiments>approach</a> achieved 4.3%-50.5 % absolute F-score gains compared to the mono-lingual single-task baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801036 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1077/>Multi-Relational Question Answering from Narratives : Machine Reading and Reasoning in Simulated Worlds</a></strong><br><a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/b/bishan-yang/>Bishan Yang</a>
|
<a href=/people/a/anusha-prakash/>Anusha Prakash</a>
|
<a href=/people/a/amos-azaria/>Amos Azaria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1077><div class="card-body p-3 small">Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions : (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1078.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801068 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1078" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1078/>Simple and Effective Multi-Paragraph Reading Comprehension</a></strong><br><a href=/people/c/christopher-clark/>Christopher Clark</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1078><div class="card-body p-3 small">We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models can not scale to document or multi-document input, and naively applying these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training scheme</a> that teaches the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> that requires the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to produce globally correct output. We additionally identify and improve upon a number of other <a href=https://en.wikipedia.org/wiki/Design_of_experiments>design decisions</a> that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>, including a 10 point gain on TriviaQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1079.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1079.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801102 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1079/>Semantically Equivalent Adversarial Rules for Debugging NLP models<span class=acl-fixed-case>NLP</span> models</a></strong><br><a href=/people/m/marco-tulio-ribeiro/>Marco Tulio Ribeiro</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/c/carlos-guestrin/>Carlos Guestrin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1079><div class="card-body p-3 small">Complex machine learning models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) semantic-preserving perturbations that induce changes in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions. We generalize these <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversaries</a> into semantically equivalent adversarial rules (SEARs) simple, universal replacement rules that induce <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversaries</a> on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains : machine comprehension, visual question-answering, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable : retraining models using <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> significantly reduces bugs, while maintaining <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1080.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1080" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1080/>Style Transfer Through Back-Translation</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1080><div class="card-body p-3 small">Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations : <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1082.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801163 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1082" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1082/>Hierarchical Neural Story Generation</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/y/yann-dauphin/>Yann Dauphin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1082><div class="card-body p-3 small">We explore story generation : creative systems that can build coherent and fluent passages of text about a topic. We collect a large <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 300 K human-written stories paired with writing prompts from an <a href=https://en.wikipedia.org/wiki/Internet_forum>online forum</a>. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1083.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1083.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801215 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1083" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1083/>No Metrics Are Perfect : Adversarial Reward Learning for Visual Storytelling</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/y/yuan-fang-wang/>Yuan-Fang Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1083><div class="card-body p-3 small">Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, <a href=https://en.wikipedia.org/wiki/Narrative>stories</a> have more expressive language styles and contain many imaginary concepts that do not appear in the <a href=https://en.wikipedia.org/wiki/Image>images</a>. Thus <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> on evaluating story quality, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning methods</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>hand-crafted rewards</a> also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1087.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801308 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1087/>Transformation Networks for Target-Oriented Sentiment Classification</a></strong><br><a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/b/bei-shi/>Bei Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1087><div class="card-body p-3 small">Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> seems a good fit for the characteristics of this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801326 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1088/>Target-Sensitive Memory Networks for Aspect Sentiment Classification</a></strong><br><a href=/people/s/shuai-wang/>Shuai Wang</a>
|
<a href=/people/s/sahisnu-mazumder/>Sahisnu Mazumder</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/m/mianwei-zhou/>Mianwei Zhou</a>
|
<a href=/people/y/yi-chang/>Yi Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1088><div class="card-body p-3 small">Aspect sentiment classification (ASC) is a fundamental task in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Given an aspect / target and a sentence, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment context</a> for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it can not be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1089.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801356 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1089/>Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification</a></strong><br><a href=/people/r/raksha-sharma/>Raksha Sharma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/h/himanshu-sharad-bhatt/>Himanshu Sharad Bhatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1089><div class="card-body p-3 small">Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classifier</a> for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on 2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> enhances the cross-domain classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1090.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285801380 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1090" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1090/>Unpaired Sentiment-to-Sentiment Translation : A Cycled Reinforcement Learning Approach</a></strong><br><a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/x/xiaodong-zhang/>Xiaodong Zhang</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1090><div class="card-body p-3 small">The goal of sentiment-to-sentiment translation is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, <a href=https://en.wikipedia.org/wiki/Yelp>Yelp</a> and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>. Experimental results show that our approach significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art systems</a>. Especially, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> substantially improves the <a href=https://en.wikipedia.org/wiki/Digital_preservation>content preservation</a> performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1092.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802130 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1092/>Working Memory Networks : Augmenting Memory Networks with a Relational Reasoning Module</a></strong><br><a href=/people/j/juan-pavez/>Juan Pavez</a>
|
<a href=/people/h/hector-allende-cid/>Héctor Allende</a>
|
<a href=/people/h/hector-allende-cid/>Héctor Allende-Cid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1092><div class="card-body p-3 small">During the last years, there has been a lot of interest in achieving some kind of complex reasoning using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. To do that, models like Memory Networks (MemNNs) have combined external memory storages and <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>. These <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a>, however, lack of more complex reasoning mechanisms that could allow, for instance, <a href=https://en.wikipedia.org/wiki/Relational_model>relational reasoning</a>. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> grows quadratically with the number of <a href=https://en.wikipedia.org/wiki/Memory>memories</a>, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network, a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, achieving a <a href=https://en.wikipedia.org/wiki/Mean>mean error</a> of less than 0.5 %. Moreover, a simple <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of two of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> solves all 20 tasks in the joint version of the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1094.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802169 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1094/>Adversarial Contrastive Estimation</a></strong><br><a href=/people/a/avishek-joey-bose/>Avishek Joey Bose</a>
|
<a href=/people/h/huan-ling/>Huan Ling</a>
|
<a href=/people/y/yanshuai-cao/>Yanshuai Cao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1094><div class="card-body p-3 small">Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and translating embeddings for <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> are examples in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a <a href=https://en.wikipedia.org/wiki/Mixture_distribution>mixture distribution</a> containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn a better representation of the data. We evaluate our proposal on learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/Order_embedding>order embeddings</a> and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1096.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802189 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1096/>Strong Baselines for Neural Semi-Supervised Learning under Domain Shift</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1096><div class="card-body p-3 small">Novel neural models have been proposed in recent years for learning under domain shift. Most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> difficult. In this paper, we re-evaluate classic <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>general-purpose bootstrapping approaches</a> in the context of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> under <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>domain shifts</a> vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> are negative : while our novel method establishes a new state-of-the-art for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Hence classic <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> constitute an important and strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802228 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1098/>A Neural Architecture for Automated ICD Coding<span class=acl-fixed-case>ICD</span> Coding</a></strong><br><a href=/people/p/pengtao-xie/>Pengtao Xie</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1098><div class="card-body p-3 small">The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for <a href=https://en.wikipedia.org/wiki/Medical_classification>classifying diseases</a>. Medical coding which assigns a subset of ICD codes to a patient visit is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for <a href=https://en.wikipedia.org/wiki/Computer_programming>automated coding</a>. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant <a href=https://en.wikipedia.org/wiki/ICD-10>ICD codes</a>. This architecture contains four major ingredients : (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on a <a href=https://en.wikipedia.org/wiki/Data_set>clinical datasets</a> with 59 K patient visits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1099.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802247 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1099/>Domain Adaptation with Adversarial Training and Graph Embeddings</a></strong><br><a href=/people/f/firoj-alam/>Firoj Alam</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/m/muhammad-imran/>Muhammad Imran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1099><div class="card-body p-3 small">The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> demonstrate significant improvements over several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1100.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1100/>TDNN : A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring<span class=acl-fixed-case>TDNN</span>: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring</a></strong><br><a href=/people/c/cancan-jin/>Cancan Jin</a>
|
<a href=/people/b/ben-he/>Ben He</a>
|
<a href=/people/k/kai-hui/>Kai Hui</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1100><div class="card-body p-3 small">Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data ; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1101.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1101.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802293 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1101/>Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/k/kyusong-lee/>Kyusong Lee</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1101><div class="card-body p-3 small">The encoder-decoder dialog model is one of the most prominent methods used to build <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a> in <a href=https://en.wikipedia.org/wiki/Complex_system>complex domains</a>. Yet <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is limited because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can not output interpretable actions as in traditional <a href=https://en.wikipedia.org/wiki/System>systems</a>, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1104.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802339 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1104/>MojiTalk : Generating Emotional Responses at Scale<span class=acl-fixed-case>M</span>oji<span class=acl-fixed-case>T</span>alk: Generating Emotional Responses at Scale</a></strong><br><a href=/people/x/xianda-zhou/>Xianda Zhou</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1104><div class="card-body p-3 small">Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach : we exploit the idea of leveraging Twitter data that are naturally labeled with <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We collect a large corpus of Twitter conversations that include <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in the response and assume the <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1106.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1106.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802370 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1106/>A Framework for Representing <a href=https://en.wikipedia.org/wiki/Language_acquisition>Language Acquisition</a> in a Population Setting</a></strong><br><a href=/people/j/jordan-kodner/>Jordan Kodner</a>
|
<a href=/people/c/christopher-cerezo-falco/>Christopher Cerezo Falco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1106><div class="card-body p-3 small">Language variation and change are driven both by individuals&#8217; internal cognitive processes and by the <a href=https://en.wikipedia.org/wiki/Social_structure>social structures</a> through which language propagates. A wide range of <a href=https://en.wikipedia.org/wiki/Software_framework>computational frameworks</a> have been proposed to connect these drivers. We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models&#8217; ability to capture realistic <a href=https://en.wikipedia.org/wiki/Social_structure>social structure</a> with practically and more elegant computational properties. The framework privileges the process of <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> and embeds learners in a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a> but is modular so that <a href=https://en.wikipedia.org/wiki/Population_structure>population structure</a> can be combined with different <a href=https://en.wikipedia.org/wiki/Language_acquisition>acquisition models</a>. We demonstrate two applications for the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> : a test of practical concerns that arise when modeling <a href=https://en.wikipedia.org/wiki/Language_acquisition>acquisition</a> in a population setting and an application of the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to recent work on phonological mergers in progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1107.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802428 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1107/>Prefix Lexicalization of Synchronous CFGs using Synchronous TAG<span class=acl-fixed-case>CFG</span>s using Synchronous <span class=acl-fixed-case>TAG</span></a></strong><br><a href=/people/l/logan-born/>Logan Born</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1107><div class="card-body p-3 small">We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar&#8217;s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend <a href=https://en.wikipedia.org/wiki/Greibach_normal_form>Greibach normal form</a> from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1108.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1108/>Straight to the Tree : Constituency Parsing with Neural Syntactic Distance</a></strong><br><a href=/people/y/yikang-shen/>Yikang Shen</a>
|
<a href=/people/z/zhouhan-lin/>Zhouhan Lin</a>
|
<a href=/people/a/athul-paul-jacob/>Athul Paul Jacob</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1108><div class="card-body p-3 small">In this work, we propose a novel constituency parsing scheme. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1109.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802453 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1109/>Gaussian Mixture Latent Vector Grammars<span class=acl-fixed-case>G</span>aussian Mixture Latent Vector Grammars</a></strong><br><a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a>
|
<a href=/people/l/liwen-zhang/>Liwen Zhang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1109><div class="card-body p-3 small">We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the <a href=https://en.wikipedia.org/wiki/Inside-outside_algorithm>inside-outside algorithm</a>, which enables efficient <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a>. We apply GM-LVeGs to <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1110.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285802475 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1110/>Extending a <a href=https://en.wikipedia.org/wiki/Parsing>Parser</a> to Distant Domains Using a Few Dozen Partially Annotated Examples</a></strong><br><a href=/people/v/vidur-joshi/>Vidur Joshi</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/m/mark-hopkins/>Mark Hopkins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1110><div class="card-body p-3 small">We revisit <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> in the neural era. First we show that recent advances in word representations greatly diminish the need for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> when the target domain is syntactically similar to the source domain. As evidence, we train a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on the <a href=https://en.wikipedia.org/wiki/The_Wall_Street_Journal>Wall Street Journal</a> alone that achieves over 90 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45 % to 73 % using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3 %. This is an absolute increase of 1.7 % over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> of 92.6 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1112.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803381 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1112/>Searching for the X-Factor : Exploring Corpus Subjectivity for Word Embeddings<span class=acl-fixed-case>X</span>-Factor: Exploring Corpus Subjectivity for Word Embeddings</a></strong><br><a href=/people/m/maksim-tkachenko/>Maksim Tkachenko</a>
|
<a href=/people/c/chong-cher-chia/>Chong Cher Chia</a>
|
<a href=/people/h/hady-lauw/>Hady Lauw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1112><div class="card-body p-3 small">We explore the notion of <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity</a>, and hypothesize that word embeddings learnt from input corpora of varying levels of <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity</a> behave differently on natural language processing tasks such as classifying a sentence by sentiment, <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity</a>, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1113.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803402 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1113/>Word Embedding and WordNet Based Metaphor Identification and Interpretation<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Based Metaphor Identification and Interpretation</a></strong><br><a href=/people/r/rui-mao/>Rui Mao</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/f/frank-guerin/>Frank Guerin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1113><div class="card-body p-3 small">Metaphoric expressions are widespread in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, posing a significant challenge for various <a href=https://en.wikipedia.org/wiki/Natural_language>natural language processing tasks</a> such as <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. Current word embedding based metaphor identification models can not identify the exact metaphorical words within a sentence. In this paper, we propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning method</a> that identifies and interprets <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>, paraphrasing them into their literal counterparts, so that they can be better translated by <a href=https://en.wikipedia.org/wiki/Machine>machines</a>. We evaluated this with two popular translation systems for <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, showing that our model improved the <a href=https://en.wikipedia.org/wiki/System>systems</a> significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1117.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1117.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152860 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1117/>Context-Aware Neural Machine Translation Learns <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>Anaphora Resolution</a></a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/p/pavel-serdyukov/>Pavel Serdyukov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1117><div class="card-body p-3 small">Standard <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the <a href=https://en.wikipedia.org/wiki/Machine_translation>translation model</a> can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a>. It is consistent with gains for sentences where <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> need to be gendered in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Beside improvements in <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric cases</a>, the model also improves in overall BLEU, both over its <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context-agnostic version</a> (+0.7) and over simple concatenation of the context and source sentences (+0.6).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1118.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152852 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1118/>Document Context Neural Machine Translation with Memory Networks</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1118><div class="card-body p-3 small">We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a structured prediction problem with interdependencies among the <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>observed and hidden variables</a>, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803482 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1120/>Learning Prototypical Goal Activities for Locations</a></strong><br><a href=/people/t/tianyu-jiang/>Tianyu Jiang</a>
|
<a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1120><div class="card-body p-3 small">People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803502 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1121/>Guess Me if You Can : Acronym Disambiguation for Enterprises</a></strong><br><a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/b/bo-zhao/>Bo Zhao</a>
|
<a href=/people/a/ariel-fuxman/>Ariel Fuxman</a>
|
<a href=/people/f/fangbo-tao/>Fangbo Tao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1121><div class="card-body p-3 small">Acronyms are abbreviations formed from the initial components of words or phrases. In <a href=https://en.wikipedia.org/wiki/Business>enterprises</a>, people often use <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> to make communications more efficient. However, <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity. To alleviate such troubles, we study how to automatically resolve the true meanings of <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a> in a given context. Acronym disambiguation for <a href=https://en.wikipedia.org/wiki/Business>enterprises</a> is challenging for several reasons. First, acronyms may be highly ambiguous since an <a href=https://en.wikipedia.org/wiki/Acronym>acronym</a> used in the <a href=https://en.wikipedia.org/wiki/Business>enterprise</a> could have multiple internal and external meanings. Second, there are usually no comprehensive <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> available in enterprises. Finally, the <a href=https://en.wikipedia.org/wiki/System>system</a> should be generic to work for any enterprise. In this work we propose an end-to-end framework to tackle all these challenges. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output. Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples. Therefore, our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can be deployed to any enterprise to support high-quality <a href=https://en.wikipedia.org/wiki/Acronym>acronym disambiguation</a>. Experimental results on real world data justified the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1122.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1122.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803517 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1122/>A Multi-Axis Annotation Scheme for Event Temporal Relations</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1122><div class="card-body p-3 small">Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60&#8217;s to 80&#8217;s (Cohen&#8217;s Kappa). This better-defined annotation scheme further enables the use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803553 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1124/>DialSQL : Dialogue Based Structured Query Generation<span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>SQL</span>: Dialogue Based Structured Query Generation</a></strong><br><a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1124><div class="card-body p-3 small">The recent advance in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> has significantly improved the translation accuracy of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> to structured queries. However, further improvement of the existing <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages <a href=https://en.wikipedia.org/wiki/Human_intelligence>human intelligence</a> to boost the performance of existing <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> via user interaction. DialSQL is capable of identifying potential errors in a generated <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a> and asking users for validation via simple <a href=https://en.wikipedia.org/wiki/Multiple_choice>multi-choice questions</a>. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3 % to 69.0 % using only 2.4 validation questions per dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1126.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1126.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1126/>Are BLEU and Meaning Representation in Opposition?<span class=acl-fixed-case>BLEU</span> and Meaning Representation in Opposition?</a></strong><br><a href=/people/o/ondrej-cifka/>Ondřej Cífka</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1126><div class="card-body p-3 small">One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1128.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803636 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1128/>The Hitchhiker’s Guide to Testing Statistical Significance in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/r/rotem-dror/>Rotem Dror</a>
|
<a href=/people/g/gili-baumer/>Gili Baumer</a>
|
<a href=/people/s/segev-shlomov/>Segev Shlomov</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1128><div class="card-body p-3 small">Statistical significance testing is a standard <a href=https://en.wikipedia.org/wiki/Statistics>statistical tool</a> designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance testing</a> in Natural Language Processing (NLP) research. We establish the fundamental concepts of <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance testing</a> and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance tests</a> in NLP research. Based on this discussion we propose a simple practical <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a> for statistical significance test selection in NLP setups and accompany this <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a> with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important <a href=https://en.wikipedia.org/wiki/Tool>tool</a> can be applied. in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a> in a statistically sound manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1130.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1130.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1130/>Stack-Pointer Networks for Dependency Parsing</a></strong><br><a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a>
|
<a href=/people/j/jingzhou-liu/>Jingzhou Liu</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1130><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> for dependency parsing : stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The <a href=https://en.wikipedia.org/wiki/Call_stack>stack</a> tracks the status of the <a href=https://en.wikipedia.org/wiki/Depth-first_search>depth-first search</a> and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n^2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them<tex-math>O(n^2)</tex-math> time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1131/>Twitter Universal Dependency Parsing for African-American and Mainstream American English<span class=acl-fixed-case>T</span>witter <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing for <span class=acl-fixed-case>A</span>frican-<span class=acl-fixed-case>A</span>merican and Mainstream <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1131><div class="card-body p-3 small">Due to the presence of both Twitter-specific conventions and <a href=https://en.wikipedia.org/wiki/Non-standard_dialect>non-standard and dialectal language</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods&#8217; impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at :.<url>http://slanglab.cs.umass.edu/TwitterAAE/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1132.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803729 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1132/>LSTMs Can Learn Syntax-Sensitive Dependencies Well, But <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>Modeling Structure</a> Makes Them Better<span class=acl-fixed-case>LSTM</span>s Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</a></strong><br><a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/j/john-hale/>John Hale</a>
|
<a href=/people/d/dani-yogatama/>Dani Yogatama</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1132><div class="card-body p-3 small">Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependenciesprovided they have enough capacity. We then explore whether <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that have access to explicit syntactic information learn <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a> more effectively, and how the way in which this structural information is incorporated into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> impacts performance. We find that the mere presence of syntactic information does not improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, but when model architecture is determined by <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> is improved. Further, we find that the choice of how syntactic structure is built affects how well <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> is learned : <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down construction</a> outperforms <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>left-corner and bottom-up variants</a> in capturing non-local structural dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1133.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1133/>Sequicity : Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures<span class=acl-fixed-case>S</span>equicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</a></strong><br><a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/x/xisen-jin/>Xisen Jin</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a>
|
<a href=/people/x/xiangnan-he/>Xiangnan He</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1133><div class="card-body p-3 small">Existing solutions to task-oriented dialogue systems follow <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline designs</a> which introduces <a href=https://en.wikipedia.org/wiki/Architecture>architectural complexity</a> and <a href=https://en.wikipedia.org/wiki/Fragility>fragility</a>. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability : significantly reducing model complexity in terms of number of parameters and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1136.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1136.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1136/>Mem2Seq : Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems<span class=acl-fixed-case>M</span>em2<span class=acl-fixed-case>S</span>eq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems</a></strong><br><a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1136><div class="card-body p-3 small">End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the <a href=https://en.wikipedia.org/wiki/Attentional_control>multi-hop attention</a> over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1137.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1137/>Tailored Sequence to Sequence Models to Different Conversation Scenarios</a></strong><br><a href=/people/h/hainan-zhang/>Hainan Zhang</a>
|
<a href=/people/y/yanyan-lan/>Yanyan Lan</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a>
|
<a href=/people/j/jun-xu/>Jun Xu</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1137><div class="card-body p-3 small">Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>. However, the requirements for different conversation scenarios are distinct. For example, <a href=https://en.wikipedia.org/wiki/Customer_service>customer service</a> requires the generated responses to be specific and accurate, while <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as &#8216;I do n&#8217;t know&#8217;. In this paper, we propose two tailored optimization criteria for Seq2Seq to different <a href=https://en.wikipedia.org/wiki/Conversation_analysis>conversation scenarios</a>, i.e., the <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum generated likelihood</a> for specific-requirement scenario, and the <a href=https://en.wikipedia.org/wiki/Conditional_value_at_risk>conditional value-at-risk</a> for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1140.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1140.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1140/>Sentiment Adaptive End-to-End Dialog Systems</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1140><div class="card-body p-3 small">End-to-end learning framework is useful for building <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialog systems</a> for its simplicity in training and efficiency in model updating. However, current <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approaches</a> only consider user semantic inputs in learning and under-utilize other <a href=https://en.wikipedia.org/wiki/User_information>user information</a>. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1141.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1141/>Embedding Learning Through Multilingual Concept Induction</a></strong><br><a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/m/mengjie-zhao/>Mengjie Zhao</a>
|
<a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1141><div class="card-body p-3 small">We present a new method for estimating vector space representations of words : embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> indicates that concept-based multilingual embedding learning performs better than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1143.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1143/>Language Modeling for <a href=https://en.wikipedia.org/wiki/Code_mixing>Code-Mixing</a> : The Role of <a href=https://en.wikipedia.org/wiki/Linguistic_theory>Linguistic Theory</a> based Synthetic Data</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1143><div class="card-body p-3 small">Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1148.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1148/>Improving Entity Linking by Modeling Latent Relations between Mentions</a></strong><br><a href=/people/p/phong-le/>Phong Le</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1148><div class="card-body p-3 small">Entity linking involves aligning textual mentions of named entities to their corresponding entries in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> or <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to predict these relations, we treat relations as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> in our neural entity-linking model. We induce the relations without any <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1149.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1149/>Dating Documents using Graph Convolution Networks</a></strong><br><a href=/people/s/shikhar-vashishth/>Shikhar Vashishth</a>
|
<a href=/people/s/shib-sankar-dasgupta/>Shib Sankar Dasgupta</a>
|
<a href=/people/s/swayambhu-nath-ray/>Swayambhu Nath Ray</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1149><div class="card-body p-3 small">Document date is essential for many important tasks, such as <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. Document Dating is a challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19 % absolute (45 % relative) accuracy points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1150.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1150" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1150/>A Graph-to-Sequence Model for AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1150><div class="card-body p-3 small">The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows superior results to existing methods in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1152 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1152.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1152.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1152/>Learning to Write with Cooperative Discriminators</a></strong><br><a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/d/david-golub/>David Golub</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1152><div class="card-body p-3 small">Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as <a href=https://en.wikipedia.org/wiki/Grice&#8217;s_maxims>Grice&#8217;s maxims</a>, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1153.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1153/>A Neural Approach to Pun Generation</a></strong><br><a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1153><div class="card-body p-3 small">Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate <a href=https://en.wikipedia.org/wiki/Pun>puns</a> without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with an elaborately designed decoding algorithm. Automatic and human evaluations show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are able to generate homographic puns of good readability and quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1154.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1154" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1154/>Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</a></strong><br><a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1154><div class="card-body p-3 small">This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a <a href=https://en.wikipedia.org/wiki/Chess>chess game</a>. The introduced <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of more than 298 K chess move-commentary pairs across 11 K chess games. We highlight how this task poses unique research challenges in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> : the <a href=https://en.wikipedia.org/wiki/Data>data</a> contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the <a href=https://en.wikipedia.org/wiki/Data>data</a> which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of <a href=https://en.wikipedia.org/wiki/Correctness_(computer_science)>correctness</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1155.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1155.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1155/>From Credit Assignment to Entropy Regularization : Two New <a href=https://en.wikipedia.org/wiki/Algorithm>Algorithms</a> for Neural Sequence Prediction</a></strong><br><a href=/people/z/zihang-dai/>Zihang Dai</a>
|
<a href=/people/q/qizhe-xie/>Qizhe Xie</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1155><div class="card-body p-3 small">In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1156.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1156/>DuoRC : Towards Complex Language Understanding with Paraphrased Reading Comprehension<span class=acl-fixed-case>D</span>uo<span class=acl-fixed-case>RC</span>: Towards Complex Language Understanding with Paraphrased Reading Comprehension</a></strong><br><a href=/people/a/amrita-saha/>Amrita Saha</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1156><div class="card-body p-3 small">We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in <a href=https://en.wikipedia.org/wiki/Language_understanding>language understanding</a> beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> reflects two versions of the same movie-one from Wikipedia and the other from IMDb-written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, etc., answering questions from the second version requires deeper <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42 % on DuoRC v / s 86 % on SQuAD dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1157.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1157/>Stochastic Answer Networks for Machine Reading Comprehension</a></strong><br><a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1157><div class="card-body p-3 small">We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1159/>Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</a></strong><br><a href=/people/z/zhen-wang/>Zhen Wang</a>
|
<a href=/people/j/jiachen-liu/>Jiachen Liu</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/t/tian-wu/>Tian Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1159><div class="card-body p-3 small">While sophisticated neural-based techniques have been developed in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> and train the two-stage process jointly with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the <a href=https://en.wikipedia.org/wiki/Information_fusion>information fusion</a> of all the candidates and the joint training of the extract-then-select procedure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1160.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1160.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1160/>Efficient and Robust <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> from Minimal Context over Documents</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1160><div class="card-body p-3 small">Neural models for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA)</a> over <a href=https://en.wikipedia.org/wiki/Document>documents</a> have achieved significant performance improvements. Although effective, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1163.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1163/>Towards Robust Neural Machine Translation</a></strong><br><a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/j/junjie-zhai/>Junjie Zhai</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1163><div class="card-body p-3 small">Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1165.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1165.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1165/>Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning</a></strong><br><a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/j/joshua-uyheng/>Joshua Uyheng</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1165><div class="card-body p-3 small">We present a study on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of human bandit feedback, and analyze the influence of <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator -agreement is comparable. Best <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into <a href=https://en.wikipedia.org/wiki/Linear_regression>RL</a> for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1166.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1166/>Accelerating Neural Transformer via an Average Attention Network</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1166><div class="card-body p-3 small">With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and <a href=https://en.wikipedia.org/wiki/Translation>translation performance</a>. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1167.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1167" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1167/>How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</a></strong><br><a href=/people/t/tobias-domhan/>Tobias Domhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1167><div class="card-body p-3 small">With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a> or <a href=https://en.wikipedia.org/wiki/Computer-generated_imagery>CNN</a> without a loss in performance in most settings. Surprisingly, even a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> without any target side self-attention performs well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1168.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1168.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804766 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1168/>Weakly Supervised Semantic Parsing with Abstract Examples</a></strong><br><a href=/people/o/omer-goldman/>Omer Goldman</a>
|
<a href=/people/v/veronica-latcinnik/>Veronica Latcinnik</a>
|
<a href=/people/e/ehud-nave/>Ehud Nave</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1168><div class="card-body p-3 small">Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> needs to be explored at training time to find a correct <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>. Second, spurious programs that accidentally lead to a correct <a href=https://en.wikipedia.org/wiki/Denotation>denotation</a> add <a href=https://en.wikipedia.org/wiki/Noise>noise</a> to <a href=https://en.wikipedia.org/wiki/Training>training</a>. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> substantially improves performance, and reaches 82.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, a 14.7 % absolute accuracy improvement compared to the best reported <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1169.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1169.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804785 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1169/>Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback</a></strong><br><a href=/people/c/carolin-lawrence/>Carolin Lawrence</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1169><div class="card-body p-3 small">Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target <a href=https://en.wikipedia.org/wiki/System>system</a>. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with <a href=https://en.wikipedia.org/wiki/User_(computing)>human users</a>, we devise an easy-to-use interface to collect <a href=https://en.wikipedia.org/wiki/Feedback>human feedback</a> on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1170.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1170.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804795 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1170/>AMR dependency parsing with a typed semantic algebra<span class=acl-fixed-case>AMR</span> dependency parsing with a typed semantic algebra</a></strong><br><a href=/people/j/jonas-groschwitz/>Jonas Groschwitz</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/m/meaghan-fowlie/>Meaghan Fowlie</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1170><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and outperform strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804833 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1172/>Batch IS NOT Heavy : Learning Word Representations From All Samples<span class=acl-fixed-case>IS</span> <span class=acl-fixed-case>NOT</span> Heavy: Learning Word Representations From All Samples</a></strong><br><a href=/people/x/xin-xin/>Xin Xin</a>
|
<a href=/people/f/fajie-yuan/>Fajie Yuan</a>
|
<a href=/people/x/xiangnan-he/>Xiangnan He</a>
|
<a href=/people/j/joemon-m-jose/>Joemon M. Jose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1172><div class="card-body p-3 small">Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling methods</a> are biased especially when the <a href=https://en.wikipedia.org/wiki/Sampling_distribution>sampling distribution</a> deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark tasks</a>. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1173.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1173.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804853 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1173/>Backpropagating through Structured Argmax using a SPIGOT<span class=acl-fixed-case>SPIGOT</span></a></strong><br><a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1173><div class="card-body p-3 small">We introduce structured projection of intermediate gradients (SPIGOT), a new method for <a href=https://en.wikipedia.org/wiki/Backpropagation>backpropagating through neural networks</a> that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing <a href=https://en.wikipedia.org/wiki/Backpropagation>backpropagation</a> before and after them ; SPIGOT&#8217;s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines : syntactic-then-semantic dependency parsing, and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1174.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804866 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1174/>Learning How to Actively Learn : A Deep Imitation Learning Approach</a></strong><br><a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/w/wray-buntine/>Wray Buntine</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1174><div class="card-body p-3 small">Heuristic-based active learning (AL) methods are limited when the <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>data distribution</a> of the underlying learning problems vary. We introduce a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> that learns an AL policy using imitation learning (IL). Our IL-based approach makes use of an efficient and effective algorithmic expert, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1175.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804886 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1175/>Training <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>Classifiers</a> with Natural Language Explanations</a></strong><br><a href=/people/b/braden-hancock/>Braden Hancock</a>
|
<a href=/people/p/paroma-varma/>Paroma Varma</a>
|
<a href=/people/s/stephanie-wang/>Stephanie Wang</a>
|
<a href=/people/m/martin-bringmann/>Martin Bringmann</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-re/>Christopher Ré</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1175><div class="card-body p-3 small">Training accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in which an annotator provides a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanation</a> for each labeling decision. A <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. On three relation extraction tasks, we find that users are able to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1177.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1177/>Harvesting Paragraph-level Question-Answer Pairs from Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1177><div class="card-body p-3 small">We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that only take into account sentence-level information (Heilman and Smith, 2010 ; Du et al., 2017 ; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1179.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1179.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152765 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1179/>Language Generation via DAG Transduction<span class=acl-fixed-case>DAG</span> Transduction</a></strong><br><a href=/people/y/yajie-ye/>Yajie Ye</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1179><div class="card-body p-3 small">A DAG automaton is a formal device for manipulating <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our <a href=https://en.wikipedia.org/wiki/Transducer>transducer</a> is a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> licensed by a <a href=https://en.wikipedia.org/wiki/Declarative_programming>declarative programming language</a> rather than linguistic structures. By executing such a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>, we can easily get a surface string. Our <a href=https://en.wikipedia.org/wiki/Transducer>transducer</a> is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1180.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152732 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1180" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1180/>A Distributional and Orthographic Aggregation Model for English Derivational Morphology<span class=acl-fixed-case>E</span>nglish Derivational Morphology</a></strong><br><a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/j/john-hewitt/>John Hewitt</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1180><div class="card-body p-3 small">Modeling <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational morphology</a> to generate words with particular <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> is useful in many text generation tasks, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or abstractive question answering. In this work, we tackle the task of derived word generation. That is, we attempt to generate the word runner for someone who runs. We identify two key problems in generating derived words from <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>root words</a> and <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a>. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then learns to choose between the hypothesis of each system. We also present two ways of incorporating <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus information</a> into derived word generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1182.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804944 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1182/>NeuralREG : An end-to-end approach to referring expression generation<span class=acl-fixed-case>N</span>eural<span class=acl-fixed-case>REG</span>: An end-to-end approach to referring expression generation</a></strong><br><a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/d/diego-moussallem/>Diego Moussallem</a>
|
<a href=/people/a/akos-kadar/>Ákos Kádár</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1182><div class="card-body p-3 small">Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical function</a>. In this paper, we present a new approach (NeuralREG), relying on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>, which makes decisions about form and content in one go without explicit <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1183.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1183.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804956 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1183" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1183/>Stock Movement Prediction from <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> and Historical Prices</a></strong><br><a href=/people/y/yumo-xu/>Yumo Xu</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1183><div class="card-body p-3 small">Stock movement prediction is a challenging problem : the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of <a href=https://en.wikipedia.org/wiki/Stochastic>stochasticity</a>, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a new stock movement prediction dataset which we collected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804998 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1186/>Multimodal Named Entity Disambiguation for Noisy Social Media Posts</a></strong><br><a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1186><div class="card-body p-3 small">We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1187.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1187.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1187/>Semi-supervised User Geolocation via Graph Convolutional Networks</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1187><div class="card-body p-3 small">Social media user geolocation is vital to many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> such as <a href=https://en.wikipedia.org/wiki/Geolocation>event detection</a>. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, and to two baselines we propose, and show that our model achieves or is competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1192.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805245 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1192/>Syntax for Semantic Role Labeling, To Be, Or Not To Be</a></strong><br><a href=/people/s/shexia-he/>Shexia He</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/h/hongxiao-bai/>Hongxiao Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1192><div class="card-body p-3 small">Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> has a remarkable contribution to <a href=https://en.wikipedia.org/wiki/Speech_recognition>SRL</a> performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, showing the quantitative significance of <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> to neural SRL together with a thorough empirical survey over existing models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1193.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1193.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805263 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1193/>Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation</a></strong><br><a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1193><div class="card-body p-3 small">We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3 % across the domains over approaches that use <a href=https://en.wikipedia.org/wiki/High-level_programming_language>high-level logical representations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1194.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805276 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1194/>Marrying Up <a href=https://en.wikipedia.org/wiki/Regular_expression>Regular Expressions</a> with <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> : A Case Study for Spoken Language Understanding</a></strong><br><a href=/people/b/bingfeng-luo/>Bingfeng Luo</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/z/zheng-wang/>Zheng Wang</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1194><div class="card-body p-3 small">The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question : Can we combine a neural network (NN) with regular expressions (RE) to improve <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>?. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>, giving a clear boost to the RE-unaware NN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1197.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805320 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1197/>To Attend or not to Attend : A Case Study on Syntactic Structures for Semantic Relatedness</a></strong><br><a href=/people/a/amulya-gupta/>Amulya Gupta</a>
|
<a href=/people/z/zhu-zhang/>Zhu Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1197><div class="card-body p-3 small">With the recent success of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks (RNNs)</a> in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation (MT)</a>, attention mechanisms have become increasingly popular. The purpose of this paper is two-fold ; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>LSTM</a>. Secondly, we study the interaction between <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and syntactic structures, by experimenting with three LSTM variants : bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks : semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1198 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1198.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1198.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805339 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1198/>What you can cram into a single $ & ! # * vector : Probing sentence embeddings for linguistic properties</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/g/german-kruszewski/>German Kruszewski</a>
|
<a href=/people/g/guillaume-lample/>Guillaume Lample</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1198><div class="card-body p-3 small">Although much effort has recently been devoted to training high-quality <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>, we still have a poor understanding of what they are capturing. Downstream tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> makes it however difficult to infer what kind of information is present in the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> trained in eight distinct ways, uncovering intriguing properties of both <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> and training methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1199.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805358 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1199/>Robust Distant Supervision Relation Extraction via <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Reinforcement Learning</a></a></strong><br><a href=/people/p/pengda-qin/>Pengda Qin</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1199><div class="card-body p-3 small">Distant supervision has become the standard method for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. However, even though <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is an efficient method, it does not come at no costThe resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solutionWe explore a deep reinforcement learning strategy to generate the <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false-positive indicator</a>, where we automatically recognize <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a> for each relation type without any <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised information</a>. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> significantly improves the performance of distant supervision comparing to <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1200.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805371 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1200/>Interpretable and Compositional Relation Learning by Joint Training with an <a href=https://en.wikipedia.org/wiki/Autoencoder>Autoencoder</a></a></strong><br><a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/r/ran-tian/>Ran Tian</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1200><div class="card-body p-3 small">Embedding models for entities and relations are extremely useful for recovering missing facts in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Intuitively, a <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> can be modeled by a matrix mapping entity vectors. However, <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>relations</a> reside on low dimension sub-manifolds in the parameter space of arbitrary matrices for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2 = M3). In this paper we investigate a dimension reduction technique by training <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> jointly with an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a>, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at.<url>github.com/tianran/glimvec</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1201.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805384 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1201/>Zero-Shot Transfer Learning for Event Extraction</a></strong><br><a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1201><div class="card-body p-3 small">Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus can not be applied to new event types without extra annotation effort. We take a fresh look at <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> and model it as a generic grounding problem : mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing <a href=https://en.wikipedia.org/wiki/Event_(computing)>event types</a>, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> trained from 3,000 sentences annotated with 500 event mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1205.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805443 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1205/>Personalizing Dialogue Agents : I have a dog, do you have pets too?<span class=acl-fixed-case>I</span> have a dog, do you have pets too?</a></strong><br><a href=/people/s/saizheng-zhang/>Saizheng Zhang</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/j/jack-urbanek/>Jack Urbanek</a>
|
<a href=/people/a/arthur-szlam/>Arthur Szlam</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1205><div class="card-body p-3 small">Chit-chat models are known to have several problems : they lack <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, do not display a consistent personality and are often not very captivating. In this work we present the task of making <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information ; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805460 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1206/>Efficient Large-Scale Neural Domain Classification with Personalized Attention</a></strong><br><a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/d/dongchan-kim/>Dongchan Kim</a>
|
<a href=/people/a/anjishnu-kumar/>Anjishnu Kumar</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1206><div class="card-body p-3 small">In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that incorporates <a href=https://en.wikipedia.org/wiki/Personalization>personalization information</a> and domain-specific classifiers that solves the problem efficiently. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of <a href=https://en.wikipedia.org/wiki/Real-time_computing>real-time production systems</a>, and design to minimize <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> and <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>runtime latency</a>. We demonstrate that incorporating <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a> significantly improves domain classification accuracy in a setting with thousands of overlapping domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805491 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1208/>Multimodal Language Analysis in the Wild : CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph<span class=acl-fixed-case>CMU</span>-<span class=acl-fixed-case>MOSEI</span> Dataset and Interpretable Dynamic Fusion Graph</a></strong><br><a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1208><div class="card-body p-3 small">Analyzing human multimodal language is an emerging area of research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Intrinsically this <a href=https://en.wikipedia.org/wiki/Language>language</a> is multimodal (heterogeneous), sequential and asynchronous ; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1212.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1212.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805571 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1212/>Joint Reasoning for Temporal and Causal Relations</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/z/zhili-feng/>Zhili Feng</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1212><div class="card-body p-3 small">Understanding temporal and causal relations between events is a fundamental <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding task</a>. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for <a href=https://en.wikipedia.org/wiki/Logical_disjunction>them</a> using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> that are inherent in the nature of time and <a href=https://en.wikipedia.org/wiki/Causality>causality</a>. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1214.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1214/>A Deep Relevance Model for Zero-Shot Document Filtering</a></strong><br><a href=/people/c/chenliang-li/>Chenliang Li</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/f/feng-ji/>Feng Ji</a>
|
<a href=/people/y/yuguang-duan/>Yu Duan</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1214><div class="card-body p-3 small">In the era of <a href=https://en.wikipedia.org/wiki/Big_data>big data</a>, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, <a href=https://en.wikipedia.org/wiki/Information_filtering>information filtering</a> therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1216.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1216/>Joint Embedding of Words and Labels for Text Classification</a></strong><br><a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1216><div class="card-body p-3 small">Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> as a label-word joint embedding problem : each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between <a href=https://en.wikipedia.org/wiki/String_(computer_science)>text sequences</a> and labels. The <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1218.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1218.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1218" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1218/>Document Similarity for Texts of Varying Lengths via Hidden Topics</a></strong><br><a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/t/tarek-sakakini/>Tarek Sakakini</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/j/jinjun-xiong/>JinJun Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1218><div class="card-body p-3 small">Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching tasks</a> and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to text matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1220 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1220.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1220/>Multi-Input Attention for Unsupervised OCR Correction<span class=acl-fixed-case>OCR</span> Correction</a></strong><br><a href=/people/r/rui-dong/>Rui Dong</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1220><div class="card-body p-3 small">We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised training</a> and as a source of evidence when decoding. A sequence-to-sequence model with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus</a> among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1221.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1221" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1221/>Building <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Text with Named Entities</a></strong><br><a href=/people/m/md-rizwan-parvez/>Md Rizwan Parvez</a>
|
<a href=/people/s/saikat-chakraborty/>Saikat Chakraborty</a>
|
<a href=/people/b/baishakhi-ray/>Baishakhi Ray</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1221><div class="card-body p-3 small">Text in many domains involves a significant amount of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. Predicting the entity names is often challenging for a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> which can learn the entity names by leveraging their entity type information. We also introduce two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> based on recipes and <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java programming codes</a>, on which we evaluate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 52.2 % better perplexity in recipe generation and 22.06 % on <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>code generation</a> than state-of-the-art language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1222.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1222/>hyperdoc2vec : Distributed Representations of Hypertext Documents</a></strong><br><a href=/people/j/jialong-han/>Jialong Han</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/h/haisong-zhang/>Haisong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1222><div class="card-body p-3 small">Hypertext documents, such as <a href=https://en.wikipedia.org/wiki/Web_page>web pages</a> and <a href=https://en.wikipedia.org/wiki/Academic_publishing>academic papers</a>, are of great importance in delivering information in our daily life. Although being effective on <a href=https://en.wikipedia.org/wiki/Plain_text>plain documents</a>, conventional text embedding methods suffer from <a href=https://en.wikipedia.org/wiki/Information_loss>information loss</a> if directly adapted to <a href=https://en.wikipedia.org/wiki/Hypertext>hyper-documents</a>. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several <a href=https://en.wikipedia.org/wiki/Competition_(biology)>competitors</a> on two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, i.e., paper classification and <a href=https://en.wikipedia.org/wiki/Citation>citation recommendation</a>, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> w.r.t. the four criteria.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1223.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1223/>Entity-Duet Neural Ranking : Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval</a></strong><br><a href=/people/z/zhenghao-liu/>Zhenghao Liu</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1223><div class="card-body p-3 small">This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> to neural search systems. EDRM represents queries and documents by their words and entity annotations. The <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> are integrated in the distributed representations of their entities, while the <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> is conducted by interaction-based neural ranking networks. The two <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of <a href=https://en.wikipedia.org/wiki/EDRM>EDRM</a>. Our analyses reveal that knowledge graph semantics significantly improve the <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a> of neural ranking models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1224.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1224/>Neural Natural Language Inference Models Enhanced with External Knowledge</a></strong><br><a href=/people/q/qian-chen/>Qian Chen</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/s/si-wei/>Si Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1224><div class="card-body p-3 small">Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these <a href=https://en.wikipedia.org/wiki/Data>data</a>? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1225 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1225.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1225.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1225/>AdvEntuRe : Adversarial Training for Textual Entailment with Knowledge-Guided Examples<span class=acl-fixed-case>A</span>dv<span class=acl-fixed-case>E</span>ntu<span class=acl-fixed-case>R</span>e: Adversarial Training for Textual Entailment with Knowledge-Guided Examples</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1225><div class="card-body p-3 small">We consider the problem of learning textual entailment models with limited supervision (5K-10 K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment modela discriminatormore robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator&#8217;s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> increase <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 4.7 % on SciTail and by 2.8 % on a 1 % sub-sample of <a href=https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism>SNLI</a>. Notably, even a single hand-written rule, negate, improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of negation examples in SNLI by 6.1 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1226.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1226" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1226/>Subword-level Word Vector Representations for Korean<span class=acl-fixed-case>K</span>orean</a></strong><br><a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/jeongmin-byun/>Jeongmin Byun</a>
|
<a href=/people/s/sion-baek/>Sion Baek</a>
|
<a href=/people/y/yongseok-cho/>Yongseok Cho</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1226><div class="card-body p-3 small">Research on distributed word representations is focused on widely-used languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Although the same <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> can be used for other languages, language-specific knowledge can enhance the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and richness of <a href=https://en.wikipedia.org/wiki/Vector_space>word vector representations</a>. In this paper, we look at improving distributed word representations for <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> using knowledge about the unique linguistic structure of <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. Specifically, we decompose Korean words into the jamo-level, beyond the character-level, allowing a systematic use of subword information. To evaluate the <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a>, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1229.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1229/>End-to-End Reinforcement Learning for Automatic Taxonomy Induction</a></strong><br><a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1229><div class="card-body p-3 small">We present a novel end-to-end reinforcement learning approach to <a href=https://en.wikipedia.org/wiki/Automatic_taxonomy_induction>automatic taxonomy induction</a> from a set of terms. While prior methods treat the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>, and can not effectively optimize <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a> via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomies</a>. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6 % on ancestor F1.<i>i.e.</i>,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine <i>which</i> term to select and <i>where</i> to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1230.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1230/>Incorporating Glosses into Neural Word Sense Disambiguation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/q/qiaolin-xia/>Qiaolin Xia</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1230><div class="card-body p-3 small">Word Sense Disambiguation (WSD) aims to identify the correct meaning of <a href=https://en.wikipedia.org/wiki/Polysemy>polysemous words</a> in the particular context. Lexical resources like <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> which are proved to be of great help for WSD in the knowledge-based methods. However, previous <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for WSD always rely on massive labeled data (context), ignoring lexical resources like <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>glosses</a> (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> and <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical knowledge</a>. Therefore, we propose GAS : a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to enrich the <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>gloss information</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art systems on several English all-words WSD datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1231.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1231/>Bilingual Sentiment Embeddings : Joint Projection of Sentiment Across Languages</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1231><div class="card-body p-3 small">Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> in a source and target language. This model only requires a small <a href=https://en.wikipedia.org/wiki/Bilingual_lexicon>bilingual lexicon</a>, a source-language corpus annotated for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our analysis of the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> provides evidence that it represents <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> in the resource-poor target language without any annotated data in that language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1232 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1232.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1232/>Learning Domain-Sensitive and Sentiment-Aware Word Embeddings</a></strong><br><a href=/people/b/bei-shi/>Bei Shi</a>
|
<a href=/people/z/zihao-fu/>Zihao Fu</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1232><div class="card-body p-3 small">Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> exploit <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>, but they can not produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they can not distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1233.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1233/>Cross-Domain Sentiment Classification with Target Domain Specific Information</a></strong><br><a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yu-gang-jiang/>Yu-gang Jiang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1233><div class="card-body p-3 small">The task of adopting a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with good performance to a target domain that is different from the source domain used for training has received considerable attention in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Most existing approaches mainly focus on learning <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> that are domain-invariant in both the source and target domains. Few of them pay attention to domain-specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on each of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> experiments demonstrated that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> could achieve better performance than state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1234.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1234/>Aspect Based Sentiment Analysis with Gated Convolutional Networks</a></strong><br><a href=/people/w/wei-xue/>Wei Xue</a>
|
<a href=/people/t/tao-li/>Tao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1234><div class="card-body p-3 small">Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks : aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> is much simpler than attention layer used in the existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-1235/>A Helping Hand : <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> for Deep Sentiment Analysis</a></strong><br><a href=/people/x/xin-luna-dong/>Xin Dong</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1235><div class="card-body p-3 small">Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a>, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in seven different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1236 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1236.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1236" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1236/>Cold-Start Aware User and Product Attention for Sentiment Classification</a></strong><br><a href=/people/r/reinald-kim-amplayo/>Reinald Kim Amplayo</a>
|
<a href=/people/j/jihyeok-kim/>Jihyeok Kim</a>
|
<a href=/people/s/sua-sung/>Sua Sung</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1236><div class="card-body p-3 small">The use of user / product information in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> is important, especially for cold-start users / products, whose number of reviews are very limited. However, current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not deal with the cold-start problem which is typical in <a href=https://en.wikipedia.org/wiki/Review_site>review websites</a>. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules : (1) a fast word encoder that returns word vectors embedded with short and long range dependency features ; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users / products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>, and thus can be trained much faster. More importantly, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performs significantly better than previous <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> when the training data is sparse and has cold-start problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1237.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1237/>Modeling Deliberative Argumentation Strategies on Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/k/kevin-lang/>Kevin Lang</a>
|
<a href=/people/j/jakob-herpel/>Jakob Herpel</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1237><div class="card-body p-3 small">This paper studies how the <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation strategies</a> of participants in <a href=https://en.wikipedia.org/wiki/Deliberation>deliberative discussions</a> can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for <a href=https://en.wikipedia.org/wiki/Deliberative_assembly>deliberative discussions</a> and we illustrate its operationalization. Previous <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> statistically from several types of <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions : discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with three supervised classifiers and provide evidence that the proposed categories can be predicted.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1241.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1241.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1241/>Attacking Visual Language Grounding with Adversarial Examples : A Case Study on Neural Image Captioning</a></strong><br><a href=/people/h/hongge-chen/>Hongge Chen</a>
|
<a href=/people/h/huan-zhang/>Huan Zhang</a>
|
<a href=/people/p/pin-yu-chen/>Pin-Yu Chen</a>
|
<a href=/people/j/jinfeng-yi/>Jinfeng Yi</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1241><div class="card-body p-3 small">Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components : a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in <a href=https://en.wikipedia.org/wiki/Machine_vision>machine vision</a> and perception, we propose Show-and-Fool, a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for crafting adversarial examples in neural image captioning. The proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1243 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1243.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1243.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1243/>Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game</a></strong><br><a href=/people/h/haichao-zhang/>Haichao Zhang</a>
|
<a href=/people/h/haonan-yu/>Haonan Yu</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1243><div class="card-body p-3 small">Building <a href=https://en.wikipedia.org/wiki/Intelligent_agent>intelligent agents</a> that can communicate with and learn from humans in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a> and for novel <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1245.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1245.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1245" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1245/>A Structured Variational Autoencoder for Contextual Morphological Inflection</a></strong><br><a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/j/jason-naradowsky/>Jason Naradowsky</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1245><div class="card-body p-3 small">Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following : How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior inference</a> over the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>, we derive an efficient variational inference procedure based on the <a href=https://en.wikipedia.org/wiki/Wake-sleep_algorithm>wake-sleep algorithm</a>. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10 % absolute accuracy in some cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1248.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1248" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1248/>Global Transition-based Non-projective Dependency Parsing</a></strong><br><a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a>
|
<a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1248><div class="card-body p-3 small">Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH algorithm, an O(n^4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.<tex-math>O(n^4)</tex-math> mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH&#8324; compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1251.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1251/>Composing <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite State Transducers</a> on GPUs<span class=acl-fixed-case>GPU</span>s</a></strong><br><a href=/people/a/arturo-argueta/>Arturo Argueta</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1251><div class="card-body p-3 small">Weighted finite state transducers (FSTs) are frequently used in <a href=https://en.wikipedia.org/wiki/Language_processing>language processing</a> to handle tasks such as <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>. There has been previous work using multiple <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU cores</a> to accelerate <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite state algorithms</a>, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152828 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1254/>Finding syntax in human encephalography with <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a></a></strong><br><a href=/people/j/john-hale/>John Hale</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/j/jonathan-brennan/>Jonathan Brennan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1254><div class="card-body p-3 small">Recurrent neural network grammars (RNNGs) are generative models of (tree, string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects : an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to <a href=https://en.wikipedia.org/wiki/Syntax>syntactic composition</a> within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic processing</a> that occurs during normal human language comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1256.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/288152712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1256/>Let’s do it again : A First Computational Approach to Detecting Adverbial Presupposition Triggers</a></strong><br><a href=/people/a/andre-cianflone/>Andre Cianflone</a>
|
<a href=/people/y/yulan-feng/>Yulan Feng</a>
|
<a href=/people/j/jad-kabbara/>Jad Kabbara</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1256><div class="card-body p-3 small">We introduce the novel task of predicting adverbial presupposition triggers, which is useful for natural language generation tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and dialogue systems. We introduce two new corpora, derived from the Penn Treebank and the Annotated English Gigaword dataset and investigate the use of a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our <a href=https://en.wikipedia.org/wiki/Mechanism_design>mechanism</a>. We demonstrate that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> statistically outperforms our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div></div><hr><div id=p18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P18-2/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2002.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2002/>Restricted Recurrent Neural Tensor Networks : Exploiting Word Frequency and Compositionality</a></strong><br><a href=/people/a/alexandre-salle/>Alexandre Salle</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2002><div class="card-body p-3 small">Increasing the capacity of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a> usually involves augmenting the size of the hidden layer, with significant increase of <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using <a href=https://en.wikipedia.org/wiki/Gated_recurrent_unit>Gated Recurrent Units</a> and <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>Long Short-Term Memory</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2003.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2003.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2003/>Deep RNNs Encode Soft Hierarchical Syntax<span class=acl-fixed-case>RNN</span>s Encode Soft Hierarchical Syntax</a></strong><br><a href=/people/t/terra-blevins/>Terra Blevins</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2003><div class="card-body p-3 small">We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> ; for each word, we predict its <a href=https://en.wikipedia.org/wiki/Part_of_speech>part of speech</a> as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives : dependency parsing, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, or <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> encode significant amounts of <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> even in the absence of an explicit syntactic training supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2004/>Word Error Rate Estimation for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> : e-WER<span class=acl-fixed-case>WER</span></a></strong><br><a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/s/steve-renals/>Steve Renals</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2004><div class="card-body p-3 small">Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set. Our e-WER framework uses a comprehensive set of <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> : ASR recognised text, <a href=https://en.wikipedia.org/wiki/Computer_vision>character recognition</a> results to complement recognition output, and internal decoder features. We report results for the two features ; black-box and glass-box using unseen 24 Arabic broadcast programs. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 16.9 % <a href=https://en.wikipedia.org/wiki/Root-mean-square_deviation>WER root mean squared error (RMSE)</a> across 1,400 sentences. The estimated overall WER e-WER was 25.3 % for the three hours test set, while the actual WER was 28.5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2006.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2006/>HotFlip : White-Box Adversarial Examples for Text Classification<span class=acl-fixed-case>H</span>ot<span class=acl-fixed-case>F</span>lip: White-Box Adversarial Examples for Text Classification</a></strong><br><a href=/people/j/javid-ebrahimi/>Javid Ebrahimi</a>
|
<a href=/people/a/anyi-rao/>Anyi Rao</a>
|
<a href=/people/d/daniel-lowd/>Daniel Lowd</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2006><div class="card-body p-3 small">We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2008.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2008.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2008/>Active learning for deep semantic parsing</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hadi-afshar/>Hadi Afshar</a>
|
<a href=/people/d/dominique-estival/>Dominique Estival</a>
|
<a href=/people/g/glen-pink/>Glen Pink</a>
|
<a href=/people/p/philip-r-cohen/>Philip Cohen</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2008><div class="card-body p-3 small">Semantic parsing requires <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> that is expensive and slow to collect. We apply <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> to both traditional and overnight data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> but not applicable for overnight collection. We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2010/>Unsupervised Semantic Frame Induction using Triclustering</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2010><div class="card-body p-3 small">We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2011.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2011/>Identification of Alias Links among Participants in Narratives</a></strong><br><a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2011><div class="card-body p-3 small">Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a>, <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> or <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four <a href=https://en.wikipedia.org/wiki/Multiculturalism>diverse history narratives</a> of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and coreference resolution techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2012.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2012/>Named Entity Recognition With Parallel Recurrent Neural Networks</a></strong><br><a href=/people/a/andrej-zukov-gregoric/>Andrej Žukov-Gregorič</a>
|
<a href=/people/y/yoram-bachrach/>Yoram Bachrach</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2012><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> we find a significant reduction in the total number of parameters. We find our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves state-of-the-art performance on the CoNLL 2003 NER dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2014.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2014/>A Walk-based Model on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>Entity Graphs</a> for Relation Extraction</a></strong><br><a href=/people/f/fenia-christopoulou/>Fenia Christopoulou</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2014><div class="card-body p-3 small">We present a novel graph-based neural network model for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in a fully-connected graph structure. The <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are represented with <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>position-aware contexts</a> around the entity pairs. In order to consider different <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>relation paths</a> between two entities, we construct up to l-length walks between each pair. The resulting <a href=https://en.wikipedia.org/wiki/Walk_(graph_theory)>walks</a> are merged and iteratively used to update the edge representations into longer walks representations. We show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.<tex-math>l</tex-math>-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2016.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2016/>Automatic Extraction of Commonsense LocatedNear Knowledge<span class=acl-fixed-case>L</span>ocated<span class=acl-fixed-case>N</span>ear Knowledge</a></strong><br><a href=/people/f/frank-f-xu/>Frank F. Xu</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2016><div class="card-body p-3 small">LocatedNear relation is a kind of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> for evaluation and future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2020.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2020/>A Named Entity Recognition Shootout for German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2020><div class="card-body p-3 small">We ask how to practically build a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for German named entity recognition (NER) that performs at the state of the art for both <a href=https://en.wikipedia.org/wiki/Contemporary_history>contemporary and historical texts</a>, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>CRF</a> when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2021/>A dataset for identifying actionable feedback in <a href=https://en.wikipedia.org/wiki/Collaborative_software_development>collaborative software development</a></a></strong><br><a href=/people/b/benjamin-s-meyers/>Benjamin S. Meyers</a>
|
<a href=/people/n/nuthan-munaiah/>Nuthan Munaiah</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a>
|
<a href=/people/a/andrew-meneely/>Andrew Meneely</a>
|
<a href=/people/j/josephine-wolff/>Josephine Wolff</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a>
|
<a href=/people/p/pradeep-murukannaiah/>Pradeep Murukannaiah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2021><div class="card-body p-3 small">Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>security vulnerabilities</a> and errors. For a <a href=https://en.wikipedia.org/wiki/Code_review>code review</a> to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for this new task, and the potential of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2023.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2023/>Analogical Reasoning on Chinese Morphological and Semantic Relations<span class=acl-fixed-case>C</span>hinese Morphological and Semantic Relations</a></strong><br><a href=/people/s/shen-li/>Shen Li</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/r/renfen-hu/>Renfen Hu</a>
|
<a href=/people/w/wensi-li/>Wensi Li</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2023><div class="card-body p-3 small">Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a>, context features, and <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> on <a href=https://en.wikipedia.org/wiki/Analogy>analogical reasoning</a>. With the experiments, <a href=https://en.wikipedia.org/wiki/CA8>CA8</a> is proved to be a reliable benchmark for evaluating Chinese word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2024.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2024.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2024/>Construction of a Chinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions<span class=acl-fixed-case>C</span>hinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions</a></strong><br><a href=/people/d/dongyu-zhang/>Dongyu Zhang</a>
|
<a href=/people/h/hongfei-lin/>Hongfei Lin</a>
|
<a href=/people/l/liang-yang/>Liang Yang</a>
|
<a href=/people/s/shaowu-zhang/>Shaowu Zhang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2024><div class="card-body p-3 small">Metaphors are frequently used to convey emotions. However, there is little research on the construction of metaphor corpora annotated with <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> for the analysis of emotionality of metaphorical expressions. Furthermore, most studies focus on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and few in other languages, particularly <a href=https://en.wikipedia.org/wiki/Sino-Tibetan_languages>Sino-Tibetan languages</a> such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, for emotion analysis from metaphorical texts, although there are likely to be many differences in emotional expressions of metaphorical usages across different languages. We therefore construct a significant new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> on <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>, with 5,605 manually annotated sentences in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We present an annotation scheme that contains annotations of linguistic metaphors, emotional categories (joy, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Sadness>sadness</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Love>love</a>, disgust and surprise), and intensity. The annotation agreement analyses for multiple annotators are described. We also use the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to explore and analyze the emotionality of metaphors. To the best of our knowledge, this is the first relatively large metaphor corpus with an annotation of emotions in Chinese.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2028/>A Language Model based Evaluator for Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/z/zhiyuan-luo/>Zhiyuan Luo</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2028><div class="card-body p-3 small">We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively generate more readable compression, comparable or superior to several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2029/>Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources</a></strong><br><a href=/people/m/maria-glenski/>Maria Glenski</a>
|
<a href=/people/t/tim-weninger/>Tim Weninger</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2029><div class="card-body p-3 small">In the age of <a href=https://en.wikipedia.org/wiki/Social_news_website>social news</a>, it is important to understand the types of <a href=https://en.wikipedia.org/wiki/Reactionary>reactions</a> that are evoked from <a href=https://en.wikipedia.org/wiki/Source_(journalism)>news sources</a> with various levels of <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a>. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. To that end, (1) we develop a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8 M <a href=https://en.wikipedia.org/wiki/Twitter>Twitter posts</a> and 6.2 M <a href=https://en.wikipedia.org/wiki/Reddit>Reddit comments</a>. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, but far smaller differences on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2031/>Fighting Offensive Language on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> with Unsupervised Text Style Transfer</a></strong><br><a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/i/igor-melnyk/>Igor Melnyk</a>
|
<a href=/people/i/inkit-padhi/>Inkit Padhi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2031><div class="card-body p-3 small">We introduce a new approach to tackle the problem of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2033.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2033/>Task-oriented Dialogue System for Automatic Diagnosis</a></strong><br><a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/q/qianlong-liu/>Qianlong Liu</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/h/huaixiao-tou/>Huaixiao Tou</a>
|
<a href=/people/t/ting-chen/>Ting Chen</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-fai Wong</a>
|
<a href=/people/x/xiang-dai/>Xiangying Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2033><div class="card-body p-3 small">In this paper, we make a move to build a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> for automatic diagnosis. We first build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected from an online medical forum by extracting symptoms from both patients&#8217; self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a> for patients automatically, which can converse with patients to collect additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> beyond their self-reports. Experimental results on our dataset show that additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> automatically and make a better <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2034/>Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce<span class=acl-fixed-case>E</span>-commerce</a></strong><br><a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/l/liu-yang/>Liu Yang</a>
|
<a href=/people/f/feng-ji/>Feng Ji</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a>
|
<a href=/people/w/w-bruce-croft/>Bruce Croft</a>
|
<a href=/people/w/wei-lin/>Wei Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2034><div class="card-body p-3 small">Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. After that, we extend our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2035.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2035/>A Multi-task Approach to Learning Multilingual Representations</a></strong><br><a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/d/dogan-can/>Dogan Can</a>
|
<a href=/people/s/shrikanth-narayanan/>Shrikanth Narayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2035><div class="card-body p-3 small">We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in a <a href=https://en.wikipedia.org/wiki/Scenario_analysis>limited resource scenario</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2036/>Characterizing Departures from <a href=https://en.wikipedia.org/wiki/Linearity>Linearity</a> in Word Translation</a></strong><br><a href=/people/n/ndapandula-nakashole/>Ndapa Nakashole</a>
|
<a href=/people/r/raphael-flauger/>Raphael Flauger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2036><div class="card-body p-3 small">We investigate the behavior of <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> learned by <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a>. The <a href=https://en.wikipedia.org/wiki/Map>maps</a> translate words by projecting between word embedding spaces of different languages. We locally approximate these <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linear methods</a>, and to drive the design of more accurate <a href=https://en.wikipedia.org/wiki/Map>maps</a> for <a href=https://en.wikipedia.org/wiki/Translation>word translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2038 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2038.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2038/>Hybrid semi-Markov CRF for Neural Sequence Labeling<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span> for Neural Sequence Labeling</a></strong><br><a href=/people/z/zhixiu-ye/>Zhixiu Ye</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2038><div class="card-body p-3 small">This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for neural sequence labeling in natural language processing. Based on conventional conditional random fields (CRFs), SCRFs have been designed for the tasks of assigning labels to segments by extracting <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from and describing transitions between segments instead of words. In this paper, we improve the existing SCRF methods by employing word-level and segment-level information simultaneously. First, word-level labels are utilized to derive the segment scores in SCRFs. Second, a CRF output layer and an SCRF output layer are integrated into a unified neural network and trained jointly. Experimental results on CoNLL 2003 named entity recognition (NER) shared task show that our model achieves state-of-the-art performance when no external knowledge is used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2042.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2042/>Paper Abstract Writing through Editing Mechanism</a></strong><br><a href=/people/q/qingyun-wang/>Qingyun Wang</a>
|
<a href=/people/z/zhihao-zhou/>Zhihao Zhou</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/s/spencer-whitehead/>Spencer Whitehead</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2042><div class="card-body p-3 small">We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of <a href=https://en.wikipedia.org/wiki/Turing_test>Turing tests</a>, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes <a href=https://en.wikipedia.org/wiki/Turing_test>Turing tests</a> by junior domain experts at a rate up to 30 % and by non-expert at a rate up to 80 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2043/>Conditional Generators of Words Definitions</a></strong><br><a href=/people/a/artyom-gadetsky/>Artyom Gadetsky</a>
|
<a href=/people/i/ilya-yakubovskiy/>Ilya Yakubovskiy</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2043><div class="card-body p-3 small">We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words&#8217; ambiguity and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> leads to performance improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2045/>Narrative Modeling with Memory Chains and Semantic Supervision</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2045><div class="card-body p-3 small">Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> demonstrates superior performance to a collection of competitive baselines, setting a new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2046.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2046" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2046/>Injecting <a href=https://en.wikipedia.org/wiki/Relational_structure>Relational Structural Representation</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Question Similarity</a></strong><br><a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2046><div class="card-body p-3 small">Effectively using full syntactic parsing information in Neural Networks (NNs) for solving <a href=https://en.wikipedia.org/wiki/Relational_model>relational tasks</a>, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2047/>A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</a></strong><br><a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/c/changming-xu/>Changming Xu</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2047><div class="card-body p-3 small">We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> yields +0.4 1.5 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvements over the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2048.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2048/>Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation</a></strong><br><a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2048><div class="card-body p-3 small">Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs ; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2050.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2050/>Extreme Adaptation for Personalized Neural Machine Translation</a></strong><br><a href=/people/p/paul-michel/>Paul Michel</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2050><div class="card-body p-3 small">Every person speaks or writes their own flavor of their native language, influenced by a number of factors : the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the <a href=https://en.wikipedia.org/wiki/System>system</a> should perform <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2052/>Learning from Chunk-based Feedback in Neural Machine Translation</a></strong><br><a href=/people/p/pavel-petrushkov/>Pavel Petrushkov</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2052><div class="card-body p-3 small">We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61 % BLEU absolute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2053/>Bag-of-Words as Target for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2053><div class="card-body p-3 small">A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a>, it is possible to distinguish the correct translations from the incorrect ones by the <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a>. In this paper, we propose an approach that uses both the sentences and the <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a> as targets in the training stage, in order to encourage the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to generate the potentially correct sentences that are not appeared in the training set. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a Chinese-English translation dataset, and experiments show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the strong baselines by the BLEU score of 4.55.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2054.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2054.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2054/>Improving Beam Search by Removing Monotonic Constraint for Neural Machine Translation</a></strong><br><a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2054><div class="card-body p-3 small">To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences. The <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> finds good candidate translations by considering multiple hypotheses of translations simultaneously. However, as the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> produces hypotheses in a monotonic left-to-right order, a hypothesis can not be revisited once it is discarded. We found such <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonicity</a> forces the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to sacrifice some good decoding paths. To mitigate this problem, we relax the <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonic constraint</a> of the beam search by maintaining all found hypotheses in a single <a href=https://en.wikipedia.org/wiki/Priority_queue>priority queue</a> and using a universal score function for hypothesis selection. The proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> allows discarded hypotheses to be recovered in a later step. Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the <a href=https://en.wikipedia.org/wiki/Translation>translations</a> even for high-performance models in English-Japanese translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2055.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803899 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2055/>Leveraging <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations<span class=acl-fixed-case>E</span>nglish verb-noun combinations</a></strong><br><a href=/people/m/milton-king/>Milton King</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2055><div class="card-body p-3 small">Verb-noun combinations (VNCs)-e.g., blow the whistle, hit the roof, and see stars-are a common type of <a href=https://en.wikipedia.org/wiki/List_of_English_idioms>English idiom</a> that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>. Our results show that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts. Idiomatic usages of <a href=https://en.wikipedia.org/wiki/Virtual_Network_Computing>VNCs</a> are known to exhibit lexico-syntactic fixedness. We further incorporate this information into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, demonstrating that this rich linguistic knowledge is complementary to the information carried by <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2056.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803915 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2056/>Using pseudo-senses for improving the extraction of synonyms from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a></a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2056><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> proposed recently for specializing word embeddings according to a particular perspective generally rely on <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this article, we propose Pseudofit, a new method for specializing word embeddings according to <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for each word and uses these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2057.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803929 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2057" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2057/>Hearst Patterns Revisited : Automatic Hypernym Detection from Large Text Corpora</a></strong><br><a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/maximilian-nickel/>Maximilian Nickel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2057><div class="card-body p-3 small">Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms : pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a> on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803960 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2059/>Sparse and Constrained Attention for Neural Machine Translation</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/p/pedro-ferreira/>Pedro Ferreira</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2059><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, words are sometimes dropped from the source or generated repeatedly in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We explore novel <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2060.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803972 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2060/>Neural Hidden Markov Model for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>M</span>arkov Model for Machine Translation</a></strong><br><a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/d/derui-zhu/>Derui Zhu</a>
|
<a href=/people/t/tamer-alkhouli/>Tamer Alkhouli</a>
|
<a href=/people/z/zixuan-gan/>Zixuan Gan</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2060><div class="card-body p-3 small">Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates <a href=https://en.wikipedia.org/wiki/Attention>NMT</a> while replacing the <a href=https://en.wikipedia.org/wiki/Attention>attention component</a>. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the <a href=https://en.wikipedia.org/wiki/Forward&#8211;backward_algorithm>forward-backward algorithm</a>. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 GermanEnglish and ChineseEnglish translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2062.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804010 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2062/>Orthographic Features for Bilingual Lexicon Induction</a></strong><br><a href=/people/p/parker-riley/>Parker Riley</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2062><div class="card-body p-3 small">Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a>, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804022 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2063/>Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking</a></strong><br><a href=/people/g/gourab-kundu/>Gourab Kundu</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2063><div class="card-body p-3 small">We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and language independent features. We perform both intrinsic and extrinsic evaluations of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In the intrinsic evaluation, we show that our model, when trained on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and tested on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, achieves competitive results to the models trained directly on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese and Spanish test sets</a> than the top 2015 TAC system without using any annotated data from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> or Spanish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804053 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2065/>Neural Open Information Extraction</a></strong><br><a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2065><div class="card-body p-3 small">Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>, yet they face problems of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, while maintaining comparable computational efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804071 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2066/>Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</a></strong><br><a href=/people/y/yue-zhao/>Yue Zhao</a>
|
<a href=/people/x/xiaolong-jin/>Xiaolong Jin</a>
|
<a href=/people/y/yuanzhuo-wang/>Yuanzhuo Wang</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2066><div class="card-body p-3 small">Document-level information is very important for event detection even at <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804096 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2068/>Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention</a></strong><br><a href=/people/l/lin-zhao/>Lin Zhao</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2068><div class="card-body p-3 small">We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> into a spoken language understanding system and achieve the state-of-the-art performance on the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2070.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2070/>Modeling discourse cohesion for discourse parsing via memory network</a></strong><br><a href=/people/y/yanyan-jia/>Yanyan Jia</a>
|
<a href=/people/y/yuan-ye/>Yuan Ye</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2070><div class="card-body p-3 small">Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2071.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804155 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2071/>SciDTB : Discourse Dependency TreeBank for Scientific Abstracts<span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>DTB</span>: Discourse Dependency <span class=acl-fixed-case>T</span>ree<span class=acl-fixed-case>B</span>ank for Scientific Abstracts</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2071><div class="card-body p-3 small">Annotation corpus for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> benefits NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In this paper, we present SciDTB, a domain-specific discourse treebank annotated on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a>. Different from widely-used RST-DT and <a href=https://en.wikipedia.org/wiki/PDTB>PDTB</a>, SciDTB uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about SciDTB. Furthermore, our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2075 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804205 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2075/>Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2075><div class="card-body p-3 small">Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser&#8217;s transition system. We explore using a <a href=https://en.wikipedia.org/wiki/Policy_gradient_method>policy gradient method</a> as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> by allowing exploration during training ; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle</a> which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2076.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804218 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2076/>Linear-time Constituency Parsing with <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>Dynamic Programming</a><span class=acl-fixed-case>RNN</span>s and Dynamic Programming</a></strong><br><a href=/people/j/juneki-hong/>Juneki Hong</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2076><div class="card-body p-3 small">Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model spans. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a <a href=https://en.wikipedia.org/wiki/Chart_parser>chart parser</a> running in cubic time, O(n^3), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> using graph-structured stack and beam search, which runs in time O(n b^2) where b is the beam size. We further speed this up to O(n b log b) by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and orders of magnitude faster for discourse parsing, and achieves the highest <a href=https://en.wikipedia.org/wiki/F-number>F1 accuracy</a> on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> among single model end-to-end systems.<tex-math>O(n^3)</tex-math>, which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time <tex-math>O(n b^2)</tex-math> where <tex-math>b</tex-math> is the beam size. We further speed this up to <tex-math>O(n b log b)</tex-math> by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804230 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2077" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2077/>Simpler but More Accurate Semantic Dependency Parsing</a></strong><br><a href=/people/t/timothy-dozat/>Timothy Dozat</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2077><div class="card-body p-3 small">While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations. We extend the LSTM-based syntactic parser of Dozat and Manning (2017) to train on and generate these graph structures. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> on its own achieves state-of-the-art performance, beating the previous, substantially more complex state-of-the-art system by 0.6 % labeled F1. Adding linguistically richer input representations pushes the margin even higher, allowing us to beat it by 1.9 % labeled F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2079.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2079/>Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2079><div class="card-body p-3 small">As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task : automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by a large margin. The dataset and code are available at<url>https://github.com/lancopku/AAPR</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2080.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2080/>Automated essay scoring with string kernels and word embeddings</a></strong><br><a href=/people/m/madalina-cozma/>Mădălina Cozma</a>
|
<a href=/people/a/andrei-butnaru/>Andrei Butnaru</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2080><div class="card-body p-3 small">In this work, we present an approach based on combining <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply <a href=https://en.wikipedia.org/wiki/String_(computer_science)>string kernels</a> to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2081/>Party Matters : Enhancing Legislative Embeddings with Author Attributes for Vote Prediction</a></strong><br><a href=/people/a/anastassia-kornilova/>Anastassia Kornilova</a>
|
<a href=/people/d/daniel-argyle/>Daniel Argyle</a>
|
<a href=/people/v/vladimir-eidelman/>Vladimir Eidelman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2081><div class="card-body p-3 small">Predicting how Congressional legislators will vote is important for understanding their past and future behavior. However, previous work on roll-call prediction has been limited to single session settings, thus not allowing for generalization across sessions. In this paper, we show that text alone is insufficient for modeling voting outcomes in new contexts, as session changes lead to changes in the underlying data generation process. We propose a novel neural method for encoding documents alongside additional <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, achieving an average of a 4 % boost in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2082.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2082/>Dynamic and Static Topic Model for Analyzing Time-Series Document Collections</a></strong><br><a href=/people/r/rem-hida/>Rem Hida</a>
|
<a href=/people/n/naoya-takeishi/>Naoya Takeishi</a>
|
<a href=/people/t/takehisa-yairi/>Takehisa Yairi</a>
|
<a href=/people/k/koichi-hori/>Koichi Hori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2082><div class="card-body p-3 small">For extracting meaningful topics from texts, their structures should be considered properly. In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time. To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time. We show the results of experiments on collections of scientific papers, in which the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperformed conventional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Moreover, we show an example of extracted topic structures, which we found helpful for analyzing research activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2083.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2083/>PhraseCTM : Correlated Topic Modeling on Phrases within <a href=https://en.wikipedia.org/wiki/Markov_random_field>Markov Random Fields</a><span class=acl-fixed-case>P</span>hrase<span class=acl-fixed-case>CTM</span>: Correlated Topic Modeling on Phrases within <span class=acl-fixed-case>M</span>arkov Random Fields</a></strong><br><a href=/people/w/weijing-huang/>Weijing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2083><div class="card-body p-3 small">Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are lack of the ability to capture the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation structure</a> among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within <a href=https://en.wikipedia.org/wiki/Markov_random_field>Markov Random Fields</a> when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2085.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2085.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2085/>Learning with Structured Representations for Negation Scope Extraction</a></strong><br><a href=/people/h/hao-li/>Hao Li</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2085><div class="card-body p-3 small">We report an empirical study on the task of negation scope extraction given the negation cue. Our key observation is that certain useful information such as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> related to negation cue, long-distance dependencies as well as some latent structural information can be exploited for such a task. We design approaches based on conditional random fields (CRF), semi-Markov CRF, as well as latent-variable CRF models to capture such information. Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2086.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2086/>End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2086><div class="card-body p-3 small">This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair (P, H), DEISTE consists of : (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations ; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets 5 % improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2088.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2088.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2088/>A Rank-Based Similarity Metric for Word Embeddings</a></strong><br><a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/h/hongmin-wang/>Hongmin Wang</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2088><div class="card-body p-3 small">Word Embeddings have recently imposed themselves as a standard for representing word meaning in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with <a href=https://en.wikipedia.org/wiki/Trigonometric_functions>vector cosine</a> being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of <a href=https://en.wikipedia.org/wiki/Outlier_detection>outlier detection</a>, thus suggesting that rank-based measures can improve clustering quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2089.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2089/>Addressing Noise in Multidialectal Word Embeddings</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2089><div class="card-body p-3 small">Word embeddings are crucial to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>. The quality of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> relies on large non-noisy corpora. Arabic dialects lack <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>large corpora</a> and are noisy, being linguistically disparate with no <a href=https://en.wikipedia.org/wiki/Standard_language>standardized spelling</a>. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions&#8217; effects on low and high frequency words. In addition to boosting performance between 2-53 %, we specifically improve on noisy, low frequency forms without compromising <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on high frequency forms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2090.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2090/>GNEG : Graph-Based Negative Sampling for word2vec<span class=acl-fixed-case>GNEG</span>: Graph-Based Negative Sampling for word2vec</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2090><div class="card-body p-3 small">Negative sampling is an important component in <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as <a href=https://en.wikipedia.org/wiki/Random_walk>random walk</a>. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5 % and improves the performance on word similarity tasks by about 1 % compared to the skip-gram negative sampling baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2091.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2091/>Unsupervised Learning of Style-sensitive Word Vectors</a></strong><br><a href=/people/r/reina-akama/>Reina Akama</a>
|
<a href=/people/k/kento-watanabe/>Kento Watanabe</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2091><div class="card-body p-3 small">This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to predict lexical stylistic similarity and to create a benchmark dataset for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our experiment with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> supports our assumption and demonstrates that the proposed <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> contribute to the acquisition of style-sensitive word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2094.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2094/>Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction<span class=acl-fixed-case>CNN</span>-based Sequence Labeling for Aspect Extraction</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2094><div class="card-body p-3 small">One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> : general-purpose embeddings and domain-specific embeddings. Without using any additional <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and achieve very good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2095.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2095/>Will it Blend? Blending Weak and Strong Labeled Data in a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> for Argumentation Mining</a></strong><br><a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/c/carlos-alzate/>Carlos Alzate</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/m/martin-gleize/>Martin Gleize</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2095><div class="card-body p-3 small">The process of obtaining high quality <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding tasks</a> is often slow, error-prone, complicated and expensive. With the vast usage of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results. We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to blend high quality but scarce strong labeled data with noisy but abundant weak labeled data during the training of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme. In addition, we provide a manually annotated data set for the task of topic-dependent evidence detection. We believe that blending weak and strong labeled data is a general notion that may be applicable to many language understanding tasks, and can especially assist researchers who wish to train a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> but have a small amount of high quality <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for their task of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2096.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2096/>Investigating Audio, Video, and Text Fusion Methods for End-to-End Automatic Personality Prediction</a></strong><br><a href=/people/o/onno-kampman/>Onno Kampman</a>
|
<a href=/people/e/elham-j-barezi/>Elham J. Barezi</a>
|
<a href=/people/d/dario-bertero/>Dario Bertero</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2096><div class="card-body p-3 small">We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data. For each <a href=https://en.wikipedia.org/wiki/Communication_channel>channel</a>, stacked Convolutional Neural Networks are employed. The <a href=https://en.wikipedia.org/wiki/Communication_channel>channels</a> are fused both on <a href=https://en.wikipedia.org/wiki/Decision-making>decision-level</a> and by concatenating their respective fully connected layers. It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4 % over the best individual modality (video). Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models. Furthermore, we can see the prediction relevance of each <a href=https://en.wikipedia.org/wiki/Locus_(genetics)>modality</a> for each trait. The described <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can be used to increase the <a href=https://en.wikipedia.org/wiki/Emotional_intelligence>emotional intelligence</a> of <a href=https://en.wikipedia.org/wiki/Virtual_agent>virtual agents</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2098.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2098/>Parser Training with Heterogeneous Treebanks</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2098><div class="card-body p-3 small">How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We go on to propose a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on treebank embeddings. We perform experiments for several languages and show that in many cases <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and treebank embeddings lead to substantial improvements over single treebanks or <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>, with average gains of 2.03.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2099.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2099/>Generalized chart constraints for efficient PCFG and TAG parsing<span class=acl-fixed-case>PCFG</span> and <span class=acl-fixed-case>TAG</span> parsing</a></strong><br><a href=/people/s/stefan-grunewald/>Stefan Grünewald</a>
|
<a href=/people/s/sophie-henning/>Sophie Henning</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2099><div class="card-body p-3 small">Chart constraints, which specify at which string positions a constituent may begin or end, have been shown to speed up chart parsers for PCFGs. We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision. Our constraints accelerate both PCFG and TAG parsing, and combine effectively with other pruning techniques (coarse-to-fine and supertagging) for an overall speedup of two orders of magnitude, while improving accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2100.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805830 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2100/>Exploring Semantic Properties of Sentence Embeddings</a></strong><br><a href=/people/x/xunjie-zhu/>Xunjie Zhu</a>
|
<a href=/people/t/tingfeng-li/>Tingfeng Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2100><div class="card-body p-3 small">Neural vector representations are ubiquitous throughout all subfields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> or <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a given sentence affect the similarities obtained between their sentence embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2103.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805874 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2103/>Breaking NLI Systems with Sentences that Require Simple <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Inferences</a><span class=acl-fixed-case>NLI</span> Systems with Sentences that Require Simple Lexical Inferences</a></strong><br><a href=/people/m/max-glockner/>Max Glockner</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2103><div class="card-body p-3 small">We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> is substantially worse across <a href=https://en.wikipedia.org/wiki/System>systems</a> trained on SNLI, demonstrating that these <a href=https://en.wikipedia.org/wiki/System>systems</a> are limited in their generalization ability, failing to capture many simple inferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805925 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2106/>Polyglot Semantic Role Labeling</a></strong><br><a href=/people/p/phoebe-mulcaire/>Phoebe Mulcaire</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2106><div class="card-body p-3 small">Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on several languages over a monolingual baseline. Analysis of the polyglot models&#8217; performance provides a new understanding of the similarities and differences between languages in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2111 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2111.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806005 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2111/>Personalized Language Model for Query Auto-Completion</a></strong><br><a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2111><div class="card-body p-3 small">Query auto-completion is a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine feature</a> whereby the <a href=https://en.wikipedia.org/wiki/System>system</a> suggests completed queries as the user types. Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions. We show how an adaptable language model can be used to generate personalized completions and how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can use online updating to make predictions for users not seen during training. The personalized predictions are significantly better than a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that uses no <a href=https://en.wikipedia.org/wiki/User_(computing)>user information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2112.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2112/>Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2112><div class="card-body p-3 small">In this paper, we focus on the problem of building <a href=https://en.wikipedia.org/wiki/Assistive_technology>assistive systems</a> that can help users to write reviews. We cast this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the <a href=https://en.wikipedia.org/wiki/System>system</a>. We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple <a href=https://en.wikipedia.org/wiki/Code>encoders</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> successfully learns <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> capable of generating coherent and diverse reviews. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2113.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2113.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806034 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2113/>Learning Simplifications for Specific Target Audiences</a></strong><br><a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2113><div class="card-body p-3 small">Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. Different from MT, TS data comprises more elaborate <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a>, such as sentence splitting. It can also contain multiple <a href=https://en.wikipedia.org/wiki/Simplification>simplifications</a> of the same original text targeting different audiences, such as <a href=https://en.wikipedia.org/wiki/Educational_stage>school grade levels</a>. We explore these two features of TS to build <a href=https://en.wikipedia.org/wiki/Physical_model>models</a> tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2114 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2114.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2114.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806055 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2114/>Split and Rephrase : Better Evaluation and Stronger Baselines</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2114><div class="card-body p-3 small">Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89 % of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 8.68 BLEU and fostering further progress on the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806074 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2115/>Autoencoder as Assistant Supervisor : Improving Text Representation for Chinese Social Media Text Summarization<span class=acl-fixed-case>C</span>hinese Social Media Text Summarization</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2115><div class="card-body p-3 small">Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a popular Chinese social media dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performances on the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2117.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2117.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806108 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2117/>On the Practical Computational Power of Finite Precision RNNs for <a href=https://en.wikipedia.org/wiki/Language_recognition>Language Recognition</a><span class=acl-fixed-case>RNN</span>s for Language Recognition</a></strong><br><a href=/people/g/gail-weiss/>Gail Weiss</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/e/eran-yahav/>Eran Yahav</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2117><div class="card-body p-3 small">While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>finite precision</a> whose <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> is linear in the input length. Under these limitations, we show that different RNN variants have different <a href=https://en.wikipedia.org/wiki/Computational_power>computational power</a>. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> and <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>ReLU-RNNs</a> can easily implement counting behavior. We show empirically that the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> does indeed learn to effectively use the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>counting mechanism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2121 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2121.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2121.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806152 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2121/>Pretraining Sentiment Classifiers with Unlabeled Dialog Data</a></strong><br><a href=/people/t/tohru-shimizu/>Toru Shimizu</a>
|
<a href=/people/n/nobuyuki-shimizu/>Nobuyuki Shimizu</a>
|
<a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2121><div class="card-body p-3 small">The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification. Recent studies showed that pretraining with unlabeled data via a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can improve the performance of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a>. In this paper, we take the concept a step further by using a conditional language model, instead of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Specifically, we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2122.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806165 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2122/>Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection</a></strong><br><a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/c/chiao-chen-chen/>Chiao-Chen Chen</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2122><div class="card-body p-3 small">The reliability of self-labeled data is an important issue when the <a href=https://en.wikipedia.org/wiki/Data>data</a> are regarded as ground-truth for training and testing <a href=https://en.wikipedia.org/wiki/Machine_learning>learning-based models</a>. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> that are widely used to collect the training data for irony detection. Furthermore, we apply our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on all data.</div></div></div><hr><div id=p18-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P18-3/>Proceedings of ACL 2018, Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2018, Student Research Workshop</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/j/jeniya-tabassum/>Jeniya Tabassum</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3006/>Recognizing Complex Entity Mentions : A Review and Future Directions</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3006><div class="card-body p-3 small">Standard named entity recognizers can effectively recognize entity mentions that consist of contiguous tokens and do not overlap with each other. However, in practice, there are many domains, such as the biomedical domain, in which there are nested, overlapping, and discontinuous entity mentions. These complex mentions can not be directly recognized by conventional sequence tagging models because they may break the assumptions based on which sequence tagging techniques are built. We review the existing methods which are revised to tackle complex entity mentions and categorize them as tokenlevel and sentence-level approaches. We then identify the research gap, and discuss some directions that we are exploring.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3007/>Automatic Detection of Cross-Disciplinary Knowledge Associations</a></strong><br><a href=/people/m/menasha-thilakaratne/>Menasha Thilakaratne</a>
|
<a href=/people/k/katrina-falkner/>Katrina Falkner</a>
|
<a href=/people/t/thushari-atapattu/>Thushari Atapattu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3007><div class="card-body p-3 small">Detecting interesting, cross-disciplinary knowledge associations hidden in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a> can greatly assist scientists to formulate and validate scientifically sensible novel research hypotheses. This will also introduce new areas of research that can be successfully linked with their research discipline. Currently, this process is mostly performed manually by exploring the <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>, requiring a substantial amount of time and effort. Due to the exponential growth of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>, it has become almost impossible for a scientist to keep track of all research advances. As a result, scientists tend to deal with fragments of the literature according to their specialisation. Consequently, important and hidden associations among these fragmented knowledge that can be linked to produce significant scientific discoveries remain unnoticed. This doctoral work aims to develop a novel knowledge discovery approach that suggests most promising research pathways by analysing the existing <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3008/>Language Identification and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> in Hinglish Code Mixed Tweets<span class=acl-fixed-case>H</span>inglish Code Mixed Tweets</a></strong><br><a href=/people/k/kushagra-singh/>Kushagra Singh</a>
|
<a href=/people/i/indira-sen/>Indira Sen</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3008><div class="card-body p-3 small">While growing code-mixed content on Online Social Networks(OSN) provides a fertile ground for studying various aspects of <a href=https://en.wikipedia.org/wiki/Code-mixing>code-mixing</a>, the lack of automated text analysis tools render such studies challenging. To meet this challenge, a family of tools for analyzing code-mixed data such as language identifiers, parts-of-speech (POS) taggers, chunkers have been developed. Named Entity Recognition (NER) is an important text analysis task which is not only informative by itself, but is also needed for downstream NLP tasks such as <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>. In this work, we present an exploration of automatic NER of code-mixed data. We compare our method with existing off-the-shelf NER tools for <a href=https://en.wikipedia.org/wiki/Social_media>social media content</a>, and find that our systems outperforms the best <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 33.18 % (F1 score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3010/>SuperNMT : <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Semantic Supersenses and Syntactic Supertags<span class=acl-fixed-case>S</span>uper<span class=acl-fixed-case>NMT</span>: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags</a></strong><br><a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3010><div class="card-body p-3 small">In this paper we incorporate semantic supersensetags and syntactic supertag features into ENFR and ENDE factored NMT systems. In experiments on various test sets, we observe that such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3012/>Biomedical Document Retrieval for Clinical Decision Support System</a></strong><br><a href=/people/j/jainisha-sankhavara/>Jainisha Sankhavara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3012><div class="card-body p-3 small">The availability of huge amount of biomedical literature have opened up new possibilities to apply <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for mining documents from them. In this work, we are focusing on biomedical document retrieval from literature for <a href=https://en.wikipedia.org/wiki/Clinical_decision_support_system>clinical decision support systems</a>. We compare statistical and NLP based approaches of query reformulation for biomedical document retrieval. Also, we have modeled the biomedical document retrieval as a learning to rank problem. We report initial results for statistical and NLP based query reformulation approaches and learning to rank approach with future direction of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3013/>A Computational Approach to <a href=https://en.wikipedia.org/wiki/Feature_extraction>Feature Extraction</a> for Identification of Suicidal Ideation in Tweets</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/p/prachi-manchanda/>Prachi Manchanda</a>
|
<a href=/people/r/raj-singh/>Raj Singh</a>
|
<a href=/people/s/swati-aggarwal/>Swati Aggarwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3013><div class="card-body p-3 small">Technological advancements in the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>World Wide Web</a> and <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> in particular coupled with an increase in social media usage has led to a positive correlation between the exhibition of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>Suicidal ideation</a> on websites such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and cases of suicide. This paper proposes a novel supervised approach for detecting <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a> in content on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. A set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> is proposed for training both linear and ensemble classifiers over a dataset of manually annotated tweets. The performance of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is compared against four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> that utilize varying approaches to validate its utility. The results are finally summarized by reflecting on the effect of the inclusion of the proposed features one by one for suicidal ideation detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3014/>BCSAT : A Benchmark Corpus for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> Using Word-level Annotations<span class=acl-fixed-case>BCSAT</span> : A Benchmark Corpus for Sentiment Analysis in <span class=acl-fixed-case>T</span>elugu Using Word-level Annotations</a></strong><br><a href=/people/s/sreekavitha-parupalli/>Sreekavitha Parupalli</a>
|
<a href=/people/v/vijjini-anvesh-rao/>Vijjini Anvesh Rao</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3014><div class="card-body p-3 small">The presented work aims at generating a systematically annotated corpus that can support the enhancement of sentiment analysis tasks in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> using word-level sentiment annotations. From OntoSenseNet, we extracted 11,000 <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, 253 <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a>, 8483 verbs and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment annotation</a> is being done by language experts. We discuss the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> followed for the polarity annotations and validate the developed <a href=https://en.wikipedia.org/wiki/Resource>resource</a>. This work aims at developing a benchmark corpus, as an extension to SentiWordNet, and baseline accuracy for a model where lexeme annotations are applied for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment predictions</a>. The fundamental aim of this paper is to validate and study the possibility of utilizing <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>word-level sentiment annotations</a> in the task of automated sentiment identification. Furthermore, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is improved by annotating the bi-grams extracted from the target corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-3015.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-3015/>Reinforced Extractive Summarization with Question-Focused Rewards</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3015><div class="card-body p-3 small">We investigate a new training paradigm for extractive summarization. Traditionally, human abstracts are used to derive goldstandard labels for extraction units. However, the labels are often inaccurate, because <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>human abstracts</a> and source documents can not be easily aligned at the word level. In this paper we convert <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>human abstracts</a> to a set of Cloze-style comprehension questions. System summaries are encouraged to preserve salient source content useful for answering questions and share common words with the <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a>. We use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to explore the space of possible extractive summaries and introduce a question-focused reward function to promote concise, fluent, and informative summaries. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is effective. It surpasses state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> on the standard summarization dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3016 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-3016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-3016/>Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models</a></strong><br><a href=/people/s/satoru-katsumata/>Satoru Katsumata</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3016><div class="card-body p-3 small">Encoder-decoder models typically only employ words that are frequently used in the training corpus because of the computational costs and/or to exclude noisy words. However, this vocabulary set may still include words that interfere with learning in encoder-decoder models. This paper proposes a method for selecting more suitable words for learning encoders by utilizing not only <a href=https://en.wikipedia.org/wiki/Frequency>frequency</a>, but also <a href=https://en.wikipedia.org/wiki/Co-occurrence>co-occurrence information</a>, which we capture using the <a href=https://en.wikipedia.org/wiki/HITS_algorithm>HITS algorithm</a>. The proposed method is applied to two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a>. For Japanese-to-English translation, this method achieved a BLEU score that was 0.56 points more than that of a <a href=https://en.wikipedia.org/wiki/Baseline_(typography)>baseline</a>. It also outperformed the baseline method for English grammatical error correction, with an <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> that was 1.48 points higher.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3020/>Mixed Feelings : Natural Text Generation with Variable, Coexistent Affective Categories</a></strong><br><a href=/people/l/lee-kezar/>Lee Kezar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3020><div class="card-body p-3 small">Conversational agents, having the goal of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, must rely on <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> which can integrate <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> into their responses. Recent projects outline <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> which can produce emotional sentences, but unlike <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>, they tend to be restricted to one affective category out of a few. To my knowledge, none allow for the intentional coexistence of multiple emotions on the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>word or sentence level</a>. Building on prior research which allows for variation in the intensity of a singular emotion, this research proposal outlines an LSTM (Long Short-Term Memory) language model which allows for variation in multiple emotions simultaneously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-3021/>Automatic Spelling Correction for Resource-Scarce Languages using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/p/pravallika-etoori/>Pravallika Etoori</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3021><div class="card-body p-3 small">Spelling correction is a well-known task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. Automatic spelling correction is important for many NLP applications like <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engines</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> etc. Most approaches use parallel data of noisy and correct word mappings from different sources as training data for automatic spelling correction. Indic languages are resource-scarce and do not have such parallel data due to low volume of queries and non-existence of such prior implementations. In this paper, we show how to build an automatic spelling corrector for resource-scarce languages. We propose a sequence-to-sequence deep learning model which trains <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end</a>. We perform experiments on synthetic datasets created for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indic languages</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, by incorporating the spelling mistakes committed at character level. A comparative evaluation shows that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is competitive with the existing <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checking and correction techniques</a> for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indic languages</a>.</div></div></div><hr><div id=p18-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P18-4/>Proceedings of ACL 2018, System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2018, System Demonstrations</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4001/>Platforms for Non-speakers Annotating Names in Any Language</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/c/cash-costello/>Cash Costello</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/james-mayfield/>James Mayfield</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4001><div class="card-body p-3 small">We demonstrate two annotation platforms that allow an English speaker to annotate names for any language without knowing the language. These platforms provided high-quality&#8217; &#8216;silver standard annotations for low-resource language name taggers (Zhang et al., 2017) that achieved state-of-the-art performance on two surprise languages (Oromo and Tigrinya) at LoreHLT20171 and ten languages at TAC-KBP EDL2017 (Ji et al., 2017). We discuss strengths and limitations and compare other methods of creating silver- and gold-standard annotations using <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a>. We will make our tools publicly available for research use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-4002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-4002/>NovelPerspective : Identifying Point of View Characters<span class=acl-fixed-case>N</span>ovel<span class=acl-fixed-case>P</span>erspective: Identifying Point of View Characters</a></strong><br><a href=/people/l/lyndon-white/>Lyndon White</a>
|
<a href=/people/r/roberto-togneri/>Roberto Togneri</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/m/mohammed-bennamoun/>Mohammed Bennamoun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4002><div class="card-body p-3 small">We present NovelPerspective : a tool to allow consumers to subset their <a href=https://en.wikipedia.org/wiki/Digital_literature>digital literature</a>, based on point of view (POV) character. Many novels have multiple main characters each with their own storyline running in parallel. A well-known example is George R. R. Martin&#8217;s novel : A <a href=https://en.wikipedia.org/wiki/Game_of_Thrones>Game of Thrones</a>, and others from that series. Our tool detects the main character that each section is from the POV of, and allows the user to generate a new ebook with only those sections. This gives consumers new options in how they consume their media ; allowing them to pursue the storylines sequentially, or skip chapters about characters they find boring. We present two heuristic-based baselines, and two machine learning based methods for the detection of the main character.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4004/>HarriGT : A Tool for Linking News to Science<span class=acl-fixed-case>H</span>arri<span class=acl-fixed-case>GT</span>: A Tool for Linking News to Science</a></strong><br><a href=/people/j/james-ravenscroft/>James Ravenscroft</a>
|
<a href=/people/a/amanda-clare/>Amanda Clare</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4004><div class="card-body p-3 small">Being able to reliably link <a href=https://en.wikipedia.org/wiki/Science>scientific works</a> to the <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a> that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper / scientific work citation linking. HarriGT retrieves <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a> from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> to surface relevant examples of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a> to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> and <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a> to the user, in order of likelihood. HarriGT is provided as an open source tool ().<url>http://harrigt.xyz</url>).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-4006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-4006/>YEDDA : A Lightweight Collaborative Text Span Annotation Tool<span class=acl-fixed-case>YEDDA</span>: A Lightweight Collaborative Text Span Annotation Tool</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/l/linwei-li/>Linwei Li</a>
|
<a href=/people/x/xingxuan-li/>Xingxuan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4006><div class="card-body p-3 small">In this paper, we introduce Yedda, a lightweight but efficient and comprehensive open-source tool for text span annotation. Yedda provides a systematic solution for text span annotation, ranging from collaborative user annotation to administrator evaluation and analysis. It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels. Yedda also gives intelligent recommendations by learning the up-to-date annotated text. An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> can reduce the annotation time by half compared with existing <a href=https://en.wikipedia.org/wiki/Annotation>annotation tools</a>. And the annotation time can be further compressed by 16.47 % through <a href=https://en.wikipedia.org/wiki/Recommender_system>intelligent recommendation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4007/>NextGen AML : Distributed Deep Learning based Language Technologies to Augment Anti Money Laundering Investigation<span class=acl-fixed-case>N</span>ext<span class=acl-fixed-case>G</span>en <span class=acl-fixed-case>AML</span>: Distributed Deep Learning based Language Technologies to Augment Anti Money Laundering Investigation</a></strong><br><a href=/people/j/jingguang-han/>Jingguang Han</a>
|
<a href=/people/u/utsab-barman/>Utsab Barman</a>
|
<a href=/people/j/jer-hayes/>Jeremiah Hayes</a>
|
<a href=/people/j/jinhua-du/>Jinhua Du</a>
|
<a href=/people/e/edward-burgin/>Edward Burgin</a>
|
<a href=/people/d/dadong-wan/>Dadong Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4007><div class="card-body p-3 small">Most of the current anti money laundering (AML) systems, using handcrafted rules, are heavily reliant on existing structured databases, which are not capable of effectively and efficiently identifying hidden and complex ML activities, especially those with dynamic and time-varying characteristics, resulting in a high percentage of false positives. Therefore, analysts are engaged for further investigation which significantly increases human capital cost and <a href=https://en.wikipedia.org/wiki/Time_complexity>processing time</a>. To alleviate these issues, this paper presents a novel framework for the next generation <a href=https://en.wikipedia.org/wiki/Advanced_Mobile_Location>AML</a> by applying and visualizing deep learning-driven natural language processing (NLP) technologies in a distributed and scalable manner to augment <a href=https://en.wikipedia.org/wiki/Advanced_Mobile_Location>AML monitoring and investigation</a>. The proposed distributed framework performs news and tweet sentiment analysis, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, entity linking and link analysis on different data sources (e.g. news articles and tweets) to provide additional evidence to human investigators for final decision-making. Each NLP module is evaluated on a task-specific data set, and the overall experiments are performed on synthetic and real-world datasets. Feedback from AML practitioners suggests that our <a href=https://en.wikipedia.org/wiki/System>system</a> can reduce approximately 30 % time and cost compared to their previous manual approaches of AML investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4008/>NLP Web Services for Resource-Scarce Languages<span class=acl-fixed-case>NLP</span> Web Services for Resource-Scarce Languages</a></strong><br><a href=/people/m/martin-puttkammer/>Martin Puttkammer</a>
|
<a href=/people/r/roald-eiselen/>Roald Eiselen</a>
|
<a href=/people/j/justin-hocking/>Justin Hocking</a>
|
<a href=/people/f/frederik-koen/>Frederik Koen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4008><div class="card-body p-3 small">In this paper, we present a project where existing text-based core technologies were ported to Java-based web services from various <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a>. These <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> were developed over a period of eight years through various government funded projects for 10 resource-scarce languages spoken in South Africa. We describe the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> and a simple web front-end capable of completing various predefined tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4012/>SANTO : A Web-based Annotation Tool for Ontology-driven Slot Filling<span class=acl-fixed-case>SANTO</span>: A Web-based Annotation Tool for Ontology-driven Slot Filling</a></strong><br><a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/h/hendrik-ter-horst/>Hendrik ter Horst</a>
|
<a href=/people/f/frank-grimm/>Frank Grimm</a>
|
<a href=/people/t/tim-diekmann/>Tim Diekmann</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4012><div class="card-body p-3 small">Supervised machine learning algorithms require training data whose generation for complex relation extraction tasks tends to be difficult. Being optimized for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> at sentence level, many annotation tools lack in facilitating the annotation of relational structures that are widely spread across the text. This leads to non-intuitive and cumbersome visualizations, making the annotation process unnecessarily time-consuming. We propose SANTO, an easy-to-use, domain-adaptive annotation tool specialized for complex slot filling tasks which may involve problems of <a href=https://en.wikipedia.org/wiki/Cardinality>cardinality</a> and referential grounding. The web-based architecture enables fast and clearly structured annotation for multiple users in parallel. Relational structures are formulated as templates following the conceptualization of an underlying <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>. Further, import and export procedures of standard formats enable interoperability with external sources and tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-4013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-4013/>NCRF++ : An Open-source Neural Sequence Labeling Toolkit<span class=acl-fixed-case>NCRF</span>++: An Open-source Neural Sequence Labeling Toolkit</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4013><div class="card-body p-3 small">This paper describes NCRF++, a <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an <a href=https://en.wikipedia.org/wiki/Inference>inference</a> for building the custom model structure through <a href=https://en.wikipedia.org/wiki/Configuration_file>configuration file</a> with flexible neural feature design and utilization. Built on <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a>, the core operations are calculated in batch, making the toolkit efficient with the acceleration of <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.<url>http://pytorch.org/</url>, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4015/>A Web-scale system for scientific knowledge exploration</a></strong><br><a href=/people/z/zhihong-shen/>Zhihong Shen</a>
|
<a href=/people/h/hao-ma/>Hao Ma</a>
|
<a href=/people/k/kuansan-wang/>Kuansan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4015><div class="card-body p-3 small">To enable efficient exploration of Web-scale scientific knowledge, it is necessary to organize <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a> into a hierarchical concept structure. In this work, we present a large-scale system to (1) identify hundreds of thousands of scientific concepts, (2) tag these identified <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> to hundreds of millions of scientific publications by leveraging both text and graph structure, and (3) build a six-level concept hierarchy with a subsumption-based model. The <a href=https://en.wikipedia.org/wiki/System>system</a> builds the most comprehensive cross-domain scientific concept ontology published to date, with more than 200 thousand concepts and over one million relationships.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4017/>The SUMMA Platform : A Scalable Infrastructure for Multi-lingual Multi-media Monitoring<span class=acl-fixed-case>SUMMA</span> Platform: A Scalable Infrastructure for Multi-lingual Multi-media Monitoring</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/renars-liepins/>Renārs Liepins</a>
|
<a href=/people/g/guntis-barzdins/>Guntis Barzdins</a>
|
<a href=/people/d/didzis-gosko/>Didzis Gosko</a>
|
<a href=/people/s/sebastiao-miranda/>Sebastião Miranda</a>
|
<a href=/people/d/david-nogueira/>David Nogueira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4017><div class="card-body p-3 small">The open-source SUMMA Platform is a highly scalable <a href=https://en.wikipedia.org/wiki/Distributed_computing>distributed architecture</a> for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. The Platform offers a fully automated media ingestion pipeline capable of recording live broadcasts, detection and transcription of spoken content, translation of all text (original or transcribed) into English, recognition and linking of Named Entities, topic detection, clustering and cross-lingual multi-document summarization of related media items, and last but not least, extraction and storage of factual claims in these news items. Browser-based graphical user interfaces provide humans with aggregated information as well as structured access to individual news items stored in the Platform&#8217;s database. This paper describes the intended use cases and provides an overview over the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s implementation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4021/>DeepPavlov : Open-Source Library for Dialogue Systems<span class=acl-fixed-case>D</span>eep<span class=acl-fixed-case>P</span>avlov: Open-Source Library for Dialogue Systems</a></strong><br><a href=/people/m/mikhail-burtsev/>Mikhail Burtsev</a>
|
<a href=/people/a/alexander-seliverstov/>Alexander Seliverstov</a>
|
<a href=/people/r/rafael-airapetyan/>Rafael Airapetyan</a>
|
<a href=/people/m/mikhail-arkhipov/>Mikhail Arkhipov</a>
|
<a href=/people/d/dilyara-baymurzina/>Dilyara Baymurzina</a>
|
<a href=/people/n/nickolay-bushkov/>Nickolay Bushkov</a>
|
<a href=/people/o/olga-gureenkova/>Olga Gureenkova</a>
|
<a href=/people/t/taras-khakhulin/>Taras Khakhulin</a>
|
<a href=/people/y/yurii-kuratov/>Yuri Kuratov</a>
|
<a href=/people/d/denis-kuznetsov/>Denis Kuznetsov</a>
|
<a href=/people/a/alexey-litinsky/>Alexey Litinsky</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alexey-lymar/>Alexey Lymar</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/m/maxim-petrov/>Maxim Petrov</a>
|
<a href=/people/v/vadim-polulyakh/>Vadim Polulyakh</a>
|
<a href=/people/l/leonid-pugachev/>Leonid Pugachev</a>
|
<a href=/people/a/alexey-sorokin/>Alexey Sorokin</a>
|
<a href=/people/m/maria-vikhreva/>Maria Vikhreva</a>
|
<a href=/people/m/marat-zaynutdinov/>Marat Zaynutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4021><div class="card-body p-3 small">Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of feature-rich dialogue systems. An open-source library DeepPavlov is tailored for development of <a href=https://en.wikipedia.org/wiki/Intelligent_agent>conversational agents</a>. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approaches</a> to implementation of <a href=https://en.wikipedia.org/wiki/Intelligent_agent>conversational agents</a>. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4023/>A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines</a></strong><br><a href=/people/s/salvatore-romeo/>Salvatore Romeo</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4023><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> have been proving to be excellent tools to deliver state-of-the-art results, when data is scarce and the tackled tasks involve complex semantic inference, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep linguistic processing</a> and traditional structure-based approaches, such as tree kernel methods, are an alternative solution. Community Question Answering is a research area that benefits from deep linguistic analysis to improve the experience of the community of forum users. In this paper, we present a UIMA framework to distribute the computation of cQA tasks over computer clusters such that traditional systems can scale to large datasets and deliver fast processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4024/>Moon IME : Neural-based Chinese Pinyin Aided Input Method with Customizable Association<span class=acl-fixed-case>IME</span>: Neural-based <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>P</span>inyin Aided Input Method with Customizable Association</a></strong><br><a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-4024><div class="card-body p-3 small">Chinese pinyin input method engine (IME) lets user conveniently input Chinese into a computer by typing pinyin through the common keyboard. In addition to offering high conversion quality, modern pinyin IME is supposed to aid user input with extended association function. However, existing solutions for such <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> are roughly based on oversimplified matching algorithms at word-level, whose resulting products provide limited extension associated with user inputs. This work presents the Moon IME, a pinyin IME that integrates the attention-based neural machine translation (NMT) model and Information Retrieval (IR) to offer amusive and customizable association ability. The released IME is implemented on <a href=https://en.wikipedia.org/wiki/Microsoft_Windows>Windows</a> via text services framework.</div></div></div><hr><div id=p18-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P18-5/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-5000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/y/yoav-artzi/>Yoav Artzi</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-5004/>Connecting Language and Vision to Actions</a></strong><br><a href=/people/p/peter-anderson/>Peter Anderson</a>
|
<a href=/people/a/abhishek-das/>Abhishek Das</a>
|
<a href=/people/q/qi-wu/>Qi Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-5004><div class="card-body p-3 small">A long-term goal of AI research is to build intelligent agents that can see the rich visual environment around us, communicate this understanding in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> to humans and other agents, and act in a physical or embodied environment. To this end, recent advances at the intersection of language and vision have made incredible progress from being able to generate natural language descriptions of images / videos, to answering questions about them, to even holding free-form conversations about visual content ! However, while these <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> can passively describe images or answer (a sequence of) questions about them, they can not act in the world (what if I can not answer a question from my current view, or I am asked to move or manipulate something?). Thus, the challenge now is to extend this progress in language and vision to embodied agents that take actions and actively interact with their visual environments. To reduce the entry barrier for new researchers, this tutorial will provide an overview of the growing number of multimodal tasks and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that combine textual and visual understanding. We will comprehensively review existing state-of-the-art approaches to selected tasks such as image captioning, visual question answering (VQA) and visual dialog, presenting the key architectural building blocks (such as co-attention) and novel algorithms (such as cooperative / adversarial games) used to train models for these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-5005/>Beyond Multiword Expressions : Processing Idioms and Metaphors</a></strong><br><a href=/people/v/valia-kordoni/>Valia Kordoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-5005><div class="card-body p-3 small">Idioms and metaphors are characteristic to all areas of human activity and to all types of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. Their processing is a rapidly growing area in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>, since they have become a big challenge for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP systems</a>. Their omnipresence in language has been established in a number of corpus studies and the role they play in <a href=https://en.wikipedia.org/wiki/Reason>human reasoning</a> has also been confirmed in psychological experiments. This makes idioms and metaphors an important research area for computational and cognitive linguistics, and their automatic identification and interpretation indispensable for any semantics-oriented NLP application. This tutorial aims to provide attendees with a clear notion of the linguistic characteristics of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> and metaphors, computational models of idioms and metaphors using state-of-the-art NLP techniques, their relevance for the intersection of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, parsing (syntactic and semantic) and language technology, not necessarily experts in idioms and metaphors, who are interested in tasks that involve or could benefit from considering idioms and metaphors as a pervasive phenomenon in human language and communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-5007/>Deep Reinforcement Learning for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a><span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/w/william-yang-wang/>William Yang Wang</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-5007><div class="card-body p-3 small">Many Natural Language Processing (NLP) tasks (including generation, language grounding, <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, and dialog) can be formulated as deep reinforcement learning (DRL) problems. However, since language is often discrete and the space for all sentences is infinite, there are many challenges for formulating reinforcement learning problems of NLP tasks. In this tutorial, we provide a gentle introduction to the foundation of deep reinforcement learning, as well as some practical DRL solutions in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We describe recent advances in designing <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>deep reinforcement learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, with a special focus on generation, <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Finally, we discuss why they succeed, and when they may fail, aiming at providing some practical advice about deep reinforcement learning for solving real-world NLP problems.</div></div></div><hr><div id=w18-23><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-23.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-23/>Proceedings of the BioNLP 2018 workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2300/>Proceedings of the <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span> 2018 workshop</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2301/>Embedding Transfer for Low-Resource Medical Named Entity Recognition : A Case Study on Patient Mobility</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2301><div class="card-body p-3 small">Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research. We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of free text electronic health records. We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction. As <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> focused on patient functioning are scarce, we explore domain adaptation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for use in a recurrent neural network NER system. We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall. Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2304/>Keyphrases Extraction from User-Generated Contents in Healthcare Domain Using Long Short-Term Memory Networks</a></strong><br><a href=/people/i/ilham-fathy-saputra/>Ilham Fathy Saputra</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a>
|
<a href=/people/a/alfan-farizki-wicaksono/>Alfan Farizki Wicaksono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2304><div class="card-body p-3 small">We propose keyphrases extraction technique to extract important terms from the healthcare user-generated contents. We employ deep learning architecture, i.e. Long Short-Term Memory, and leverage <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, medical concepts from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, and linguistic components as our <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 61.37 % F-1 score. Experimental results indicate that our proposed approach outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline methods</a>, i.e. RAKE and CRF, on the task of extracting keyphrases from Indonesian health forum posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2305/>Identifying Key Sentences for Precision Oncology Using <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-Supervised Learning</a></a></strong><br><a href=/people/j/jurica-seva/>Jurica Ševa</a>
|
<a href=/people/m/martin-wackerbauer/>Martin Wackerbauer</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2305><div class="card-body p-3 small">We present a machine learning pipeline that identifies key sentences in abstracts of oncological articles to aid <a href=https://en.wikipedia.org/wiki/Evidence-based_medicine>evidence-based medicine</a>. This problem is characterized by the lack of gold standard datasets, data imbalance and thematic differences between available silver standard corpora. Additionally, available training and target data differs with regard to their domain (professional summaries vs. sentences in abstracts). This makes <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> inapplicable. We propose the use of two semi-supervised machine learning approaches : To mitigate difficulties arising from heterogeneous data sources, overcome data imbalance and create reliable training data we propose using <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> from positive and unlabelled data (PU Learning). For obtaining a realistic <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>, we propose the use of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> summarised in relevant sentences as unlabelled examples through Self-Training. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 84 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 0.84 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2310/>A Neural Autoencoder Approach for Document Ranking and Query Refinement in Pharmacogenomic Information Retrieval</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/m/mathias-goschl/>Mathias Göschl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2310><div class="card-body p-3 small">In this study, we investigate learning-to-rank and query refinement approaches for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> in the <a href=https://en.wikipedia.org/wiki/Pharmacogenomics>pharmacogenomic domain</a>. The goal is to improve the information retrieval process of biomedical curators, who manually build <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> for <a href=https://en.wikipedia.org/wiki/Personalized_medicine>personalized medicine</a>. We study how to exploit the relationships between <a href=https://en.wikipedia.org/wiki/Gene>genes</a>, <a href=https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism>variants</a>, <a href=https://en.wikipedia.org/wiki/Drug>drugs</a>, <a href=https://en.wikipedia.org/wiki/Disease>diseases</a> and outcomes as features for document ranking and query refinement. For a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approach</a>, we are faced with a small amount of <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> and a large amount of unannotated data. Therefore, we explore ways to use a neural document auto-encoder in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised approach</a>. We show that a combination of established <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature-engineering</a> and a neural auto-encoder model yield promising results in this setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2311/>Biomedical Event Extraction Using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and Dependency Parsing</a></strong><br><a href=/people/j/jari-bjorne/>Jari Björne</a>
|
<a href=/people/t/tapio-salakoski/>Tapio Salakoski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2311><div class="card-body p-3 small">Event and relation extraction are central tasks in <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. Where <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> concerns the detection of semantic connections between pairs of entities, <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the <a href=https://en.wikipedia.org/wiki/Parse_graph>parse graph</a> into this <a href=https://en.wikipedia.org/wiki/Vector_space>linear space</a> using dependency path embeddings. We integrate our <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> into the open source Turku Event Extraction System (TEES) framework. Using this <a href=https://en.wikipedia.org/wiki/System>system</a>, our <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> can be easily applied to a large set of corpora from e.g. the <a href=https://en.wikipedia.org/wiki/BioNLP>BioNLP</a>, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2318/>SingleCite : Towards an improved Single Citation Search in PubMed<span class=acl-fixed-case>S</span>ingle<span class=acl-fixed-case>C</span>ite: Towards an improved Single Citation Search in <span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>M</span>ed</a></strong><br><a href=/people/l/lana-yeganova/>Lana Yeganova</a>
|
<a href=/people/d/donald-c-comeau/>Donald C Comeau</a>
|
<a href=/people/w/won-kim/>Won Kim</a>
|
<a href=/people/w/w-john-wilbur/>W John Wilbur</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2318><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> that is targeted at finding a specific document in databases is called a Single Citation search. Single citation searches are particularly important for scholarly databases, such as <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, because users are frequently searching for a specific publication. In this work we describe SingleCite, a single citation matching system designed to facilitate user&#8217;s search for a specific document. We report on the progress that has been achieved towards building that <a href=https://en.wikipedia.org/wiki/Function_(engineering)>functionality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2319/>A Framework for Developing and Evaluating Word Embeddings of Drug-named Entity</a></strong><br><a href=/people/m/mengnan-zhao/>Mengnan Zhao</a>
|
<a href=/people/a/aaron-j-masino/>Aaron J. Masino</a>
|
<a href=/people/c/christopher-c-yang/>Christopher C. Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2319><div class="card-body p-3 small">We investigate the quality of task specific word embeddings created with relatively small, targeted corpora. We present a comprehensive evaluation framework including both intrinsic and extrinsic evaluation that can be expanded to named entities beyond drug name. Intrinsic evaluation results tell that drug name embeddings created with a domain specific document corpus outperformed the previously published versions that derived from a very large general text corpus. Extrinsic evaluation uses <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> for the task of drug name recognition with Bi-LSTM model and the results demonstrate the advantage of using domain-specific word embeddings as the only input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> for drug name recognition with F1-score achieving 0.91. This work suggests that it may be advantageous to derive domain specific embeddings for certain tasks even when the domain specific corpus is of limited size.</div></div></div><hr><div id=w18-24><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-24.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-24/>Proceedings of the Seventh Named Entities Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2400/>Proceedings of the Seventh Named Entities Workshop</a></strong><br><a href=/people/n/nancy-chen/>Nancy Chen</a>
|
<a href=/people/r/rafael-e-banchs/>Rafael E. Banchs</a>
|
<a href=/people/x/xiangyu-duan/>Xiangyu Duan</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/haizhou-li/>Haizhou Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2401/>Automatic Extraction of Entities and Relation from Legal Documents</a></strong><br><a href=/people/j/judith-jeyafreeda-andrew/>Judith Jeyafreeda Andrew</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2401><div class="card-body p-3 small">In recent years, the journalists and <a href=https://en.wikipedia.org/wiki/Computer_science>computer sciences</a> speak to each other to identify useful technologies which would help them in extracting useful information. This is called <a href=https://en.wikipedia.org/wiki/Computational_journalism>computational Journalism</a>. In this paper, we present a method that will enable the journalists to automatically identifies and annotates <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a> such as names of people, <a href=https://en.wikipedia.org/wiki/Organization>organizations</a>, role and functions of people in <a href=https://en.wikipedia.org/wiki/Legal_instrument>legal documents</a> ; the relationship between these entities are also explored. The <a href=https://en.wikipedia.org/wiki/System>system</a> uses a combination of both statistical and rule based technique. The <a href=https://en.wikipedia.org/wiki/Statistics>statistical method</a> used is <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> and for the rule based technique, document and language specific regular expressions are used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2402/>Connecting Distant Entities with Induction through Conditional Random Fields for Named Entity Recognition : Precursor-Induced CRF<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/w/wangjin-lee/>Wangjin Lee</a>
|
<a href=/people/j/jinwook-choi/>Jinwook Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2402><div class="card-body p-3 small">This paper presents a method of designing specific high-order dependency factor on the linear chain conditional random fields (CRFs) for named entity recognition (NER). Named entities tend to be separated from each other by multiple outside tokens in a text, and thus the first-order CRF, as well as the second-order CRF, may innately lose transition information between distant named entities. The proposed design uses outside label in NER as a transmission medium of precedent entity information on the CRF. Then, empirical results apparently demonstrate that it is possible to exploit long-distance label dependency in the original first-order linear chain CRF structure upon NER while reducing computational loss rather than in the second-order CRF.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2404/>Attention-based Semantic Priming for Slot-filling</a></strong><br><a href=/people/j/jiewen-wu/>Jiewen Wu</a>
|
<a href=/people/r/rafael-e-banchs/>Rafael E. Banchs</a>
|
<a href=/people/l/luis-fernando-dharo/>Luis Fernando D’Haro</a>
|
<a href=/people/p/pavitra-krishnaswamy/>Pavitra Krishnaswamy</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2404><div class="card-body p-3 small">The problem of sequence labelling in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> would benefit from approaches inspired by semantic priming phenomena. We propose that an attention-based RNN architecture can be used to simulate <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a> for sequence labelling. Specifically, we employ pre-trained word embeddings to characterize the semantic relationship between utterances and labels. We validate the approach using varying sizes of the ATIS and MEDIA datasets, and show up to 1.4-1.9 % improvement in F1 score. The developed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can enable more explainable and generalizable spoken language understanding systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2405/>Named Entity Recognition for Hindi-English Code-Mixed Social Media Text<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Text</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2405><div class="card-body p-3 small">Named Entity Recognition (NER) is a major task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, and also is a sub-task of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. The challenge of NER for <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> lie in the insufficient information available in a tweet. There has been a significant amount of work done related to <a href=https://en.wikipedia.org/wiki/Entity_extraction>entity extraction</a>, but only for resource rich languages and domains such as <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it&#8217;s unstructured and incomplete information. We propose experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classification algorithms</a> with word, character and lexical features. The algorithms we experimented with are <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision tree</a>, <a href=https://en.wikipedia.org/wiki/Long-term_memory>Long Short-Term Memory (LSTM)</a>, and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Field (CRF)</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for NER in Hindi-English Code-Mixed along with extensive experiments on our <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> which achieved the best <a href=https://en.wikipedia.org/wiki/F-number>f1-score</a> of 0.95 with both CRF and <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2411/>A Deep Learning Based Approach to <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a></a></strong><br><a href=/people/s/soumyadeep-kundu/>Soumyadeep Kundu</a>
|
<a href=/people/s/sayantan-paul/>Sayantan Paul</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2411><div class="card-body p-3 small">In this paper, we propose different <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for language independent machine transliteration which is extremely important for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) applications</a>. Though a number of statistical models for <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> have already been proposed in the past few decades, we proposed some neural network based deep learning architectures for the <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration of named entities</a>. Our transliteration systems adapt two different neural machine translation (NMT) frameworks : <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> and convolutional sequence to sequence based NMT. It is shown that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides quite satisfactory results when it comes to multi lingual machine transliteration. Our submitted runs are an ensemble of different <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration systems</a> for all the language pairs. In the NEWS 2018 Shared Task on <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a>, our method achieves top performance for the EnPe and PeEn language pairs and comparable results for other cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2412/>Comparison of Assorted Models for <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a></a></strong><br><a href=/people/s/saeed-najafi/>Saeed Najafi</a>
|
<a href=/people/b/bradley-hauer/>Bradley Hauer</a>
|
<a href=/people/r/rashed-rubby-riyadh/>Rashed Rubby Riyadh</a>
|
<a href=/people/l/leyuan-yu/>Leyuan Yu</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2412><div class="card-body p-3 small">We report the results of our experiments in the context of the NEWS 2018 Shared Task on <a href=https://en.wikipedia.org/wiki/Transliteration>Transliteration</a>. We focus on the comparison of several diverse <a href=https://en.wikipedia.org/wiki/System>systems</a>, including three neural MT models. A combination of discriminative, generative, and neural models obtains the best results on the development sets. We also put forward ideas for improving the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2414/>Low-Resource Machine Transliteration Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> of Asian Languages<span class=acl-fixed-case>A</span>sian Languages</a></strong><br><a href=/people/n/ngoc-tan-le/>Ngoc Tan Le</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2414><div class="card-body p-3 small">Grapheme-to-phoneme models are key components in <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech systems</a>. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are based on initial <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network-based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pre-trained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. We participated in the NEWS 2018 shared task for the English-Vietnamese transliteration task.</div></div></div><hr><div id=w18-25><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-25.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-25/>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2500/>Proceedings of Workshop for <span class=acl-fixed-case>NLP</span> Open Source Software (<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>OSS</span>)</a></strong><br><a href=/people/e/eunjeong-l-park/>Eunjeong L. Park</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/d/dmitrijs-milajevs/>Dmitrijs Milajevs</a>
|
<a href=/people/l/liling-tan/>Liling Tan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2501/>AllenNLP : A Deep Semantic Natural Language Processing Platform<span class=acl-fixed-case>A</span>llen<span class=acl-fixed-case>NLP</span>: A Deep Semantic Natural Language Processing Platform</a></strong><br><a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/m/michael-schmitz/>Michael Schmitz</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2501><div class="card-body p-3 small">Modern natural language processing (NLP) research requires writing code. Ideally this <a href=https://en.wikipedia.org/wiki/Source_code>code</a> would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the <a href=https://en.wikipedia.org/wiki/Research>research</a>. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2502.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-2502/>Stop Word Lists in Free Open-source Software Packages</a></strong><br><a href=/people/j/joel-nothman/>Joel Nothman</a>
|
<a href=/people/h/hanmin-qin/>Hanmin Qin</a>
|
<a href=/people/r/roman-yurchak/>Roman Yurchak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2502><div class="card-body p-3 small">Open-source software packages for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. has n&#8217;t but not had n&#8217;t) and inclusions (computer), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2503/>Texar : A Modularized, Versatile, and Extensible Toolbox for Text Generation<span class=acl-fixed-case>T</span>exar: A Modularized, Versatile, and Extensible Toolbox for Text Generation</a></strong><br><a href=/people/z/zhiting-hu/>Zhiting Hu</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/h/haoran-shi/>Haoran Shi</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/d/devendra-singh-chaplot/>Devendra Singh Chaplot</a>
|
<a href=/people/b/bowen-tan/>Bowen Tan</a>
|
<a href=/people/x/xingjiang-yu/>Xingjiang Yu</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2503><div class="card-body p-3 small">We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing <a href=https://en.wikipedia.org/wiki/Toolkit>toolkits</a> that are specialized for specific <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> across different <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text generation applications</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-2504.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-2504/>The ACL Anthology : Current State and Future Directions<span class=acl-fixed-case>ACL</span> <span class=acl-fixed-case>A</span>nthology: Current State and Future Directions</a></strong><br><a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a>
|
<a href=/people/m/martin-villalba/>Martín Villalba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2504><div class="card-body p-3 small">The Association of Computational Linguistic&#8217;s Anthology is the open source archive, and the main source for <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and natural language processing&#8217;s scientific literature. The <a href=https://en.wikipedia.org/wiki/ACL_Anthology>ACL Anthology</a> is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of <a href=https://en.wikipedia.org/wiki/Docker_(software)>Docker images</a> will improve the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>&#8217;s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the <a href=https://en.wikipedia.org/wiki/Anthology>Anthology</a>. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2507/>OpenSeq2Seq : Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models</a></strong><br><a href=/people/o/oleksii-kuchaiev/>Oleksii Kuchaiev</a>
|
<a href=/people/b/boris-ginsburg/>Boris Ginsburg</a>
|
<a href=/people/i/igor-gitman/>Igor Gitman</a>
|
<a href=/people/v/vitaly-lavrukhin/>Vitaly Lavrukhin</a>
|
<a href=/people/c/carl-case/>Carl Case</a>
|
<a href=/people/p/paulius-micikevicius/>Paulius Micikevicius</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2507><div class="card-body p-3 small">We present OpenSeq2Seq an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source toolkit</a> for training sequence-to-sequence models. The main goal of our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is to allow researchers to most effectively explore different sequence-to-sequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a>. We plan to extend <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with other modalities in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2508/>Integrating Multiple NLP Technologies into an Open-source Platform for Multilingual Media Monitoring<span class=acl-fixed-case>NLP</span> Technologies into an Open-source Platform for Multilingual Media Monitoring</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/renars-liepins/>Renārs Liepins</a>
|
<a href=/people/d/didzis-gosko/>Didzis Gosko</a>
|
<a href=/people/g/guntis-barzdins/>Guntis Barzdins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2508><div class="card-body p-3 small">The open-source SUMMA Platform is a highly scalable <a href=https://en.wikipedia.org/wiki/Distributed_computing>distributed architecture</a> for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> with comparatively little effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2509/>The Annotated Transformer</a></strong><br><a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2509><div class="card-body p-3 small">A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for <a href=https://en.wikipedia.org/wiki/Replication_(statistics)>replication</a>, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.</div></div></div><hr><div id=w18-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-26/>Proceedings of the Workshop on Machine Reading for Question Answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2600/>Proceedings of the Workshop on Machine Reading for Question Answering</a></strong><br><a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2601/>Ruminating Reader : Reasoning with Gated Multi-hop Attention</a></strong><br><a href=/people/y/yichen-gong/>Yichen Gong</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2601><div class="card-body p-3 small">To answer the question in machine comprehension (MC) task, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> need to establish the interaction between the question and the context. To tackle the problem that the single-pass model can not reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by 2.1 <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> and 2.7 EM score. Our analysis shows that different hops of the attention have different responsibilities in selecting answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2603/>A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension</a></strong><br><a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/s/seohyun-back/>Seohyun Back</a>
|
<a href=/people/h/haejun-lee/>Haejun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2603><div class="card-body p-3 small">Reading Comprehension (RC) of text is one of the fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In recent years, several end-to-end neural network models have been proposed to solve RC tasks. However, most of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> suffer in reasoning over long documents. In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension. We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA. We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2606" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2606/>Robust and Scalable Differentiable Neural Computer for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a></a></strong><br><a href=/people/j/jorg-franke/>Jörg Franke</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2606><div class="card-body p-3 small">Deep learning models are often not easily adaptable to new <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>tasks</a> and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We analyze the DNC and identify possible improvements within the application of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> with passable results on the CNN RC task without adaptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2610/>Comparative Analysis of Neural QA models on SQuAD<span class=acl-fixed-case>QA</span> models on <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span></a></strong><br><a href=/people/s/soumya-wadhwa/>Soumya Wadhwa</a>
|
<a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2610><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> has gained prominence in the past few decades for testing the ability of machines to understand <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Large datasets for <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a> have led to the development of neural models that cater to deeper language understanding compared to <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval tasks</a>. Different <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2611/>Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task<span class=acl-fixed-case>ROUGE</span> and <span class=acl-fixed-case>BLEU</span> to Better Evaluate Machine Reading Comprehension Task</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2611><div class="card-body p-3 small">Current evaluation metrics to <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, bias may appear when these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to better correlate n-gram overlap with the <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> for answers to these two question types. Statistical analysis proves the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>. Our adaptations may provide positive guidance for the development of real-scene MRC systems.<tex-math>n</tex-math>-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.</div></div></div><hr><div id=w18-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-27/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2700/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2701/>Findings of the Second Workshop on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Minh-Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2701><div class="card-body p-3 small">This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop&#8217;s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2704/>Inducing Grammars with and for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/k/ke-m-tran/>Ke Tran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2704><div class="card-body p-3 small">Machine translation systems require <a href=https://en.wikipedia.org/wiki/Semantics>semantic knowledge</a> and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical understanding</a>. Neural machine translation (NMT) systems often assume this information is captured by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and a decoder that ensures <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Recent work has shown that incorporating explicit <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> alleviates the burden of modeling both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>. However, requiring <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> is expensive and does not explore the question of what syntax a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> needs during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. To address both of these issues we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2705" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2705/>Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2705><div class="card-body p-3 small">Supervised domain adaptationwhere a large generic corpus and a smaller in-domain corpus are both available for trainingis a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, then continue training the second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a> between the in-domain model&#8217;s output word distribution and that of the out-of-domain model to prevent the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> shows improvements over standard continued training by up to 1.5 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2706/>Controllable Abstractive Summarization</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2706><div class="card-body p-3 small">Current models for <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> can produce high quality summaries that follow <a href=https://en.wikipedia.org/wiki/Preference>user preferences</a>. Without user input, we set the <a href=https://en.wikipedia.org/wiki/Control_variable>control variables</a> automatically on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2709/>On the Impact of Various Types of <a href=https://en.wikipedia.org/wiki/Noise>Noise</a> on Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2709><div class="card-body p-3 small">We examine how various types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the parallel training data impact the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a>. We create five types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>artificial noise</a> and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> than <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a>. For one especially egregious type of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> they learn to just copy the input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2711/>Multi-Source Neural Machine Translation with Missing Data</a></strong><br><a href=/people/y/yuta-nishimura/>Yuta Nishimura</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2711><div class="card-body p-3 small">Multi-source translation is an <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> to exploit multiple inputs (e.g. in two different languages) to increase <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a>. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a>, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol NULL. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the NULL tokens achieved higher translation accuracies measured by <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> than those by any one-to-one NMT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2715/>OpenNMT System Description for WNMT 2018 : 800 words / sec on a single-core CPU<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>NMT</span> System Description for <span class=acl-fixed-case>WNMT</span> 2018: 800 words/sec on a single-core <span class=acl-fixed-case>CPU</span></a></strong><br><a href=/people/j/jean-senellart/>Jean Senellart</a>
|
<a href=/people/d/dakun-zhang/>Dakun Zhang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/g/guillaume-klein/>Guillaume Klein</a>
|
<a href=/people/j/jean-pierre-ramatchandirin/>Jean-Pierre Ramatchandirin</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2715><div class="card-body p-3 small">We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a <a href=https://en.wikipedia.org/wiki/Supercomputer>high-performance CPU system</a>. The final system uses a combination of four techniques, all of them lead to significant speed-ups in combination : (a) sequence distillation, (b) architecture modifications, (c) <a href=https://en.wikipedia.org/wiki/Precomputation>precomputation</a>, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and available to the community.</div></div></div><hr><div id=w18-28><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-28.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-28/>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2800/>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</a></strong><br><a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/thierry-poibeau/>Thierry Poibeau</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2801" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2801/>Predicting Brain Activation with WordNet Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Embeddings</a></strong><br><a href=/people/j/joao-rodrigues/>João António Rodrigues</a>
|
<a href=/people/r/ruben-branco/>Ruben Branco</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/c/chakaveh-saedi/>Chakaveh Saedi</a>
|
<a href=/people/a/antonio-branco/>António Branco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2801><div class="card-body p-3 small">The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of <a href=https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging>fMRI spatial patterns</a> was pioneered by Mitchell et al. That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to feature-based models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a <a href=https://en.wikipedia.org/wiki/Semantic_network>semantic network</a> to encode lexical semantics, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2803/>Language Production Dynamics with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/j/jesus-calvillo/>Jesús Calvillo</a>
|
<a href=/people/m/matthew-crocker/>Matthew Crocker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2803><div class="card-body p-3 small">We present an analysis of the internal mechanism of the recurrent neural model of sentence production presented by Calvillo et al. The results show clear patterns of computation related to each layer in the network allowing to infer an algorithmic account, where the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> activates the semantically related words, then each word generated at each time step activates syntactic and semantic constraints on possible continuations, while the <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence</a> preserves information through time. We propose that such insights could generalize to other models with similar architecture, including some used in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and image caption generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2805/>Predicting Japanese Word Order in Double Object Constructions<span class=acl-fixed-case>J</span>apanese Word Order in Double Object Constructions</a></strong><br><a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/s/satoshi-nambu/>Satoshi Nambu</a>
|
<a href=/people/s/shin-ichiro-sano/>Shin-Ichiro Sano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2805><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> to predict Japanese word order in the double object constructions. We employed a Bayesian linear mixed model with manually annotated predicate-argument structure data. The findings from the refined corpus analysis confirmed the effects of information status of an NP as &#8216;givennew ordering&#8217; in addition to the effects of &#8216;long-before-short&#8217; as a tendency of the general Japanese word order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2806/>Affordances in Grounded Language Learning</a></strong><br><a href=/people/s/stephen-mcgregor/>Stephen McGregor</a>
|
<a href=/people/k/kyungtae-lim/>KyungTae Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2806><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> involving mappings between different modes of semantic representation. We propose distributional semantic models as a mechanism for representing the kind of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> inherent in the system of abstract symbols characteristic of a sophisticated community of language users. Then, motivated by insight from <a href=https://en.wikipedia.org/wiki/Ecological_psychology>ecological psychology</a>, we describe a model approximating <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, by which we mean a <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learner</a>&#8217;s direct perception of opportunities for action in an environment. We present a preliminary experiment involving mapping between these two representational modalities, and propose that our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> can become the basis for a cognitively inspired model of grounded language learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2807/>Rating Distributions and <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian Inference</a> : Enhancing Cognitive Models of Spatial Language Use<span class=acl-fixed-case>B</span>ayesian Inference: Enhancing Cognitive Models of Spatial Language Use</a></strong><br><a href=/people/t/thomas-kluth/>Thomas Kluth</a>
|
<a href=/people/h/holger-schultheis/>Holger Schultheis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2807><div class="card-body p-3 small">We present two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that improve the assessment of <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a>. The first method is applicable to models computing average acceptability ratings. For these models, we propose an extension that simulates a full rating distribution (instead of average ratings) and allows generating individual ratings. Our second method enables <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a> for models generating individual data. To this end, we propose to use the cross-match test (Rosenbaum, 2005) as a <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood function</a>. We exemplarily present both methods using <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a> from the domain of spatial language use. For spatial language use, determining linguistic acceptability judgments of a spatial preposition for a depicted spatial relation is assumed to be a crucial process (Logan and Sadler, 1996). Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of this <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a> compute an average acceptability rating. We extend the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and based on existing data show that the extended <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> allow extracting more information from the <a href=https://en.wikipedia.org/wiki/Empirical_evidence>empirical data</a> and yield more readily interpretable information about model successes and failures. Applying <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian inference</a>, we find that model performance relies less on mechanisms of capturing geometrical aspects than on mapping the captured geometry to a rating interval.</div></div></div><hr><div id=w18-29><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-29.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-29/>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2900/>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/georgiana-dinu/>Georgiana Dinu</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a>
|
<a href=/people/s/samuel-bowman/>Sam Bowman</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a>
|
<a href=/people/a/anders-sogaard/>Anders Sogaard</a>
|
<a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2901/>Compositional Morpheme Embeddings with Affixes as Functions and Stems as Arguments</a></strong><br><a href=/people/d/daniel-edmiston/>Daniel Edmiston</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2901><div class="card-body p-3 small">This work introduces a novel, linguistically motivated architecture for composing <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> to derive <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The principal novelty in the work is to treat stems as vectors and <a href=https://en.wikipedia.org/wiki/Affix>affixes</a> as functions over vectors. In this way, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s architecture more closely resembles the compositionality of <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Such a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> stands in opposition to models which treat morphemes uniformly, making no distinction between <a href=https://en.wikipedia.org/wiki/Word_stem>stem</a> and <a href=https://en.wikipedia.org/wiki/Affix>affix</a>. We run this new architecture on a dependency parsing task in Koreana language rich in derivational morphologyand compare it against a lexical baseline, along with other sub-word architectures. StAffNet, the name of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, shows competitive performance with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2902/>Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2902><div class="card-body p-3 small">Incorporating source syntactic information into neural machine translation (NMT) has recently proven successful (Eriguchi et al., 2016 ; Luong et al., 2016). However, this is generally done using an outside <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to syntactically annotate the training data, making this technique difficult to use for languages or domains for which a reliable <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is not available. In this paper, we introduce an unsupervised tree-to-sequence (tree2seq) model for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> ; this model is able to induce an unsupervised hierarchical structure on the source sentence based on the downstream task of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We adapt the Gumbel tree-LSTM of Choi et al. (2018) to NMT in order to create the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> against sequential and supervised parsing baselines on three low- and medium-resource language pairs. For low-resource cases, the unsupervised tree2seq encoder significantly outperforms the baselines ; no improvements are seen for medium-resource translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2905/>Subcharacter Information in Japanese Embeddings : When Is It Worth It?<span class=acl-fixed-case>J</span>apanese Embeddings: When Is It Worth It?</a></strong><br><a href=/people/m/marzena-karpinska/>Marzena Karpinska</a>
|
<a href=/people/b/bofang-li/>Bofang Li</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2905><div class="card-body p-3 small">Languages with logographic writing systems present a difficulty for traditional character-level models. Leveraging the subcharacter information was recently shown to be beneficial for a number of intrinsic and extrinsic tasks in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We examine whether the same strategies could be applied for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, and contribute a new analogy dataset for this <a href=https://en.wikipedia.org/wiki/Language>language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2906/>A neural parser as a direct classifier for head-final languages</a></strong><br><a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/m/masayasu-muraoka/>Masayasu Muraoka</a>
|
<a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2906><div class="card-body p-3 small">This paper demonstrates a neural parser implementation suitable for consistently head-final languages such as <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. Unlike the transition- and graph-based algorithms in most state-of-the-art parsers, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> directly selects the head word of a dependent from a limited number of candidates. This method drastically simplifies the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> so that we can easily interpret the output of the neural model. Moreover, by exploiting grammatical knowledge to restrict possible modification types, we can control the output of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to reduce specific errors without adding annotated corpora. The neural parser performed well both on conventional Japanese corpora and the Japanese version of Universal Dependency corpus, and the advantages of <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> were observed in the comparison with the non-neural conventional model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2907/>Syntactic Dependency Representations in Neural Relation Classification</a></strong><br><a href=/people/f/farhad-nooralahzadeh/>Farhad Nooralahzadeh</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2907><div class="card-body p-3 small">We investigate the use of different syntactic dependency representations in a neural relation classification task and compare the CoNLL, Stanford Basic and Universal Dependencies schemes. We further compare with a syntax-agnostic approach and perform an <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> in order to gain a better understanding of the results.</div></div></div><hr><div id=w18-30><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-30.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-30/>Proceedings of The Third Workshop on Representation Learning for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3000/>Proceedings of The Third Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/j/jamie-kiros/>Jamie Kiros</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/d/dipendra-misra/>Dipendra Misra</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3001/>Corpus Specificity in LSA and <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> : The Role of Out-of-Domain Documents<span class=acl-fixed-case>LSA</span> and Word2vec: The Role of Out-of-Domain Documents</a></strong><br><a href=/people/e/edgar-altszyler/>Edgar Altszyler</a>
|
<a href=/people/m/mariano-sigman/>Mariano Sigman</a>
|
<a href=/people/d/diego-fernandez-slezak/>Diego Fernández Slezak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3001><div class="card-body p-3 small">Despite the popularity of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the precise way by which they acquire semantic relations between words remain unclear. In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However, if <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus size</a> grows in topics which are not specific to the domain of interest, <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>signal to noise ratio</a> may weaken. Here we investigate the effect of corpus specificity and size in <a href=https://en.wikipedia.org/wiki/Word_embedding>word-embeddings</a>, and for this, we study two ways for progressive elimination of documents : the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of <a href=https://en.wikipedia.org/wiki/Dimensionality>dimensionality</a>, can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view, we point out that LSA&#8217;s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> does.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3002/>Hierarchical Convolutional Attention Networks for Text Classification</a></strong><br><a href=/people/s/shang-gao/>Shang Gao</a>
|
<a href=/people/a/arvind-ramanathan/>Arvind Ramanathan</a>
|
<a href=/people/g/georgia-tourassi/>Georgia Tourassi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3002><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has demonstrated that self-attention mechanisms can be used in place of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> to increase <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training speed</a> without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a>. We propose combining this approach with the benefits of convolutional filters and a hierarchical structure to create a document classification model that is both highly accurate and fast to train we name our method Hierarchical Convolutional Attention Networks. We demonstrate the effectiveness of this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> by surpassing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on several classification tasks while being twice as fast to train.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3003/>Extrofitting : Enriching Word Representation and its Vector Space with Semantic Lexicons<span class=acl-fixed-case>E</span>xtrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons</a></strong><br><a href=/people/h/hwiyeol-jo/>Hwiyeol Jo</a>
|
<a href=/people/s/stanley-jungkyu-choi/>Stanley Jungkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3003><div class="card-body p-3 small">We propose post-processing method for enriching not only word representation but also its <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using semantic lexicons, which we call extrofitting. The method consists of 3 steps as follows : (i) Expanding 1 or more <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>dimension(s)</a> on all the word vectors, filling with their representative value. (ii) Transferring semantic knowledge by averaging each representative values of synonyms and filling them in the expanded dimension(s). These two steps make representations of the synonyms close together. (iii) Projecting the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using Linear Discriminant Analysis, which eliminates the expanded dimension(s) with semantic knowledge. When experimenting with GloVe, we find that our method outperforms Faruqui&#8217;s retrofitting on some of word similarity task. We also report further analysis on our method in respect to word vector dimensions, <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> as well as other well-known pretrained word vectors (e.g., Word2Vec, Fasttext).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3005/>Text Completion using Context-Integrated Dependency Parsing</a></strong><br><a href=/people/a/amr-rekaby-salama/>Amr Rekaby Salama</a>
|
<a href=/people/o/ozge-alacam/>Özge Alaçam</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3005><div class="card-body p-3 small">Incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges that a successful <a href=https://en.wikipedia.org/wiki/Communication_system>communication system</a> has to deal with. In this paper, we study text completion with a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> composed of sentences with gaps where a successful completion can not be achieved through a uni-modal (language-based) approach. We present a solution based on a context-integrating dependency parser incorporating an additional non-linguistic modality. An incompleteness in one <a href=https://en.wikipedia.org/wiki/Communication_channel>channel</a> is compensated by information from another one and the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> learns the association between the two modalities from a multiple level knowledge representation. We examined several model variations by adjusting the degree of influence of different modalities in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a> on possible filler words and their exact reference to a non-linguistic context element. Our model is able to fill the gap with 95.4 % word and 95.2 % exact reference accuracy hence the successful prediction can be achieved not only on the word level (such as mug) but also with respect to the correct identification of its context reference (such as mug 2 among several mug instances).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3006/>Quantum-Inspired Complex Word Embedding</a></strong><br><a href=/people/q/qiuchi-li/>Qiuchi Li</a>
|
<a href=/people/s/sagar-uprety/>Sagar Uprety</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a>
|
<a href=/people/d/dawei-song/>Dawei Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3006><div class="card-body p-3 small">A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> will assign high probabilities to the words Penguin and Fly if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense-Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in <a href=https://en.wikipedia.org/wiki/Quantum_mechanics>Quantum Mechanics</a> where we subscribe a relative phase to each word, which is a <a href=https://en.wikipedia.org/wiki/Complex_number>complex number</a>, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve better performances than state-of-the-art non-quantum models on binary sentence classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3007/>Natural Language Inference with Definition Embedding Considering Context On the Fly</a></strong><br><a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/h/hisako-asano/>Hisako Asano</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3007><div class="card-body p-3 small">Natural language inference (NLI) is one of the most important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In this study, we propose a novel method using word dictionaries, which are pairs of a word and its definition, as external knowledge. Our neural definition embedding mechanism encodes input sentences with the definitions of each word of the sentences on the fly. It can encode the definition of words considering the context of input sentences by using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We evaluated our method using <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> as a dictionary and confirmed that our method performed better than baseline models when using the full or a subset of 100d GloVe as word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3008/>Comparison of Representations of Named Entities for Document Classification</a></strong><br><a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3008><div class="card-body p-3 small">We explore representations for multi-word names in text classification tasks, on Reuters (RCV1) topic and sector classification. We find that : the best way to treat <a href=https://en.wikipedia.org/wiki/Name>names</a> is to split them into tokens and use each token as a separate feature ; NEs have more impact on sector classification than topic classification ; replacing NEs with entity types is not an effective strategy ; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our <a href=https://en.wikipedia.org/wiki/Computer_simulation>CNN models</a> yield.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3011/>A Hybrid Learning Scheme for Chinese Word Embedding<span class=acl-fixed-case>C</span>hinese Word Embedding</a></strong><br><a href=/people/w/wenfan-chen/>Wenfan Chen</a>
|
<a href=/people/w/weiguo-sheng/>Weiguo Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3011><div class="card-body p-3 small">To improve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, subword information has been widely employed in state-of-the-art methods. These <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can be classified to either compositional or predictive models. In this paper, we propose a hybrid learning scheme, which integrates compositional and predictive model for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. Such a scheme can take advantage of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, thus effectively learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The proposed scheme has been applied to learn <a href=https://en.wikipedia.org/wiki/Linguistic_description>word representation</a> on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Our results show that the proposed scheme can significantly improve the performance of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> in terms of <a href=https://en.wikipedia.org/wiki/Analogy>analogical reasoning</a> and is robust to the size of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3012.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3012/>Unsupervised Random Walk Sentence Embeddings : A Strong but Simple Baseline</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3012><div class="card-body p-3 small">Using a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings : take a weighted average of word embeddings and modify with SVD. This simple <a href=https://en.wikipedia.org/wiki/Methodology>method</a> even outperforms far more complex approaches such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.&#8217;s model. We propose a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our <a href=https://en.wikipedia.org/wiki/Stiffness>approach</a> beats Arora et al.&#8217;s by up to 44.4 % on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.&#8217;s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3013/>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3013><div class="card-body p-3 small">Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> given a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The task we use is fine-grained name typing : given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, we can build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task that are complementary to the current embedding evaluation datasets in : they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3015/>Exploiting Common Characters in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to Learn Cross-Lingual Word Embeddings via Matrix Factorization<span class=acl-fixed-case>C</span>hinese and <span class=acl-fixed-case>J</span>apanese to Learn Cross-Lingual Word Embeddings via Matrix Factorization</a></strong><br><a href=/people/j/jilei-wang/>Jilei Wang</a>
|
<a href=/people/s/shiying-luo/>Shiying Luo</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tao-dai/>Tao Dai</a>
|
<a href=/people/s/shu-tao-xia/>Shu-Tao Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3015><div class="card-body p-3 small">Learning vector space representation of words (i.e., word embeddings) has recently attracted wide research interests, and has been extended to cross-lingual scenario. Currently most cross-lingual word embedding learning models are based on sentence alignment, which inevitably introduces much noise. In this paper, we show in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the acquisition of semantic relation among words can benefit from the large number of common characters shared by both languages ; inspired by this unique feature, we design a method named CJC targeting to generate cross-lingual context of words. We combine CJC with GloVe based on matrix factorization, and then propose an integrated model named CJ-Glo. Taking two sentence-aligned models and CJ-BOC (also exploits common characters but is based on CBOW) as baseline algorithms, we compare them with CJ-Glo on a series of NLP tasks including cross-lingual synonym, word analogy and sentence alignment. The result indicates CJ-Glo achieves the best performance among these methods, and is more stable in cross-lingual tasks ; moreover, compared with CJ-BOC, CJ-Glo is less sensitive to the alteration of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3018/>Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3018><div class="card-body p-3 small">Word vector space specialisation models offer a portable, light-weight approach to fine-tuning arbitrary distributional vector spaces to discern between <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a>. Their effectiveness is drawn from <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>external linguistic constraints</a> that specify the exact <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical relation</a> between words. In this work, we show that a careful selection of the <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>external constraints</a> can steer and improve the <a href=https://en.wikipedia.org/wiki/Specialization_(functional)>specialisation</a>. By simply selecting appropriate constraints, we report state-of-the-art results on a suite of tasks with well-defined benchmarks where modeling lexical contrast is crucial : 1) true semantic similarity, with highest reported scores on SimLex-999 and SimVerb-3500 to date ; 2) detecting antonyms ; and 3) distinguishing antonyms from synonyms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3019.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3019/>Characters or <a href=https://en.wikipedia.org/wiki/Morphemes>Morphemes</a> : How to Represent Words?</a></strong><br><a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/m/murathan-kurfali/>Murathan Kurfalı</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3019><div class="card-body p-3 small">In this paper, we investigate the effects of using subword information in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. We argue that using syntactic subword units effects the quality of the word representations positively. We introduce a morpheme-based model and compare it against to word-based, character-based, and character n-gram level models. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes a list of candidate segmentations of a word and learns the representation of the word based on different segmentations that are weighted by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We performed experiments on <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as a morphologically rich language and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with a comparably poorer morphology. The results show that morpheme-based models are better at learning word representations of morphologically complex languages compared to character-based and character n-gram level models since the morphemes help to incorporate more syntactic knowledge in learning, that makes morpheme-based models better at syntactic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3020/>Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences</a></strong><br><a href=/people/a/athul-paul-jacob/>Athul Paul Jacob</a>
|
<a href=/people/z/zhouhan-lin/>Zhouhan Lin</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3020><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Hierarchical_model>hierarchical model</a> for sequential data that learns a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> on-the-fly, i.e. while reading the sequence. In the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections that ease the learning of long-term dependencies. The <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> can either be inferred without <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervision</a> through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, or learned in a supervised manner. We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is created to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a well-known propositional logic and language modelling tasks. Experimental results have shown the potential of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3021/>Limitations of Cross-Lingual Learning from Image Search</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3021><div class="card-body p-3 small">Cross-lingual representation learning is an important step in making <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> scale to all the world&#8217;s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3022/>Learning Semantic Textual Similarity from Conversations</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/s/steve-yuan/>Steve Yuan</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/s/sheng-yi-kong/>Sheng-yi Kong</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/p/petr-pilar/>Petr Pilar</a>
|
<a href=/people/h/heming-ge/>Heming Ge</a>
|
<a href=/people/y/yun-hsuan-sung/>Yun-Hsuan Sung</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3022><div class="card-body p-3 small">We present a novel approach to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for sentence-level semantic similarity using conversational data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> trains an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> to predict conversational responses. The resulting <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017&#8217;s Community Question Answering (CQA) question similarity subtask. Performance is further improved by introducing multitask training, combining conversational response prediction and natural language inference. Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3023/>Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification</a></strong><br><a href=/people/k/katherine-yu/>Katherine Yu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3023><div class="card-body p-3 small">In this paper we continue experiments where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> to the <a href=https://en.wikipedia.org/wiki/Loss_function>learning objective</a> which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both. Furthermore, we notice that while our Reuters results are very competitive, our English results are not as competitive, showing room for improvement in the current cross-lingual state-of-the-art. Our results are based on a set of 6 <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3024/>LSTMs Exploit Linguistic Attributes of Data<span class=acl-fixed-case>LSTM</span>s Exploit Linguistic Attributes of Data</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3024><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a> affect an LSTM&#8217;s ability to learn a nonlinguistic task : recalling elements from its input. We find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on natural language data are able to recall tokens from much longer sequences than <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> on the learnability of LSTMs remains an open question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3026/>Jointly Embedding Entities and Text with Distant Supervision</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/albert-m-lai/>Albert M Lai</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3026><div class="card-body p-3 small">Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> between <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and <a href=https://en.wikipedia.org/wiki/Word>words</a> capture complementary information that can be effectively combined for downstream use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3027/>A Sequence-to-Sequence Model for Semantic Role Labeling</a></strong><br><a href=/people/a/angel-daza/>Angel Daza</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3027><div class="card-body p-3 small">We explore a novel approach for Semantic Role Labeling (SRL) by casting it as a sequence-to-sequence process. We employ an attention-based model enriched with a copying mechanism to ensure faithful regeneration of the input sequence, while enabling interleaved generation of argument role labels. We apply this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a monolingual setting, performing PropBank SRL on English language data. The constrained sequence generation set-up enforced with the copying mechanism allows us to analyze the performance and special properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on manually labeled data and benchmarking against state-of-the-art sequence labeling models. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> truly competitive. Our work represents the first step towards more advanced, generative SRL labeling setups.</div></div></div><hr><div id=w18-31><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-31.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-31/>Proceedings of the First Workshop on Economics and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3100/>Proceedings of the First Workshop on Economics and Natural Language Processing</a></strong><br><a href=/people/u/udo-hahn/>Udo Hahn</a>
|
<a href=/people/v/veronique-hoste/>Véronique Hoste</a>
|
<a href=/people/m/ming-feng-tsai/>Ming-Feng Tsai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3102/>Causality Analysis of Twitter Sentiments and Stock Market Returns<span class=acl-fixed-case>T</span>witter Sentiments and Stock Market Returns</a></strong><br><a href=/people/n/narges-tabari/>Narges Tabari</a>
|
<a href=/people/p/piyusha-biswas/>Piyusha Biswas</a>
|
<a href=/people/b/bhanu-praneeth/>Bhanu Praneeth</a>
|
<a href=/people/a/armin-seyeditabari/>Armin Seyeditabari</a>
|
<a href=/people/m/mirsad-hadzikadic/>Mirsad Hadzikadic</a>
|
<a href=/people/w/wlodek-zadrozny/>Wlodek Zadrozny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3102><div class="card-body p-3 small">Sentiment analysis is the process of identifying the opinion expressed in text. Recently, it has been used to study <a href=https://en.wikipedia.org/wiki/Behavioral_economics>behavioral finance</a>, and in particular the effect of <a href=https://en.wikipedia.org/wiki/Opinion>opinions</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> on economic or financial decisions. In this paper, we use a public dataset of labeled tweets that has been labeled by Amazon Mechanical Turk and then we propose a baseline classification model. Then, by using Granger causality of both sentiment datasets with the different stocks, we shows that there is causality between <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and stock market returns (in both directions) for many stocks. Finally, We evaluate this <a href=https://en.wikipedia.org/wiki/Causality>causality analysis</a> by showing that in the event of a specific news on certain dates, there are evidences of trending the same news on Twitter for that stock.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3103/>A Corpus of Corporate Annual and Social Responsibility Reports : 280 Million Tokens of Balanced Organizational Writing</a></strong><br><a href=/people/s/sebastian-g-m-handschke/>Sebastian G.M. Händschke</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/j/jan-goldenstein/>Jan Goldenstein</a>
|
<a href=/people/p/philipp-poschmann/>Philipp Poschmann</a>
|
<a href=/people/t/tinghui-duan/>Tinghui Duan</a>
|
<a href=/people/p/peter-walgenbach/>Peter Walgenbach</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3103><div class="card-body p-3 small">We introduce JOCo, a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP analytics</a> in the field of <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S&P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world&#8217;s most important economies. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> spans a time frame from 2000 up to 2015 and contains, in total, 282 M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, business and management research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3104/>Word Embeddings-Based Uncertainty Detection in Financial Disclosures</a></strong><br><a href=/people/c/christoph-kilian-theil/>Christoph Kilian Theil</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3104><div class="card-body p-3 small">In this paper, we use NLP techniques to detect <a href=https://en.wikipedia.org/wiki/Uncertainty>linguistic uncertainty</a> in <a href=https://en.wikipedia.org/wiki/Financial_statement>financial disclosures</a>. Leveraging general-domain and domain-specific word embedding models, we automatically expand an existing dictionary of uncertainty triggers. We furthermore examine how an expert filtering affects the quality of such an <a href=https://en.wikipedia.org/wiki/Expansion_factor>expansion</a>. We show that the dictionary expansions significantly improve regressions on stock return volatility. Lastly, we prove that the expansions significantly boost the automatic detection of uncertain sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3108/>Implicit and Explicit Aspect Extraction in Financial Microblogs</a></strong><br><a href=/people/t/thomas-gaillat/>Thomas Gaillat</a>
|
<a href=/people/b/bernardo-stearns/>Bernardo Stearns</a>
|
<a href=/people/g/gopal-sridhar/>Gopal Sridhar</a>
|
<a href=/people/r/ross-mcdermott/>Ross McDermott</a>
|
<a href=/people/m/manel-zarrouk/>Manel Zarrouk</a>
|
<a href=/people/b/brian-davis/>Brian Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3108><div class="card-body p-3 small">This paper focuses on <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> which is a sub-task of Aspect-based Sentiment Analysis. The goal is to report an <a href=https://en.wikipedia.org/wiki/Information_extraction>extraction method</a> of <a href=https://en.wikipedia.org/wiki/Finance>financial aspects</a> in <a href=https://en.wikipedia.org/wiki/Microblogging>microblog messages</a>. Our approach uses a stock-investment taxonomy for the identification of explicit and implicit aspects. We compare <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised and unsupervised methods</a> to assign <a href=https://en.wikipedia.org/wiki/Categorization>predefined categories</a> at message level. Results on 7 <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect classes</a> show 0.71 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while the 32 class classification gives 0.82 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for messages containing explicit aspects and 0.35 for implicit aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3109/>Unsupervised Word Influencer Networks from News Streams</a></strong><br><a href=/people/a/ananth-balashankar/>Ananth Balashankar</a>
|
<a href=/people/s/sunandan-chakraborty/>Sunandan Chakraborty</a>
|
<a href=/people/l/lakshminarayanan-subramanian/>Lakshminarayanan Subramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3109><div class="card-body p-3 small">In this paper, we propose a new unsupervised learning framework to use news events for predicting trends in stock prices. We present Word Influencer Networks (WIN), a graph framework to extract longitudinal temporal relationships between any pair of informative words from news streams. Using the temporal occurrence of words, WIN measures how the appearance of one word in a news stream influences the emergence of another set of words in the future. The latent word-word influencer relationships in WIN are the building blocks for <a href=https://en.wikipedia.org/wiki/Causal_reasoning>causal reasoning</a> and <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive modeling</a>. We demonstrate the efficacy of WIN by using it for unsupervised extraction of latent features for <a href=https://en.wikipedia.org/wiki/Stock_market_prediction>stock price prediction</a> and obtain 2 orders lower prediction error compared to a similar causal graph based method. WIN discovered influencer links from seemingly unrelated words from topics like <a href=https://en.wikipedia.org/wiki/Politics>politics</a> to <a href=https://en.wikipedia.org/wiki/Finance>finance</a>. WIN also validated 67 % of the causal evidence found manually in the text through a direct edge and the rest 33 % through a path of length 2.</div></div></div><hr><div id=w18-32><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-32.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-32/>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3200/>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/f/fahad-alghamdi/>Fahad AlGhamdi</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3201/>Joint Part-of-Speech and Language ID Tagging for Code-Switched Data<span class=acl-fixed-case>ID</span> Tagging for Code-Switched Data</a></strong><br><a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3201><div class="card-body p-3 small">Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a>. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on known monolingual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English Spanish conversation, focusing on two types of experiments : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>POS tagging</a> alone, for which we achieve 96.34 % accuracy, and joint part-of-speech and language ID tagging, which achieves similar <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>POS tagging accuracy</a> (96.39 %) and very high language ID accuracy (98.78 %). Finally, we show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform other state-of-the-art code-switched taggers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3202/>Phone Merging For Code-Switched Speech Recognition</a></strong><br><a href=/people/s/sunit-sivasankaran/>Sunit Sivasankaran</a>
|
<a href=/people/b/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3202><div class="card-body p-3 small">Speakers in multilingual communities often switch between or mix multiple languages in the same conversation. Automatic Speech Recognition (ASR) of code-switched speech faces many challenges including the influence of phones of different languages on each other. This paper shows evidence that phone sharing between languages improves the Acoustic Model performance for Hindi-English code-switched speech. We compare baseline system built with separate phones for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with systems where the phones were manually merged based on linguistic knowledge. Encouraged by the improved ASR performance after manually merging the phones, we further investigate multiple data-driven methods to identify phones to be merged across the languages. We show detailed analysis of automatic phone merging in this language pair and the impact it has on individual phone accuracies and WER. Though the best performance gain of 1.2 % <a href=https://en.wikipedia.org/wiki/Effective_radiated_power>WER</a> was observed with manually merged phones, we show experimentally that the manual phone merge is not optimal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3203/>Improving Neural Network Performance by Injecting Background Knowledge : Detecting Code-switching and Borrowing in Algerian texts<span class=acl-fixed-case>A</span>lgerian texts</a></strong><br><a href=/people/w/wafia-adouane/>Wafia Adouane</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/simon-dobnik/>Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3203><div class="card-body p-3 small">We explore the effect of injecting background knowledge to different deep neural network (DNN) configurations in order to mitigate the problem of the scarcity of annotated data when applying these models on datasets of low-resourced languages. The background knowledge is encoded in the form of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> and pre-trained sub-word embeddings. The DNN models are evaluated on the task of detecting code-switching and borrowing points in non-standardised user-generated Algerian texts. Overall results show that DNNs benefit from adding background knowledge. However, the gain varies between models and categories. The proposed DNN architectures are generic and could be applied to other low-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3204/>Code-Mixed Question Answering Challenge : Crowd-sourcing Data and Techniques</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/ekaterina-loginova/>Ekaterina Loginova</a>
|
<a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/g/gunter-neumann/>Günter Neumann</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3204><div class="card-body p-3 small">Code-Mixing (CM) is the phenomenon of alternating between two or more languages which is prevalent in bi- and multi-lingual communities. Most NLP applications today are still designed with the assumption of a single interaction language and are most likely to break given a CM utterance with multiple languages mixed at a morphological, phrase or sentence level. For example, popular commercial search engines do not yet fully understand the intents expressed in CM queries. As a first step towards fostering research which supports CM in NLP applications, we systematically crowd-sourced and curated an evaluation dataset for factoid question answering in three CM languages-Hinglish (Hindi+English), Tenglish (Telugu+English) and Tamlish (Tamil+English) which belong to two language families (Indo-Aryan and Dravidian). We share the details of our data collection process, techniques which were used to avoid inducing lexical bias amongst the crowd workers and other CM specific linguistic properties of the dataset. Our final dataset, which is available freely for research purposes, has 1,694 <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, 2,848 Tamlish and 1,391 Tenglish factoid questions and their answers. We discuss the <a href=https://en.wikipedia.org/wiki/List_of_art_media>techniques</a> used by the participants for the first edition of this ongoing challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3205/>Transliteration Better than <a href=https://en.wikipedia.org/wiki/Translation>Translation</a>? Answering Code-mixed Questions over a Knowledge Base</a></strong><br><a href=/people/v/vishal-gupta/>Vishal Gupta</a>
|
<a href=/people/m/manoj-chinnakotla/>Manoj Chinnakotla</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3205><div class="card-body p-3 small">Humans can learn multiple languages. If they know a fact in one language, they can answer a question in another language they understand. They can also answer Code-mix (CM) questions : questions which contain both languages. This behavior is attributed to the unique learning ability of humans. Our task aims to study if machines can achieve this. We demonstrate how effectively a <a href=https://en.wikipedia.org/wiki/Machine>machine</a> can answer CM questions. In this work, we adopt a two phase approach : candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. We show experiments on the SimpleQuestions dataset. Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English. Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5 % & 35 % in accuracy respectively, highlighting the efficacy of our approach in a resource constrained setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3208/>Predicting the presence of a Matrix Language in <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a></a></strong><br><a href=/people/b/barbara-bullock/>Barbara Bullock</a>
|
<a href=/people/w/wally-guzman/>Wally Guzmán</a>
|
<a href=/people/j/jacqueline-serigos/>Jacqueline Serigos</a>
|
<a href=/people/v/vivek-sharath/>Vivek Sharath</a>
|
<a href=/people/a/almeida-jacqueline-toribio/>Almeida Jacqueline Toribio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3208><div class="card-body p-3 small">One language is often assumed to be dominant in <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> but this assumption has not been empirically tested. We operationalize the matrix language (ML) at the level of the sentence, using three common definitions from <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. We test whether these converge and then model this <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence</a> via a set of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that together quantify the nature of C-S. We conduct our experiment on four Spanish-English corpora. Our results demonstrate that our model can separate some corpora according to whether they have a dominant ML or not but that the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> span a range of mixing types that can not be sorted neatly into an insertional vs. alternational dichotomy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3210/>Accommodation of Conversational Code-Choice</a></strong><br><a href=/people/a/anshul-bawa/>Anshul Bawa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3210><div class="card-body p-3 small">Bilingual speakers often freely mix languages. However, in such bilingual conversations, are the language choices of the speakers coordinated? How much does one speaker&#8217;s choice of language affect other speakers? In this paper, we formulate code-choice as a <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>linguistic style</a>, and show that speakers are indeed sensitive to and accommodating of each other&#8217;s code-choice. We find that the saliency or markedness of a language in context directly affects the degree of accommodation observed. More importantly, we discover that accommodation of code-choices persists over several conversational turns. We also propose an alternative interpretation of conversational accommodation as a retrieval problem, and show that the differences in accommodation characteristics of code-choices are based on their markedness in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3211/>Language Informed Modeling of Code-Switched Text</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/s/sumeet-singh/>Sumeet Singh</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3211><div class="card-body p-3 small">Code-switching (CS), the practice of alternating between two or more languages in conversations, is pervasive in most <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual communities</a>. CS texts have a complex interplay between languages and occur in informal contexts that make them harder to collect and construct NLP tools for. We approach this problem through Language Modeling (LM) on a new Hindi-English mixed corpus containing 59,189 unique sentences collected from <a href=https://en.wikipedia.org/wiki/Blog>blogging websites</a>. We implement and discuss different <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> derived from a multi-layered LSTM architecture. We hypothesize that <a href=https://en.wikipedia.org/wiki/Code-switching>encoding language information</a> strengthens a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> by helping to learn code-switching points. We show that our highest performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves a test perplexity of 19.52 on the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>CS corpus</a> that we collected and processed. On this data we demonstrate that our performance is an improvement over AWD-LSTM LM (a recent state of the art on monolingual English).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3212/>GHHT at CALCS 2018 : Named Entity Recognition for Dialectal Arabic Using Neural Networks<span class=acl-fixed-case>GHHT</span> at <span class=acl-fixed-case>CALCS</span> 2018: Named Entity Recognition for Dialectal <span class=acl-fixed-case>A</span>rabic Using Neural Networks</a></strong><br><a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/w/wolfgang-maier/>Wolfgang Maier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3212><div class="card-body p-3 small">This paper describes our system submission to the CALCS 2018 shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> on code-switched data for the language variant pair of <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic</a> and <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian dialectal Arabic</a>. We build a a <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Network</a> that combines word and character-based representations in convolutional and recurrent networks with a CRF layer. The model is augmented with stacked layers of enriched information such pre-trained embeddings, Brown clusters and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity gazetteers</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is ranked second among those participating in the shared task achieving an FB1 average of 70.09 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3213/>Simple Features for Strong Performance on Named Entity Recognition in Code-Switched Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/d/devanshu-jain/>Devanshu Jain</a>
|
<a href=/people/m/maria-kustikova/>Maria Kustikova</a>
|
<a href=/people/m/mayank-darbari/>Mayank Darbari</a>
|
<a href=/people/r/rishabh-gupta/>Rishabh Gupta</a>
|
<a href=/people/s/stephen-mayhew/>Stephen Mayhew</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3213><div class="card-body p-3 small">In this work, we address the problem of Named Entity Recognition (NER) in code-switched tweets as a part of the Workshop on Computational Approaches to Linguistic Code-switching (CALCS) at ACL&#8217;18. Code-switching is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential code-switching, respectively. Processing such <a href=https://en.wikipedia.org/wiki/Data>data</a> is challenging using state of the art methods since such <a href=https://en.wikipedia.org/wiki/Technology>technology</a> is generally geared towards processing monolingual text. In this paper we explored ways to use <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to recognize named entities in such data, however, utilizing simple features (sans multi-lingual features) with Conditional Random Field (CRF) classifier achieved the best results. Our experiments were mainly aimed at the (ENG-SPA) English-Spanish dataset but we submitted a language-independent version of our system to the (MSA-EGY) Arabic-Egyptian dataset as well and achieved good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3214/>Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3214><div class="card-body p-3 small">We propose an LSTM-based model with hierarchical architecture on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> from code-switching Twitter data. Our model uses bilingual character representation and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to address out-of-vocabulary words. In order to mitigate <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>data noise</a>, we propose to use token replacement and normalization. In the 3rd Workshop on Computational Approaches to Linguistic Code-Switching Shared Task, we achieved second place with 62.76 % harmonic mean F1-score for English-Spanish language pair without using any <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteer</a> and knowledge-based information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3216/>The University of Texas System Submission for the Code-Switching Workshop Shared Task 2018<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>T</span>exas System Submission for the Code-Switching Workshop Shared Task 2018</a></strong><br><a href=/people/f/florian-janke/>Florian Janke</a>
|
<a href=/people/t/tongrui-li/>Tongrui Li</a>
|
<a href=/people/e/eric-rincon/>Eric Rincón</a>
|
<a href=/people/g/gualberto-a-guzman/>Gualberto Guzmán</a>
|
<a href=/people/b/barbara-bullock/>Barbara Bullock</a>
|
<a href=/people/a/almeida-jacqueline-toribio/>Almeida Jacqueline Toribio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3216><div class="card-body p-3 small">This paper describes the system for the Named Entity Recognition Shared Task of the Third Workshop on Computational Approaches to Linguistic Code-Switching (CALCS) submitted by the Bilingual Annotations Tasks (BATs) research group of the University of Texas. Our system uses several features to train a Conditional Random Field (CRF) model for classifying input words as Named Entities (NEs) using the Inside-Outside-Beginning (IOB) tagging scheme. We participated in the Modern Standard Arabic-Egyptian Arabic (MSA-EGY) and English-Spanish (ENG-SPA) tasks, achieving <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average F-scores</a> of 65.62 and 54.16 respectively. We also describe the performance of a deep neural network (NN) trained on a subset of the CRF features, which did not surpass CRF performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3217/>Tackling Code-Switched NER : Participation of CMU<span class=acl-fixed-case>NER</span>: Participation of <span class=acl-fixed-case>CMU</span></a></strong><br><a href=/people/p/parvathy-geetha/>Parvathy Geetha</a>
|
<a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3217><div class="card-body p-3 small">Named Entity Recognition plays a major role in several downstream applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Though this task has been heavily studied in formal monolingual texts and also <a href=https://en.wikipedia.org/wiki/Noisy_text>noisy texts</a> like Twitter data, it is still an emerging task in code-switched (CS) content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper describes our participation in the shared task of NER on code-switched data for Spanglish (Spanish + English) and Arabish (Arabic + English). In this paper we describe <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that intuitively developed from the <a href=https://en.wikipedia.org/wiki/Data>data</a> for the shared task Named Entity Recognition on Code-switched Data. Owing to the sparse and non-linear relationships between words in Twitter data, we explored neural architectures that are capable of non-linearities fairly well. In specific, we trained character level models and word level models based on Bidirectional LSTMs (Bi-LSTMs) to perform sequential tagging. We train multiple models to identify nominal mentions and subsequently use this information to predict the labels of named entity in a sequence. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a character level model along with word level pre-trained multilingual embeddings that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 56.72 in <a href=https://en.wikipedia.org/wiki/Spanglish>Spanglish</a> and a word level model that gave an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 65.02 in <a href=https://en.wikipedia.org/wiki/Arabic>Arabish</a> on the test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3218/>Multilingual Named Entity Recognition on Spanish-English Code-switched Tweets using Support Vector Machines<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish Code-switched Tweets using Support Vector Machines</a></strong><br><a href=/people/d/daniel-claeser/>Daniel Claeser</a>
|
<a href=/people/s/samantha-kent/>Samantha Kent</a>
|
<a href=/people/d/dennis-felske/>Dennis Felske</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3218><div class="card-body p-3 small">This paper describes our system submission for the ACL 2018 shared task on named entity recognition (NER) in code-switched Twitter data. Our best result (F1 = 53.65) was obtained using a Support Vector Machine (SVM) with 14 <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> combined with rule-based post processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3220/>IIT (BHU) Submission for the ACL Shared Task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> on Code-switched Data<span class=acl-fixed-case>IIT</span> (<span class=acl-fixed-case>BHU</span>) Submission for the <span class=acl-fixed-case>ACL</span> Shared Task on Named Entity Recognition on Code-switched Data</a></strong><br><a href=/people/s/shashwat-trivedi/>Shashwat Trivedi</a>
|
<a href=/people/h/harsh-rangwani/>Harsh Rangwani</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3220><div class="card-body p-3 small">This paper describes the best performing system for the shared task on Named Entity Recognition (NER) on code-switched data for the language pair Spanish-English (ENG-SPA). We introduce a gated neural architecture for the NER task. Our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 63.76 %, outperforming the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3221/>Code-Switched Named Entity Recognition with Embedding Attention</a></strong><br><a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3221><div class="card-body p-3 small">We describe our work for the CALCS 2018 shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> on code-switched data. Our system ranked first place for MS Arabic-Egyptian named entity recognition and third place for English-Spanish.</div></div></div><hr><div id=w18-33><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-33.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-33/>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3300/>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-<span class=acl-fixed-case>HML</span>)</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3301/>Getting the subtext without the text : Scalable multimodal sentiment classification from visual and acoustic modalities</a></strong><br><a href=/people/n/nathaniel-blanchard/>Nathaniel Blanchard</a>
|
<a href=/people/d/daniel-moreira/>Daniel Moreira</a>
|
<a href=/people/a/aparna-bharati/>Aparna Bharati</a>
|
<a href=/people/w/walter-scheirer/>Walter Scheirer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3301><div class="card-body p-3 small">In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment. The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection. Multimodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers. In the detection of sentiment in these <a href=https://en.wikipedia.org/wiki/Video>videos</a>, acoustic and video features provide clarity to otherwise ambiguous transcripts. In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>spoken sentences</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>. We discard traditional transcription features in order to minimize human intervention and to maximize the deployability of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on at-scale real-world data. We select high-level features for our model that have been successful in non-affect domains in order to test their generalizability in the sentiment detection domain. We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, obtaining an F1 score of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out challenge test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3302/>Recognizing Emotions in Video Using Multimodal DNN Feature Fusion<span class=acl-fixed-case>DNN</span> Feature Fusion</a></strong><br><a href=/people/j/jennifer-williams/>Jennifer Williams</a>
|
<a href=/people/s/steven-kleinegesse/>Steven Kleinegesse</a>
|
<a href=/people/r/ramona-comanescu/>Ramona Comanescu</a>
|
<a href=/people/o/oana-radu/>Oana Radu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3302><div class="card-body p-3 small">We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language. Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs). We show that our fusion approach outperforms <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal predictors</a>. Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment. This leads to an overall binary accuracy of 90 %, overall 4-class accuracy of 89.2 % and an overall <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>mean-absolute-error (MAE)</a> of 0.12. Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities. The presented multimodal approach creates a simple and robust <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on this new Grand Challenge dataset. Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3304/>Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data</a></strong><br><a href=/people/w/woo-yong-choi/>Woo Yong Choi</a>
|
<a href=/people/k/kyu-ye-song/>Kyu Ye Song</a>
|
<a href=/people/c/chan-woo-lee/>Chan Woo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3304><div class="card-body p-3 small">Emotion recognition has become a popular topic of interest, especially in the field of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human computer interaction</a>. Previous works involve <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal analysis of emotion</a>, while recent efforts focus on <a href=https://en.wikipedia.org/wiki/Emotion_recognition>multimodal emotion recognition</a> from <a href=https://en.wikipedia.org/wiki/Visual_perception>vision</a> and <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3306/>Polarity and Intensity : the Two Aspects of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/l/leimin-tian/>Leimin Tian</a>
|
<a href=/people/c/catherine-lai/>Catherine Lai</a>
|
<a href=/people/j/johanna-d-moore/>Johanna Moore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3306><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> frames sentiment score prediction as a general Machine Learning task. However, what the sentiment score actually represents has often been overlooked. As a measurement of opinions and affective states, a sentiment score generally consists of two aspects : polarity and intensity. We decompose sentiment scores into these two aspects and study how they are conveyed through individual modalities and combined multimodal models in a naturalistic monologue setting. In particular, we build unimodal and multimodal multi-task learning models with sentiment score prediction as the main task and polarity and/or intensity classification as the auxiliary tasks. Our experiments show that <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, and individual modalities differ when conveying the polarity and intensity aspects of sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3308/>Seq2Seq2Sentiment : Multimodal Sequence to Sequence Models for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>entiment: Multimodal Sequence to Sequence Models for Sentiment Analysis</a></strong><br><a href=/people/h/hai-pham/>Hai Pham</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/b/barnabas-poczos/>Barnabás Poczós</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3308><div class="card-body p-3 small">Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods : a Seq2Seq Modality Translation Model and a Hierarchical Seq2Seq Modality Translation Model. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a>, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.</div></div></div><hr><div id=w18-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-34/>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3400/>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/gholamreza-haffari/>Reza Haffari</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3403.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3403/>Multi-task learning for historical text normalization : Size matters</a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3403><div class="card-body p-3 small">Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> across 10 different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, representing different languages and periods. Our main findingcontrary to what has been observed for other NLP tasksis that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> mainly works when target task data is very scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3404/>Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication</a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3404><div class="card-body p-3 small">Icon-based communication systems are widely used in the field of <a href=https://en.wikipedia.org/wiki/Augmentative_and_alternative_communication>Augmentative and Alternative Communication</a>. Typically, icon-based systems have lagged behind word- and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models. We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies. We propose a method to generate <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for corpus-less symbol-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3405/>Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3405><div class="card-body p-3 small">In this paper, we investigate the effectiveness of training a multimodal neural machine translation (MNMT) system with image features for a low-resource language pair, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, using synthetic data. A three-way parallel corpus which contains <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual texts</a> and corresponding images is required to train a MNMT system with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>image features</a>. However, such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development / test dataset for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> based on an existing English-image parallel corpus. We used these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a <a href=https://en.wikipedia.org/wiki/System>system</a> can benefit from image features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3406/>Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus</a></strong><br><a href=/people/f/fariz-ikhwantri/>Fariz Ikhwantri</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/b/bagas-abisena/>Bagas Abisena</a>
|
<a href=/people/v/valdi-rachman/>Valdi Rachman</a>
|
<a href=/people/a/alfan-farizki-wicaksono/>Alfan Farizki Wicaksono</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3406><div class="card-body p-3 small">Most Semantic Role Labeling (SRL) approaches are supervised methods which require a significant amount of annotated corpus, and the annotation requires linguistic expertise. In this paper, we propose a Multi-Task Active Learning framework for Semantic Role Labeling with Entity Recognition (ER) as the auxiliary task to alleviate the need for extensive data and use additional information from ER to help SRL. We evaluate our approach on Indonesian conversational dataset. Our experiments show that multi-task active learning can outperform single-task active learning method and standard <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. According to our results, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is more efficient by using 12 % less of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> compared to <a href=https://en.wikipedia.org/wiki/Passive_learning>passive learning</a> in both single-task and multi-task setting. We also introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for SRL in Indonesian conversational domain to encourage further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3407.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3407/>Domain Adapted Word Embeddings for Improved Sentiment Classification</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha Kameswara Sarma</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a>
|
<a href=/people/b/bill-sethares/>Bill Sethares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3407><div class="card-body p-3 small">Generic word embeddings are trained on large-scale generic corpora ; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings. The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by first aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA (KCCA) and then combining them via <a href=https://en.wikipedia.org/wiki/Convex_optimization>convex optimization</a>. Results from evaluation on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3408/>Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus</a></strong><br><a href=/people/k/kanako-komiya/>Kanako Komiya</a>
|
<a href=/people/h/hiroyuki-shinnou/>Hiroyuki Shinnou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3408><div class="card-body p-3 small">Fine-tuning is a popular <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> to achieve better performance when only a small target corpus is available. However, <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> requires tuning of a number of metaparameters and thus it might carry risk of adverse effect when inappropriate metaparameters are used. Therefore, we investigate effective parameters for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> when only a small target corpus is available. In the current study, we target at improving Japanese word embeddings created from a <a href=https://en.wikipedia.org/wiki/Text_corpus>huge corpus</a>. First, we demonstrate that even the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> created from the huge corpus are affected by domain shift. After that, we investigate effective <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> of the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> using a small target corpus. We used perplexity of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> obtained from a Long Short-Term Memory network to assess the word embeddings input into the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. The experiments revealed that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. In addition, we confirmed that effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is higher when size of a target corpus was larger.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3409.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3409/>Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification</a></strong><br><a href=/people/m/mingkuan-liu/>Mingkuan Liu</a>
|
<a href=/people/m/musen-wen/>Musen Wen</a>
|
<a href=/people/s/selcuk-kopru/>Selcuk Kopru</a>
|
<a href=/people/x/xianjing-liu/>Xianjing Liu</a>
|
<a href=/people/a/alan-lu/>Alan Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3409><div class="card-body p-3 small">The lack of high-quality labeled training data has been one of the critical challenges facing many industrial machine learning tasks. To tackle this challenge, in this paper, we propose a semi-supervised learning method to utilize unlabeled data and user feedback signals to improve the performance of ML models. The method employs a primary model Main and an auxiliary evaluation model Eval, where Main and Eval models are trained iteratively by automatically generating labeled data from unlabeled data and/or users&#8217; feedback signals. The proposed approach is applied to different text classification tasks. We report results on both the publicly available Yahoo ! Answers dataset and our e-commerce product classification dataset. The experimental results show that the proposed method reduces the classification error rate by 4 % and up to 15 % across various experimental setups and datasets. A detailed comparison with other semi-supervised learning approaches is also presented later in the paper. The results from various text classification tasks demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms those developed in previous related studies.</div></div></div><hr><div id=w18-35><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-35.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-35/>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3500/>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/c/cheng-te-li/>Cheng-Te Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3501/>Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students<span class=acl-fixed-case>W</span>hats<span class=acl-fixed-case>A</span>pp Chats in <span class=acl-fixed-case>S</span>panish among College Students</a></strong><br><a href=/people/a/alejandro-dorantes/>Alejandro Dorantes</a>
|
<a href=/people/g/gerardo-sierra/>Gerardo Sierra</a>
|
<a href=/people/t/tlauhlia-yamin-donohue-perez/>Tlauhlia Yamín Donohue Pérez</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/m/monica-jasso-rosales/>Mónica Jasso Rosales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3501><div class="card-body p-3 small">This work presents the Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students, a corpus of raw data for general use. Its purpose is to offer data for the study of of language and interactions via Instant Messaging (IM) among bachelors. Our paper consists of an overview of both the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>&#8217;s content and demographic metadata. Furthermore, it presents the current research being conducted with it namely <a href=https://en.wikipedia.org/wiki/Parenthetical_expression>parenthetical expressions</a>, <a href=https://en.wikipedia.org/wiki/Orality>orality traits</a>, and <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. This work also includes a brief outline of similar corpora and recent studies in the field of IM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3503/>A Twitter Corpus for Hindi-English Code Mixed POS Tagging<span class=acl-fixed-case>T</span>witter Corpus for <span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code Mixed <span class=acl-fixed-case>POS</span> Tagging</a></strong><br><a href=/people/k/kushagra-singh/>Kushagra Singh</a>
|
<a href=/people/i/indira-sen/>Indira Sen</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3503><div class="card-body p-3 small">Code-mixing is a linguistic phenomenon where multiple languages are used in the same occurrence that is increasingly common in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual societies</a>. Code-mixed content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is also on the rise, prompting the need for tools to automatically understand such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>. Automatic Parts-of-Speech (POS) tagging is an essential step in any Natural Language Processing (NLP) pipeline, but there is a lack of annotated data to train such models. In this work, we present a unique language tagged and POS-tagged dataset of code-mixed English-Hindi tweets related to five incidents in India that led to a lot of Twitter activity. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is unique in two dimensions : (i) it is larger than previous annotated datasets and (ii) it closely resembles typical real-world tweets. Additionally, we present a POS tagging model that is trained on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to provide an example of how this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also shows the efficacy of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in enabling the creation of code-mixed social media POS taggers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3504/>Detecting Offensive Tweets in Hindi-English Code-Switched Language<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Switched Language</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/d/debanjan-mahata/>Debanjan Mahata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3504><div class="card-body p-3 small">The exponential rise of social media websites like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> in linguistically diverse geographical regions has led to hybridization of popular native languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a> in an effort to ease communication. The paper focuses on the classification of offensive tweets written in <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish language</a>, which is a portmanteau of the Indic language Hindi with the <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a>. The paper introduces a novel tweet dataset, titled Hindi-English Offensive Tweet (HEOT) dataset, consisting of tweets in Hindi-English code switched language split into three classes : non-offensive, abusive and hate-speech. Further, we approach the problem of classification of the tweets in HEOT dataset using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> wherein the proposed model employing <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> is pre-trained on tweets in English followed by retraining on <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish tweets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3507/>EmotionX-AR : CNN-DCNN autoencoder based Emotion Classifier<span class=acl-fixed-case>E</span>motion<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>AR</span>: <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>DCNN</span> autoencoder based Emotion Classifier</a></strong><br><a href=/people/s/sopan-khosla/>Sopan Khosla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3507><div class="card-body p-3 small">In this paper, we model emotions in EmotionLines dataset using a convolutional-deconvolutional autoencoder (CNN-DCNN) framework. We show that adding a joint reconstruction loss improves performance. Quantitative evaluation with jointly trained network, augmented with linguistic features, reports best accuracies for emotion prediction ; namely <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Sadness>sadness</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, and neutral emotion in text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3508/>EmotionX-SmartDubai_NLP : Detecting User Emotions In Social Media Text<span class=acl-fixed-case>E</span>motion<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>S</span>mart<span class=acl-fixed-case>D</span>ubai_<span class=acl-fixed-case>NLP</span>: Detecting User Emotions In Social Media Text</a></strong><br><a href=/people/h/hessa-albalooshi/>Hessa AlBalooshi</a>
|
<a href=/people/s/shahram-rahmanian/>Shahram Rahmanian</a>
|
<a href=/people/r/rahul-venkatesh-kumar/>Rahul Venkatesh Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3508><div class="card-body p-3 small">This paper describes the working note on EmotionX shared task. It is hosted by SocialNLP 2018. The objective of this task is to detect the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>, based on each speaker&#8217;s utterances that are in English. Taking this as multiclass text classification problem, we have experimented to develop a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to classify the target class. The primary challenge in this task is to detect the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> in short messages, communicated through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper describes the participation of SmartDubai_NLP team in EmotionX shared task and our investigation to detect the emotions from utterance using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural networks</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural language understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3513/>Political discourse classification in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> using context sensitive convolutional neural networks</a></strong><br><a href=/people/a/aritz-bilbao-jayo/>Aritz Bilbao-Jayo</a>
|
<a href=/people/a/aitor-almeida/>Aitor Almeida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3513><div class="card-body p-3 small">In this study we propose a new approach to analyse the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>on-line social networks</a> such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. To do so, we have built a discourse classifier using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a>. Our model has been trained using <a href=https://en.wikipedia.org/wiki/Manifesto>election manifestos</a> annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E-RMP). Using this <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a>, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 <a href=https://en.wikipedia.org/wiki/2016_Spanish_general_election>Spanish general election</a> and providing a study of their discourse.</div></div></div><hr><div id=w18-36><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-36.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-36/>Proceedings of the First Workshop on Multilingual Surface Realisation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3600/>Proceedings of the First Workshop on Multilingual Surface Realisation</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3602/>BinLin : A Simple Method of Dependency Tree Linearization<span class=acl-fixed-case>B</span>in<span class=acl-fixed-case>L</span>in: A Simple Method of Dependency Tree Linearization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3602><div class="card-body p-3 small">Surface Realization Shared Task 2018 is a workshop on generating sentences from lemmatized sets of dependency triples. This paper describes the results of our participation in the challenge. We develop a data-driven pipeline system which first orders the lemmas and then conjugates the words to finish the surface realization process. Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results. We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3607/>AX Semantics’ Submission to the Surface Realization Shared Task 2018<span class=acl-fixed-case>AX</span> Semantics’ Submission to the Surface Realization Shared Task 2018</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/j/johanna-heininger/>Johanna Heininger</a>
|
<a href=/people/n/nyamsuren-davaasambuu/>Nyamsuren Davaasambuu</a>
|
<a href=/people/v/vitaliia-voronik/>Vitaliia Voronik</a>
|
<a href=/people/m/michael-kaufl/>Michael Käufl</a>
|
<a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3607><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> and experimental results on the development set of the Surface Realisation Shared Task. Our system is an entry for the Shallow-Task, with two different models based on deep-learning implementations for building the sentence combined with a rule-based morphology component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3608/>NILC-SWORNEMO at the Surface Realization Shared Task : Exploring Syntax-Based Word Ordering using Neural Models<span class=acl-fixed-case>NILC</span>-<span class=acl-fixed-case>SWORNEMO</span> at the Surface Realization Shared Task: Exploring Syntax-Based Word Ordering using Neural Models</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3608><div class="card-body p-3 small">This paper describes the submission by the NILC Computational Linguistics research group of the University of So Paulo / Brazil to the Track 1 of the Surface Realization Shared Task (SRST Track 1). We present a neural-based method that works at the syntactic level to order the words (which we refer by NILC-SWORNEMO, standing for Syntax-based Word ORdering using NEural MOdels). Additionally, we apply a <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up approach</a> to build the sentence and, using language-specific lexicons, we produce the proper word form of each lemma in the sentence. The results obtained by our method outperformed the average of the results for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> in the track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3609/>The DipInfo-UniTo system for SRST 2018<span class=acl-fixed-case>D</span>ip<span class=acl-fixed-case>I</span>nfo-<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>T</span>o system for <span class=acl-fixed-case>SRST</span> 2018</a></strong><br><a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3609><div class="card-body p-3 small">This paper describes the system developed by the DipInfo-UniTo team to participate to the shallow track of the Surface Realization Shared Task 2018. The system employs two separate <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with different architectures to predict the <a href=https://en.wikipedia.org/wiki/Word_order>word ordering</a> and the <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> independently from each other. The UniTO realizer is language independent, and its simple architecture allowed it to be scored in the central part of the final ranking of the shared task.</div></div></div><hr><div id=w18-37><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-37.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-37/>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3700/>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3703/>Feature Optimization for Predicting Readability of Arabic L1 and L2<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>L</span>1 and <span class=acl-fixed-case>L</span>2</a></strong><br><a href=/people/h/hind-saddiki/>Hind Saddiki</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/v/violetta-cavalli-sforza/>Violetta Cavalli-Sforza</a>
|
<a href=/people/m/muhamed-al-khalil/>Muhamed Al Khalil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3703><div class="card-body p-3 small">Advances in automatic readability assessment can impact the way people consume information in a number of domains. Arabic, being a low-resource and morphologically complex language, presents numerous challenges to the task of automatic readability assessment. In this paper, we present the largest and most in-depth computational readability study for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> to date. We study a large set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> with varying depths, from shallow words to syntactic trees, for both L1 and L2 readability tasks. Our best L1 readability accuracy result is 94.8 % (75 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> from a commonly used baseline). The comparable results for L2 are 72.4 % (45 % error reduction). We also demonstrate the added value of leveraging L1 features for L2 readability prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3704/>A Tutorial Markov Analysis of Effective Human Tutorial Sessions<span class=acl-fixed-case>M</span>arkov Analysis of Effective Human Tutorial Sessions</a></strong><br><a href=/people/n/nabin-maharjan/>Nabin Maharjan</a>
|
<a href=/people/v/vasile-rus/>Vasile Rus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3704><div class="card-body p-3 small">This paper investigates what differentiates effective tutorial sessions from less effective sessions. Towards this end, we characterize and explore human tutors&#8217; actions in tutorial dialogue sessions by mapping the tutor-tutee interactions, which are streams of dialogue utterances, into streams of actions, based on the language-as-action theory. Next, we use human expert judgment measures, evidence of learning (EL) and evidence of soundness (ES), to identify effective and ineffective sessions. We perform sub-sequence pattern mining to identify sub-sequences of dialogue modes that discriminate good sessions from bad sessions. We finally use the results of sub-sequence analysis method to generate a tutorial Markov process for effective tutorial sessions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3705/>Thank Goodness ! A Way to Measure Style in Student Essays</a></strong><br><a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3705><div class="card-body p-3 small">Essays have two major components for scoring-content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3706/>Overview of NLPTEA-2018 Share Task Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>NLPTEA</span>-2018 Share Task <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/g/gaoqi-rao/>Gaoqi Rao</a>
|
<a href=/people/q/qi-gong/>Qi Gong</a>
|
<a href=/people/b/baolin-zhang/>Baolin Zhang</a>
|
<a href=/people/e/endong-xun/>Endong Xun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3706><div class="card-body p-3 small">This paper presents the NLPTEA 2018 shared task for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Grammatical Error Diagnosis (CGED)</a> which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as foreign language. We describe the task definition, data preparation, <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a>, and evaluation results. Of the 20 teams registered for this shared task, 13 teams developed the <a href=https://en.wikipedia.org/wiki/System>system</a> and submitted a total of 32 runs. Progress in <a href=https://en.wikipedia.org/wiki/System>system</a> performances was obviously, reaching F1 of 36.12 % in position level and 25.27 % in correction level. All data sets with gold standards and scoring scripts are made publicly available to researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3708/>A Hybrid System for Chinese Grammatical Error Diagnosis and Correction<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis and Correction</a></strong><br><a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zuyi-bao/>Zuyi Bao</a>
|
<a href=/people/h/hengyou-liu/>Hengyou Liu</a>
|
<a href=/people/g/guangwei-xu/>Guangwei Xu</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3708><div class="card-body p-3 small">This paper introduces the DM_NLP team&#8217;s system for NLPTEA 2018 shared task of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Grammatical Error Diagnosis (CGED)</a>, which can be used to detect and correct grammatical errors in texts written by <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a Foreign Language (CFL) learners. This task aims at not only detecting four types of grammatical errors including redundant words (R), missing words (M), bad word selection (S) and disordered words (W), but also recommending corrections for errors of M and S types. We proposed a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> including four <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for this task with two stages : the detection stage and the correction stage. In the detection stage, we first used a BiLSTM-CRF model to tag potential errors by <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, along with some handcraft features. Then we designed three Grammatical Error Correction (GEC) models to generate corrections, which could help to tune the detection result. In the correction stage, candidates were generated by the three GEC models and then merged to output the final corrections for M and S types. Our system reached the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3709/>Ling@CASS Solution to the NLP-TEA CGED Shared Task 2018<span class=acl-fixed-case>CASS</span> Solution to the <span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>TEA</span> <span class=acl-fixed-case>CGED</span> Shared Task 2018</a></strong><br><a href=/people/q/qinan-hu/>Qinan Hu</a>
|
<a href=/people/y/yongwei-zhang/>Yongwei Zhang</a>
|
<a href=/people/f/fang-liu/>Fang Liu</a>
|
<a href=/people/y/yueguo-gu/>Yueguo Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3709><div class="card-body p-3 small">In this study, we employ the sequence to sequence learning to model the task of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammar error correction</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> takes potentially erroneous sentences as inputs, and outputs correct sentences. To breakthrough the bottlenecks of very limited size of manually labeled data, we adopt a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised approach</a>. Specifically, we adapt correct sentences written by native Chinese speakers to generate pseudo grammatical errors made by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a second language. We use the pseudo data to pre-train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and the CGED data to fine-tune it. Being aware of the significance of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> in a grammar error correction system in real scenarios, we use <a href=https://en.wikipedia.org/wiki/Assembly_language>ensembles</a> to boost <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. When using inputs as simple as <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a>, the ensembled system achieves a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> at 86.56 % in the detection of erroneous sentences, and a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> at 51.53 % in the correction of errors of Selection and Missing types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3710/>Chinese Grammatical Error Diagnosis Based on Policy Gradient LSTM Model<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Based on Policy Gradient <span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/changliang-li/>Changliang Li</a>
|
<a href=/people/j/ji-qi/>Ji Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3710><div class="card-body p-3 small">Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA2018 workshop held during ACL2018. The goal of this task is to diagnose Chinese sentences containing four kinds of <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> through the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and find out the sentence errors. Chinese grammatical error diagnosis system is a very important tool, which can help <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese learners</a> automatically diagnose <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> in many scenarios. However, due to the limitations of the Chinese language&#8217;s own characteristics and datasets, the traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> faces the problem of extreme imbalances in the positive and negative samples and the disappearance of gradients. In this paper, we propose a sequence labeling method based on the Policy Gradient LSTM model and apply it to this task to solve the above problems. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve higher precision scores in the case of lower False positive rate (FPR) and it is convenient to optimize the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on-line.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3714/>Joint learning of frequency and word embeddings for multilingual readability assessment</a></strong><br><a href=/people/d/dieu-thu-le/>Dieu-Thu Le</a>
|
<a href=/people/c/cam-tu-nguyen/>Cam-Tu Nguyen</a>
|
<a href=/people/x/xiaoliang-wang/>Xiaoliang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3714><div class="card-body p-3 small">This paper describes two models that employ word frequency embeddings to deal with the problem of readability assessment in multiple languages. The task is to determine the difficulty level of a given document, i.e., how hard it is for a reader to fully comprehend the text. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> show how <a href=https://en.wikipedia.org/wiki/Frequency>frequency information</a> can be integrated to improve the readability assessment. The experimental results testing on both English and Chinese datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> improve the results notably when comparing to those using only traditional <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3715/>MULLE : A grammar-based Latin language learning tool to supplement the classroom setting<span class=acl-fixed-case>MULLE</span>: A grammar-based <span class=acl-fixed-case>L</span>atin language learning tool to supplement the classroom setting</a></strong><br><a href=/people/h/herbert-lange/>Herbert Lange</a>
|
<a href=/people/p/peter-ljunglof/>Peter Ljunglöf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3715><div class="card-body p-3 small">MULLE is a tool for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a> that focuses on teaching Latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> distinct from other <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning tools</a> that provide standalone learning experience. It uses grammar-based lessons and embraces methods of <a href=https://en.wikipedia.org/wiki/Gamification>gamification</a> to improve the learner motivation. The main type of exercise provided by our <a href=https://en.wikipedia.org/wiki/Application_software>application</a> is to practice <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, but it is also possible to shift the focus to vocabulary or morphology training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3716/>Textual Features Indicative of Writing Proficiency in Elementary School Spanish Documents<span class=acl-fixed-case>S</span>panish Documents</a></strong><br><a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/d/diana-duenas-chavez/>Diana Dueñas Chávez</a>
|
<a href=/people/a/arturo-curiel/>Arturo Curiel Díaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3716><div class="card-body p-3 small">Childhood acquisition of written language is not straightforward. Writing skills evolve differently depending on external factors, such as the conditions in which children practice their productions and the quality of their instructors&#8217; guidance. This can be challenging in low-income areas, where schools may struggle to ensure ideal acquisition conditions. Developing computational tools to support the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a> may counterweight negative environmental influences ; however, few work exists on the use of <a href=https://en.wikipedia.org/wiki/Information_technology>information technologies</a> to improve childhood literacy. This work centers around the computational study of Spanish word and syllable structure in documents written by 2nd and 3rd year elementary school students. The studied texts were compared against a corpus of short stories aimed at the same age group, so as to observe whether the children tend to produce similar written patterns as the ones they are expected to interpret at their <a href=https://en.wikipedia.org/wiki/Literacy>literacy level</a>. The obtained results show some significant differences between the two kinds of texts, pointing towards possible strategies for the implementation of new <a href=https://en.wikipedia.org/wiki/Educational_software>education software</a> in support of written language acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3718 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3718/>A Short Answer Grading System in Chinese by Support Vector Approach<span class=acl-fixed-case>C</span>hinese by Support Vector Approach</a></strong><br><a href=/people/s/shih-hung-wu/>Shih-Hung Wu</a>
|
<a href=/people/w/wen-feng-shih/>Wen-Feng Shih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3718><div class="card-body p-3 small">In this paper, we report a short answer grading system in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We build a system based on standard machine learning approaches and test it with translated corpus from two publicly available corpus in English. The experiment results show similar results on two different corpus as in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3719 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3719/>From Fidelity to Fluency : <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> for Translator Training</a></strong><br><a href=/people/o/olivia-o-y-kwong/>Oi Yee Kwong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3719><div class="card-body p-3 small">This study explores the use of natural language processing techniques to enhance bilingual lexical access beyond simple equivalents, to enable translators to navigate along a wider cross-lingual lexical space and more examples showing different translation strategies, which is essential for them to learn to produce not only faithful but also fluent translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3720.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3720/>Countering Position Bias in Instructor Interventions in MOOC Discussion Forums<span class=acl-fixed-case>MOOC</span> Discussion Forums</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3720><div class="card-body p-3 small">We systematically confirm that instructors are strongly influenced by the user interface presentation of Massive Online Open Course (MOOC) discussion forums. In a large scale dataset, we conclusively show that instructor interventions exhibit strong position bias, as measured by the position where the thread appeared on the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> at the time of intervention. We measure and remove this <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, enabling unbiased statistical modelling and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. We show that our de-biased classifier improves predicting interventions over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> with sufficient number of <a href=https://en.wikipedia.org/wiki/Intervention_(counseling)>interventions</a> by 8.2 % in F1 and 24.4 % in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3722.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3722 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3722 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3722/>Learning to Automatically Generate Fill-In-The-Blank Quizzes</a></strong><br><a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/a/ai-nakajima/>Ai Nakajima</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a>
|
<a href=/people/o/ono-yuichi/>Ono Yuichi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3722><div class="card-body p-3 small">In this paper we formalize the problem automatic fill-in-the-blank question generation using two standard NLP machine learning schemes, proposing concrete deep learning models for each. We present an empirical study based on data obtained from a <a href=https://en.wikipedia.org/wiki/Machine_learning>language learning platform</a> showing that both of our proposed settings offer promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3723 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3723/>Multilingual Short Text Responses Clustering for Mobile Educational Activities : a Preliminary Exploration</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/y/yu-ta-chien/>Yu-Ta Chien</a>
|
<a href=/people/c/chun-yen-chang/>Chun-Yen Chang</a>
|
<a href=/people/t/tsung-yen-li/>Tsung-Yen Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3723><div class="card-body p-3 small">Text clustering is a powerful technique to detect topics from <a href=https://en.wikipedia.org/wiki/Text_corpus>document corpora</a>, so as to provide information browsing, <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a>, and <a href=https://en.wikipedia.org/wiki/Organization>organization</a>. On the other hand, the Instant Response System (IRS) has been widely used in recent years to enhance student engagement in class and thus improve their learning effectiveness. However, the lack of functions to process short text responses from the <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a> prevents the further application of <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a> in classes. Therefore, this study aims to propose a proper short text clustering module for the <a href=https://en.wikipedia.org/wiki/Internal_Revenue_Service>IRS</a>, and demonstrate our implemented techniques through real-world examples, so as to provide experiences and insights for further study. In particular, we have compared three <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering methods</a> and the result shows that theoretically better methods need not lead to better results, as there are various factors that may affect the final performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3724 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3724/>Chinese Grammatical Error Diagnosis Based on CRF and LSTM-CRF model<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis Based on <span class=acl-fixed-case>CRF</span> and <span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> model</a></strong><br><a href=/people/y/yujie-zhou/>Yujie Zhou</a>
|
<a href=/people/y/yinan-shao/>Yinan Shao</a>
|
<a href=/people/y/yong-zhou/>Yong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3724><div class="card-body p-3 small">When learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language, the learners may have some <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> due to negative migration of their native languages. However, few grammar checking applications have been developed to support the learners. The goal of this paper is to develop a tool to automatically diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W) in Chinese sentences written by those foreign learners. In this paper, a conventional linear CRF model with specific <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and a LSTM-CRF model are used to solve the CGED (Chinese Grammatical Error Diagnosis) task. We make some improvement on both models and the submitted results have better performance on <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than the average of all runs from CGED2018 for all three evaluation levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3727 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3727/>Detecting Simultaneously Chinese Grammar Errors Based on a BiLSTM-CRF Model<span class=acl-fixed-case>C</span>hinese Grammar Errors Based on a <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model</a></strong><br><a href=/people/y/yajun-liu/>Yajun Liu</a>
|
<a href=/people/h/hongying-zan/>Hongying Zan</a>
|
<a href=/people/m/mengjie-zhong/>Mengjie Zhong</a>
|
<a href=/people/h/hongchao-ma/>Hongchao Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3727><div class="card-body p-3 small">In the process of learning and using <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, many learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese as foreign language(CFL)</a> may have <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammar errors</a> due to negative migration of their native languages. This paper introduces our system that can simultaneously diagnose four types of grammatical errors including redundant (R), missing (M), selection (S), disorder (W) in NLPTEA-5 shared task. We proposed a Bidirectional LSTM CRF neural network (BiLSTM-CRF) that combines BiLSTM and CRF without hand-craft features for Chinese Grammatical Error Diagnosis (CGED). Evaluation includes three levels, which are detection level, identification level and position level. At the detection level and identification level, our <a href=https://en.wikipedia.org/wiki/System>system</a> got the third recall scores, and achieved good <a href=https://en.wikipedia.org/wiki/F-number>F1 values</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3728/>A Hybrid Approach Combining Statistical Knowledge with <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> for Chinese Grammatical Error Detection<span class=acl-fixed-case>C</span>hinese Grammatical Error Detection</a></strong><br><a href=/people/y/yiyi-wang/>Yiyi Wang</a>
|
<a href=/people/c/chilin-shih/>Chilin Shih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3728><div class="card-body p-3 small">This paper presents a method of combining Conditional Random Fields (CRFs) model with a post-processing layer using Google n-grams statistical information tailored to detect word selection and word order errors made by learners of Chinese as Foreign Language (CFL). We describe the architecture of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and its performance in the shared task of the ACL 2018 Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA). This hybrid approach yields comparably high <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a> (FPR = 0.1274) and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> (Pd= 0.7519 ; Pi= 0.6311), but low <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> (Rd = 0.3035 ; Ri = 0.1696) in grammatical error detection and identification tasks. Additional <a href=https://en.wikipedia.org/wiki/Statistics>statistical information</a> and <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a> can be added to enhance the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3729.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3729 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3729 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3729/>CYUT-III Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2018 CGED Shared Task<span class=acl-fixed-case>CYUT</span>-<span class=acl-fixed-case>III</span> Team <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis System Report in <span class=acl-fixed-case>NLPTEA</span>-2018 <span class=acl-fixed-case>CGED</span> Shared Task</a></strong><br><a href=/people/s/shih-hung-wu/>Shih-Hung Wu</a>
|
<a href=/people/j/jun-wei-wang/>Jun-Wei Wang</a>
|
<a href=/people/l/liang-pu-chen/>Liang-Pu Chen</a>
|
<a href=/people/p/ping-che-yang/>Ping-Che Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3729><div class="card-body p-3 small">This paper reports how we build a Chinese Grammatical Error Diagnosis system in the NLPTEA-2018 CGED shared task. In 2018, we sent three runs with three different <a href=https://en.wikipedia.org/wiki/Pitch_(sports_field)>approaches</a>. The first one is a pattern-based approach by frequent error pattern matching. The second one is a sequential labelling approach by conditional random fields (CRF). The third one is a rewriting approach by sequence to sequence (seq2seq) model. The three approaches have different properties that aim to optimize different <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> and the formal run results show the differences as we expected.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>