<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>The Chinese National Conference on Computational Linguistics (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>The Chinese National Conference on Computational Linguistics (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020ccl-1>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=2020ccl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.ccl-1/>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.0/>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a></strong><br><a href=/people/m/maosong-sun/>Maosong Sun (孙茂松)</a>
|
<a href=/people/s/sujian-li/>Sujian Li (李素建)</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang (张岳)</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu (刘洋)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.76/>A Joint Model for Graph-based Chinese Dependency Parsing<span class=acl-fixed-case>C</span>hinese Dependency Parsing</a></strong><br><a href=/people/x/xingchen-li/>Xingchen Li</a>
|
<a href=/people/m/mingtong-liu/>Mingtong Liu</a>
|
<a href=/people/y/yujie-zhang/>Yujie Zhang</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--76><div class="card-body p-3 small">In Chinese dependency parsing, the joint model of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, POS tagging and dependency parsing has become the mainstream framework because it can eliminate error propagation and share knowledge, where the transition-based model with feature templates maintains the best performance. Recently, the graph-based joint model (Yan et al., 2019) on <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and dependency parsing has achieved better performance, demonstrating the advantages of the graph-based models. However, this work can not provide POS information for downstream tasks, and the POS tagging task was proved to be helpful to the dependency parsing according to the research of the transition-based model. Therefore, we propose a graph-based joint model for <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>, <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> and dependency parsing. We designed a charater-level POS tagging task, and then train it jointly with the model of Yan et al. We adopt two methods of joint POS tagging task, one is by sharing parameters, the other is by using tag attention mechanism, which enables the three tasks to better share intermediate information and improve each other&#8217;s performance. The experimental results on the Penn Chinese treebank (CTB5) show that our proposed joint model improved by 0.38 % on dependency parsing than the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of Yan et al. Compared with the best transition-based joint model, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> improved by 0.18 %, 0.35 % and 5.99 % respectively in terms of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> and dependency parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.77/>Semantic-aware Chinese Zero Pronoun Resolution with Pre-trained Semantic Dependency Parser<span class=acl-fixed-case>C</span>hinese Zero Pronoun Resolution with Pre-trained Semantic Dependency Parser</a></strong><br><a href=/people/l/lanqiu-zhang/>Lanqiu Zhang</a>
|
<a href=/people/z/zizhuo-shen/>Zizhuo Shen</a>
|
<a href=/people/y/yanqiu-shao/>Yanqiu Shao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--77><div class="card-body p-3 small">Deep learning-based Chinese zero pronoun resolution model has achieved better performance than traditional machine learning-based model. However, the existing work related to Chinese zero pronoun resolution has not yet well integrated linguistic information into the deep learningbased Chinese zero pronoun resolution model. This paper adopts the idea based on the pre-trained model, and integrates the semantic representations in the pre-trained Chinese semantic dependency graph parser into the Chinese zero pronoun resolution model. The experimental results on OntoNotes-5.0 dataset show that our proposed Chinese zero pronoun resolution model with pretrained Chinese semantic dependency parser improves the F-score by 0.4 % compared with our baseline model, and obtains better results than other deep learning-based Chinese zero pronoun resolution models. In addition, we integrate the BERT representations into our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> so that the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was improved by 0.7 % compared with our baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.82/>Refining Data for <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>Text Generation</a></a></strong><br><a href=/people/w/wenyu-guan/>Wenyu Guan</a>
|
<a href=/people/q/qianying-liu/>Qianying Liu</a>
|
<a href=/people/t/tianyi-li/>Tianyi Li</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--82><div class="card-body p-3 small">Recent work on data-to-text generation has made progress under the neural encoder-decoder architectures. However, the data input size is often enormous, while not all data records are important for text generation and inappropriate input may bring noise into the final output. To solve this problem, we propose a two-step approach which first selects and orders the important data records and then generates text from the noise-reduced data. Here we propose a learning to rank model to rank the importance of each record which is supervised by a relation extractor. With the noise-reduced data as input, we implement a <a href=https://en.wikipedia.org/wiki/Text_generator>text generator</a> which sequentially models the input data records and emits a summary. Experiments on the ROTOWIRE dataset verifies the effectiveness of our proposed method in both performance and efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.86/>Chinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism<span class=acl-fixed-case>C</span>hinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--86><div class="card-body p-3 small">Named entity recognition (NER) aims to identify text spans that mention named entities and classify them into pre-defined categories. For Chinese NER task, most of the existing methods are character-based sequence labeling models and achieve great success. However, these methods usually ignore <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical knowledge</a>, which leads to false prediction of entity boundaries. Moreover, these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> have difficulties in capturing tag dependencies. In this paper, we propose an Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism (AMMNHT) to address all above problems. Specifically, to reduce the errors of predicting entity boundaries, we propose an adaptive multi-pass memory network to exploit lexical knowledge. In addition, we propose a hierarchical tagging layer to learn tag dependencies. Experimental results on three widely used Chinese NER datasets demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.89/>Entity Relative Position Representation based Multi-head Selection for Joint Entity and Relation Extraction</a></strong><br><a href=/people/t/tianyang-zhao/>Tianyang Zhao</a>
|
<a href=/people/z/zhao-yan/>Zhao Yan</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--89><div class="card-body p-3 small">Joint entity and relation extraction has received increasing interests recently, due to the capability of utilizing the interactions between both steps. Among existing studies, the Multi-Head Selection (MHS) framework is efficient in extracting entities and relations simultaneously. However, the method is weak for its limited performance. In this paper, we propose several effective insights to address this problem. First, we propose an entity-specific Relative Position Representation (eRPR) to allow the model to fully leverage the distance information between entities and context tokens. Second, we introduce an auxiliary Global Relation Classification (GRC) to enhance the learning of local contextual features. Moreover, we improve the semantic representation by adopting a pre-trained language model BERT as the feature encoder. Finally, these new keypoints are closely integrated with the multi-head selection framework and optimized jointly. Extensive experiments on two benchmark datasets demonstrate that our approach overwhelmingly outperforms previous works in terms of all evaluation metrics, achieving significant improvements for relation F1 by +2.40 % on CoNLL04 and +1.90 % on ACE05, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.92/>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a></strong><br><a href=/people/x/xiuhong-li/>Xiuhong Li</a>
|
<a href=/people/z/zhe-li/>Zhe Li</a>
|
<a href=/people/j/jiabao-sheng/>Jiabao Sheng</a>
|
<a href=/people/w/wushour-slamu/>Wushour Slamu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--92><div class="card-body p-3 small">Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. For low-resource agglutinative languages including <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a>, <a href=https://en.wikipedia.org/wiki/Kazakh_language>Kazakh</a>, and Kyrgyz (UKK languages), in which words are manufactured via <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> concatenated with several suffixes and <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> are used as the representation of text content, this feature allows infinite derivatives vocabulary that leads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. It is an effective solution which fine-tuning a pre-trained language model to provide meaningful and favorable-to-use <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractors</a> for downstream text classification tasks. To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based fine-tuning strategy that better selects relevant semantic and syntactic information from the pre-trained language model and uses those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> on downstream text classification tasks. We evaluate our methods on nine Uyghur, Kazakh, and Kyrgyz classification datasets, where they have significantly better performance compared with several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.93/>Constructing Uyghur Name Entity Recognition System using Neural Machine Translation Tag Projection<span class=acl-fixed-case>U</span>yghur Name Entity Recognition System using Neural Machine Translation Tag Projection</a></strong><br><a href=/people/a/anwar-azmat/>Anwar Azmat</a>
|
<a href=/people/l/li-xiao/>Li Xiao</a>
|
<a href=/people/y/yang-yating/>Yang Yating</a>
|
<a href=/people/d/dong-rui/>Dong Rui</a>
|
<a href=/people/o/osman-turghun/>Osman Turghun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--93><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> achieved great success by introducing the <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, it is challenging to apply these models to low resource languages including <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a> while it depends on a large amount of annotated training data. Constructing a well-annotated named entity corpus manually is very time-consuming and labor-intensive. Most existing methods based on the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> combined with the word alignment tools. However, word alignment methods introduce alignment errors inevitably. In this paper, we address this problem by a named entity tag transfer method based on the common neural machine translation. The proposed method marks the entity boundaries in Chinese sentence and translates the sentences to <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a> by <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation system</a>, hope that <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> will align the source and target entity by the self-attention mechanism. The experimental results show that the Uyghur named entity recognition system trained by the constructed corpus achieve good performance on the test set, with 73.80 % F1 score(3.79 % improvement by baseline)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.95/>Mongolian Questions Classification Based on Mulit-Head Attention<span class=acl-fixed-case>M</span>ongolian Questions Classification Based on Mulit-Head Attention</a></strong><br><a href=/people/g/guangyi-wang/>Guangyi Wang</a>
|
<a href=/people/f/feilong-bao/>Feilong Bao</a>
|
<a href=/people/w/weihua-wang/>Weihua Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--95><div class="card-body p-3 small">Question classification is a crucial subtask in <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a>. Mongolian is a kind of few resource language. It lacks public labeled corpus. And the complex morphological structure of <a href=https://en.wikipedia.org/wiki/Mongolian_language>Mongolian vocabulary</a> makes the data-sparse problem. This paper proposes a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>, which combines the Bi-LSTM model with the Multi-Head Attention mechanism. The Multi-Head Attention mechanism extracts relevant information from different dimensions and representation subspace. According to the characteristics of Mongolian word-formation, this paper introduces Mongolian morphemes representation in the embedding layer. Morpheme vector focuses on the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the Mongolian word. In this paper, character vector and morpheme vector are concatenated to get <a href=https://en.wikipedia.org/wiki/Word_vector>word vector</a>, which sends to the Bi-LSTM getting context representation. Finally, the Multi-Head Attention obtains global information for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> experimented on the <a href=https://en.wikipedia.org/wiki/Mongolian_writing_systems>Mongolian corpus</a>. Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms baseline systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.97/>Categorizing Offensive Language in <a href=https://en.wikipedia.org/wiki/Social_network>Social Networks</a> : A Chinese Corpus, Systems and an Explainable Tool<span class=acl-fixed-case>C</span>hinese Corpus, Systems and an Explainable Tool</a></strong><br><a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/x/xianjun-shen/>Xianjun Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--97><div class="card-body p-3 small">Recently, more and more data have been generated in the online world, filled with <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> such as <a href=https://en.wikipedia.org/wiki/Threat>threats</a>, <a href=https://en.wikipedia.org/wiki/Profanity>swear words</a> or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.ccl-1.98" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.98/>LiveQA : A Question Answering Dataset over Sports Live<span class=acl-fixed-case>L</span>ive<span class=acl-fixed-case>QA</span>: A Question Answering Dataset over Sports Live</a></strong><br><a href=/people/l/liu-qianying/>Liu Qianying</a>
|
<a href=/people/j/jiang-sicong/>Jiang Sicong</a>
|
<a href=/people/w/wang-yizhong/>Wang Yizhong</a>
|
<a href=/people/l/li-sujian/>Li Sujian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--98><div class="card-body p-3 small">In this paper, we introduce LiveQA, a new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu1 website. Derived from the characteristics of <a href=https://en.wikipedia.org/wiki/Sports_game>sports games</a>, LiveQA can potentially test the reasoning ability across timeline-based live broadcasts, which is challenging compared to the existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In LiveQA, the questions require understanding the <a href=https://en.wikipedia.org/wiki/Timeline>timeline</a>, tracking events or doing <a href=https://en.wikipedia.org/wiki/Computation>mathematical computations</a>. Our preliminary experiments show that the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> introduces a challenging problem for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering models</a>, and a strong baseline model only achieves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 53.1 % and can not beat the dominant option rule. We release the code and data of this paper for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.102/>CAN-GRU : a Hierarchical Model for Emotion Recognition in Dialogue<span class=acl-fixed-case>CAN</span>-<span class=acl-fixed-case>GRU</span>: a Hierarchical Model for Emotion Recognition in Dialogue</a></strong><br><a href=/people/t/ting-jiang/>Ting Jiang</a>
|
<a href=/people/b/bing-xu/>Bing Xu</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a>
|
<a href=/people/s/sheng-li/>Sheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--102><div class="card-body p-3 small">Emotion recognition in dialogue systems has gained attention in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> recent years, because it can be applied in <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> from public conversational data on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Hierarchical_model>hierarchical model</a> to recognize <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> in the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. In the first layer, in order to extract textual features of utterances, we propose a convolutional self-attention network(CAN). Convolution is used to capture <a href=https://en.wikipedia.org/wiki/N-gram>n-gram information</a> and attention mechanism is used to obtain the relevant semantic information among words in the utterance. In the second layer, a GRU-based network helps to capture <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> in the conversation. Furthermore, we discuss the effects of unidirectional and bidirectional networks. We conduct experiments on Friends dataset and EmotionPush dataset. The results show that our proposed model(CAN-GRU) and its variants achieve better performance than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.103/>A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer</a></strong><br><a href=/people/y/yuncong-li/>Yuncong Li</a>
|
<a href=/people/z/zhe-yang/>Zhe Yang</a>
|
<a href=/people/c/cunxiang-yin/>Cunxiang Yin</a>
|
<a href=/people/x/xu-pan/>Xu Pan</a>
|
<a href=/people/l/lunan-cui/>Lunan Cui</a>
|
<a href=/people/q/qiang-huang/>Qiang Huang</a>
|
<a href=/people/t/ting-wei/>Ting Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--103><div class="card-body p-3 small">Aspect-category sentiment analysis (ACSA) aims to predict the <a href=https://en.wikipedia.org/wiki/Aspect_(grammar)>aspect categories</a> mentioned in texts and their corresponding <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment polarities</a>. Some joint models have been proposed to address this task. Given a text, these joint models detect all the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect categories</a> mentioned in the text and predict the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment polarities</a> toward them at once. Although these joint models obtain promising performances, they train separate parameters for each aspect category and therefore suffer from data deficiency of some aspect categories. To solve this problem, we propose a novel joint model which contains a shared sentiment prediction layer. The shared sentiment prediction layer transfers <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment knowledge</a> between aspect categories and alleviates the problem caused by data deficiency. Experiments conducted on SemEval-2016 Datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ccl-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ccl-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ccl-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ccl-1.106/>Clickbait Detection with Style-aware Title Modeling and Co-attention</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ccl-1--106><div class="card-body p-3 small">Clickbait is a form of <a href=https://en.wikipedia.org/wiki/Web_content>web content</a> designed to attract attention and entice users to click on specific hyperlinks. The detection of clickbaits is an important task for online platforms to improve the quality of web content and the satisfaction of users. Clickbait detection is typically formed as a binary classification task based on the title and body of a webpage, and existing methods are mainly based on the content of title and the relevance between title and body. However, these methods ignore the stylistic patterns of titles, which can provide important clues on identifying <a href=https://en.wikipedia.org/wiki/Clickbait>clickbaits</a>. In addition, they do not consider the interactions between the contexts within title and body, which are very important for measuring their <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> for clickbait detection. In this paper, we propose a clickbait detection approach with style-aware title modeling and co-attention. Specifically, we use Transformers to learn content representations of title and body, and respectively compute two content-based clickbait scores for title and body based on their <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. In addition, we propose to use a character-level Transformer to learn a style-aware title representation by capturing the stylistic patterns of title, and we compute a title stylistic score based on this representation. Besides, we propose to use a co-attention network to model the relatedness between the contexts within title and body, and further enhance their representations by encoding the interaction information. We compute a title-body matching score based on the representations of title and body enhanced by their interactions. The final clickbait score is predicted by a weighted summation of the aforementioned four kinds of scores.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>