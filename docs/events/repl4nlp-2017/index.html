<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Representation Learning for NLP (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Representation Learning for NLP (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w17-26>Proceedings of the 2nd Workshop on Representation Learning for NLP</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li></ul></div></div><div id=w17-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-26/>Proceedings of the 2nd Workshop on Representation Learning for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2600/>Proceedings of the 2nd Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/shay-b-cohen/>Shay Cohen</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/e/edward-grefenstette/>Edward Grefenstette</a>
|
<a href=/people/k/karl-moritz-hermann/>Karl Moritz Hermann</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2601/>Sense Contextualization in a Dependency-Based Compositional Distributional Model</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2601><div class="card-body p-3 small">Little attention has been paid to distributional compositional methods which employ syntactically structured vector models. As word vectors belonging to different <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a> have incompatible syntactic distributions, no trivial compositional operation can be applied to combine them into a new compositional vector. In this article, we generalize the method described by Erk and Pad (2009) by proposing a dependency-base framework that contextualize not only lemmas but also selectional preferences. The main contribution of the article is to expand their <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to a fully compositional framework in which syntactic dependencies are put at the core of semantic composition. We claim that semantic composition is mainly driven by syntactic dependencies. Each syntactic dependency generates two new compositional vectors representing the contextualized sense of the two related lemmas. The sequential application of the compositional operations associated to the dependencies results in as many contextualized vectors as lemmas the composite expression contains. At the end of the semantic process, we do not obtain a single compositional vector representing the semantic denotation of the whole composite expression, but one contextualized vector for each lemma of the whole expression. Our method avoids the troublesome high-order tensor representations by defining lemmas and selectional restrictions as <a href=https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)>first-order tensors</a> (i.e. standard vectors). A corpus-based experiment is performed to both evaluate the quality of the compositional vectors built with our strategy, and to compare them to other approaches on distributional compositional semantics. The experiments show that our dependency-based compositional method performs as (or even better than) the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2602" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2602/>Context encoders as a simple but powerful extension of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a></a></strong><br><a href=/people/f/franziska-horn/>Franziska Horn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2602><div class="card-body p-3 small">With a strikingly simple architecture and the ability to learn meaningful <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> efficiently from texts containing billions of words, <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> remains one of the most popular neural language models used today. However, as only a single <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model&#8217;s negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word&#8217;s average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words&#8217; local contexts. The benefits of this approach are illustrated by using these <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> in the CoNLL 2003 named entity recognition (NER) task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2603" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2603/>Machine Comprehension by Text-to-Text Neural Question Generation</a></strong><br><a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/t/tong-wang/>Tong Wang</a>
|
<a href=/people/c/caglar-gulcehre/>Caglar Gulcehre</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/p/philip-bachman/>Philip Bachman</a>
|
<a href=/people/s/saizheng-zhang/>Saizheng Zhang</a>
|
<a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2603><div class="card-body p-3 small">We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using a combination of supervised and reinforcement learning. After teacher forcing for standard <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood training</a>, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering system</a>. We motivate question generation as a means to improve the performance of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a>. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained and evaluated on the recent question-answering dataset SQuAD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2604/>Emergent Predication Structure in Hidden State Vectors of Neural Readers</a></strong><br><a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/t/takashi-onishi/>Takeshi Onishi</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/d/david-mcallester/>David McAllester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2604><div class="card-body p-3 small">A significant number of neural architectures for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>predication structure</a> in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas [ c ] where is a semantic property (predicate) and c is a constant symbol entity identifier.<tex-math>\\Phi[c]</tex-math> where <tex-math>\\Phi</tex-math> is a semantic property (predicate) and <tex-math>c</tex-math> is a constant symbol entity identifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2606/>Combining Word-Level and Character-Level Representations for Relation Classification of Informal Text</a></strong><br><a href=/people/d/dongyun-liang/>Dongyun Liang</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/y/yinge-zhao/>Yinge Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2606><div class="card-body p-3 small">Word representation models have achieved great success in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Binary_relation>relation classification</a>. However, it does not always work on informal text, and the morphemes of some misspelling words may carry important short-distance semantic information. We propose a hybrid model, combining the merits of word-level and character-level representations to learn better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> on informal text. Experiments on two dataset of relation classification, SemEval-2010 Task8 and a large-scale one we compile from informal text, show that our model achieves a competitive result in the former and state-of-the-art with the other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2607/>Transfer Learning for Neural Semantic Parsing</a></strong><br><a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/e/emilio-monti/>Emilio Monti</a>
|
<a href=/people/l/lambert-mathias/>Lambert Mathias</a>
|
<a href=/people/m/markus-dreyer/>Markus Dreyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2607><div class="card-body p-3 small">The goal of <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is to map <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is the lack of sufficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>annotation training data</a>. In this paper, we propose using sequence-to-sequence in a multi-task setup for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> with focus on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Our experiments show that the multi-task setup aids <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from an auxiliary task with large labeled data to the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with smaller labeled data. We see an absolute accuracy gain ranging from 1.0 % to 4.4 % in in our in-house data set and we also see good gains ranging from 2.5 % to 7.0 % on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2608/>Modeling Large-Scale Structured Relationships with <a href=https://en.wikipedia.org/wiki/Shared_memory>Shared Memory</a> for Knowledge Base Completion</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2608><div class="card-body p-3 small">Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, learning multi-step relations directly on top of observed triplets could be costly. Hence, a manually designed procedure is often used when training the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a <a href=https://en.wikipedia.org/wiki/Controller_(computing)>controller</a> and <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a>. Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a> and controller. While the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> does not explicitly operate on top of observed triplets, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all previous <a href=https://en.wikipedia.org/wiki/Statistical_inference>approaches</a> on the popular FB15k benchmark by more than 5.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2610/>Sequential Attention : A Context-Aware Alignment Function for <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a></a></strong><br><a href=/people/s/sebastian-brarda/>Sebastian Brarda</a>
|
<a href=/people/p/philip-yeres/>Philip Yeres</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2610><div class="card-body p-3 small">In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> (on the Who did What and CNN datasets) and show that it dramatically improves a strong baselinethe Stanford Readerand is competitive with the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2611/>Semantic Vector Encoding and <a href=https://en.wikipedia.org/wiki/Similarity_search>Similarity Search</a> Using Fulltext Search Engines</a></strong><br><a href=/people/j/jan-rygl/>Jan Rygl</a>
|
<a href=/people/j/jan-pomikalek/>Jan Pomikálek</a>
|
<a href=/people/r/radim-rehurek/>Radim Řehůřek</a>
|
<a href=/people/m/michal-ruzicka/>Michal Růžička</a>
|
<a href=/people/v/vit-novotny/>Vít Novotný</a>
|
<a href=/people/p/petr-sojka/>Petr Sojka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2611><div class="card-body p-3 small">Vector representations and vector space modeling (VSM) play a central role in modern <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We propose a novel approach to &#8216;vector similarity searching&#8217; over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiquity. We show that this approach allows the indexing and querying of dense vectors in text domains. This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring. The end result is a fast and scalable vector database with a tunable trade-off between vector search performance and quality, backed by a standard <a href=https://en.wikipedia.org/wiki/Full-text_search>fulltext engine</a> such as <a href=https://en.wikipedia.org/wiki/Elasticsearch>Elasticsearch</a>. We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2612/>Multi-task Domain Adaptation for Sequence Tagging</a></strong><br><a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2612><div class="card-body p-3 small">Many domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains. Traditional <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> only considers adapting for one task. In this paper, we explore multi-task representation learning under the domain adaptation scenario. We propose a neural network framework that supports <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for multiple tasks simultaneously, and learns shared representations that better generalize for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. We apply the proposed framework to <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for sequence tagging problems considering two tasks : <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task, and achieves the state-of-the-art results for both tasks in the <a href=https://en.wikipedia.org/wiki/Social_media>social media domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2613/>Beyond Bilingual : Multi-sense Word Embeddings using Multilingual Context</a></strong><br><a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/m/matt-taddy/>Matt Taddy</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a>
|
<a href=/people/j/james-zou/>James Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2613><div class="card-body p-3 small">Word embeddings, which represent a word as a point in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense wor d embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual model</a> trained on five times more training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2617/>Learning Bilingual Projections of Embeddings for Vocabulary Expansion in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2617><div class="card-body p-3 small">We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the EnglishSpanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 <a href=https://en.wikipedia.org/wiki/Point_(typography)>BLEU points</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2618/>Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with Frame Embeddings<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Hierarchy with Frame Embeddings</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/h/hatem-mousselly-sergieh/>Hatem Mousselly-Sergieh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2618><div class="card-body p-3 small">Automatic completion of frame-to-frame (F2F) relations in the FrameNet (FN) hierarchy has received little attention, although they incorporate meta-level commonsense knowledge and are used in downstream approaches. We address the problem of sparsely annotated F2F relations. First, we examine whether the manually defined F2F relations emerge from text by learning text-based frame embeddings. Our analysis reveals insights about the difficulty of reconstructing F2F relations purely from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. Second, we present different systems for predicting F2F relations ; our best-performing one uses the FN hierarchy to train on and to ground embeddings in. A comparison of systems and <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> exposes the crucial influence of knowledge-based embeddings to a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance in predicting F2F relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2619/>Learning Joint Multilingual Sentence Representations with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/m/matthijs-douze/>Matthijs Douze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2619><div class="card-body p-3 small">In this paper, we use the framework of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to learn joint sentence representations across six very different languages. Our aim is that a <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> which is independent of the language, is likely to capture the underlying <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We define a new cross-lingual similarity measure, compare up to 1.4 M <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2621/>Gradual Learning of Matrix-Space Models of Language for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/s/shima-asaadi/>Shima Asaadi</a>
|
<a href=/people/s/sebastian-rudolph/>Sebastian Rudolph</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2621><div class="card-body p-3 small">Learning word representations to capture the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and compositionality of language has received much research interest in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Beyond the popular vector space models, matrix representations for words have been proposed, since then, <a href=https://en.wikipedia.org/wiki/Matrix_multiplication>matrix multiplication</a> can serve as natural composition operation. In this work, we investigate the problem of learning <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix representations of words</a>. We present a learning approach for compositional matrix-space models for the task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We show that our approach, which learns the matrices gradually in two steps, outperforms other approaches and a gradient-descent baseline in terms of <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> and <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2622/>Improving <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> using Densely Connected Recurrent Neural Networks</a></strong><br><a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/j/joni-dambre/>Joni Dambre</a>
|
<a href=/people/w/wesley-de-neve/>Wesley De Neve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2622><div class="card-body p-3 small">In this paper, we introduce the novel concept of densely connected layers into <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al., 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2623/>NewsQA : A Machine Comprehension Dataset<span class=acl-fixed-case>N</span>ews<span class=acl-fixed-case>QA</span>: A Machine Comprehension Dataset</a></strong><br><a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/t/tong-wang/>Tong Wang</a>
|
<a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/j/justin-harris/>Justin Harris</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/p/philip-bachman/>Philip Bachman</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2623><div class="card-body p-3 small">We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> from <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>, with answers consisting of spans of text in the articles. We collect this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> through a four-stage process designed to solicit exploratory questions that require <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure <a href=https://en.wikipedia.org/wiki/Human>human</a> performance on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and compare it to several strong neural models. The performance gap between <a href=https://en.wikipedia.org/wiki/Human>humans</a> and <a href=https://en.wikipedia.org/wiki/Machine>machines</a> (13.3 % F1) indicates that significant progress can be made on NewsQA through future research. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is freely available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2624/>Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams<span class=acl-fixed-case>T</span>witter Streams</a></strong><br><a href=/people/l/lawrence-phillips/>Lawrence Phillips</a>
|
<a href=/people/k/kyle-shaffer/>Kyle Shaffer</a>
|
<a href=/people/d/dustin-arendt/>Dustin Arendt</a>
|
<a href=/people/n/nathan-hodas/>Nathan Hodas</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2624><div class="card-body p-3 small">Language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the <a href=https://en.wikipedia.org/wiki/Twitter>corpus of tweets</a> collected from European countries over a two-week period centered around the <a href=https://en.wikipedia.org/wiki/2016_Brussels_bombings>terrorist attacks</a> in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2625.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2625/>Rethinking Skip-thought : A Neighborhood based Approach</a></strong><br><a href=/people/s/shuai-tang/>Shuai Tang</a>
|
<a href=/people/h/hailin-jin/>Hailin Jin</a>
|
<a href=/people/c/chen-fang/>Chen Fang</a>
|
<a href=/people/z/zhaowen-wang/>Zhaowen Wang</a>
|
<a href=/people/v/virginia-de-sa/>Virginia de Sa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2625><div class="card-body p-3 small">We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification benchmarks</a>. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> did n&#8217;t aid our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform better, while it hurts the performance of the skip-thought model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2629/>Adversarial Generation of Natural Language</a></strong><br><a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/s/sai-rajeswar/>Sai Rajeswar</a>
|
<a href=/people/f/francis-dutil/>Francis Dutil</a>
|
<a href=/people/c/christopher-pal/>Chris Pal</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2629><div class="card-body p-3 small">Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2630 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2630/>Deep Active Learning for Named Entity Recognition</a></strong><br><a href=/people/y/yanyao-shen/>Yanyao Shen</a>
|
<a href=/people/h/hyokun-yun/>Hyokun Yun</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary Lipton</a>
|
<a href=/people/y/yakov-kronrod/>Yakov Kronrod</a>
|
<a href=/people/a/animashree-anandkumar/>Animashree Anandkumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2630><div class="card-body p-3 small">Deep neural networks have advanced the state of the art in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. However, under typical <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedures</a>, advantages over <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>classical methods</a> emerge only with <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a>. As a result, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> is employed only when large public datasets or a large budget for manually labeling data is available. In this work, we show otherwise : by combining <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> with active learning, we can outperform classical methods even with a significantly smaller amount of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2631 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2631/>Learning when to skim and when to read</a></strong><br><a href=/people/a/alexander-johansen/>Alexander Johansen</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2631><div class="card-body p-3 small">Many recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> have come at increasing <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>, but the power of these state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> is not needed for every example in a dataset. We demonstrate two approaches to reducing unnecessary computation in cases where a fast but weak baseline classier and a stronger, slower model are both available. Applying an AUC-based metric to the task of sentiment classification, we find significant efficiency gains with both a probability-threshold method for reducing computational cost and one that uses a secondary decision network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2632/>Learning to Embed Words in Context for Syntactic Tasks</a></strong><br><a href=/people/l/lifu-tu/>Lifu Tu</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2632><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic category</a>, and <a href=https://en.wikipedia.org/wiki/Semantic_role>semantic role</a>. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a> and dependency parsers trained on much smaller amounts of annotated data. We find that <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>predictors</a> endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>