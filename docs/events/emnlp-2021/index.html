<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021emnlp-main>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">347&nbsp;papers</span></li><li><a class=align-middle href=#2021emnlp-demo>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021emnlp-tutorials>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021argmining-1>Proceedings of the 8th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021blackboxnlp-1>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#2021cinlp-1>Proceedings of the First Workshop on Causal Inference and NLP</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021codi-main>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021codi-sharedtask>Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021conll-1>Proceedings of the 25th Conference on Computational Natural Language Learning</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#2021crac-1>Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021disrpt-1>Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021eancs-1>The First Workshop on Evaluations and Assessments of Neural Conversation Systems</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021econlp-1>Proceedings of the Third Workshop on Economics and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021eval4nlp-1>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#2021fever-1>Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021insights-1>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021latechclfl-1>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2021law-1>Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021mrl-1>Proceedings of the 1st Workshop on Multilingual Representation Learning</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021mrqa-1>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021newsum-1>Proceedings of the Third Workshop on New Frontiers in Summarization</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021nllp-1>Proceedings of the Natural Legal Language Processing Workshop 2021</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021nlp4convai-1>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021sustainlp-1>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2021wmt-1>Proceedings of the Sixth Conference on Machine Translation</a>
<span class="badge badge-info align-middle ml-1">52&nbsp;papers</span></li><li><a class=align-middle href=#2021wnut-1>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li></ul></div></div><div id=2021emnlp-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.emnlp-main/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.0/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/w/wen-tau-yih/>Scott Wen-tau Yih</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.11/>Multiplex Graph Neural Network for Extractive Text Summarization</a></strong><br><a href=/people/b/baoyu-jing/>Baoyu Jing</a>
|
<a href=/people/z/zeyu-you/>Zeyu You</a>
|
<a href=/people/t/tao-yang/>Tao Yang</a>
|
<a href=/people/w/wei-fan/>Wei Fan</a>
|
<a href=/people/h/hanghang-tong/>Hanghang Tong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--11><div class="card-body p-3 small">Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a> plays an important role. Recent studies have leveraged <a href=https://en.wikipedia.org/wiki/Graph_theory>graph neural networks</a> to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and natural connection relationships), nor model intra-sentential relationships (e.g, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic relationship</a> among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the CNN / DailyMail benchmark dataset to demonstrate effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.14/>Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</a></strong><br><a href=/people/x/xinnian-liang/>Xinnian Liang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--14><div class="card-body p-3 small">Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> as transitional embedding based models do. In terms of the local view, we first build a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> based on the document where phrases are regarded as vertices and the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. Finally, we further combine the modeling of global and local context for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.17/>A Partition Filter Network for Joint Entity and Relation Extraction</a></strong><br><a href=/people/z/zhiheng-yan/>Zhiheng Yan</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--17><div class="card-body p-3 small">In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature encoding</a> is decomposed into two steps : partition and <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filter</a>. In our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, we leverage two gates : entity and relation gate, to segment <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.19.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.19/>Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge</a></strong><br><a href=/people/b/bin-liang/>Bin Liang</a>
|
<a href=/people/h/hang-su/>Hang Su</a>
|
<a href=/people/r/rongdi-yin/>Rongdi Yin</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/q/qin-zhao/>Qin Zhao</a>
|
<a href=/people/x/xiaoqi-yu/>Xiaoqi Yu</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--19><div class="card-body p-3 small">In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> from external affective commonsense knowledge. Then, we employ <a href=https://en.wikipedia.org/wiki/Beta_distribution>Beta Distribution</a> to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.21/>Improving Multimodal fusion via Mutual Dependency Maximisation</a></strong><br><a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/e/emile-chapuis/>Emile Chapuis</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--21><div class="card-body p-3 small">Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of <a href=https://en.wikipedia.org/wiki/Communication_channel>channels</a> (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representations</a> into a synthetic one. So far, a consequent effort has been made on developing <a href=https://en.wikipedia.org/wiki/Computer_architecture>complex architectures</a> allowing the fusion of these <a href=https://en.wikipedia.org/wiki/Modularity>modalities</a>. However, such systems are mainly trained by minimising simple <a href=https://en.wikipedia.org/wiki/Loss_function>losses</a> such as L_1 or <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a>. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets : CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> but also produces <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.<tex-math>L_1</tex-math> or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.23/>Progressive Self-Training with <a href=https://en.wikipedia.org/wiki/Discriminator>Discriminator</a> for Aspect Term Extraction</a></strong><br><a href=/people/q/qianlong-wang/>Qianlong Wang</a>
|
<a href=/people/z/zhiyuan-wen/>Zhiyuan Wen</a>
|
<a href=/people/q/qin-zhao/>Qin Zhao</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--23><div class="card-body p-3 small">Aspect term extraction aims to extract <a href=https://en.wikipedia.org/wiki/Aspect_(grammar)>aspect terms</a> from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the <a href=https://en.wikipedia.org/wiki/Subset>subset</a> become harder and more numerous as the iteration proceeds. The other is that we use a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.24/>Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification</a></strong><br><a href=/people/h/hao-chen/>Hao Chen</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a>
|
<a href=/people/j/jianfei-yu/>Jianfei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--24><div class="card-body p-3 small">Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, which can not address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences ; 2) the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples ; 3) the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach&#8217;s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.27.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.27/>(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys</a></strong><br><a href=/people/k/kenneth-joseph/>Kenneth Joseph</a>
|
<a href=/people/s/sarah-shugars/>Sarah Shugars</a>
|
<a href=/people/r/ryan-gallagher/>Ryan Gallagher</a>
|
<a href=/people/j/jon-green/>Jon Green</a>
|
<a href=/people/a/alexi-quintana-mathe/>Alexi Quintana Mathé</a>
|
<a href=/people/z/zijian-an/>Zijian An</a>
|
<a href=/people/d/david-lazer/>David Lazer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--27><div class="card-body p-3 small">Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover <a href=https://en.wikipedia.org/wiki/Public_opinion>public opinion</a> from large streams of <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a>. Yet even human annotation of social media content does not always capture stance as measured by <a href=https://en.wikipedia.org/wiki/Opinion_poll>public opinion polls</a>. We demonstrate this by directly comparing an individual&#8217;s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> is high for both Pro&#8217;&#8217; and Anti&#8217;&#8217; stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance : temporal inconsistencies, differences in constructs, and <a href=https://en.wikipedia.org/wiki/Observational_error>measurement errors</a> from both survey respondents and annotators. By presenting a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.30/>Distilling <a href=https://en.wikipedia.org/wiki/Context_(language_use)>Linguistic Context</a> for Language Model Compression</a></strong><br><a href=/people/g/geondo-park/>Geondo Park</a>
|
<a href=/people/g/gyeongman-kim/>Gyeongman Kim</a>
|
<a href=/people/e/eunho-yang/>Eunho Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--30><div class="card-body p-3 small">A computationally expensive and memory intensive <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations : Word Relation and Layer Transforming Relation. Unlike other recent <a href=https://en.wikipedia.org/wiki/Distillation>distillation techniques</a> for the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.31/>Dynamic Knowledge Distillation for Pre-trained Language Models</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--31><div class="card-body p-3 small">Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects : teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model ; (2) conducting KD with 10 % informative instances achieves comparable performance while greatly accelerates the training ; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.36/>Graph Based Network with Contextualized Representations of Turns in Dialogue</a></strong><br><a href=/people/b/bongseok-lee/>Bongseok Lee</a>
|
<a href=/people/y/yong-suk-choi/>Yong Suk Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--36><div class="card-body p-3 small">Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> have the characteristics of high personal pronoun occurrences and low <a href=https://en.wikipedia.org/wiki/Information_density>information density</a>, and since most relational facts in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a> on most of the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.37/>Automatically Exposing Problems with Neural Dialog Models</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/k/kenji-sagae/>Kenji Sagae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--37><div class="card-body p-3 small">Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, while leaving systematic problems undercover. In this paper, we propose two methods including <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.38/>Event Coreference Data (Almost) for Free : Mining Hyperlinks from Online News<span class=acl-fixed-case>E</span>vent Coreference Data (Almost) for Free: <span class=acl-fixed-case>M</span>ining Hyperlinks from Online News</a></strong><br><a href=/people/m/michael-bugert/>Michael Bugert</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--38><div class="card-body p-3 small">Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> in online news : When referring to a significant real-world event, writers often add a <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlink</a> to another article covering this event. We demonstrate that collecting <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> which point to the same article(s) produces extensive and high-quality CDCR data and create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 2 M documents and 2.7 M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on small subsets of HyperCoref are highly competitive, with performance similar to <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.39/>Inducing Stereotypical Character Roles from Plot Structure</a></strong><br><a href=/people/l/labiba-jahan/>Labiba Jahan</a>
|
<a href=/people/r/rahul-mittal/>Rahul Mittal</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--39><div class="card-body p-3 small">Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in <a href=https://en.wikipedia.org/wiki/Narrative>narratives</a> : they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters&#8217; roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp&#8217;s structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp&#8217;s dramatis personae with F1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The best-performing feature set comprises <a href=https://en.wikipedia.org/wiki/Plot_(graphics)>plot functions</a>, <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>unigrams</a>, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.43.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.43/>Adversarial Scrubbing of Demographic Information for Text Classification</a></strong><br><a href=/people/s/somnath-basu-roy-chowdhury/>Somnath Basu Roy Chowdhury</a>
|
<a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/y/yiyuan-li/>Yiyuan Li</a>
|
<a href=/people/j/junier-oliva/>Junier Oliva</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--43><div class="card-body p-3 small">Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. In this paper, we present an adversarial learning framework Adversarial Scrubber (AdS), to debias contextual representations. We perform theoretical analysis to show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> converges without leaking <a href=https://en.wikipedia.org/wiki/Demography>demographic information</a> under certain conditions. We extend previous evaluation techniques by evaluating <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> with minimal information about <a href=https://en.wikipedia.org/wiki/Demography>demographic attributes</a> while being maximally informative about the target task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.44/>Open-domain clarification question generation without question examples</a></strong><br><a href=/people/j/julia-white/>Julia White</a>
|
<a href=/people/g/gabriel-poesia/>Gabriel Poesia</a>
|
<a href=/people/r/robert-hawkins/>Robert Hawkins</a>
|
<a href=/people/d/dorsa-sadigh/>Dorsa Sadigh</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--44><div class="card-body p-3 small">An overarching goal of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to enable machines to communicate seamlessly with humans. However, <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> can be ambiguous or unclear. In cases of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>, humans engage in an interactive process known as repair : asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model&#8217;s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.45/>Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/c/canwen-xu/>Canwen Xu</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--45><div class="card-body p-3 small">In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require <a href=https://en.wikipedia.org/wiki/Rewriting>sentence rewriting</a> (e.g., <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question generation</a>, <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a>, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.<b>S</b>equence <b>S</b>pan <b>R</b>ewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.50/>Artificial Text Detection via Examining the Topology of Attention Maps</a></strong><br><a href=/people/l/laida-kushnareva/>Laida Kushnareva</a>
|
<a href=/people/d/daniil-cherniavskii/>Daniil Cherniavskii</a>
|
<a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/s/serguei-barannikov/>Serguei Barannikov</a>
|
<a href=/people/a/alexander-bernstein/>Alexander Bernstein</a>
|
<a href=/people/i/irina-piontkovskaya/>Irina Piontkovskaya</a>
|
<a href=/people/d/dmitri-piontkovski/>Dmitri Piontkovski</a>
|
<a href=/people/e/evgeny-burnaev/>Evgeny Burnaev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--50><div class="card-body p-3 small">The impressive capabilities of recent <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> towards unseen <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10 % on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>, specifically the ones that incorporate surface and structural information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.52/>Conditional Poisson Stochastic Beams<span class=acl-fixed-case>P</span>oisson Stochastic Beams</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/a/afra-amini/>Afra Amini</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--52><div class="card-body p-3 small">Beam search is the default decoding strategy for many sequence generation tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The set of approximate K-best items returned by the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is a useful summary of the distribution for many applications ; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> into a <a href=https://en.wikipedia.org/wiki/Stochastic_process>stochastic process</a> : Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)&#8217;s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.54.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.54" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.54/>Moral Stories : Situated Reasoning about Norms, Intents, Actions, and their Consequences</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--54><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Social_environment>social settings</a>, much of human behavior is governed by <a href=https://en.wikipedia.org/wiki/Social_norm>unspoken rules of conduct</a> rooted in <a href=https://en.wikipedia.org/wiki/Social_norm>societal norms</a>. For artificial systems to be fully integrated into <a href=https://en.wikipedia.org/wiki/Social_environment>social environments</a>, adherence to such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.56/>Injecting Entity Types into Entity-Guided Text Generation</a></strong><br><a href=/people/x/xiangyu-dong/>Xiangyu Dong</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--56><div class="card-body p-3 small">Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity</a> in NLG, in this paper, we aim to model the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type</a> in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.59/>The Impact of <a href=https://en.wikipedia.org/wiki/Positional_notation>Positional Encodings</a> on Multilingual Compression</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--59><div class="card-body p-3 small">In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture ; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is : sinusoidal encodings were explicitly designed to facilitate compositionality by allowing <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>linear projections</a> over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.60/>Disentangling Representations of Text by Masking Transformers</a></strong><br><a href=/people/x/xiongyi-zhang/>Xiongyi Zhang</a>
|
<a href=/people/j/jan-willem-van-de-meent/>Jan-Willem van de Meent</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--60><div class="card-body p-3 small">Representations from large pretrained models such as BERT encode a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation ; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> from <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as and often better than previously proposed methods based on variational autoencoders and adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.62/>Do Long-Range Language Models Actually Use Long-Range Context?</a></strong><br><a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/a/andrew-mattarella-micke/>Andrew Mattarella-Micke</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--62><div class="card-body p-3 small">Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of the past. However, the ways in which such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8 K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2 K tokens) to these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to <a href=https://en.wikipedia.org/wiki/Textbook>textbooks</a> or magazines).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.63" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.63/>The World of an Octopus : How Reporting Bias Influences a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a>’s Perception of Color<span class=acl-fixed-case>T</span>he <span class=acl-fixed-case>W</span>orld of an <span class=acl-fixed-case>O</span>ctopus: <span class=acl-fixed-case>H</span>ow <span class=acl-fixed-case>R</span>eporting <span class=acl-fixed-case>B</span>ias <span class=acl-fixed-case>I</span>nfluences a <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odel’s <span class=acl-fixed-case>P</span>erception of <span class=acl-fixed-case>C</span>olor</a></strong><br><a href=/people/c/cory-paik/>Cory Paik</a>
|
<a href=/people/s/stephane-aroca-ouellette/>Stéphane Aroca-Ouellette</a>
|
<a href=/people/a/alessandro-roncone/>Alessandro Roncone</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--63><div class="card-body p-3 small">Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a>, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects ; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human&#8217;s perception of color ; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a> negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.66/>Semantic Novelty Detection in Natural Language Descriptions</a></strong><br><a href=/people/n/nianzu-ma/>Nianzu Ma</a>
|
<a href=/people/a/alexander-politowicz/>Alexander Politowicz</a>
|
<a href=/people/s/sahisnu-mazumder/>Sahisnu Mazumder</a>
|
<a href=/people/j/jiahua-chen/>Jiahua Chen</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/eric-robertson/>Eric Robertson</a>
|
<a href=/people/s/scott-grigsby/>Scott Grigsby</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--66><div class="card-body p-3 small">This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says A man is walking a chicken in the park, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (called GAT-MA) to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> and also contributes a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Experimental evaluation shows that GAT-MA outperforms 11 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.67/>Jump-Starting Item Parameters for Adaptive Language Tests</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-p-yancey/>Kevin P. Yancey</a>
|
<a href=/people/g/geoffrey-t-laflair/>Geoffrey T. LaFlair</a>
|
<a href=/people/j/jesse-egbert/>Jesse Egbert</a>
|
<a href=/people/m/manqian-liao/>Manqian Liao</a>
|
<a href=/people/b/burr-settles/>Burr Settles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--67><div class="card-body p-3 small">A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed &#8216;cold start&#8217; estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (6 each) from a large item bank (4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test item security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that the difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.68/>Voice Query Auto Completion</a></strong><br><a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/k/karun-kumar/>Karun Kumar</a>
|
<a href=/people/k/kendra-chalkley/>Kendra Chalkley</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/l/liming-zhang/>Liming Zhang</a>
|
<a href=/people/w/wenyan-li/>Wenyan Li</a>
|
<a href=/people/g/gefei-yang/>Gefei Yang</a>
|
<a href=/people/y/yajie-mao/>Yajie Mao</a>
|
<a href=/people/j/junho-shin/>Junho Shin</a>
|
<a href=/people/g/geoffrey-craig-murray/>Geoffrey Craig Murray</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--68><div class="card-body p-3 small">Query auto completion (QAC) is the task of predicting a search engine user&#8217;s final query from their intermediate, incomplete query. In this paper, we extend <a href=https://en.wikipedia.org/wiki/Speech_recognition>QAC</a> to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often do n&#8217;t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtains an 18 % relative improvement in mean reciprocal rank over previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.69/>CoPHE : A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>PHE</span>: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification</a></strong><br><a href=/people/m/matus-falis/>Matúš Falis</a>
|
<a href=/people/h/hang-dong/>Hang Dong</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/b/beatrice-alex/>Beatrice Alex</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--69><div class="card-body p-3 small">Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in <a href=https://en.wikipedia.org/wiki/Prior_art>prior art</a> is evaluated with standard <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in <a href=https://en.wikipedia.org/wiki/Prior_art>prior art</a>, and propose an alternative <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> based on the depth of the ontology. We propose a set of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with previously used <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological representation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.70/>Learning Universal Authorship Representations</a></strong><br><a href=/people/r/rafael-a-rivera-soto/>Rafael A. Rivera-Soto</a>
|
<a href=/people/o/olivia-elizabeth-miano/>Olivia Elizabeth Miano</a>
|
<a href=/people/j/juanita-ordonez/>Juanita Ordonez</a>
|
<a href=/people/b/barry-y-chen/>Barry Y. Chen</a>
|
<a href=/people/a/aleem-khan/>Aleem Khan</a>
|
<a href=/people/m/marcus-bishop/>Marcus Bishop</a>
|
<a href=/people/n/nicholas-andrews/>Nicholas Andrews</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--70><div class="card-body p-3 small">Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. Recently, authorship representations learned using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> learned in a particular domain transfer to other domains? Or are these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains : <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon reviews</a>, <a href=https://en.wikipedia.org/wiki/Fan_fiction>fanfiction short stories</a>, and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit comments</a>. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and propose simple but effective methods to improve transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.71/>Predicting emergent linguistic compositions through time : Syntactic frame extension via multimodal chaining</a></strong><br><a href=/people/l/lei-yu/>Lei Yu</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--71><div class="card-body p-3 small">Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from percept, concept, and language to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> and <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.74/>Revisiting the Uniform Information Density Hypothesis<span class=acl-fixed-case>U</span>niform <span class=acl-fixed-case>I</span>nformation <span class=acl-fixed-case>D</span>ensity Hypothesis</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/p/patrick-haller/>Patrick Haller</a>
|
<a href=/people/l/lena-jager/>Lena Jäger</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--74><div class="card-body p-3 small">The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on <a href=https://en.wikipedia.org/wiki/Language_production>language production</a> have been well explored, the <a href=https://en.wikipedia.org/wiki/Hypothesis>hypothesis</a> potentially makes predictions about <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>linguistic acceptability</a> as well. Further, it is unclear how uniformity in a linguistic signalor lack thereofshould be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID&#8217;s predictions. For <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability judgments</a>, we find clearer evidence that non-uniformity in <a href=https://en.wikipedia.org/wiki/Information_density>information density</a> is predictive of lower <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability</a>. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or documenta finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.76/>Monitoring geometrical properties of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for detecting the emergence of new topics.</a></strong><br><a href=/people/c/clement-christophe/>Clément Christophe</a>
|
<a href=/people/j/julien-velcin/>Julien Velcin</a>
|
<a href=/people/j/jairo-cugliari/>Jairo Cugliari</a>
|
<a href=/people/m/manel-boumghar/>Manel Boumghar</a>
|
<a href=/people/p/philippe-suignard/>Philippe Suignard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--76><div class="card-body p-3 small">Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and <a href=https://en.wikipedia.org/wiki/Evolutionary_linguistics>language evolution</a>, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of <a href=https://en.wikipedia.org/wiki/Weak_signals>weak signals</a> at the word level. We propose to monitor the behavior of words representation in an <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> and use one of its <a href=https://en.wikipedia.org/wiki/Geometry>geometrical properties</a> to characterize the <a href=https://en.wikipedia.org/wiki/Emergence>emergence of topics</a>. As <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is typically hard for this kind of task, we present a framework for quantitative evaluation and show positive results that outperform state-of-the-art methods. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is evaluated on two public datasets of press and scientific articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.80/>Neural Attention-Aware Hierarchical Topic Model</a></strong><br><a href=/people/y/yuan-jin/>Yuan Jin</a>
|
<a href=/people/h/he-zhao/>He Zhao</a>
|
<a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/l/lan-du/>Lan Du</a>
|
<a href=/people/w/wray-buntine/>Wray Buntine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--80><div class="card-body p-3 small">Neural topic models (NTMs) apply <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to topic modelling. Despite their success, NTMs generally ignore two important aspects : (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.81/>Relational World Knowledge Representation in Contextual Language Models : A Review<span class=acl-fixed-case>R</span>elational <span class=acl-fixed-case>W</span>orld <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>R</span>epresentation in <span class=acl-fixed-case>C</span>ontextual <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odels: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>R</span>eview</a></strong><br><a href=/people/t/tara-safavi/>Tara Safavi</a>
|
<a href=/people/d/danai-koutra/>Danai Koutra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--81><div class="card-body p-3 small">Relational knowledge bases (KBs) are commonly used to represent <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> in machines. However, while advantageous for their high degree of <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold : (1) We provide a high-level, extensible taxonomy for <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> in LMs ; (2) Within our <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a>, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs ; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.84" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.84/>Contrastive Out-of-Distribution Detection for Pretrained Transformers</a></strong><br><a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--84><div class="card-body p-3 small">Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should identify such instances, and then either reject them during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> or pass them over to <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that handle another <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a>. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the <a href=https://en.wikipedia.org/wiki/Mahalanobis_distance>Mahalanobis distance</a> in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s penultimate layer</a>. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.85/>MindCraft : Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks<span class=acl-fixed-case>M</span>ind<span class=acl-fixed-case>C</span>raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks</a></strong><br><a href=/people/c/cristian-paul-bara/>Cristian-Paul Bara</a>
|
<a href=/people/s/sky-ch-wang/>Sky CH-Wang</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--85><div class="card-body p-3 small">An ideal integration of <a href=https://en.wikipedia.org/wiki/Autonomous_agent>autonomous agents</a> in a human world implies that they are able to collaborate on human terms. In particular, <a href=https://en.wikipedia.org/wiki/Theory_of_mind>theory of mind</a> plays an important role in maintaining common ground during <a href=https://en.wikipedia.org/wiki/Collaboration>human collaboration</a> and <a href=https://en.wikipedia.org/wiki/Communication>communication</a>. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners&#8217; beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.87" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.87/>Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking</a></strong><br><a href=/people/n/nikita-moghe/>Nikita Moghe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--87><div class="card-body p-3 small">Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200 K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English-Chinese, Chinese-English) and Multilingual WoZ (English-German, English-Italian) datasets. We achieve impressive improvements (20 % on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10 % of the target language task data and zero-shot setup respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.88/>ConvFiT : Conversational Fine-Tuning of Pretrained Language Models<span class=acl-fixed-case>ConvFiT:</span> <span class=acl-fixed-case>C</span>onversational Fine-Tuning of Pretrained Language Models</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/samuel-coope/>Samuel Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--88><div class="card-body p-3 small">Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data ; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets : ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.91/>Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/p/patrick-lehnen/>Patrick Lehnen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--91><div class="card-body p-3 small">With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> is often available in real-world dialog systems, however, the <a href=https://en.wikipedia.org/wiki/Modular_programming>modularized architecture</a> commonly used in large-scale systems prevents the direct application of such <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.93/>Extend, do n’t rebuild : Phrasing conditional graph modification as autoregressive sequence labelling</a></strong><br><a href=/people/l/leon-weber/>Leon Weber</a>
|
<a href=/people/j/jannes-munchmeyer/>Jannes Münchmeyer</a>
|
<a href=/people/s/samuele-garda/>Samuele Garda</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--93><div class="card-body p-3 small">Deriving and modifying graphs from natural language text has become a versatile basis technology for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with applications in many subfields, such as <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and then generating the modified one based on this <a href=https://en.wikipedia.org/wiki/Code>encoding</a>. In this work, we show that we can considerably increase performance on this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> by phrasing it as graph extension instead of graph generation. We propose the first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> between 13 and 24 percentage points. Furthermore, we introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> from the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a> which has much larger linguistic variability and more complex <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> than the scene graph modification data sets. For this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the art</a> fails to generalize, while our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can produce meaningful predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.95/>Learning Logic Rules for Document-Level Relation Extraction</a></strong><br><a href=/people/d/dongyu-ru/>Dongyu Ru</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/y/yong-yu/>Yong Yu</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--95><div class="card-body p-3 small">Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned through (graph) neural networks, which makes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and consists of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> : a rule generator and a relation extractor. The rule generator is to generate <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a>. Those two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and <a href=https://en.wikipedia.org/wiki/Consistency>logical consistency</a>. Our code is available at https://github.com/rudongyu/LogiRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.96.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.96/>A Large-Scale Dataset for Empathetic Response Generation</a></strong><br><a href=/people/a/anuradha-welivita/>Anuradha Welivita</a>
|
<a href=/people/y/yubo-xie/>Yubo Xie</a>
|
<a href=/people/p/pearl-pu/>Pearl Pu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--96><div class="card-body p-3 small">Recent development in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1 M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using both offline experiments and visual validation methods. The resultant <a href=https://en.wikipedia.org/wiki/Subroutine>procedure</a> can be used to create similar <a href=https://en.wikipedia.org/wiki/Data_set_(IBM_mainframe)>datasets</a> in the same domain as well as in other domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.97/>The Perils of Using <a href=https://en.wikipedia.org/wiki/Mechanical_Turk>Mechanical Turk</a> to Evaluate Open-Ended Text Generation<span class=acl-fixed-case>M</span>echanical <span class=acl-fixed-case>T</span>urk to Evaluate Open-Ended Text Generation</a></strong><br><a href=/people/m/marzena-karpinska/>Marzena Karpinska</a>
|
<a href=/people/n/nader-akoury/>Nader Akoury</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--97><div class="card-body p-3 small">Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English teachers</a> provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.98/>Documenting Large Webtext Corpora : A Case Study on the Colossal Clean Crawled Corpus</a></strong><br><a href=/people/j/jesse-dodge/>Jesse Dodge</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/a/ana-marasovic/>Ana Marasović</a>
|
<a href=/people/w/william-agnew/>William Agnew</a>
|
<a href=/people/g/gabriel-ilharco/>Gabriel Ilharco</a>
|
<a href=/people/d/dirk-groeneveld/>Dirk Groeneveld</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--98><div class="card-body p-3 small">Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4 ; Raffel et al., 2020), a dataset created by applying a set of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to a single snapshot of <a href=https://en.wikipedia.org/wiki/Common_Crawl>Common Crawl</a>. We begin by investigating where the <a href=https://en.wikipedia.org/wiki/Data>data</a> came from, and find a significant amount of text from unexpected sources like <a href=https://en.wikipedia.org/wiki/Patent>patents</a> and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>) and evaluation examples from other benchmark NLP datasets. To understand the impact of the <a href=https://en.wikipedia.org/wiki/Content-control_software>filters</a> applied to create this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about <a href=https://en.wikipedia.org/wiki/Minority_group>minority individuals</a>. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.103/>Conundrums in Event Coreference Resolution : Making Sense of the State of the Art</a></strong><br><a href=/people/j/jing-lu/>Jing Lu</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--103><div class="card-body p-3 small">Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.109/>Fast, Effective, and Self-Supervised : Transforming Masked Language Models into Universal Lexical and Sentence Encoders</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--109><div class="card-body p-3 small">Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NLI</a>, sentence similarity, or <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing tasks</a> using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts <a href=https://en.wikipedia.org/wiki/Machine_learning>MLMs</a> (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.110/>RuleBERT : Teaching Soft Rules to Pre-Trained Language Models<span class=acl-fixed-case>R</span>ule<span class=acl-fixed-case>BERT</span>: Teaching Soft Rules to Pre-Trained Language Models</a></strong><br><a href=/people/m/mohammed-saeed/>Mohammed Saeed</a>
|
<a href=/people/n/naser-ahmadi/>Naser Ahmadi</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/p/paolo-papotti/>Paolo Papotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--110><div class="card-body p-3 small">While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and we propose a revised <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> that enables the PLM to learn how to predict precise probabilities for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.111/>Stepmothers are mean and academics are pretentious : What do pretrained language models learn about you?</a></strong><br><a href=/people/r/rochelle-choenni/>Rochelle Choenni</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/r/robert-van-rooij/>Robert van Rooij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--111><div class="card-body p-3 small">In this paper, we investigate what types of <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>stereotypical information</a> are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.114/>When <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> meets NLP : The devil is in the detail<span class=acl-fixed-case>NLP</span>: The devil is in the detail</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--114><div class="card-body p-3 small">Differential privacy provides a formal approach to privacy of individuals. Applications of <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> in various scenarios, such as protecting users&#8217; original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for <a href=https://en.wikipedia.org/wiki/Rewriting>text rewriting</a> (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight <a href=https://en.wikipedia.org/wiki/Privacy>privacy guarantees</a>. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder&#8217;s dimension and that the amount of utterances that are not privatized could easily reach 100 % of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.115/>Achieving Model Robustness through Discrete Adversarial Training</a></strong><br><a href=/people/m/maor-ivgi/>Maor Ivgi</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--115><div class="card-body p-3 small">Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> has been limited to offline augmentation only. Concretely, given a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, attacks are used to generate perturbed (adversarial) examples, and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We propose (i) a new discrete attack, based on <a href=https://en.wikipedia.org/wiki/Best-first_search>best-first search</a>, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> leads to impressive gains in <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training cost</a>, significantly improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> on three datasets. Last, we show that our new <a href=https://en.wikipedia.org/wiki/Attack_(computing)>attack</a> substantially improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> compared to <a href=https://en.wikipedia.org/wiki/Attack_(computing)>prior methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.118/>How much pretraining data do <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> need to learn syntax?</a></strong><br><a href=/people/l/laura-perez-mayos/>Laura Pérez-Mayos</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--118><div class="card-body p-3 small">Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. We explore this impact on the syntactic capabilities of RoBERTa, using <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.120/>Contrastive Explanations for Model Interpretability</a></strong><br><a href=/people/a/alon-jacovi/>Alon Jacovi</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/s/shauli-ravfogel/>Shauli Ravfogel</a>
|
<a href=/people/y/yanai-elazar/>Yanai Elazar</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--120><div class="card-body p-3 small">Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token / span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.121/>On the Transferability of Adversarial Attacks against Neural Text Classifier</a></strong><br><a href=/people/l/liping-yuan/>Liping Yuan</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--121><div class="card-body p-3 small">Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can fool another <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a>, <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization scheme</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> to find an ensemble of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can be used to induce adversarial examples to fool almost all existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.125/>mT6 : Multilingual Pretrained Text-to-Text Transformer with Translation Pairs<span class=acl-fixed-case>T</span>6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--125><div class="card-body p-3 small">Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and abstractive summarization. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/MT6>mT6</a> improves cross-lingual transferability over mT5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.127/>Speechformer : Reducing <a href=https://en.wikipedia.org/wiki/Information_loss>Information Loss</a> in Direct Speech Translation</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--127><div class="card-body p-3 small">Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. However, <a href=https://en.wikipedia.org/wiki/Transformer>Transformer&#8217;s quadratic complexity</a> with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> is not accessible to higher-level layers in the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial <a href=https://en.wikipedia.org/wiki/Lossy_compression>lossy compression</a> and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (ende / es / nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.133/>Effects of Parameter Norm Growth During Transformer Training : Inductive Bias from Gradient Descent</a></strong><br><a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/v/vivek-ramanujan/>Vivek Ramanujan</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--133><div class="card-body p-3 small">The capacity of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> like the widely adopted <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> is known to be very high. Evidence is emerging that they learn successfully due to <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (_ 2 norm) during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, and its implications for the <a href=https://en.wikipedia.org/wiki/Emergence>emergent representations</a> within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> approximates a discretized network with saturated activation functions. Such saturated networks are known to have a reduced capacity compared to the full network family that can be described in terms of <a href=https://en.wikipedia.org/wiki/Formal_language>formal languages</a> and <a href=https://en.wikipedia.org/wiki/Automata_theory>automata</a>. Our results suggest saturation is a new characterization of an <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> implicit in GD of particular interest for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.<tex-math>\\ell_2</tex-math> norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such &#8220;saturated&#8221; networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.136/>Knowledge-Aware Meta-learning for Low-Resource Text Classification</a></strong><br><a href=/people/h/huaxiu-yao/>Huaxiu Yao</a>
|
<a href=/people/y/ying-xin-wu/>Ying-xin Wu</a>
|
<a href=/people/m/maruan-al-shedivat/>Maruan Al-Shedivat</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--136><div class="card-body p-3 small">Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.138.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.138/>Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning</a></strong><br><a href=/people/s/seonghyeon-ye/>Seonghyeon Ye</a>
|
<a href=/people/j/jiseon-kim/>Jiseon Kim</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--138><div class="card-body p-3 small">We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and curriculum learning. For <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, we stack two types of operation sequentially : cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70 % of <a href=https://en.wikipedia.org/wiki/Computer_data_storage>computational memory</a> compared to the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.140/>DIALKI : Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization<span class=acl-fixed-case>DIALKI</span>: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</a></strong><br><a href=/people/z/zeqiu-wu/>Zeqiu Wu</a>
|
<a href=/people/b/bo-ru-lu/>Bo-Ru Lu</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--140><div class="card-body p-3 small">Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.141/>Iconary : A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</a></strong><br><a href=/people/c/christopher-clark/>Christopher Clark</a>
|
<a href=/people/j/jordi-salvador/>Jordi Salvador</a>
|
<a href=/people/d/dustin-schwenk/>Dustin Schwenk</a>
|
<a href=/people/d/derrick-bonafilia/>Derrick Bonafilia</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/e/eric-kolve/>Eric Kolve</a>
|
<a href=/people/a/alvaro-herrasti/>Alvaro Herrasti</a>
|
<a href=/people/j/jonghyun-choi/>Jonghyun Choi</a>
|
<a href=/people/s/sachin-mehta/>Sachin Mehta</a>
|
<a href=/people/s/sam-skjonsberg/>Sam Skjonsberg</a>
|
<a href=/people/c/carissa-schoenick/>Carissa Schoenick</a>
|
<a href=/people/a/aaron-sarnat/>Aaron Sarnat</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/a/aniruddha-kembhavi/>Aniruddha Kembhavi</a>
|
<a href=/people/o/oren-etzioni/>Oren Etzioni</a>
|
<a href=/people/a/ali-farhadi/>Ali Farhadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--141><div class="card-body p-3 small">Communicating with humans is challenging for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AIs</a> because it requires a shared understanding of the world, complex <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> (e.g., <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> or analogies), and at times <a href=https://en.wikipedia.org/wiki/Gesture_recognition>multi-modal gestures</a> (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on <a href=https://en.wikipedia.org/wiki/Pictionary>Pictionary</a>, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing <a href=https://en.wikipedia.org/wiki/Icon_(computing)>icons</a>, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses <a href=https://en.wikipedia.org/wiki/Canonical_criticism>canonical scenes</a>, <a href=https://en.wikipedia.org/wiki/Visual_metaphor>visual metaphor</a>, or <a href=https://en.wikipedia.org/wiki/Iconography>icon compositions</a> to express challenging words, making it an ideal test for mixing language and visual / symbolic communication in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>. We propose <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> to play Iconary and train them on over 55,000 games between human players. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are skillful players and are able to employ world knowledge in language models to play with words unseen during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.142/>Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems</a></strong><br><a href=/people/f/fei-mi/>Fei Mi</a>
|
<a href=/people/w/wanhao-zhou/>Wanhao Zhou</a>
|
<a href=/people/l/lingjing-kong/>Lingjing Kong</a>
|
<a href=/people/f/fengyu-cai/>Fengyu Cai</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--142><div class="card-body p-3 small">As the labeling cost for different <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.143/>Contextual Rephrase Detection for Reducing Friction in Dialogue Systems</a></strong><br><a href=/people/z/zhuoyi-wang/>Zhuoyi Wang</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/a/alexander-hanbo-li/>Alexander Hanbo Li</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--143><div class="card-body p-3 small">For voice assistants like <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, <a href=https://en.wikipedia.org/wiki/Google_Assistant>Google Assistant</a>, and <a href=https://en.wikipedia.org/wiki/Siri>Siri</a>, correctly interpreting users&#8217; intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or <a href=https://en.wikipedia.org/wiki/User_error>user errors</a> such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users&#8217; implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user&#8217;s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.146/>AttentionRank : Unsupervised Keyphrase Extraction using Self and Cross Attentions<span class=acl-fixed-case>A</span>ttention<span class=acl-fixed-case>R</span>ank: Unsupervised Keyphrase Extraction using Self and Cross Attentions</a></strong><br><a href=/people/h/haoran-ding/>Haoran Ding</a>
|
<a href=/people/x/xiao-luo/>Xiao Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--146><div class="card-body p-3 small">Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.149/>Everything Is All It Takes : A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction</a></strong><br><a href=/people/m/mahsa-yarmohammadi/>Mahsa Yarmohammadi</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/s/seth-ebner/>Seth Ebner</a>
|
<a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/y/yunmo-chen/>Yunmo Chen</a>
|
<a href=/people/j/jialiang-guo/>Jialiang Guo</a>
|
<a href=/people/c/craig-harman/>Craig Harman</a>
|
<a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--149><div class="card-body p-3 small">Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically <a href=https://en.wikipedia.org/wiki/English_language>English</a>. While the advance of pretrained multilingual encoders suggests an easy optimism of train on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, run on any language, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>data projection</a> and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and dependency parsing. We then apply <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>data projection</a> and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.151.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.151" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.151/>Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search</a></strong><br><a href=/people/j/jialu-wang/>Jialu Wang</a>
|
<a href=/people/y/yang-liu-umich/>Yang Liu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--151><div class="card-body p-3 small">Internet search affects people&#8217;s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Image_retrieval>image search</a> in this work : the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> suffer from severe <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>. Therefore, we introduce two novel debiasing approaches : an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30 K benchmarks show that our methods significantly reduce the gender bias in image search models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.152/>Style Pooling : Automatic Text Style Obfuscation for Improved Classification Fairness</a></strong><br><a href=/people/f/fatemehsadat-mireshghallah/>Fatemehsadat Mireshghallah</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--152><div class="card-body p-3 small">Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in <a href=https://en.wikipedia.org/wiki/Recruitment>hiring decisions</a>, regardless of whether <a href=https://en.wikipedia.org/wiki/Recruitment>hiring decisions</a> are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style : (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling&#8217;s effect on <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, semantic consistency, and attribute removal from text, in two and three domain style transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.153/>Modeling Disclosive Transparency in NLP Application Descriptions<span class=acl-fixed-case>NLP</span> Application Descriptions</a></strong><br><a href=/people/m/michael-saxon/>Michael Saxon</a>
|
<a href=/people/s/sharon-levy/>Sharon Levy</a>
|
<a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/a/alon-albalak/>Alon Albalak</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--153><div class="card-body p-3 small">Broader disclosive transparencytruth and clarity in communication regarding the function of AI systemsis widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where too much information clouds a reader&#8217;s understanding of what a system description means. Disclosive transparency&#8217;s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between <a href=https://en.wikipedia.org/wiki/Transparency_(behavior)>transparency</a>, <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a>, and user perceptions in a corpus of real NLP system descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.155/>Fairness-aware Class Imbalanced Learning</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--155><div class="card-body p-3 small">Class imbalance is a common challenge in many NLP tasks, and has clear connections to <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, in that bias in training data often leads to higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.157/>Local Word Discovery for Interactive Transcription</a></strong><br><a href=/people/w/william-lane/>William Lane</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--157><div class="card-body p-3 small">Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for <a href=https://en.wikipedia.org/wiki/Kunwinjku_language>Kunwinjku</a>, a morphologically-complex Australian language. We combine a finite state implementation of a published <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17 %. Further, we find that 75 % of breath groups in the test set receive at least one correct partial or full-word suggestion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.158/>Segment, Mask, and Predict : Augmenting Chinese Word Segmentation with Self-Supervision<span class=acl-fixed-case>C</span>hinese Word Segmentation with Self-Supervision</a></strong><br><a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/g/gang-chen/>Gang Chen</a>
|
<a href=/people/k/kaiyu-huang/>Kaiyu Huang</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--158><div class="card-body p-3 small">Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the previous <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.160/>Fast WordPiece Tokenization<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>P</span>iece Tokenization</a></strong><br><a href=/people/x/xinying-song/>Xinying Song</a>
|
<a href=/people/a/alex-salcianu/>Alex Salcianu</a>
|
<a href=/people/y/yang-song/>Yang Song</a>
|
<a href=/people/d/dave-dopson/>Dave Dopson</a>
|
<a href=/people/d/denny-zhou/>Denny Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--160><div class="card-body p-3 small">Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for the WordPiece tokenization used in BERT, from <a href=https://en.wikipedia.org/wiki/Lexical_analysis>single-word tokenization</a> to <a href=https://en.wikipedia.org/wiki/Lexical_analysis>general text (e.g., sentence) tokenization</a>. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as <a href=https://en.wikipedia.org/wiki/Maximum_matching>maximum matching</a>. The best known algorithms so far are O(n2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> whose tokenization complexity is strictly O(n). Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is inspired by the <a href=https://en.wikipedia.org/wiki/Aho&#8211;Corasick_algorithm>Aho-Corasick algorithm</a>. We introduce additional linkages on top of the <a href=https://en.wikipedia.org/wiki/Trie>trie</a> built from the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, allowing smart transitions when the <a href=https://en.wikipedia.org/wiki/Trie>trie matching</a> can not continue. For general text, we further propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.161/>You should evaluate your <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on marginal likelihood over tokenisations</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--161><div class="card-body p-3 small">Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, <a href=https://en.wikipedia.org/wiki/Formal_language>language models</a> should be evaluated on their <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> over <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenisations</a>. We compare different estimators for the <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> based on <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a>, and show that it is feasible to estimate the <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in <a href=https://en.wikipedia.org/wiki/Uncertainty>perplexity</a> to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.162/>Broaden the Vision : Geo-Diverse Visual Commonsense Reasoning</a></strong><br><a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/l/liunian-harold-li/>Liunian Harold Li</a>
|
<a href=/people/z/ziniu-hu/>Ziniu Hu</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--162><div class="card-body p-3 small">Commonsense is defined as the knowledge on which everyone agrees. However, certain types of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models&#8217; ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on <a href=https://en.wikipedia.org/wiki/Videocassette_recorder>VCR</a>, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including <a href=https://en.wikipedia.org/wiki/East_Asia>East Asia</a>, <a href=https://en.wikipedia.org/wiki/South_Asia>South Asia</a>, and <a href=https://en.wikipedia.org/wiki/Africa>Africa</a> is significantly lower than that for <a href=https://en.wikipedia.org/wiki/Western_world>Western region</a>. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that : 1) are concerned with <a href=https://en.wikipedia.org/wiki/Culture>culture-related scenarios</a>, e.g., <a href=https://en.wikipedia.org/wiki/Wedding>weddings</a>, <a href=https://en.wikipedia.org/wiki/Religion>religious activities</a>, and <a href=https://en.wikipedia.org/wiki/Festival>festivals</a> ; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.163" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.163/>Reference-Centric Models for Grounded Collaborative Dialogue</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/j/justin-chiu/>Justin Chiu</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--163><div class="card-body p-3 small">We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our dialogue agent accurately grounds referents from the partner&#8217;s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, obtaining a 20 % relative improvement in successful task completion in self-play evaluations and a 50 % relative improvement in success in human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.164/>CrossVQA : Scalably Generating Benchmarks for Systematically Testing VQA Generalization<span class=acl-fixed-case>C</span>ross<span class=acl-fixed-case>VQA</span>: Scalably Generating Benchmarks for Systematically Testing <span class=acl-fixed-case>VQA</span> Generalization</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/soravit-changpinyo/>Soravit Changpinyo</a>
|
<a href=/people/b/boqing-gong/>Boqing Gong</a>
|
<a href=/people/p/piyush-sharma/>Piyush Sharma</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--164><div class="card-body p-3 small">One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the <a href=https://en.wikipedia.org/wiki/Language_shift>language shifts</a>. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.168/>Neural Path Hunter : Reducing <a href=https://en.wikipedia.org/wiki/Hallucination>Hallucination</a> in Dialogue Systems via Path Grounding</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaïane</a>
|
<a href=/people/a/avishek-joey-bose/>Avishek Joey Bose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--168><div class="card-body p-3 small">Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a> of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a> followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35 % based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.169/>Thinking Clearly, Talking Fast : Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems</a></strong><br><a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/z/zhihua-liu/>Zhihua Liu</a>
|
<a href=/people/x/xingwu-hu/>Xingwu Hu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--169><div class="card-body p-3 small">Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.170/>Perspective-taking and <a href=https://en.wikipedia.org/wiki/Pragmatics>Pragmatics</a> for Generating Empathetic Responses Focused on Emotion Causes</a></strong><br><a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/b/byeongchang-kim/>Byeongchang Kim</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--170><div class="card-body p-3 small">Empathy is a complex cognitive ability based on the reasoning of others&#8217; affective states. In order to better understand others and express stronger <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in dialogues, we argue that two issues must be tackled at the same time : (i) identifying which word is the cause for the other&#8217;s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> require sub-utterance level annotations, which can be demanding. Taking inspiration from <a href=https://en.wikipedia.org/wiki/Social_cognition>social cognition</a>, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> to make dialogue models focus on targeted words in the input during generation. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.172/>CoLV : A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>LV</span>: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation</a></strong><br><a href=/people/h/haolan-zhan/>Haolan Zhan</a>
|
<a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/h/hongshen-chen/>Hongshen Chen</a>
|
<a href=/people/h/hainan-zhang/>Hainan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--172><div class="card-body p-3 small">Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.176/>Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks</a></strong><br><a href=/people/q/qingbin-liu/>Qingbin Liu</a>
|
<a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/j/jiansong-chen/>Jiansong Chen</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--176><div class="card-body p-3 small">Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that <a href=https://en.wikipedia.org/wiki/KPN>KPN</a> effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25 % and 8.27 % of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.178/>Different Strokes for Different Folks : Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks</a></strong><br><a href=/people/y/yao-qiu/>Yao Qiu</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--178><div class="card-body p-3 small">Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the domain-specific unlabeled data can bring positive effects. However, most of these further <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a> works just keep running the conventional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training task</a>, e.g., masked language model, which can be regarded as the <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.179/>Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation</a></strong><br><a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--179><div class="card-body p-3 small">Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> leverage an <a href=https://en.wikipedia.org/wiki/Knowledge_base>external knowledge base</a> to generate appropriate responses. In real-world practical, the entity may not be included by the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or suffer from the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of <a href=https://en.wikipedia.org/wiki/Knowledge_retrieval>knowledge retrieval</a>. To deal with this problem, instead of introducing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> as the input, we force the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn a better semantic representation by predicting the information in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, only based on the input context. Specifically, with the help of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, we introduce two auxiliary training objectives : 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context ; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.181" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.181/>Unsupervised Conversation Disentanglement through <a href=https://en.wikipedia.org/wiki/Co-training>Co-Training</a></a></strong><br><a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/z/zhan-shi/>Zhan Shi</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--181><div class="card-body p-3 small">Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> : a message-pair classifier and a session classifier. The former is responsible of retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both the two <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> are initialized respectively with pseudo data built from the <a href=https://en.wikipedia.org/wiki/Text_corpus>unannotated corpus</a>. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.184/>EARL : Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning<span class=acl-fixed-case>EARL</span>: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/y/yong-liu/>Yong Liu</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--184><div class="card-body p-3 small">Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> into conversation generation. Automatic and manual evaluations demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more informative, coherent, and natural responses than baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.185/>DialogueCSE : Dialogue-based Contrastive Learning of Sentence Embeddings<span class=acl-fixed-case>D</span>ialogue<span class=acl-fixed-case>CSE</span>: Dialogue-based Contrastive Learning of Sentence Embeddings</a></strong><br><a href=/people/c/che-liu/>Che Liu</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/j/jinghua-liu/>Jinghua Liu</a>
|
<a href=/people/j/jian-sun/>Jian Sun</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--185><div class="card-body p-3 small">Learning sentence embeddings from <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets : the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in terms of MAP and Spearman&#8217;s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.187/>Not Just <a href=https://en.wikipedia.org/wiki/Classification>Classification</a> : Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation</a></strong><br><a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/y/yaxin-fan/>Yaxin Fan</a>
|
<a href=/people/x/xiaomin-chu/>Xiaomin Chu</a>
|
<a href=/people/p/peifeng-li/>Peifeng Li</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--187><div class="card-body p-3 small">Implicit discourse relation recognition (IDRR) is a critical task in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a <a href=https://en.wikipedia.org/wiki/Machine_learning>generation task</a> and further propose a method joint modeling of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>generation</a>. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.189.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.189" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.189/>Multimodal Phased Transformer for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/j/junyan-cheng/>Junyan Cheng</a>
|
<a href=/people/i/iordanis-fostiropoulos/>Iordanis Fostiropoulos</a>
|
<a href=/people/b/barry-boehm/>Barry Boehm</a>
|
<a href=/people/m/mohammad-soleymani/>Mohammad Soleymani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--189><div class="card-body p-3 small">Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>quadratic complexity</a> of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a>. SPT uses a <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>sampling function</a> to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90 % reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.190.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.190/>Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations</a></strong><br><a href=/people/l/linli-xu/>Linli Xu</a>
|
<a href=/people/s/sijie-teng/>Sijie Teng</a>
|
<a href=/people/r/ruoyu-zhao/>Ruoyu Zhao</a>
|
<a href=/people/j/junliang-guo/>Junliang Guo</a>
|
<a href=/people/c/chi-xiao/>Chi Xiao</a>
|
<a href=/people/d/deqiang-jiang/>Deqiang Jiang</a>
|
<a href=/people/b/bo-ren/>Bo Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--190><div class="card-body p-3 small">Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a <a href=https://en.wikipedia.org/wiki/Hybrid_algorithm>hybrid algorithm</a> to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with significant improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.192/>FLiText : A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks<span class=acl-fixed-case>FL</span>i<span class=acl-fixed-case>T</span>ext: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks</a></strong><br><a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/z/zhang-mengchao/>Zhang Mengchao</a>
|
<a href=/people/f/fu-zhibing/>Fu Zhibing</a>
|
<a href=/people/p/panpan-hou/>Panpan Hou</a>
|
<a href=/people/y/yu-li/>Yu Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--192><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>SSL</a>. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSL benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00 % to 90.49 % on <a href=https://en.wikipedia.org/wiki/IMDb>IMDb</a>, 39.8 % to 58.06 % on <a href=https://en.wikipedia.org/wiki/Yelp>Yelp-5</a>, and from 55.3 % to 65.08 % on Yahoo ! Answer. In addition, compared with the <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised method</a> on the full dataset, FLiText just uses less than 1 % of labeled data to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 6.59 %, 3.94 %, and 3.22 % on the datasets of <a href=https://en.wikipedia.org/wiki/IMDb>IMDb</a>, <a href=https://en.wikipedia.org/wiki/Yelp>Yelp-5</a>, and Yahoo ! Answer respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.195.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.195/>Transductive Learning for Unsupervised Text Style Transfer</a></strong><br><a href=/people/f/fei-xiao/>Fei Xiao</a>
|
<a href=/people/l/liang-pang/>Liang Pang</a>
|
<a href=/people/y/yanyan-lan/>Yanyan Lan</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/h/huawei-shen/>Huawei Shen</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--195><div class="card-body p-3 small">Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like &#8216;the salad is rude&#8217;. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves top-K relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.199/>ConRPG : <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>Paraphrase Generation</a> using Contexts as Regularizer<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>RPG</span>: Paraphrase Generation using Contexts as Regularizer</a></strong><br><a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/q/qing-he/>Qing He</a>
|
<a href=/people/x/xiaofei-sun/>Xiaofei Sun</a>
|
<a href=/people/q/qinghong-han/>Qinghong Han</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/c/chun-fan/>Chun Fan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--199><div class="card-body p-3 small">A long-standing issue with <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> is the lack of reliable supervision signals. In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised paradigm</a> for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods : (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs ; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity ; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.202/>Asking Questions Like Educational Experts : Automatically Generating Question-Answer Pairs on Real-World Examination Data<span class=acl-fixed-case>A</span>utomatically Generating Question-Answer Pairs on Real-World Examination Data</a></strong><br><a href=/people/f/fanyi-qu/>Fanyi Qu</a>
|
<a href=/people/x/xin-jia/>Xin Jia</a>
|
<a href=/people/y/yunfang-wu/>Yunfang Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--202><div class="card-body p-3 small">Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the strong generative pre-training model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting new directions for this challenging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.203/>Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data</a></strong><br><a href=/people/e/erguang-yang/>Erguang Yang</a>
|
<a href=/people/m/mingtong-liu/>Mingtong Liu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/y/yujie-zhang/>Yujie Zhang</a>
|
<a href=/people/y/yao-meng/>Yao Meng</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--203><div class="card-body p-3 small">Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained only on non-parallel data is capable of generating diverse paraphrases with specified <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. Additionally, we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.204/>Exploring Task Difficulty for Few-Shot Relation Extraction</a></strong><br><a href=/people/j/jiale-han/>Jiale Han</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--204><div class="card-body p-3 small">Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not distinguish <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>hard tasks</a> from easy ones in the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>learning process</a>. In this paper, we introduce a novel approach based on contrastive learning that learns better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> by exploiting relation label information. We further design a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> that allows the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.208/>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</a></strong><br><a href=/people/f/feiliang-ren/>Feiliang Ren</a>
|
<a href=/people/l/longhui-zhang/>Longhui Zhang</a>
|
<a href=/people/s/shujuan-yin/>Shujuan Yin</a>
|
<a href=/people/x/xiaofeng-zhao/>Xiaofeng Zhao</a>
|
<a href=/people/s/shilei-liu/>Shilei Liu</a>
|
<a href=/people/b/bochao-li/>Bochao Li</a>
|
<a href=/people/y/yaduo-liu/>Yaduo Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--208><div class="card-body p-3 small">Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>table feature</a> for each relation. Then two kinds of global associations are mined from the generated <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>table features</a>. Next, the mined global associations are integrated into the table feature of each relation. This generate-mine-integrate process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation&#8217;s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. Experimental results show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective and it achieves state-of-the-art results on all of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. The source code of our work is available at : https://github.com/neukg/GRTE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.212/>MapRE : An Effective Semantic Mapping Approach for Low-resource Relation Extraction<span class=acl-fixed-case>M</span>ap<span class=acl-fixed-case>RE</span>: An Effective Semantic Mapping Approach for Low-resource Relation Extraction</a></strong><br><a href=/people/m/manqing-dong/>Manqing Dong</a>
|
<a href=/people/c/chunguang-pan/>Chunguang Pan</a>
|
<a href=/people/z/zhipeng-luo/>Zhipeng Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--212><div class="card-body p-3 small">Neural relation extraction models have shown promising results in recent years ; however, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> can significantly improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on low-resource relation extraction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.222" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.222/>Weakly-supervised Text Classification Based on Keyword Graph</a></strong><br><a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/j/jiandong-ding/>Jiandong Ding</a>
|
<a href=/people/y/yi-xu/>Yi Xu</a>
|
<a href=/people/y/yingyao-liu/>Yingyao Liu</a>
|
<a href=/people/s/shuigeng-zhou/>Shuigeng Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--222><div class="card-body p-3 small">Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is an <a href=https://en.wikipedia.org/wiki/Iterative_and_incremental_development>iterative process</a>. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a <a href=https://en.wikipedia.org/wiki/Text_classification>text classifier</a> to classify the unlabeled texts. Finally, we re-extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.223/>Efficient-FedRec : Efficient Federated Learning Framework for Privacy-Preserving News Recommendation<span class=acl-fixed-case>F</span>ed<span class=acl-fixed-case>R</span>ec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation</a></strong><br><a href=/people/j/jingwei-yi/>Jingwei Yi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/r/ruixuan-liu/>Ruixuan Liu</a>
|
<a href=/people/g/guangzhong-sun/>Guangzhong Sun</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--223><div class="card-body p-3 small">News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users&#8217; historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the <a href=https://en.wikipedia.org/wiki/User_model>user model</a> and <a href=https://en.wikipedia.org/wiki/News>news representations</a> from the server, and send their locally computed gradients to the server for <a href=https://en.wikipedia.org/wiki/News_aggregator>aggregation</a>. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.224/>RocketQAv2 : A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking<span class=acl-fixed-case>R</span>ocket<span class=acl-fixed-case>QA</span>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</a></strong><br><a href=/people/r/ruiyang-ren/>Ruiyang Ren</a>
|
<a href=/people/y/yingqi-qu/>Yingqi Qu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/q/qiaoqiao-she/>QiaoQiao She</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--224><div class="card-body p-3 small">In various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other&#8217;s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.227/>Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval</a></strong><br><a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/m/minghan-li/>Minghan Li</a>
|
<a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--227><div class="card-body p-3 small">Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>, but at the cost of large space and memory requirements. In this paper, we analyze the <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>principal component analysis (PCA)</a>, product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracyspace trade-offs, for example, 48 compression with less than 3 % drop in top-100 retrieval accuracy on average or 96 compression with less than 4 % drop. Code and data are available at.<tex-math>48\\times</tex-math> compression with less than 3% drop in top-100 retrieval accuracy on average or <tex-math>96\\times</tex-math> compression with less than 4% drop. Code and data are available at <url>http://pyserini.io/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.228/>Relation Extraction with Word Graphs from N-grams</a></strong><br><a href=/people/h/han-qin/>Han Qin</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--228><div class="card-body p-3 small">Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Therefore, different word pairs from the contexts within and across <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> are weighted in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.229/>A Bayesian Framework for Information-Theoretic Probing<span class=acl-fixed-case>B</span>ayesian Framework for Information-Theoretic Probing</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--229><div class="card-body p-3 small">Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that <a href=https://en.wikipedia.org/wiki/Probing>probing</a> should be seen as approximating a <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>, however, assumes the true <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> of a pair of <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a> is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agentsallowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and <a href=https://en.wikipedia.org/wiki/Information>information</a> can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.230/>Masked Language Modeling and the Distributional Hypothesis : Order Word Matters Pre-training for Little</a></strong><br><a href=/people/k/koustuv-sinha/>Koustuv Sinha</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--230><div class="card-body p-3 small">A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation : MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> after fine-tuning on many downstream tasksincluding tasks specifically designed to be challenging for models that ignore <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.234/>Linguistic Dependencies and Statistical Dependence</a></strong><br><a href=/people/j/jacob-louis-hoover/>Jacob Louis Hoover</a>
|
<a href=/people/w/wenyu-du/>Wenyu Du</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/t/timothy-odonnell/>Timothy J. O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--234><div class="card-body p-3 small">Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, and <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>statistical dependence</a> between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most 0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.<tex-math>\\approx 0.5</tex-math>. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.236/>A Simple and Effective Positional Encoding for Transformers</a></strong><br><a href=/people/p/pu-chin-chen/>Pu-Chin Chen</a>
|
<a href=/people/h/henry-tsai/>Henry Tsai</a>
|
<a href=/people/s/srinadh-bhojanapalli/>Srinadh Bhojanapalli</a>
|
<a href=/people/h/hyung-won-chung/>Hyung Won Chung</a>
|
<a href=/people/y/yin-wen-chang/>Yin-Wen Chang</a>
|
<a href=/people/c/chun-sung-ferng/>Chun-Sung Ferng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--236><div class="card-body p-3 small">Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.237/>Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models</a></strong><br><a href=/people/a/anlin-qu/>Anlin Qu</a>
|
<a href=/people/j/jianwei-niu/>Jianwei Niu</a>
|
<a href=/people/s/shasha-mo/>Shasha Mo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--237><div class="card-body p-3 small">Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the <a href=https://en.wikipedia.org/wiki/Gaussian_function>Gaussian function</a> to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different <a href=https://en.wikipedia.org/wiki/Randomized_algorithm>RPEs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.238/>Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup</a></strong><br><a href=/people/g/guang-liu/>Guang Liu</a>
|
<a href=/people/y/yuzhao-mao/>Yuzhao Mao</a>
|
<a href=/people/h/huang-hailong/>Huang Hailong</a>
|
<a href=/people/g/gao-weiguo/>Gao Weiguo</a>
|
<a href=/people/l/li-xuan/>Li Xuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--238><div class="card-body p-3 small">Mixup is a recent <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> for current deep classification networks. Through training a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> on <a href=https://en.wikipedia.org/wiki/Convex_combination>convex combinations</a> of pairs of examples and their labels, it imposes locally linear constraints on the model&#8217;s input space. However, such strict linear constraints often lead to <a href=https://en.wikipedia.org/wiki/Underfitting>under-fitting</a> which degrades the effects of <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>adversarial perturbation</a> to the mixing coefficients rather than the examples. Thus, slight <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linearity</a> is injected in-between the synthetic examples and synthetic labels. By training on these <a href=https://en.wikipedia.org/wiki/Data>data</a>, the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep networks</a> are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3 %), especially in low-resource conditions (up to 17.5 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.246/>Layer-wise Model Pruning based on <a href=https://en.wikipedia.org/wiki/Mutual_information>Mutual Information</a></a></strong><br><a href=/people/c/chun-fan/>Chun Fan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/t/tianwei-zhang/>Tianwei Zhang</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/x/xiaofei-sun/>Xiaofei Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--246><div class="card-body p-3 small">Inspired by mutual information (MI) based feature selection in SVMs and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>, in this paper, we propose MI-based layer-wise pruning : for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques : (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup ; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.249" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.249/>Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</a></strong><br><a href=/people/a/atsuki-yamaguchi/>Atsuki Yamaguchi</a>
|
<a href=/people/g/george-chrysostomou/>George Chrysostomou</a>
|
<a href=/people/k/katerina-margatina/>Katerina Margatina</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--249><div class="card-body p-3 small">Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [ MASK ] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside <a href=https://en.wikipedia.org/wiki/Multilevel_marketing>MLM</a> other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of <a href=https://en.wikipedia.org/wiki/Machine_learning>MLM</a>. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41 % of the BERT-BASE&#8217;s parameters, BERT-MEDIUM results in only a 1 % drop in GLUE scores with our best objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.250/>HRKD : Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression<span class=acl-fixed-case>HRKD</span>: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression</a></strong><br><a href=/people/c/chenhe-dong/>Chenhe Dong</a>
|
<a href=/people/y/yaliang-li/>Yaliang Li</a>
|
<a href=/people/y/ying-shen/>Ying Shen</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--250><div class="card-body p-3 small">On many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network methods</a>. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at.<url>https://github.com/cheneydon/hrkd</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.252/>Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification</a></strong><br><a href=/people/j/jiachen-tian/>Jiachen Tian</a>
|
<a href=/people/s/shizhan-chen/>Shizhan Chen</a>
|
<a href=/people/x/xiaowang-zhang/>Xiaowang Zhang</a>
|
<a href=/people/z/zhiyong-feng/>Zhiyong Feng</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/s/shaojuan-wu/>Shaojuan Wu</a>
|
<a href=/people/c/chunliu-dou/>Chunliu Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--252><div class="card-body p-3 small">Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> achieve significant improvements over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.255/>MetaTS : Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision<span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>TS</span>: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision</a></strong><br><a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/d/danqing-zhang/>Danqing Zhang</a>
|
<a href=/people/t/tianyu-cao/>Tianyu Cao</a>
|
<a href=/people/y/ying-wei/>Ying Wei</a>
|
<a href=/people/y/yiwei-song/>Yiwei Song</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--255><div class="card-body p-3 small">Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> hinders the effectiveness of <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for <a href=https://en.wikipedia.org/wiki/Multilingualism>multiple languages</a>. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student&#8217;s feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.256/>Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--256><div class="card-body p-3 small">Neural Machine Translation (NMT) has shown a strong ability to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> to disambiguate the meaning of words. However, it remains a challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> to leverage <a href=https://en.wikipedia.org/wiki/Context_(language_use)>broader context information</a> like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English-German and English-French translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.259/>Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</a></strong><br><a href=/people/y/yingmei-guo/>Yingmei Guo</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/m/mingxing-xu/>Mingxing Xu</a>
|
<a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--259><div class="card-body p-3 small">Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various <a href=https://en.wikipedia.org/wiki/Augmented_reality>augmented methods</a>. Those <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provide supervision signals to each other. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the existing <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art by 3.05 and 4.24 percentage points on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>, respectively. The code will be made open sourced on github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.262/>Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation</a></strong><br><a href=/people/x/xinglin-lyu/>Xinglin Lyu</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/z/zhengxian-gong/>Zhengxian Gong</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--262><div class="card-body p-3 small">Recently a number of approaches have been proposed to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply one translation per discourse in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of those words within a <a href=https://en.wikipedia.org/wiki/Hyperlink>link</a> to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on ChineseEnglish and EnglishFrench translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.267/>Self-Supervised Quality Estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--267><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> on several QE tasks in different language pairs and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--269 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.269.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.269" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.269/>STANKER : Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media<span class=acl-fixed-case>STANKER</span>: Stacking Network based on Level-grained Attention-masked <span class=acl-fixed-case>BERT</span> for Rumor Detection on Social Media</a></strong><br><a href=/people/d/dongning-rao/>Dongning Rao</a>
|
<a href=/people/x/xin-miao/>Xin Miao</a>
|
<a href=/people/z/zhihua-jiang/>Zhihua Jiang</a>
|
<a href=/people/r/ran-li/>Ran Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--269><div class="card-body p-3 small">Rumor detection on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare ; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--276 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.276/>GMH : A General Multi-hop Reasoning Model for KG Completion<span class=acl-fixed-case>GMH</span>: A General Multi-hop Reasoning Model for <span class=acl-fixed-case>KG</span> Completion</a></strong><br><a href=/people/y/yao-zhang/>Yao Zhang</a>
|
<a href=/people/h/hongru-liang/>Hongru Liang</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/x/xin-wei/>Xin Wei</a>
|
<a href=/people/n/ning-jiang/>Ning Jiang</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--276><div class="card-body p-3 small">Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model : i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules : 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with significant improvements against baselines in both short and long distance reasoning scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.281/>Self-Supervised Curriculum Learning for Spelling Error Correction</a></strong><br><a href=/people/z/zifa-gan/>Zifa Gan</a>
|
<a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/h/hongying-zan/>Hongying Zan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--281><div class="card-body p-3 small">Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for : 1) scoring the difficulty of training data, and 2) evaluating the competence of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38 % F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--282 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.282/>Fix-Filter-Fix : Intuitively Connect Any Models for Effective Bug Fixing</a></strong><br><a href=/people/h/haiwen-hong/>Haiwen Hong</a>
|
<a href=/people/j/jingfeng-zhang/>Jingfeng Zhang</a>
|
<a href=/people/y/yin-zhang/>Yin Zhang</a>
|
<a href=/people/y/yao-wan/>Yao Wan</a>
|
<a href=/people/y/yulei-sui/>Yulei Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--282><div class="card-body p-3 small">Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a> (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a> that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or F3) for <a href=https://en.wikipedia.org/wiki/Patch_(computing)>bug fixing</a>. F3 connects models with our filter mechanism to filter out the last <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s unchanged fix to the next. We propose an F3 theory that can quantitatively and accurately calculate the F3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic F3 instances, called F3_ST+AT and F3_AT+ST. Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of F3 and corroborates our findings and methodology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.283/>Neuro-Symbolic Reinforcement Learning with First-Order Logic</a></strong><br><a href=/people/d/daiki-kimura/>Daiki Kimura</a>
|
<a href=/people/m/masaki-ono/>Masaki Ono</a>
|
<a href=/people/s/subhajit-chaudhury/>Subhajit Chaudhury</a>
|
<a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/a/akifumi-wachi/>Akifumi Wachi</a>
|
<a href=/people/d/don-joven-agravante/>Don Joven Agravante</a>
|
<a href=/people/m/michiaki-tatsubori/>Michiaki Tatsubori</a>
|
<a href=/people/a/asim-munawar/>Asim Munawar</a>
|
<a href=/people/a/alexander-gray/>Alexander Gray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--283><div class="card-body p-3 small">Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained <a href=https://en.wikipedia.org/wiki/Policy>policies</a> is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a <a href=https://en.wikipedia.org/wiki/Policy>policy</a> in the <a href=https://en.wikipedia.org/wiki/Social_network>network</a> with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.284" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.284/>Biomedical Concept Normalization by Leveraging Hypernyms</a></strong><br><a href=/people/c/cheng-yan/>Cheng Yan</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yafei-shi/>Yafei Shi</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--284><div class="card-body p-3 small">Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the NCBI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.285" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.285/>Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically</a></strong><br><a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/j/junying-chen/>Junying Chen</a>
|
<a href=/people/w/wenxiu-zhou/>Wenxiu Zhou</a>
|
<a href=/people/t/tingyu-liu/>Tingyu Liu</a>
|
<a href=/people/x/xinlan-yang/>Xinlan Yang</a>
|
<a href=/people/w/weihua-peng/>Weihua Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--285><div class="card-body p-3 small">Integrating knowledge into text is a promising way to enrich text representation, especially in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a>. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and <a href=https://en.wikipedia.org/wiki/Dynamic_routing>dynamic routing</a>, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a>, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.287/>SpellBERT : A Lightweight Pretrained Model for Chinese Spelling Check<span class=acl-fixed-case>S</span>pell<span class=acl-fixed-case>BERT</span>: A Lightweight Pretrained Model for <span class=acl-fixed-case>C</span>hinese Spelling Check</a></strong><br><a href=/people/t/tuo-ji/>Tuo Ji</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--287><div class="card-body p-3 small">Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> between correct characters and its visually similar or phonetically similar misuses but the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with <a href=https://en.wikipedia.org/wiki/Character_encoding>character representations</a>, we devise masked language model alike <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training tasks</a>. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.288/>Automated Generation of Accurate & Fluent Medical X-ray Reports<span class=acl-fixed-case>X</span>-ray Reports</a></strong><br><a href=/people/h/hoang-nguyen/>Hoang Nguyen</a>
|
<a href=/people/d/dong-nie/>Dong Nie</a>
|
<a href=/people/t/taivanbat-badamdorj/>Taivanbat Badamdorj</a>
|
<a href=/people/y/yujie-liu/>Yujie Liu</a>
|
<a href=/people/y/yingying-zhu/>Yingying Zhu</a>
|
<a href=/people/j/jason-truong/>Jason Truong</a>
|
<a href=/people/l/li-cheng/>Li Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--288><div class="card-body p-3 small">Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules : taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding ; the embedding representation is then passed to our transformer-based generator, to produce the medical report ; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning <a href=https://en.wikipedia.org/wiki/Fluency>language fluency</a> and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.295/>Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations</a></strong><br><a href=/people/y/yiming-ju/>Yiming Ju</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhixing-tian/>Zhixing Tian</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/x/xiaohuan-cao/>Xiaohuan Cao</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/j/jinlong-li/>Jinlong Li</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--295><div class="card-body p-3 small">Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines&#8217; ability to understand <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. Multiple-choice MRC is one of the most studied tasks in <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MRC</a> due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and reveal how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.296" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.296/>Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models</a></strong><br><a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/r/rumei-li/>Rumei Li</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/h/hongzhi-zhang/>Hongzhi Zhang</a>
|
<a href=/people/z/zan-daoguang/>Zan Daoguang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--296><div class="card-body p-3 small">The key challenge of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a>. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language form</a> and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> learns to align the natural language expressions to the relations in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>KB</a> as well as reason over the missing connections in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>KB</a>. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.300" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.300/>FinQA : A Dataset of <a href=https://en.wikipedia.org/wiki/Numerical_analysis>Numerical Reasoning</a> over Financial Data<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>QA</span>: A Dataset of Numerical Reasoning over Financial Data</a></strong><br><a href=/people/z/zhiyu-chen/>Zhiyu Chen</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/c/charese-smiley/>Charese Smiley</a>
|
<a href=/people/s/sameena-shah/>Sameena Shah</a>
|
<a href=/people/i/iana-borova/>Iana Borova</a>
|
<a href=/people/d/dylan-langdon/>Dylan Langdon</a>
|
<a href=/people/r/reema-moussa/>Reema Moussa</a>
|
<a href=/people/m/matt-beane/>Matt Beane</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Huang</a>
|
<a href=/people/b/bryan-r-routledge/>Bryan Routledge</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--300><div class="card-body p-3 small">The sheer volume of <a href=https://en.wikipedia.org/wiki/Financial_statement>financial statements</a> makes it difficult for humans to access and analyze a business&#8217;s financials. Robust <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a> likewise faces unique challenges in this <a href=https://en.wikipedia.org/wiki/Domain_of_a_function>domain</a>. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the <a href=https://en.wikipedia.org/wiki/Mathematical_finance>finance domain</a> includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and conduct comprehensive experiments in our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset the first of its kind should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.301/>FiD-Ex : Improving Sequence-to-Sequence Models for Extractive Rationale Generation<span class=acl-fixed-case>F</span>i<span class=acl-fixed-case>D</span>-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation</a></strong><br><a href=/people/k/kushal-lakhotia/>Kushal Lakhotia</a>
|
<a href=/people/b/bhargavi-paranjape/>Bhargavi Paranjape</a>
|
<a href=/people/a/asish-ghoshal/>Asish Ghoshal</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/s/srini-iyer/>Srini Iyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--301><div class="card-body p-3 small">Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings ; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by : 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.302/>RockNER : A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models<span class=acl-fixed-case>R</span>ock<span class=acl-fixed-case>NER</span>: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/w/wenyang-gao/>Wenyang Gao</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/r/ryan-moreno/>Ryan Moreno</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--302><div class="card-body p-3 small">To audit the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> ; at the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context level</a>, we use pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of NER models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.305/>COUGH : A Challenge Dataset and Models for COVID-19 FAQ Retrieval<span class=acl-fixed-case>COUGH</span>: A Challenge Dataset and Models for <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>FAQ</span> Retrieval</a></strong><br><a href=/people/x/xinliang-frederick-zhang/>Xinliang Frederick Zhang</a>
|
<a href=/people/h/heming-sun/>Heming Sun</a>
|
<a href=/people/x/xiang-yue/>Xiang Yue</a>
|
<a href=/people/s/simon-lin/>Simon Lin</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--305><div class="card-body p-3 small">We present a large, challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, COUGH, for COVID-19 FAQ retrieval. Similar to a standard <a href=https://en.wikipedia.org/wiki/FAQ>FAQ dataset</a>, COUGH consists of three parts : FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16 K <a href=https://en.wikipedia.org/wiki/FAQ>FAQ items</a> scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> and BERT, among which the best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github.com/sunlab-osu/covid-faq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--307 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.307.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.307/>WinoLogic : A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>L</span>ogic: <span class=acl-fixed-case>A</span> Zero-Shot Logic-based Diagnostic Dataset for <span class=acl-fixed-case>W</span>inograd <span class=acl-fixed-case>S</span>chema <span class=acl-fixed-case>C</span>hallenge</a></strong><br><a href=/people/w/weinan-he/>Weinan He</a>
|
<a href=/people/c/canming-huang/>Canming Huang</a>
|
<a href=/people/y/yongmei-liu/>Yongmei Liu</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--307><div class="card-body p-3 small">The recent success of neural language models (NLMs) on the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge</a> has called for further investigation of the commonsense reasoning ability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Previous diagnostic datasets rely on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. Specifically, we identify and collect formal knowledge formulas verified by <a href=https://en.wikipedia.org/wiki/Automated_theorem_proving>theorem provers</a> and translate such <a href=https://en.wikipedia.org/wiki/Well-formed_formula>formulas</a> into <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sentences</a>. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> named WinoLogic with these sentences. Given a problem in WinoLogic, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WinoLogic to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.309/>Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast</a></strong><br><a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/j/jingming-liu/>Jingming Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--309><div class="card-body p-3 small">In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a>. Pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt <a href=https://en.wikipedia.org/wiki/Molybdenum_disulfide>MoCo</a> (He et al., 2020) to further improve the quality of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a>. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe andSchwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.315/>Virtual Data Augmentation : A Robust and General Framework for Fine-tuning Pre-trained Models</a></strong><br><a href=/people/k/kun-zhou/>Kun Zhou</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--315><div class="card-body p-3 small">Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the <a href=https://en.wikipedia.org/wiki/Gaussian_noise>Gaussian noise</a> provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at blue.<url>https://github.com/RUCAIBox/VDA</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.317.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.317" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.317/>To be Closer : Learning to Link up Aspects with Opinions</a></strong><br><a href=/people/y/yuxiang-zhou/>Yuxiang Zhou</a>
|
<a href=/people/l/lejian-liao/>Lejian Liao</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--317><div class="card-body p-3 small">Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> compared to the standard dependency parse tree. The learning process allows the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19 % on the standard SemEval Restaurant14 (CITATION) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--319 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.319/>Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph</a></strong><br><a href=/people/j/jianzhu-bao/>Jianzhu Bao</a>
|
<a href=/people/b/bin-liang/>Bin Liang</a>
|
<a href=/people/j/jingyi-sun/>Jingyi Sun</a>
|
<a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--319><div class="card-body p-3 small">Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.321/>Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories</a></strong><br><a href=/people/h/han-qin/>Han Qin</a>
|
<a href=/people/g/guimin-chen/>Guimin Chen</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--321><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful <a href=https://en.wikipedia.org/wiki/Categorical_variable>categorical information</a> for localized predictions. Experimental results on a simulated environment for <a href=https://en.wikipedia.org/wiki/FL_(programming_language)>FL</a> with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed <a href=https://en.wikipedia.org/wiki/FL_(programming_language)>FL frameworks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.323/>CTAL : Pre-training Cross-modal Transformer for Audio-and-Language Representations<span class=acl-fixed-case>CTAL</span>: Pre-training Cross-modal Transformer for Audio-and-Language Representations</a></strong><br><a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/w/wenbiao-ding/>Wenbiao Ding</a>
|
<a href=/people/y/yu-kang/>Yu Kang</a>
|
<a href=/people/t/tianqiao-liu/>Tianqiao Liu</a>
|
<a href=/people/z/zhongqin-wu/>Zhongqin Wu</a>
|
<a href=/people/z/zitao-liu/>Zitao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--323><div class="card-body p-3 small">Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are facing challenges of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs : masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Speaker_verification>speaker verification</a>. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.325/>Mutual-Learning Improves End-to-End Speech Translation</a></strong><br><a href=/people/j/jiawei-zhao/>Jiawei Zhao</a>
|
<a href=/people/w/wei-luo/>Wei Luo</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/a/andrew-gilman/>Andrew Gilman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--325><div class="card-body p-3 small">A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) task</a> to improve the <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation (ST) task</a>. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternativea trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher / student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.328/>Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments<span class=acl-fixed-case>LAW</span>) Supervision for Vision-and-Language Navigation in Continuous Environments</a></strong><br><a href=/people/s/sonia-raychaudhuri/>Sonia Raychaudhuri</a>
|
<a href=/people/s/saim-wani/>Saim Wani</a>
|
<a href=/people/s/shivansh-patel/>Shivansh Patel</a>
|
<a href=/people/u/unnat-jain/>Unnat Jain</a>
|
<a href=/people/a/angel-chang/>Angel Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--328><div class="card-body p-3 small">In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle &#8216;off the path&#8217; scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent&#8217;s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> that measures the number of sub-instructions the agent has completed during <a href=https://en.wikipedia.org/wiki/Navigation>navigation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.329/>How to leverage the multimodal EHR data for better medical prediction?<span class=acl-fixed-case>EHR</span> data for better medical prediction?</a></strong><br><a href=/people/b/bo-yang/>Bo Yang</a>
|
<a href=/people/l/lijun-wu/>Lijun Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--329><div class="card-body p-3 small">Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Specifically, the data produced in the hospital admissions are monitored by the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR system</a>, which includes structured data like <a href=https://en.wikipedia.org/wiki/Thermoregulation>daily body temperature</a> and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a> like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR data</a>, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different <a href=https://en.wikipedia.org/wiki/Data>data</a> from various views are all beneficial to the medical tasks and how to best utilize these <a href=https://en.wikipedia.org/wiki/Data>data</a> remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR</a> and propose a method to integrate these <a href=https://en.wikipedia.org/wiki/Data>data</a>, we also comprehensively study the different models and the <a href=https://en.wikipedia.org/wiki/Data>data leverage methods</a> for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--331 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.331/>Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization</a></strong><br><a href=/people/y/yong-guan/>Yong Guan</a>
|
<a href=/people/s/shaoru-guo/>Shaoru Guo</a>
|
<a href=/people/r/ru-li/>Ru Li</a>
|
<a href=/people/x/xiaoli-li/>Xiaoli Li</a>
|
<a href=/people/h/hongye-tan/>Hongye Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--331><div class="card-body p-3 small">Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame semantics</a> to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN / DM and NYT demonstrate that our model outperforms six state-of-the-art methods significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.332/>CAST : Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees<span class=acl-fixed-case>CAST</span>: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</a></strong><br><a href=/people/e/ensheng-shi/>Ensheng Shi</a>
|
<a href=/people/y/yanlin-wang/>Yanlin Wang</a>
|
<a href=/people/l/lun-du/>Lun Du</a>
|
<a href=/people/h/hongyu-zhang/>Hongyu Zhang</a>
|
<a href=/people/s/shi-han/>Shi Han</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a>
|
<a href=/people/h/hongbin-sun/>Hongbin Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--332><div class="card-body p-3 small">Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size / depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AST</a> into a set of <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>subtrees</a> and utilize a <a href=https://en.wikipedia.org/wiki/Recursive_neural_network>recursive neural network</a> to encode the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>subtrees</a>. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for <a href=https://en.wikipedia.org/wiki/Automatic_programming>code summarization</a>. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.com/DeepSoftwareAnalytics/CAST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.335" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.335/>Transformer-based Lexically Constrained Headline Generation</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--335><div class="card-body p-3 small">This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a <a href=https://en.wikipedia.org/wiki/Headline>headline</a> including a given phrase by providing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> with additional information corresponding to the given phrase. However, these methods can not always include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--337 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.337/>Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--337><div class="card-body p-3 small">Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.339.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.339/>A Unified Encoding of Structures in Transition Systems</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--339><div class="card-body p-3 small">Transition systems usually contain various <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic structures</a> (e.g., <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stacks</a>, buffers). An ideal transition-based model should encode these <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a> completely and efficiently. Previous works relying on <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a>. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding <a href=https://en.wikipedia.org/wiki/Transition_state>transition states</a> with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.342" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.342/>Topic Transferable Table Question Answering</a></strong><br><a href=/people/s/saneem-chemmengath/>Saneem Chemmengath</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/j/jaydeep-sen/>Jaydeep Sen</a>
|
<a href=/people/m/mustafa-canim/>Mustafa Canim</a>
|
<a href=/people/s/soumen-chakrabarti/>Soumen Chakrabarti</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--342><div class="card-body p-3 small">Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT&#8217;s pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTable-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTable-Questions datasets. We empirically show that, despite pre-training on large open-domain text, performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of : (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic-specific training data, and (3) a logical form re-ranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.343/>WebSRC : A Dataset for Web-Based Structural Reading Comprehension<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>SRC</span>: A Dataset for Web-Based Structural Reading Comprehension</a></strong><br><a href=/people/x/xingyu-chen/>Xingyu Chen</a>
|
<a href=/people/z/zihan-zhao/>Zihan Zhao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/j/jiabao-ji/>JiaBao Ji</a>
|
<a href=/people/d/danyang-zhang/>Danyang Zhang</a>
|
<a href=/people/a/ao-luo/>Ao Luo</a>
|
<a href=/people/y/yuxuan-xiong/>Yuxuan Xiong</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--343><div class="card-body p-3 small">Web search is an essential way for humans to obtain information, but it&#8217;s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a <a href=https://en.wikipedia.org/wiki/Web_page>web page</a> and a question about it, the task is to find an answer from the web page. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires a system not only to understand the <a href=https://en.wikipedia.org/wiki/Semantics>semantics of texts</a> but also the <a href=https://en.wikipedia.org/wiki/Web_design>structure of the web page</a>. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400 K question-answer pairs, which are collected from 6.4 K web pages with corresponding HTML source code, <a href=https://en.wikipedia.org/wiki/Screenshot>screenshots</a>, and <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes / no. We evaluate various strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to show the difficulty of our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also investigate the usefulness of structural information and <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and baselines have been publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.344/>Cryptonite : A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</a></strong><br><a href=/people/a/avia-efrat/>Avia Efrat</a>
|
<a href=/people/u/uri-shaham/>Uri Shaham</a>
|
<a href=/people/d/dan-kilman/>Dan Kilman</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--344><div class="card-body p-3 small">Current NLP datasets targeting <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> can be solved by a native speaker with relative ease. We present <a href=https://en.wikipedia.org/wiki/Cryptonite>Cryptonite</a>, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in <a href=https://en.wikipedia.org/wiki/Cryptonite>Cryptonite</a> is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Cryptonite is a challenging task for current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> ; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, on par with the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a rule-based clue solver (8.6 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.346/>Improving Query Graph Generation for Complex Question Answering over Knowledge Base</a></strong><br><a href=/people/k/kechen-qin/>Kechen Qin</a>
|
<a href=/people/c/cheng-li/>Cheng Li</a>
|
<a href=/people/v/virgil-pavlu/>Virgil Pavlu</a>
|
<a href=/people/j/javed-aslam/>Javed Aslam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--346><div class="card-body p-3 small">Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner : we start with the entire <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and gradually shrink it to the desired query graph. This approach improves both the <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of query graph generation, especially for complex multi-hop questions. Experimental results show that our method achieves state-of-the-art performance on ComplexWebQuestion (CWQ) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.349/>Generic resources are what you need : Style transfer tasks without task-specific parallel training data</a></strong><br><a href=/people/h/huiyuan-lai/>Huiyuan Lai</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--349><div class="card-body p-3 small">Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (sourcetarget) data outperforms existing unsupervised approaches on the two most popular style transfer tasks : formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First, we strengthen the model&#8217;s ability to rewrite by further pre-training BART on both an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative back-translation approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs, dynamically in the training process. Lastly, we let our best resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generate static synthetic pairs to be used in a supervised training regime. Besides methodology and state-of-the-art results, a core contribution of this work is a reflection on the nature of the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> we address, and how their differences are highlighted by their response to our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--350 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.350/>Revisiting Pivot-Based Paraphrase Generation : Language Is Not the Only Optional Pivot</a></strong><br><a href=/people/y/yitao-cai/>Yitao Cai</a>
|
<a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--350><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the <a href=https://en.wikipedia.org/wiki/Round-trip_translation>round-trip translation</a>, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with better quality than taking <a href=https://en.wikipedia.org/wiki/Language>language</a> as the pivot. The end-to-end framework can reduce <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a> when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.356/>DuRecDial 2.0 : A Bilingual Parallel Corpus for Conversational Recommendation<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>R</span>ec<span class=acl-fixed-case>D</span>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</a></strong><br><a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--356><div class="card-body p-3 small">In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.357" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.357/>End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs</a></strong><br><a href=/people/d/dinesh-raghu/>Dinesh Raghu</a>
|
<a href=/people/s/shantanu-agarwal/>Shantanu Agarwal</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a>
|
<a href=/people/m/mausam/>Mausam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--357><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the <a href=https://en.wikipedia.org/wiki/Flowchart>flowchart</a> without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--358 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.358/>Dimensional Emotion Detection from Categorical Emotion</a></strong><br><a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/jiseon-kim/>Jiseon Kim</a>
|
<a href=/people/s/seonghyeon-ye/>Seonghyeon Ye</a>
|
<a href=/people/j/jaeyeol-jeon/>Jaeyeol Jeon</a>
|
<a href=/people/h/hee-young-park/>Hee Young Park</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--358><div class="card-body p-3 small">We present a model to predict fine-grained emotions along the continuous dimensions of <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>valence</a>, <a href=https://en.wikipedia.org/wiki/Arousal>arousal</a>, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover&#8217;s Distance) loss between the predicted <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD score distribution</a> and the categorical emotion distributions sorted along <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD</a>, and it can simultaneously classify the emotion categories and predict the <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD scores</a> for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--360 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.360" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.360/>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</a></strong><br><a href=/people/x/xincheng-ju/>Xincheng Ju</a>
|
<a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/r/rong-xiao/>Rong Xiao</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--360><div class="card-body p-3 small">Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both <a href=https://en.wikipedia.org/wiki/Aspect_(linguistics)>aspect terms</a> and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between <a href=https://en.wikipedia.org/wiki/MATE_(software)>MATE</a> and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--361 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.361" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.361/>Solving Aspect Category Sentiment Analysis as a Text Generation Task</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/h/hanmeng-liu/>Hanmeng Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--361><div class="card-body p-3 small">Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more direct use of pre-trained knowledge in seq2seq language models by directly following the task setting during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a>. Experiments on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.364/>CHoRaL : Collecting Humor Reaction Labels from Millions of Social Media Users<span class=acl-fixed-case>CH</span>o<span class=acl-fixed-case>R</span>a<span class=acl-fixed-case>L</span>: Collecting Humor Reaction Labels from Millions of Social Media Users</a></strong><br><a href=/people/z/zixiaofan-yang/>Zixiaofan Yang</a>
|
<a href=/people/s/shayan-hooshmand/>Shayan Hooshmand</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--364><div class="card-body p-3 small">Humor detection has gained attention in recent years due to the desire to understand <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> with <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> and non-humor. We present the largest <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to date with labeled humor on 785 K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.366/>CodRED : A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild<span class=acl-fixed-case>C</span>od<span class=acl-fixed-case>RED</span>: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</a></strong><br><a href=/people/y/yuan-yao/>Yuan Yao</a>
|
<a href=/people/j/jiaju-du/>Jiaju Du</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--366><div class="card-body p-3 small">Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges : Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations ; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.368.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.368/>We Need to Talk About train-dev-test Splits</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--368><div class="card-body p-3 small">Standard train-dev-test splits used to benchmark multiple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> against each other are ubiquitously used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has led to a different use of these standard splits ; the development set is now often used for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> during the training procedure. Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the test data, leading to faster overfitting and expiration of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can safely be done on the development data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--369 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.369" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.369/>PhoMT : A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation<span class=acl-fixed-case>P</span>ho<span class=acl-fixed-case>MT</span>: A High-Quality and Large-Scale Benchmark Dataset for <span class=acl-fixed-case>V</span>ietnamese-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/l/long-doan/>Long Doan</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/n/nguyen-luong-tran/>Nguyen Luong Tran</a>
|
<a href=/people/t/thai-hoang/>Thai Hoang</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--369><div class="card-body p-3 small">We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02 M sentence pairs, which is 2.9 M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations : the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at : https://github.com/VinAIResearch/PhoMT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.374/>Mind the Style of Text ! Adversarial and Backdoor Attacks Based on Text Style Transfer</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yangyi-chen/>Yangyi Chen</a>
|
<a href=/people/x/xurui-zhang/>Xurui Zhang</a>
|
<a href=/people/m/mukai-li/>Mukai Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--374><div class="card-body p-3 small">Adversarial attacks and <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attacks</a> are two common security threats that hang over <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attack method</a>, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transferthe attack success rates can exceed 90 % without much effort. It reflects the limited ability of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.376/>Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes</a></strong><br><a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/d/david-marecek/>David Mareček</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--376><div class="card-body p-3 small">State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareek, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT&#8217;s contextual representations for nine diverse languages. We observe that for languages closely related to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply <a href=https://en.wikipedia.org/wiki/Orthogonal_transformation>orthogonal transformation</a> learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.381/>Extracting Fine-Grained Knowledge Graphs of Scientific Claims : Dataset and Transformer-Based Results</a></strong><br><a href=/people/i/ian-magnusson/>Ian Magnusson</a>
|
<a href=/people/s/scott-friedman/>Scott Friedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--381><div class="card-body p-3 small">Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.385/>AVocaDo : Strategy for Adapting Vocabulary to Downstream Domain<span class=acl-fixed-case>AV</span>oca<span class=acl-fixed-case>D</span>o: Strategy for Adapting Vocabulary to Downstream Domain</a></strong><br><a href=/people/j/jimin-hong/>Jimin Hong</a>
|
<a href=/people/t/taehee-kim/>TaeHee Kim</a>
|
<a href=/people/h/hyesu-lim/>Hyesu Lim</a>
|
<a href=/people/j/jaegul-choo/>Jaegul Choo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--385><div class="card-body p-3 small">During the fine-tuning phase of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, the pretrained vocabulary remains unchanged, while <a href=https://en.wikipedia.org/wiki/Parameter>model parameters</a> are updated. The <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> generated based on the pretrained data is suboptimal for <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream data</a> when domain discrepancy exists. We propose to consider the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> as an optimizable parameter, allowing us to update the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words from <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> to downstream data by utilizing knowledge learned from a pretrained language model with a regularization term. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieved consistent performance improvements on diverse domains (i.e., <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical</a>, <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>, <a href=https://en.wikipedia.org/wiki/News>news</a>, and reviews).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--388 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.388" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.388/>Can <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> be Biomedical Knowledge Bases?</a></strong><br><a href=/people/m/mujeen-sung/>Mujeen Sung</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/s/sean-yi/>Sean Yi</a>
|
<a href=/people/m/minji-jeon/>Minji Jeon</a>
|
<a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--388><div class="card-body p-3 small">Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>LMs</a> contain and how we can extract that knowledge, treating <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>LMs</a> as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49 K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51 % Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.391/>Learning grounded word meaning representations on similarity graphs</a></strong><br><a href=/people/m/mariella-dimiccoli/>Mariella Dimiccoli</a>
|
<a href=/people/h/herwig-wendt/>Herwig Wendt</a>
|
<a href=/people/p/pau-batlle-franch/>Pau Batlle Franch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--391><div class="card-body p-3 small">This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a> models <a href=https://en.wikipedia.org/wiki/Modality_(semiotics)>modality-specific word representations</a>, conditioned to another <a href=https://en.wikipedia.org/wiki/Modality_(semiotics)>modality</a>, through dedicated but communicating graphs, while the higher level puts these representations together on a single <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to learn a representation jointly from both modalities. The topology of each <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> models similarity relations among words, and is estimated jointly with the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph embedding</a>. The assumption underlying this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is that words sharing similar meaning correspond to <a href=https://en.wikipedia.org/wiki/Community>communities</a> in an underlying <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>low-dimensional space</a>. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgments and concept categorization, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.394/>On the Relation between Syntactic Divergence and Zero-Shot Performance</a></strong><br><a href=/people/o/ofir-arviv/>Ofir Arviv</a>
|
<a href=/people/d/dmitry-nikolaev/>Dmitry Nikolaev</a>
|
<a href=/people/t/taelin-karidi/>Taelin Karidi</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--394><div class="card-body p-3 small">We explore the link between the extent to which syntactic relations are preserved in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and the ease of correctly constructing a <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edgesa gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a standard English RE benchmark translated to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.395/>Improved Latent Tree Induction with Distant Supervision via Span Constraints</a></strong><br><a href=/people/z/zhiyang-xu/>Zhiyang Xu</a>
|
<a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/s/subendhu-rongali/>Subendhu Rongali</a>
|
<a href=/people/d/dylan-finkbeiner/>Dylan Finkbeiner</a>
|
<a href=/people/s/shilpa-suresh/>Shilpa Suresh</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--395><div class="card-body p-3 small">For over thirty years, researchers have developed and analyzed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern <a href=https://en.wikipedia.org/wiki/System>systems</a> still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>span constraints</a> (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.397" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.397/>Just Say No : Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts</a></strong><br><a href=/people/a/ashutosh-baheti/>Ashutosh Baheti</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--397><div class="card-body p-3 small">Dialogue models trained on <a href=https://en.wikipedia.org/wiki/Conversation>human conversations</a> inadvertently learn to generate <a href=https://en.wikipedia.org/wiki/Toxicity>toxic responses</a>. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> and <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>. Our analysis reveals that 42 % of human responses agree with toxic comments, whereas only 13 % agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19 % reduction in agreement with offensive comments and produces 29 % fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.398/>Multi-Modal Open-Domain Dialogue</a></strong><br><a href=/people/k/kurt-shuster/>Kurt Shuster</a>
|
<a href=/people/e/eric-michael-smith/>Eric Michael Smith</a>
|
<a href=/people/d/da-ju/>Da Ju</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--398><div class="card-body p-3 small">Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020 ; Roller et al., 2020). However, if we want to build <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and show that such efforts do not diminish model performance with respect to human preference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.402/>RAST : Domain-Robust Dialogue Rewriting as Sequence Tagging<span class=acl-fixed-case>RAST</span>: Domain-Robust Dialogue Rewriting as Sequence Tagging</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--402><div class="card-body p-3 small">The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s outputs may lack fluency. To alleviate this issue, we inject the loss signal from <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the current state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> when transferring to another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--404 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.404/>Dialogue State Tracking with a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> using Schema-Driven Prompting</a></strong><br><a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--404><div class="card-body p-3 small">Task-oriented conversational systems often use dialogue state tracking to represent the user&#8217;s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely <a href=https://en.wikipedia.org/wiki/Generative_model>generative system</a> achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> : MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.409/>Pre-train or Annotate? Domain Adaptation with a Constrained Budget</a></strong><br><a href=/people/f/fan-bai/>Fan Bai</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--409><div class="card-body p-3 small">Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question : given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> under varying <a href=https://en.wikipedia.org/wiki/Budget_constraint>budget constraints</a> to assess which combination strategy works best. We find that, for small budgets, spending all funds on <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> leads to the best performance ; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.416/>Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning</a></strong><br><a href=/people/l/li-zhou/>Li Zhou</a>
|
<a href=/people/k/kevin-small/>Kevin Small</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a>
|
<a href=/people/s/sandeep-atluri/>Sandeep Atluri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--416><div class="card-body p-3 small">Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>, a common problem in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.417/>Unsupervised Paraphrasing with Pretrained Language Models</a></strong><br><a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/h/huan-wang/>Huan Wang</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--417><div class="card-body p-3 small">Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training pipeline</a> that enables pre-trained language models to generate high-quality paraphrases in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a <a href=https://en.wikipedia.org/wiki/Surface_form>surface form</a> dissimilar from the input, whenever the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers to <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> in other languages without any additional <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.418/>Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness</a></strong><br><a href=/people/h/hengtong-zhang/>Hengtong Zhang</a>
|
<a href=/people/t/tianhang-zheng/>Tianhang Zheng</a>
|
<a href=/people/y/yaliang-li/>Yaliang Li</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a>
|
<a href=/people/l/lu-su/>Lu Su</a>
|
<a href=/people/b/bo-li-vanderbilt/>Bo Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--418><div class="card-body p-3 small">Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of <a href=https://en.wikipedia.org/wiki/Profanity>profanity examples</a> to prevent seq2seq models from generating a broader spectrum of <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a> in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--419 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.419/>Journalistic Guidelines Aware News Image Captioning</a></strong><br><a href=/people/x/xuewen-yang/>Xuewen Yang</a>
|
<a href=/people/s/svebor-karaman/>Svebor Karaman</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/alejandro-jaimes/>Alejandro Jaimes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--419><div class="card-body p-3 small">The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.420/>AESOP : Paraphrase Generation with Adaptive Syntactic Control<span class=acl-fixed-case>AESOP</span>: Paraphrase Generation with Adaptive Syntactic Control</a></strong><br><a href=/people/j/jiao-sun/>Jiao Sun</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--420><div class="card-body p-3 small">We propose to control <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> through carefully chosen target syntactic structures to generate more proper and higher quality <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models&#8217; <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to syntactic perturbation by <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> on two GLUE tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.421/>Refocusing on Relevance : Personalization in NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--421><div class="card-body p-3 small">Many NLG tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, dialogue response, or <a href=https://en.wikipedia.org/wiki/Question_answering>open domain question answering</a>, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user&#8217;s intent or context of work is not easily recoverable based solely on that source text a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, and argue that value-sensitive design represents a crucial path forward through these challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.427" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.427/>Learning Prototype Representations Across Few-Shot Tasks for Event Detection</a></strong><br><a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--427><div class="card-body p-3 small">We address the <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> and outlier issues in few-shot learning for event detection, a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> across tasks to make the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> more robust to <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a>. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.428/>Lifelong Event Detection with Knowledge Transfer</a></strong><br><a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--428><div class="card-body p-3 small">Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> exist among hierarchical knowledge element types. In our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, knowledge is being transferred between learned <a href=https://en.wikipedia.org/wiki/Event_(computing)>old event types</a> and <a href=https://en.wikipedia.org/wiki/Event_(computing)>new event types</a>. Specifically, we update old knowledge with new event types&#8217; mentions using a self-training loss. In addition, we aggregate old event types&#8217; representations based on their similarities with new event types to initialize the new event types&#8217; representations. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> outperforms competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> with a 5.1 % absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30 % absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.432/>Adversarial Attack against Cross-lingual Knowledge Graph Alignment</a></strong><br><a href=/people/z/zeru-zhang/>Zeru Zhang</a>
|
<a href=/people/z/zijie-zhang/>Zijie Zhang</a>
|
<a href=/people/y/yang-zhou/>Yang Zhou</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/x/xiaoying-han/>Xiaoying Han</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a>
|
<a href=/people/t/tianshi-che/>Tianshi Che</a>
|
<a href=/people/d/da-yan/>Da Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--432><div class="card-body p-3 small">Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--435 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.435/>Incorporating medical knowledge in BERT for clinical relation extraction<span class=acl-fixed-case>BERT</span> for clinical relation extraction</a></strong><br><a href=/people/a/arpita-roy/>Arpita Roy</a>
|
<a href=/people/s/shimei-pan/>Shimei Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--435><div class="card-body p-3 small">In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a>. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> versus clinic notes), these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> may not be ideal for <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific tasks</a> (e.g., extracting clinical relations). Furthermore, it may require additional <a href=https://en.wikipedia.org/wiki/Medicine>medical knowledge</a> to understand <a href=https://en.wikipedia.org/wiki/Medical_literature>clinical text</a> properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add <a href=https://en.wikipedia.org/wiki/Medicine>medical knowledge</a> into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2 / VA 2010 clinical relation extraction dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.436/>ECONET : Effective Continual Pretraining of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Event Temporal Reasoning<span class=acl-fixed-case>ECONET</span>: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--436><div class="card-body p-3 small">While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This * * E**ffective * * CON**tinual pre-training framework for * * E**vent * * T**emporal reasoning (ECONET) improves the PTLMs&#8217; fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--441 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.441" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.441/>Corpus-based Open-Domain Event Type Induction</a></strong><br><a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/y/yunyi-zhang/>Yunyi Zhang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--441><div class="card-body p-3 small">Traditional event extraction methods require predefined <a href=https://en.wikipedia.org/wiki/Event_(computing)>event types</a> and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of predicate sense, object head pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering predicate sense, object head pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.442/>PDALN : Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition<span class=acl-fixed-case>PDALN</span>: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition</a></strong><br><a href=/people/t/tao-zhang/>Tao Zhang</a>
|
<a href=/people/c/congying-xia/>Congying Xia</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a>
|
<a href=/people/z/zhiwei-liu/>Zhiwei Liu</a>
|
<a href=/people/s/shu-zhao/>Shu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--442><div class="card-body p-3 small">Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach PDALN. It achieves superior domain adaptability by employing three components : (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously ; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains ; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> indicates the state-of-the-art performance of PDALN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--453 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.453/>Average Approximates First Principal Component? An Empirical Analysis on Representations from Neural Language Models</a></strong><br><a href=/people/z/zihan-wang/>Zihan Wang</a>
|
<a href=/people/c/chengyu-dong/>Chengyu Dong</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--453><div class="card-body p-3 small">Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> remains a mystery. In this paper, we present an empirical property of these representationsaverage approximates <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>first principal component</a>. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a <a href=https://en.wikipedia.org/wiki/Normal_distribution>normal distribution</a> for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.460" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.460/>Continual Few-Shot Learning for Text Classification</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--460><div class="card-body p-3 small">Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, handle numbers, perform <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data : two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> from diverse <a href=https://en.wikipedia.org/wiki/Paradigm_(disambiguation)>paradigms</a> (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--461 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.461" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.461/>Efficient Nearest Neighbor Language Models</a></strong><br><a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--461><div class="card-body p-3 small">Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>inference overhead</a> and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.464" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.464/>Gradient-based Adversarial Attacks against Text Transformers</a></strong><br><a href=/people/c/chuan-guo/>Chuan Guo</a>
|
<a href=/people/a/alexandre-sablayrolles/>Alexandre Sablayrolles</a>
|
<a href=/people/h/herve-jegou/>Hervé Jégou</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--464><div class="card-body p-3 small">We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--465 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.465" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.465/>Do Transformer Modifications Transfer Across Implementations and Applications?</a></strong><br><a href=/people/s/sharan-narang/>Sharan Narang</a>
|
<a href=/people/h/hyung-won-chung/>Hyung Won Chung</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/l/liam-fedus/>Liam Fedus</a>
|
<a href=/people/t/thibault-fevry/>Thibault Fevry</a>
|
<a href=/people/m/michael-matena/>Michael Matena</a>
|
<a href=/people/k/karishma-malkan/>Karishma Malkan</a>
|
<a href=/people/n/noah-fiedel/>Noah Fiedel</a>
|
<a href=/people/n/noam-shazeer/>Noam Shazeer</a>
|
<a href=/people/z/zhenzhong-lan/>Zhenzhong Lan</a>
|
<a href=/people/y/yanqi-zhou/>Yanqi Zhou</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/n/nan-ding/>Nan Ding</a>
|
<a href=/people/j/jake-marcus/>Jake Marcus</a>
|
<a href=/people/a/adam-roberts/>Adam Roberts</a>
|
<a href=/people/c/colin-raffel/>Colin Raffel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--465><div class="card-body p-3 small">The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Surprisingly, we find that most <a href=https://en.wikipedia.org/wiki/Mod_(video_gaming)>modifications</a> do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same <a href=https://en.wikipedia.org/wiki/Codebase>codebase</a> that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--466 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.466/>Paired Examples as Indirect Supervision in Latent Decision Models</a></strong><br><a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--466><div class="card-body p-3 small">Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP : (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--467 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.467" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.467/>Pairwise Supervised Contrastive Learning of Sentence Representations</a></strong><br><a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/w/wei-xiao/>Wei Xiao</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/a/andrew-o-arnold/>Andrew O. Arnold</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--467><div class="card-body p-3 small">Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with <a href=https://en.wikipedia.org/wiki/Triplet_loss>triplet loss</a> or siamese loss. Nevertheless, they share a common weakness : sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a> that involve understanding sentence semantics at different <a href=https://en.wikipedia.org/wiki/Granularity>granularities</a>. We outperform the previous state-of-the-art method with 10%13 % averaged improvement on eight clustering tasks, and 5%6 % averaged improvement on seven semantic textual similarity (STS) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.474.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--474 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.474 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.474/>Classification-based Quality Estimation : Small and Efficient Models for Real-world Applications</a></strong><br><a href=/people/s/shuo-sun/>Shuo Sun</a>
|
<a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--474><div class="card-body p-3 small">Sentence-level Quality estimation (QE) of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is traditionally formulated as a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>, and the performance of QE models is typically measured by <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>. However, we argue that the level of expressiveness of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a continuous range is unnecessary given the downstream applications of <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>, and show that reframing <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> as a classification problem and evaluating <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE models</a> using classification metrics would better reflect their actual performance in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.477" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.477/>Rule-based Morphological Inflection Improves Neural Terminology Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--477><div class="card-body p-3 small">Current approaches to incorporating terminology constraints in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> typically assume that the constraint terms are provided in their correct <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological forms</a>. This limits their application to real-world scenarios where <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint terms</a> are provided as lemmas. In this paper, we introduce a modular framework for incorporating <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.479.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.479/>Good-Enough Example Extrapolation</a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--479><div class="card-body p-3 small">This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. To operationalize this question, I propose a simple data augmentation protocol called good-enough example extrapolation (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than <a href=https://en.wikipedia.org/wiki/Upsampling>upsampling</a> and other hidden-space data augmentation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.489.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--489 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.489 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.489/>A Scalable Framework for Learning From Implicit User Feedback to Improve <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> in Large-Scale Conversational AI Systems<span class=acl-fixed-case>AI</span> Systems</a></strong><br><a href=/people/s/sunghyun-park/>Sunghyun Park</a>
|
<a href=/people/h/han-li/>Han Li</a>
|
<a href=/people/a/ameen-patel/>Ameen Patel</a>
|
<a href=/people/s/sidharth-mudgal/>Sidharth Mudgal</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--489><div class="card-body p-3 small">Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and improving NLU for a large-scale production system across 10 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.495.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--495 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.495 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.495" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.495/>Single-dataset Experts for Multi-dataset Question Answering</a></strong><br><a href=/people/d/dan-friedman/>Dan Friedman</a>
|
<a href=/people/b/ben-dodge/>Ben Dodge</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--495><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been created for training reading comprehension models, and a natural question is whether we can combine them to build <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.496" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.496/>Simple Entity-Centric Questions Challenge Dense Retrievers</a></strong><br><a href=/people/c/christopher-sciavolino/>Christopher Sciavolino</a>
|
<a href=/people/z/zexuan-zhong/>Zexuan Zhong</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--496><div class="card-body p-3 small">Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse models</a> using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a>. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> (e.g., Where was Arve Furset born?), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--497 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.497" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.497/>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</a></strong><br><a href=/people/a/ansong-ni/>Ansong Ni</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--497><div class="card-body p-3 small">Question Answering (QA) tasks requiring information from multiple documents often rely on a <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval model</a> to identify relevant information for <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model can not rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, <a href=https://en.wikipedia.org/wiki/IIRC>IIRC</a> and HotpotQA. On IIRC, we show that joint modeling with marginalization improves <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--500 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.500/>BiSECT : Learning to Split and Rephrase Sentences with Bitexts<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>SECT</span>: Learning to Split and Rephrase Sentences with Bitexts</a></strong><br><a href=/people/j/joongwon-kim/>Joongwon Kim</a>
|
<a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--500><div class="card-body p-3 small">An important task in NLP applications such as <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a> is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this &#8216;split and rephrase&#8217; task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to convert both sides of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--502 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.502/>Universal Sentence Representation Learning with Conditional Masked Language Model</a></strong><br><a href=/people/z/ziyi-yang/>Ziyi Yang</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/j/jax-law/>Jax Law</a>
|
<a href=/people/e/eric-darve/>Eric Darve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--502><div class="card-body p-3 small">This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning method</a>, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10 % improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--504 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.504/>Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/a/aadit-trivedi/>Aadit Trivedi</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--504><div class="card-body p-3 small">Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an <a href=https://en.wikipedia.org/wiki/Enthymeme>enthymeme</a>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. The largest available <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Enthymemes>enthymemes</a> (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset : <a href=https://en.wikipedia.org/wiki/Abductive_reasoning>Abductive reasoning</a> in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.<i>implicit premise in an enthymeme</i>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--505 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.505" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.505/>Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--505><div class="card-body p-3 small">Systematic compositionality is an essential mechanism in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>, allowing the recombination of known parts to create novel <a href=https://en.wikipedia.org/wiki/Idiom>expressions</a>. However, existing neural models have been shown to lack this basic ability in learning <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic structures</a>. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from 10 % to 100 %. With only 418 (5 %) training instances, our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> still achieves 97.8 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the MCD1 split. Therefore, we argue that <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> can be induced in <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention&#8217;s query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--508 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.508" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.508/>Think about it ! Improving defeasible reasoning by first modeling the question scenario.</a></strong><br><a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--508><div class="card-body p-3 small">Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on <a href=https://en.wikipedia.org/wiki/Defeasible_reasoning>defeasible reasoning</a> suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first create a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph of relevant influences</a>, and then leverage that <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a <a href=https://en.wikipedia.org/wiki/System>system</a> to think about a question and explicitly model the scenario, rather than answering reflexively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--511 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.511" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.511/>Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation</a></strong><br><a href=/people/y/yingjie-li/>Yingjie Li</a>
|
<a href=/people/c/chenye-zhao/>Chenye Zhao</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--511><div class="card-body p-3 small">Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, one of the remaining challenges is the scarcity of <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on each dataset and datasets of different domains, respectively. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a large margin. We publicly release our code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--513 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.513/>Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--513><div class="card-body p-3 small">Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation generality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.515/>Hitting your MARQ : Multimodal ARgument Quality Assessment in Long Debate Video<span class=acl-fixed-case>MARQ</span>: Multimodal <span class=acl-fixed-case>AR</span>gument Quality Assessment in Long Debate Video</a></strong><br><a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/j/james-spann/>James Spann</a>
|
<a href=/people/m/masum-hasan/>Masum Hasan</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/k/kurtis-haut/>Kurtis Haut</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/ehsan-hoque/>Ehsan Hoque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--515><div class="card-body p-3 small">The combination of <a href=https://en.wikipedia.org/wiki/Gesture>gestures</a>, <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>intonations</a>, and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a> plays a key role in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argument delivery</a>. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation quality</a>. Second, we design the Multimodal ARgument Quality assessor (MARQ) a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 81.91 % on the argument quality prediction task and outperforms established <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> with an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error rate reduction</a> of 22.7 %. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--516 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.516/>Mind the Context : The Impact of <a href=https://en.wikipedia.org/wiki/Contextualization>Contextualization</a> in Neural Module Networks for Grounding Visual Referring Expressions</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/keze-wang/>Keze Wang</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--516><div class="card-body p-3 small">Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. dark cube on the left vs. black cube on the left). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> in NMN by up to 75 % without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module&#8217;s capacity in exploiting the visiolinguistic associations. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1 % improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the single-referent test set and +4.3 % on the full test set. Additionally, we demonstrate that contextualization provides +11.2 % and +1.7 % improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our <a href=https://en.wikipedia.org/wiki/Contextualization>contextualization</a> by constructing a <a href=https://en.wikipedia.org/wiki/Contrast_(vision)>contrast set</a> for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> by as much as +10.4 % absolute <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on CC-Ref+, illustrating the generalization skills of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--524 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.524/>Knowledge Base Completion Meets Transfer Learning</a></strong><br><a href=/people/v/vid-kocijan/>Vid Kocijan</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--524><div class="card-body p-3 small">The aim of knowledge base completion is to predict unseen facts from existing facts in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. In this work, we introduce the first approach for <a href=https://en.wikipedia.org/wiki/Transfer_of_knowledge>transfer of knowledge</a> from one collection of facts to another without the need for entity or relation matching. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> where more than one copy of a real-world entity or relation may exist. Such <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are a natural output of automated information extraction tools that extract <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Our main contribution is a method that can make use of a large-scale pretraining on facts, collected from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>, to improve predictions on <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20 K, where we obtained a 6 % absolute increase of mean reciprocal rank and 65 % relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.526/>Towards Zero-Shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/v/vasileios-lioutas/>Vasileios Lioutas</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--526><div class="card-body p-3 small">Knowledge distillation (KD) is a common <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer algorithm</a> used for <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a> across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher&#8217;s training data for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a>. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> to learn the teacher&#8217;s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75 % and 92 % of the teacher&#8217;s classification score (accuracy or F1) while compressing the model 30 times.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.529" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.529/>QuestEval : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> Asks for Fact-based Evaluation<span class=acl-fixed-case>Q</span>uest<span class=acl-fixed-case>E</span>val: Summarization Asks for Fact-based Evaluation</a></strong><br><a href=/people/t/thomas-scialom/>Thomas Scialom</a>
|
<a href=/people/p/paul-alexis-dray/>Paul-Alexis Dray</a>
|
<a href=/people/s/sylvain-lamprier/>Sylvain Lamprier</a>
|
<a href=/people/b/benjamin-piwowarski/>Benjamin Piwowarski</a>
|
<a href=/people/j/jacopo-staiano/>Jacopo Staiano</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--529><div class="card-body p-3 small">Summarization evaluation remains an open research problem : current <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any <a href=https://en.wikipedia.org/wiki/Ground_truth>ground-truth reference</a>. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.531/>Finding a Balanced Degree of Automation for Summary Evaluation</a></strong><br><a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--531><div class="card-body p-3 small">Human evaluation for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization tasks</a> is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs&#8217; presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>in-between metrics</a>, Lite2.xPyramid, where we use a simple <a href=https://en.wikipedia.org/wiki/Regressor>regressor</a> to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between <a href=https://en.wikipedia.org/wiki/Automation>automation</a> and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly collected PyrXSum (with 100/10 XSum examples / systems). It shows that Lite2Pyramid consistently has the best summary-level correlations ; Lite3Pyramid works better than or comparable to other automatic metrics ; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--535 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.535/>Controlling Machine Translation for Multiple Attributes with Additive Interventions</a></strong><br><a href=/people/a/andrea-schioppa/>Andrea Schioppa</a>
|
<a href=/people/d/david-vilar/>David Vilar</a>
|
<a href=/people/a/artem-sokolov/>Artem Sokolov</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--535><div class="card-body p-3 small">Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users&#8217; trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks : continuous values must be binned into discrete categories, which is unnatural for certain applications ; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained without <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better <a href=https://en.wikipedia.org/wiki/Scientific_control>control</a> over a wider range of tasks compared to <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a>, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable <a href=https://en.wikipedia.org/wiki/Control_theory>control</a> in an already trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> after a relatively cheap fine-tuning stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--536 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.536/>A Generative Framework for Simultaneous Machine Translation</a></strong><br><a href=/people/y/yishu-miao/>Yishu Miao</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--536><div class="card-body p-3 small">We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Here we formulate <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a> as a structural sequence-to-sequence learning problem. A <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to explicitly balance translation quality and <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--538 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.538" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.538/>Boosting Cross-Lingual Transfer via <a href=https://en.wikipedia.org/wiki/Self-learning>Self-Learning</a> with Uncertainty Estimation</a></strong><br><a href=/people/l/liyan-xu/>Liyan Xu</a>
|
<a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/x/xujiang-zhao/>Xujiang Zhao</a>
|
<a href=/people/h/haifeng-chen/>Haifeng Chen</a>
|
<a href=/people/f/feng-chen/>Feng Chen</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--538><div class="card-body p-3 small">Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer : Language Heteroscedastic / Homoscedastic Uncertainty (LEU / LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.540/>Interactive Machine Comprehension with Dynamic Knowledge Graphs</a></strong><br><a href=/people/x/xingdi-yuan/>Xingdi Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--540><div class="card-body p-3 small">Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph representations</a> are good inductive biases, which can serve as an agent&#8217;s memory mechanism in iMRC tasks. We explore four different categories of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> that can capture <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text information</a> at various levels. We describe methods that dynamically build and update these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> during information gathering, as well as neural models to encode <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representations</a> in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--542 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.542" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.542/>Visual News : Benchmark and Challenges in News Image Captioning</a></strong><br><a href=/people/f/fuxiao-liu/>Fuxiao Liu</a>
|
<a href=/people/y/yinghan-wang/>Yinghan Wang</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--542><div class="card-body p-3 small">We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. Unlike the standard image captioning task, news images depict situations where people, locations, and <a href=https://en.wikipedia.org/wiki/News>events</a> are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> and <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a>. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.543/>Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--543><div class="card-body p-3 small">While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit <a href=https://en.wikipedia.org/wiki/Narrative_structure>narrative structure</a> that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> and <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters / objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--547 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.547.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.547" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.547/>Tribrid : Stance Classification with Neural Inconsistency Detection</a></strong><br><a href=/people/s/song-yang/>Song Yang</a>
|
<a href=/people/j/jacopo-urbani/>Jacopo Urbani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--547><div class="card-body p-3 small">We study the problem of performing automatic stance classification on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with neural architectures such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is jointly learned to make simultaneously multiple predictions, which can be used either to improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence scores</a> to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.549" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.549/>Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks</a></strong><br><a href=/people/g/gael-guibon/>Gaël Guibon</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/h/helene-flamein/>Hélène Flamein</a>
|
<a href=/people/l/luce-lefeuvre/>Luce Lefeuvre</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--549><div class="card-body p-3 small">Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with different languages : daily conversations in English and customer service chat conversations in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. When applied to emotion classification in conversations, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> proved to be competitive even when compared to other ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.553/>When is Wall a Pared and when a Muro? : Extracting Rules Governing Lexical Selection</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--553><div class="card-body p-3 small">Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun wall has different lexical manifestations in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> pared refers to an indoor wall while muro refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.556" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.556/>Continuous Entailment Patterns for Lexical Inference in Context</a></strong><br><a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--556><div class="card-body p-3 small">Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> that closely resemble the text seen during self-supervised pretraining because the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM&#8217;s vocabulary, <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> can be adapted more flexibly to a PLM&#8217;s idiosyncrasies. Contrasting patterns where a token can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>small training data</a>. In a direct comparison with discrete patterns, <a href=https://en.wikipedia.org/wiki/CONAN>CONAN</a> consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> that enhances a PLM&#8217;s performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--557 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.557.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.557/>Numeracy enhances the Literacy of Language Models</a></strong><br><a href=/people/a/avijit-thawani/>Avijit Thawani</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/f/filip-ilievski/>Filip Ilievski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--557><div class="card-body p-3 small">Specialized number representations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a> to make better sense of <a href=https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences>world concepts</a>, e.g., you can seat 5 people in your &#8216;room&#8217; but not 500. Does a better grasp of numbers improve a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s understanding of other concepts and words? This paper studies the effect of using six different <a href=https://en.wikipedia.org/wiki/Encoder>number encoders</a> on the task of masked word prediction (MWP), as a proxy for evaluating <a href=https://en.wikipedia.org/wiki/Literacy>literacy</a>. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.558/>Students Who Study Together Learn Better : On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification</a></strong><br><a href=/people/m/mitch-paul-mithun/>Mitch Paul Mithun</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--558><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply : a little helps reduce <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>, but too much discards useful information. We propose <a href=https://en.wikipedia.org/wiki/Group_learning>Group Learning</a>, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that rely on the original data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--560 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.560/>Joint Passage Ranking for Diverse Multi-Answer Retrieval</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--560><div class="card-body p-3 small">We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it can not reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>. To model the <a href=https://en.wikipedia.org/wiki/Joint_probability>joint probability</a> of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and decoding algorithms</a>. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--561 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.561/>Generative Context Pair Selection for Multi-hop Question Answering</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/p/patrick-ng/>Patrick Ng</a>
|
<a href=/people/b/ben-athiwaratkun/>Ben Athiwaratkun</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--561><div class="card-body p-3 small">Compositional reasoning tasks such as multi-hop question answering require <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9 % higher than baseline) on adversarial held-out set which tests robustness of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s multi-hop reasoning capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--563 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.563/>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</a></strong><br><a href=/people/j/jeonghwan-kim/>Jeonghwan Kim</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/k/kyung-min-kim/>Kyung-min Kim</a>
|
<a href=/people/j/junmo-kang/>Junmo Kang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--563><div class="card-body p-3 small">Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> fail to extrapolate to unseen numbers. Presenting <a href=https://en.wikipedia.org/wiki/Number>numbers</a> as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat <a href=https://en.wikipedia.org/wiki/Number>numbers</a> differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.<i>E-digit</i> number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.568.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--568 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.568 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.568" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.568/>I Wish I Would Have Loved This One, But I Did n’t A Multilingual Dataset for Counterfactual Detection in Product Review<span class=acl-fixed-case>I</span> Wish <span class=acl-fixed-case>I</span> Would Have Loved This One, But <span class=acl-fixed-case>I</span> Didn’t – A Multilingual Dataset for Counterfactual Detection in Product Review</a></strong><br><a href=/people/j/james-oneill/>James O’Neill</a>
|
<a href=/people/p/polina-rozenshtein/>Polina Rozenshtein</a>
|
<a href=/people/r/ryuichi-kiryo/>Ryuichi Kiryo</a>
|
<a href=/people/m/motoko-kubota/>Motoko Kubota</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--568><div class="card-body p-3 small">Counterfactual statements describe events that did not or can not take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual statements</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and Japanese languages. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is unique as it contains <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> in multiple languages, covers a new application area of <a href=https://en.wikipedia.org/wiki/Review_site>e-commerce reviews</a>, and provides high quality professional annotations. We train <a href=https://en.wikipedia.org/wiki/Computational_fluid_dynamics>CFD models</a> using different text representation methods and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We find that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are robust against the <a href=https://en.wikipedia.org/wiki/Selection_bias>selectional biases</a> introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--570 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.570" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.570/>Evaluating the Morphosyntactic Well-formedness of Generated Texts</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--570><div class="card-body p-3 small">Text generation systems are ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. However, evaluation of these <a href=https://en.wikipedia.org/wiki/System>systems</a> remains a challenge, especially in multilingual settings. In this paper, we propose L&#8217;AMBRE a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to train robust parsers. We show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> on the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> through a diachronic study of systems translating into morphologically-rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--574 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.574" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.574/>ValNorm Quantifies <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> to Reveal Consistent Valence Biases Across Languages and Over Centuries<span class=acl-fixed-case>V</span>al<span class=acl-fixed-case>N</span>orm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries</a></strong><br><a href=/people/a/autumn-toney/>Autumn Toney</a>
|
<a href=/people/a/aylin-caliskan/>Aylin Caliskan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--574><div class="card-body p-3 small">Word embeddings learn <a href=https://en.wikipedia.org/wiki/Implicit_stereotype>implicit biases</a> from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology</a>. We apply ValNorm on static word embeddings from seven languages (Chinese, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In contrast, we measure <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a> using the same set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and find that <a href=https://en.wikipedia.org/wiki/Bias>social biases</a> vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.576" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.576/>Robust Open-Vocabulary Translation from Visual Text Representations</a></strong><br><a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/d/david-etter/>David Etter</a>
|
<a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--576><div class="card-body p-3 small">Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an &#8216;open vocabulary.&#8217; This approach relies on consistent and correct underlying unicode sequences, and makes <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> susceptible to degradation from common types of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> and <a href=https://en.wikipedia.org/wiki/Genetic_variation>variation</a>. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a>, achieving e.g., 25.9 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> on a character permuted GermanEnglish task where subword models degrade to 1.9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.578.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--578 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.578 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.578" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.578/>Improving Multilingual Translation by Representation and Gradient Regularization</a></strong><br><a href=/people/y/yilin-yang/>Yilin Yang</a>
|
<a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/a/alexandre-muzio/>Alexandre Muzio</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--578><div class="card-body p-3 small">Multilingual Neural Machine Translation (NMT) enables one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to serve all <a href=https://en.wikipedia.org/wiki/Translation>translation directions</a>, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also works well when the small amount of direct data is not available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.579.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--579 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.579 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.579" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.579/>Learning Kernel-Smoothed Machine Translation with Retrieved Examples</a></strong><br><a href=/people/q/qingnan-jiang/>Qingnan Jiang</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/j/jun-cao/>Jun Cao</a>
|
<a href=/people/s/shanbo-cheng/>Shanbo Cheng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--579><div class="card-body p-3 small">How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, updating the deployed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, <a href=https://en.wikipedia.org/wiki/Nonparametric_statistics>non-parametric methods</a> are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are released at https://github.com/jiangqn/KSTER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.580/>Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--580><div class="card-body p-3 small">Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model&#8217;s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--583 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.583/>Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering</a></strong><br><a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--583><div class="card-body p-3 small">In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding : the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the <a href=https://en.wikipedia.org/wiki/System>system</a> due to their answer confidence scores being lower than the <a href=https://en.wikipedia.org/wiki/System>system threshold</a>. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision / Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, e.g., reducing computation by ~60 %, while only losing ~3-4 % of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--585 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.585/>Explaining Answers with Entailment Trees</a></strong><br><a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/z/zhengnan-xie/>Zhengnan Xie</a>
|
<a href=/people/h/hannah-smith/>Hannah Smith</a>
|
<a href=/people/l/leighanna-pipatanangkura/>Leighanna Pipatanangkura</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--585><div class="card-body p-3 small">Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a rationale). If this could be done, new opportunities for understanding and debugging the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks : generate a valid <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment tree</a> given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. We show that a strong <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can partially solve these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, in particular when the relevant sentences are included in the input (e.g., 35 % of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.593.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--593 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.593 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.593/>Effective Sequence-to-Sequence Dialogue State Tracking</a></strong><br><a href=/people/j/jeffrey-zhao/>Jeffrey Zhao</a>
|
<a href=/people/m/mahdis-mahdieh/>Mahdis Mahdieh</a>
|
<a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--593><div class="card-body p-3 small">Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.598/>RICA : Evaluating Robust Inference Capabilities Based on Commonsense Axioms<span class=acl-fixed-case>RICA</span>: Evaluating Robust Inference Capabilities Based on Commonsense Axioms</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/r/rahul-khanna/>Rahul Khanna</a>
|
<a href=/people/s/seyeon-lee/>Seyeon Lee</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/d/daniel-ho/>Daniel Ho</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--598><div class="card-body p-3 small">Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA : Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--600 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.600" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.600/>MATE : Multi-view Attention for Table Transformer Efficiency<span class=acl-fixed-case>MATE</span>: Multi-view Attention for Table Transformer Efficiency</a></strong><br><a href=/people/j/julian-eisenschlos/>Julian Eisenschlos</a>
|
<a href=/people/m/maharshi-gor/>Maharshi Gor</a>
|
<a href=/people/t/thomas-mueller/>Thomas Müller</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--600><div class="card-body p-3 small">This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20 % of relational tables on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose <a href=https://en.wikipedia.org/wiki/MATE_(software)>MATE</a>, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Table_(information)>tabular data</a>, and sets a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.602" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.602/>When Attention Meets Fast Recurrence : Training <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> with Reduced Compute</a></strong><br><a href=/people/t/tao-lei/>Tao Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--602><div class="card-body p-3 small">Large language models have become increasingly difficult to train because of the growing <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time and cost</a>. In this work, we present SRU++, a highly-efficient <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that combines fast recurrence and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an <a href=https://en.wikipedia.org/wiki/Supercomputer>8-GPU machine</a>. We further demonstrate that SRU++ requires minimal <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--603 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.603/>Universal-KD : Attention-based Output-Grounded Intermediate Layer Knowledge Distillation<span class=acl-fixed-case>KD</span>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</a></strong><br><a href=/people/y/yimeng-wu/>Yimeng Wu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/md-akmal-haidar/>Md Akmal Haidar</a>
|
<a href=/people/a/ali-ghodsi/>Ali Ghodsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--603><div class="card-body p-3 small">Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a> in the hidden spaces of two different <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD can not easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits : (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large ; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--605 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.605/>Word-Level Coreference Resolution</a></strong><br><a href=/people/v/vladimir-dobrovolskii/>Vladimir Dobrovolskii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--605><div class="card-body p-3 small">Recent coreference resolution models rely heavily on span representations to find <a href=https://en.wikipedia.org/wiki/Coreference>coreference links</a> between word spans. As the number of spans is O(n^2) in the length of text and the number of potential links is O(n^4), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider <a href=https://en.wikipedia.org/wiki/Coreference>coreference links</a> between individual words rather than word spans and then reconstruct the word spans. This reduces the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of the coreference model to O(n^2) and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> will be significantly outperformed by RoBERTa. While being highly efficient, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs competitively with recent coreference resolution systems on the OntoNotes benchmark.<tex-math>O(n^2)</tex-math> in the length of text and the number of potential links is <tex-math>O(n^4)</tex-math>, various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to <tex-math>O(n^2)</tex-math> and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.611" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.611/>LM-Critic : Language Models for Unsupervised Grammatical Error Correction<span class=acl-fixed-case>LM</span>-Critic: Language Models for Unsupervised Grammatical Error Correction</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jure-leskovec/>Jure Leskovec</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--611><div class="card-body p-3 small">Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a> (+7.7 F0.5) and the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a> (+0.5 F0.5).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--615 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.615/>Come hither or go away? Recognising pre-electoral coalition signals in the news</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/anna-adendorf/>Anna Adendorf</a>
|
<a href=/people/o/oke-bahnsen/>Oke Bahnsen</a>
|
<a href=/people/l/lukas-stoetzer/>Lukas Stoetzer</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--615><div class="card-body p-3 small">In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a> leading up to an election the (un)willingness of political parties to form a <a href=https://en.wikipedia.org/wiki/Coalition_government>government coalition</a>. We decompose our problem into two related, but distinct tasks : (i) predicting whether a reported statement from a politician or a journalist refers to a potential <a href=https://en.wikipedia.org/wiki/Coalition>coalition</a> and (ii) predicting the polarity of the signal namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and investigate which setup and task formulation is best suited for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a>. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, <a href=https://en.wikipedia.org/wiki/Germany>Germany</a>, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--616 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.616/># HowYouTagTweets : Learning User Hashtagging Preferences via Personalized Topic Attention<span class=acl-fixed-case>H</span>ow<span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ag<span class=acl-fixed-case>T</span>weets: Learning User Hashtagging Preferences via Personalized Topic Attention</a></strong><br><a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/y/yubo-zhang/>Yubo Zhang</a>
|
<a href=/people/c/chunpu-xu/>Chunpu Xu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--616><div class="card-body p-3 small">Millions of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> are created on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user&#8217;s hashtagging preferences via predicting how likely they will post with a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a>. It is hypothesized that one&#8217;s interests in a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> are related with what they said before (user history) and the existing posts present the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>. In this way, user interests learned from the past can be customized to match future <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, which is beyond the capability of existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--618 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.618" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.618/>Proxy Indicators for the Quality of Open-domain Dialogues</a></strong><br><a href=/people/r/rostislav-nedelchev/>Rostislav Nedelchev</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--618><div class="card-body p-3 small">The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues&#8217; quality. As a consequence, performing such <a href=https://en.wikipedia.org/wiki/Evaluation>evaluations</a> at scale is usually expensive. This work investigates using a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning model</a> trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can infer various quality metrics and can derive a component-based overall score. We achieve statistically significant <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation coefficients</a> of up to 0.7.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--619 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.619/>Q^2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=tex-math>Q<sup>2</sup></span>: <span class=acl-fixed-case>E</span>valuating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</a></strong><br><a href=/people/o/or-honovich/>Or Honovich</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/e/ella-neeman/>Ella Neeman</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--619><div class="card-body p-3 small">Neural knowledge-grounded generative models for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, denoted Q^2, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-evaluation</a> of Q^2 against other <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> using this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and two others, where it consistently shows higher correlation with human judgements.<tex-math>Q^2</tex-math>, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of <tex-math>Q^2</tex-math> against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.622/>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/z/zhenpeng-zhou/>Zhenpeng Zhou</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--622><div class="card-body p-3 small">Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--623 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.623/>Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance</a></strong><br><a href=/people/c/carel-van-niekerk/>Carel van Niekerk</a>
|
<a href=/people/a/andrey-malinin/>Andrey Malinin</a>
|
<a href=/people/c/christian-geishauser/>Christian Geishauser</a>
|
<a href=/people/m/michael-heck/>Michael Heck</a>
|
<a href=/people/h/hsien-chin-lin/>Hsien-chin Lin</a>
|
<a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/s/shutong-feng/>Shutong Feng</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--623><div class="card-body p-3 small">The ability to identify and resolve <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> is crucial for the <a href=https://en.wikipedia.org/wiki/Robustness_(evolution)>robustness</a> of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Indeed, this has been confirmed empirically on <a href=https://en.wikipedia.org/wiki/System>systems</a> that utilise <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian approaches</a> to dialogue belief tracking. However, such <a href=https://en.wikipedia.org/wiki/System>systems</a> consider only <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence estimates</a> and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainties</a> into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> to the <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> into account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--624 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.624/>Dynamic Forecasting of Conversation Derailment</a></strong><br><a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--624><div class="card-body p-3 small">Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend <a href=https://en.wikipedia.org/wiki/It_(pronoun)>it</a> in several ways. We apply a pretrained language encoder to the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results : in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1 ; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--628 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.628" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.628/>CAPE : Context-Aware Private Embeddings for Private Language Learning<span class=acl-fixed-case>CAPE</span>: Context-Aware Private Embeddings for Private Language Learning</a></strong><br><a href=/people/r/richard-plant/>Richard Plant</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/v/valerio-giuffrida/>Valerio Giuffrida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--628><div class="card-body p-3 small">Neural language models have contributed to state-of-the-art results in a number of downstream applications including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding <a href=https://en.wikipedia.org/wiki/Personal_data>personally identifiable information</a> learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> to preserve <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> during training of embeddings. Specifically, CAPE firstly applies calibrated noise through <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3 % reduction in attacker performance compared to the best-performing current method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--629 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.629" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.629/>Text Detoxification using Large Pre-trained Neural Models</a></strong><br><a href=/people/d/david-dale/>David Dale</a>
|
<a href=/people/a/anton-voronov/>Anton Voronov</a>
|
<a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--629><div class="card-body p-3 small">We present two novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> for eliminating toxicity in text. Our first method combines two recent ideas : (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphraser</a> guided by style-trained language models to keep the text content and remove <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with a number of <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for style transfer. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> we suggest yield new SOTA results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--630 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.630/>Document-Level Text Simplification : Dataset, Criteria and Baseline</a></strong><br><a href=/people/r/renliang-sun/>Renliang Sun</a>
|
<a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--630><div class="card-body p-3 small">Text simplification is a valuable technique. However, current research is limited to <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a>. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia dumps</a>, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--632 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.632/>Paraphrasing Compound Nominalizations</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/h/ho-hung-lim/>Ho Hung Lim</a>
|
<a href=/people/c/carol-webster/>Carol Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--632><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Nominalization>nominalization</a> uses a <a href=https://en.wikipedia.org/wiki/Deverbal_noun>deverbal noun</a> to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret <a href=https://en.wikipedia.org/wiki/Nominalization>nominalizations</a> by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--635 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.635" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.635/>TDEER : An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations<span class=acl-fixed-case>TDEER</span>: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations</a></strong><br><a href=/people/x/xianming-li/>Xianming Li</a>
|
<a href=/people/x/xiaotian-luo/>Xiaotian Luo</a>
|
<a href=/people/c/chenghao-dong/>Chenghao Dong</a>
|
<a href=/people/d/daichuan-yang/>Daichuan Yang</a>
|
<a href=/people/b/beidi-luan/>Beidi Luan</a>
|
<a href=/people/z/zhen-he/>Zhen He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--635><div class="card-body p-3 small">Joint extraction of entities and relations from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured texts</a> to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a>. However, it is still challenging to handle this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands for Translating Decoding Schema for Joint Extraction of Entities and Relations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as subject + relation objects. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>, we introduce negative samples to alleviate <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>error accumulation</a> at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at.<b>TDEER</b>, which stands for <b>T</b>ranslating <b>D</b>ecoding Schema for Joint <b>E</b>xtraction of <b>E</b>ntities and <b>R</b>elations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as <tex-math>subject + relation \\rightarrow objects</tex-math>. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at <url>https://github.com/4AI/TDEER</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--636 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.636" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.636/>Extracting Event Temporal Relations via <a href=https://en.wikipedia.org/wiki/Hyperbolic_geometry>Hyperbolic Geometry</a></a></strong><br><a href=/people/x/xingwei-tan/>Xingwei Tan</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--636><div class="card-body p-3 small">Detecting events and their evolution through time is a crucial task in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a> and train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to detect temporal relations between event pairs. However, embeddings in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a> can not capture richer <a href=https://en.wikipedia.org/wiki/Asymmetric_relation>asymmetric relations</a> such as event temporal relations. We thus propose to embed <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> into hyperbolic spaces, which are intrinsically oriented at modeling <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structures</a>. We introduce two approaches to encode <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event relations</a> through simple <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>geometrical operations</a>. In the second one, we devise an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end architecture</a> composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different <a href=https://en.wikipedia.org/wiki/Space>geometrical space</a>, resulting in state-of-the-art performance on several standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--637 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.637/>Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention</a></strong><br><a href=/people/j/jiawei-chen/>Jiawei Chen</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--637><div class="card-body p-3 small">Event detection has long been troubled by the trigger curse : overfitting the trigger will harm the generalization ability while underfitting it will hurt the <a href=https://en.wikipedia.org/wiki/Detection>detection</a> performance. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during <a href=https://en.wikipedia.org/wiki/Training>training</a>. Experiments show that our method significantly improves the FSED on both ACE05 and MAVEN datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--639 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.639/>Time-dependent Entity Embedding is not All You Need : A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework</a></strong><br><a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/g/gengyuan-zhang/>Gengyuan Zhang</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--639><div class="card-body p-3 small">Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> usually contain two parts, a temporal embedding layer and a <a href=https://en.wikipedia.org/wiki/Score_(statistics)>score function</a> derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different <a href=https://en.wikipedia.org/wiki/Score_(statistics)>score functions</a> and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training strategies</a>, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes : (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--643 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.643" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.643/>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</a></strong><br><a href=/people/y/yi-huang/>Yi Huang</a>
|
<a href=/people/b/buse-giledereli/>Buse Giledereli</a>
|
<a href=/people/a/abdullatif-koksal/>Abdullatif Köksal</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a>
|
<a href=/people/e/elif-ozkirimli/>Elif Ozkirimli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--643><div class="card-body p-3 small">Multi-label text classification is a challenging task because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance problem</a>, however, they are not effective when there is label dependency besides <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance</a> because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a> with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a>. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Source code is available at https://github.com/blessu/BalancedLossNLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--644 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.644" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.644/>Bayesian Topic Regression for Causal Inference<span class=acl-fixed-case>B</span>ayesian Topic Regression for Causal Inference</a></strong><br><a href=/people/m/maximilian-ahrens/>Maximilian Ahrens</a>
|
<a href=/people/j/julian-ashwin/>Julian Ashwin</a>
|
<a href=/people/j/jan-peter-calliess/>Jan-Peter Calliess</a>
|
<a href=/people/v/vu-nguyen/>Vu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--644><div class="card-body p-3 small">Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the <a href=https://en.wikipedia.org/wiki/Frisch&#8211;Waugh&#8211;Lovell_theorem>Frisch-Waugh-Lovell theorem</a>. Our paper makes two main contributions. First, we provide a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression framework</a> that allows <a href=https://en.wikipedia.org/wiki/Causal_inference>causal inference</a> in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--646 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.646/>What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models<span class=acl-fixed-case>W</span>hat’s in Your Head? <span class=acl-fixed-case>E</span>mergent Behaviour in Multi-Task Transformer Models</a></strong><br><a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/u/uri-katz/>Uri Katz</a>
|
<a href=/people/a/aviv-ben-arie/>Aviv Ben-Arie</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--646><div class="card-body p-3 small">The primary paradigm for multi-task training in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a>, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a> suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--647 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.647/>Do n’t Search for a Search Method Simple <a href=https://en.wikipedia.org/wiki/Heuristic>Heuristics</a> Suffice for Adversarial Text Attacks</a></strong><br><a href=/people/n/nathaniel-berger/>Nathaniel Berger</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a>
|
<a href=/people/s/sebastian-ebert/>Sebastian Ebert</a>
|
<a href=/people/a/artem-sokolov/>Artem Sokolov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--647><div class="card-body p-3 small">Recently more attention has been given to adversarial attacks on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. A central research topic has been the investigation of <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> and search constraints, accompanied by <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark algorithms</a> and <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. We implement an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristics</a> exploiting <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbors</a> without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--648 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.648" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.648/>Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</a></strong><br><a href=/people/p/peru-bhardwaj/>Peru Bhardwaj</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a>
|
<a href=/people/l/luca-costabello/>Luca Costabello</a>
|
<a href=/people/d/declan-osullivan/>Declan O’Sullivan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--648><div class="card-body p-3 small">Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>security vulnerabilities</a> that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model&#8217;s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic method</a> to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62 % over the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.649.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--649 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.649 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.649/>Locke’s Holiday : Belief Bias in Machine Reading</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--649><div class="card-body p-3 small">I highlight a simple <a href=https://en.wikipedia.org/wiki/Failure_mode>failure mode</a> of state-of-the-art machine reading systems : when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of &#8216;My kingdom for a cough drop, cried Queen Elizabeth.&#8217; Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading systems</a> on Auto-Locke show the pervasiveness of <a href=https://en.wikipedia.org/wiki/Belief_bias>belief bias</a> in <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>.<i>What did Elizabeth want?</i> correctly in the context of &#8216;My kingdom for a cough drop, cried Queen Elizabeth.&#8217; Biased by co-occurrence statistics in the training data of pretrained language models, systems predict <i>my kingdom</i>, rather than <i>a cough drop</i>. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--650 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.650/>Sequence Length is a Domain : Length-based Overfitting in Transformer Models</a></strong><br><a href=/people/d/dusan-varis/>Dusan Varis</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--650><div class="card-body p-3 small">Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> during training. In practice, this is usually countered either by applying <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization methods</a> (e.g. dropout, L2-regularization) or by providing huge amounts of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--658 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.658/>Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs</a></strong><br><a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/z/zifeng-ding/>Zifeng Ding</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/y/yujia-gu/>Yujia Gu</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--658><div class="card-body p-3 small">There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>dynamic graph</a>, i.e., <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edge formation</a> and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s superior performance on the future link forecasting task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.661.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--661 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.661 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.661" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.661/>A Strong Baseline for Query Efficient Attacks in a Black Box Setting</a></strong><br><a href=/people/r/rishabh-maheshwary/>Rishabh Maheshwary</a>
|
<a href=/people/s/saket-maheshwary/>Saket Maheshwary</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--661><div class="card-body p-3 small">Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different <a href=https://en.wikipedia.org/wiki/Search_algorithm>search methods</a>. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> across three different <a href=https://en.wikipedia.org/wiki/Feasible_region>search spaces</a>. Further, we benchmark our results across the same <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> used in prior attacks. In comparison to <a href=https://en.wikipedia.org/wiki/Cyberattack>attacks</a> proposed, on an average, we are able to reduce the query count by 75 % across all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and target models. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Cyberattack>attack</a> achieves a higher success rate when compared to prior attacks in a limited query setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--667 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.667/>Language Modeling, Lexical Translation, Reordering : The Training Process of NMT through the Lens of Classical SMT<span class=acl-fixed-case>NMT</span> through the Lens of Classical <span class=acl-fixed-case>SMT</span></a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--667><div class="card-body p-3 small">Differently from the traditional <a href=https://en.wikipedia.org/wiki/Machine_translation>statistical MT</a> that decomposes the translation task into distinct separately learned components, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> uses a single <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to model the entire translation process. Despite <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>SMT</a>. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.670.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--670 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.670 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.670.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.670" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.670/>Wino-X : Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution<span class=acl-fixed-case>X</span>: Multilingual <span class=acl-fixed-case>W</span>inograd Schemas for Commonsense Reasoning and Coreference Resolution</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--670><div class="card-body p-3 small">Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a> and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.671.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--671 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.671 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.671" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.671/>One Source, Two Targets : Challenges and Rewards of Dual Decoding<span class=acl-fixed-case>C</span>hallenges and Rewards of Dual Decoding</a></strong><br><a href=/people/j/jitao-xu/>Jitao Xu</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--671><div class="card-body p-3 small">Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement : to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.678.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--678 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.678 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.678/>Cross-Policy Compliance Detection via Question Answering</a></strong><br><a href=/people/m/marzieh-saeidi/>Marzieh Saeidi</a>
|
<a href=/people/m/majid-yazdani/>Majid Yazdani</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--678><div class="card-body p-3 small">Policy compliance detection is the task of ensuring that a scenario conforms to a <a href=https://en.wikipedia.org/wiki/Policy>policy</a> (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>, which results in poor <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> due to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Policy>policies</a>. In this paper we propose to address <a href=https://en.wikipedia.org/wiki/Policy>policy compliance detection</a> via decomposing it into <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, where questions check whether the conditions stated in the <a href=https://en.wikipedia.org/wiki/Policy>policy</a> apply to the scenario, and an <a href=https://en.wikipedia.org/wiki/Expression_tree>expression tree</a> combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, especially in the cross-policy setup where the <a href=https://en.wikipedia.org/wiki/Policy>policies</a> during testing are unseen in training. In addition, it allows us to use existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering models</a> pre-trained on existing <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a>. Finally, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> explicitly identifies the information missing from a scenario in case policy compliance can not be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.680.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--680 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.680 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.680/>Unsupervised Multi-View Post-OCR Error Correction With Language Models<span class=acl-fixed-case>OCR</span> Error Correction With Language Models</a></strong><br><a href=/people/h/harsh-gupta/>Harsh Gupta</a>
|
<a href=/people/l/luciano-del-corro/>Luciano Del Corro</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/j/johannes-hoffart/>Johannes Hoffart</a>
|
<a href=/people/e/eliot-brenner/>Eliot Brenner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--680><div class="card-body p-3 small">We investigate post-OCR correction in a setting where we have access to different <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR views</a> of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error correction</a> is too risky. We evaluated different pretrained LMs on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and found significant gains in realistic scenarios with up to 15 % <a href=https://en.wikipedia.org/wiki/Receiver_operating_characteristic>WER</a> improvement over the best OCR view. We also show the importance of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for post-OCR correction on out-of-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.682.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--682 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.682 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.682/>BERT-Beta : A Proactive Probabilistic Approach to Text Moderation<span class=acl-fixed-case>BERT</span>-Beta: A Proactive Probabilistic Approach to Text Moderation</a></strong><br><a href=/people/f/fei-tan/>Fei Tan</a>
|
<a href=/people/y/yifan-hu/>Yifan Hu</a>
|
<a href=/people/k/kevin-yen/>Kevin Yen</a>
|
<a href=/people/c/changwei-hu/>Changwei Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--682><div class="card-body p-3 small">Text moderation for <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>interpretation</a> benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a> offers useful insights beyond this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.685.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--685 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.685 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.685" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.685/>CodeT5 : Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation<span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>T</span>5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/w/weishi-wang/>Weishi Wang</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--685><div class="card-body p-3 small">Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Besides, we propose a novel identifier-aware pre-training task that enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.686.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--686 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.686 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.686.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.686" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.686/>Detect and Classify Joint Span Detection and Classification for Health Outcomes</a></strong><br><a href=/people/m/micheal-abaho/>Micheal Abaho</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/p/paula-williamson/>Paula Williamson</a>
|
<a href=/people/s/susanna-dodd/>Susanna Dodd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--686><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Outcome_(probability)>health outcome</a> is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.687.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--687 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.687 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.687/>Multi-Class Grammatical Error Detection for Correction : A Tale of Two Systems<span class=acl-fixed-case>M</span>ulti-Class Grammatical Error Detection for Correction: <span class=acl-fixed-case>A</span> Tale of Two Systems</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/c/christopher-davis/>Christopher Davis</a>
|
<a href=/people/c/christopher-bryant/>Christopher Bryant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--687><div class="card-body p-3 small">In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines <a href=https://en.wikipedia.org/wiki/General_Educational_Development>GED</a> and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.688.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--688 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.688 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.688" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.688/>Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models</a></strong><br><a href=/people/t/tassilo-klein/>Tassilo Klein</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--688><div class="card-body p-3 small">Can we get existing <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge</a> by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.689.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--689 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.689 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.689" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.689/>To Share or not to Share : Predicting Sets of Sources for Model Transfer Learning<span class=acl-fixed-case>P</span>redicting Sets of Sources for Model Transfer Learning</a></strong><br><a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/j/jannik-strotgen/>Jannik Strötgen</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--689><div class="card-body p-3 small">In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity as suggested in prior work may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.690.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--690 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.690 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.690/>Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting : Phenotype Annotation Use Case</a></strong><br><a href=/people/j/jingqing-zhang/>Jingqing Zhang</a>
|
<a href=/people/l/luis-bolanos-trujillo/>Luis Bolanos Trujillo</a>
|
<a href=/people/t/tong-li/>Tong Li</a>
|
<a href=/people/a/ashwani-tanwar/>Ashwani Tanwar</a>
|
<a href=/people/g/guilherme-freire/>Guilherme Freire</a>
|
<a href=/people/x/xian-yang/>Xian Yang</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/v/vibhor-gupta/>Vibhor Gupta</a>
|
<a href=/people/y/yike-guo/>Yike Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--690><div class="card-body p-3 small">Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records</a>. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with as little as 20 % of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the <a href=https://en.wikipedia.org/wiki/Phenotype>phenotypes</a> annotated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.691.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--691 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.691 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.691/>ClauseRec : A Clause Recommendation Framework for AI-aided Contract Authoring<span class=acl-fixed-case>C</span>lause<span class=acl-fixed-case>R</span>ec: A Clause Recommendation Framework for <span class=acl-fixed-case>AI</span>-aided Contract Authoring</a></strong><br><a href=/people/v/vinay-aggarwal/>Vinay Aggarwal</a>
|
<a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/a/anandhavelu-n/>Anandhavelu N</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--691><div class="card-body p-3 small">Contracts are a common type of <a href=https://en.wikipedia.org/wiki/Legal_document>legal document</a> that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such <a href=https://en.wikipedia.org/wiki/Document>documents</a>, and even lesser in generating them. These <a href=https://en.wikipedia.org/wiki/Contract>contracts</a> are made up of clauses, and the unique nature of these <a href=https://en.wikipedia.org/wiki/Clause>clauses</a> calls for specific methods to understand and generate such <a href=https://en.wikipedia.org/wiki/Document>documents</a>. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a <a href=https://en.wikipedia.org/wiki/Contract>contract</a>, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.692.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--692 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.692 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.692" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.692/>Finnish Dialect Identification : The Effect of Audio and Text<span class=acl-fixed-case>F</span>innish Dialect Identification: The Effect of Audio and Text</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a>
|
<a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--692><div class="card-body p-3 small">Finnish is a language with multiple dialects that not only differ from each other in terms of <a href=https://en.wikipedia.org/wiki/Accent_(sociolinguistics)>accent</a> (pronunciation) but also in terms of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological forms</a> and <a href=https://en.wikipedia.org/wiki/Lexicon>lexical choice</a>. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is received by combining both of the modalities, as text only reaches to an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 57 %, where as text and audio reach to 85 %. Our code, models and data have been released openly on Github and <a href=https://en.wikipedia.org/wiki/Zenodo>Zenodo</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.694.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--694 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.694 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.694/>Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</a></strong><br><a href=/people/p/priyanka-sen/>Priyanka Sen</a>
|
<a href=/people/a/armin-oliya/>Armin Oliya</a>
|
<a href=/people/a/amir-saffari/>Amir Saffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--694><div class="card-body p-3 small">End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only <a href=https://en.wikipedia.org/wiki/Supervised_learning>weak supervision</a>, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>. We find that introducing intersection improves performance over a baseline model on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, WebQuestionsSP (69.6 % to 73.3 % Hits@1) and ComplexWebQuestions (39.8 % to 48.7 % Hits@1), and in particular, improves performance on questions with multiple entities by over 14 % on WebQuestionsSP and by 19 % on ComplexWebQuestions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.696.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--696 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.696 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.696/>Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation</a></strong><br><a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--696><div class="card-body p-3 small">Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attacks</a>. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make <a href=https://en.wikipedia.org/wiki/Question_answering>question answering models</a> more robust to <a href=https://en.wikipedia.org/wiki/Adversarial_system>human adversaries</a>. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples : crowdworkers can fool our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> only 8.8 % of the time on average, compared to 17.6 % for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained without synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.697/>BeliefBank : Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief<span class=acl-fixed-case>B</span>elief<span class=acl-fixed-case>B</span>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</a></strong><br><a href=/people/n/nora-kassner/>Nora Kassner</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--697><div class="card-body p-3 small">Although pretrained language models (PTLMs) contain significant amounts of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> actually believes about the world, making it susceptible to <a href=https://en.wikipedia.org/wiki/Consistency>inconsistent behavior</a> and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs a BeliefBank that records but then may modify the raw PTLM answers. We describe two <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> to improve belief consistency in the overall system. First, a reasoning component a weighted MaxSAT solver revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.698" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.698/>MLEC-QA : A Chinese Multi-Choice Biomedical Question Answering Dataset<span class=acl-fixed-case>MLEC-QA</span>: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>C</span>hoice <span class=acl-fixed-case>B</span>iomedical <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering <span class=acl-fixed-case>D</span>ataset</a></strong><br><a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/s/shangping-zhong/>Shangping Zhong</a>
|
<a href=/people/k/kaizhi-chen/>Kaizhi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--698><div class="card-body p-3 small">Question Answering (QA) has been successfully applied in scenarios of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a> such as <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> and <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a>. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields : <a href=https://en.wikipedia.org/wiki/Clinic>Clinic</a>, <a href=https://en.wikipedia.org/wiki/Oral_medicine>Stomatology</a>, <a href=https://en.wikipedia.org/wiki/Public_health>Public Health</a>, <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a>, and <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a> Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.699.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--699 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.699 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.699" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.699/>IndoNLG : Benchmark and Resources for Evaluating Indonesian Natural Language Generation<span class=acl-fixed-case>I</span>ndo<span class=acl-fixed-case>NLG</span>: Benchmark and Resources for Evaluating <span class=acl-fixed-case>I</span>ndonesian Natural Language Generation</a></strong><br><a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/b/bryan-wilie/>Bryan Wilie</a>
|
<a href=/people/k/karissa-vincentio/>Karissa Vincentio</a>
|
<a href=/people/x/xiaohong-li/>Xiaohong Li</a>
|
<a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/z/zhi-yuan-lim/>Zhi Yuan Lim</a>
|
<a href=/people/s/syafri-bahar/>Syafri Bahar</a>
|
<a href=/people/m/masayu-khodra/>Masayu Khodra</a>
|
<a href=/people/a/ayu-purwarianti/>Ayu Purwarianti</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--699><div class="card-body p-3 small">Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resourceyet widely spokenlanguages of Indonesia : <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese</a>, and <a href=https://en.wikipedia.org/wiki/Sundanese_language>Sundanese</a>. Altogether, these <a href=https://en.wikipedia.org/wiki/Language>languages</a> are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks : <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a>, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models : IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasksdespite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient <a href=https://en.wikipedia.org/wiki/Learning>learning</a> and faster <a href=https://en.wikipedia.org/wiki/Inference>inference</a> at very low-resource languages like <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese</a> and <a href=https://en.wikipedia.org/wiki/Sundanese_language>Sundanese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.701" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.701/>Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors<span class=acl-fixed-case>BERT</span>-Based Evaluation Metrics by Disentangling along Linguistic Factors</a></strong><br><a href=/people/m/marvin-kaster/>Marvin Kaster</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--701><div class="card-body p-3 small">Evaluation metrics are a key ingredient for progress of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation systems</a>. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, and lexical overlap. We show that the different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> capture all aspects to some degree, but that they are all substantially sensitive to <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, just like <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and ROUGE. This exposes limitations of these novelly proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, which we also highlight in an adversarial test scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--702 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.702" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.702/>Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization<span class=acl-fixed-case>SQL</span> Generalization</a></strong><br><a href=/people/y/yujian-gan/>Yujian Gan</a>
|
<a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--702><div class="card-body p-3 small">Recently, there has been significant progress in studying <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, even if the <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> appears in the training set, and the model provides the correct predictions for related training samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--704 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.704/>NeuTral Rewriter : A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives<span class=acl-fixed-case>N</span>eu<span class=acl-fixed-case>T</span>ral <span class=acl-fixed-case>R</span>ewriter: <span class=acl-fixed-case>A</span> Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives</a></strong><br><a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a>
|
<a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--704><div class="card-body p-3 small">Recent years have seen an increasing need for <a href=https://en.wikipedia.org/wiki/Gender-neutral_language>gender-neutral and inclusive language</a>. Within the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, there are various mono- and bilingual use cases where <a href=https://en.wikipedia.org/wiki/Gender_inclusive_language>gender inclusive language</a> is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for <a href=https://en.wikipedia.org/wiki/English_language>English</a> along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> generated by the rule-based approach, obtains <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rates (WER)</a> below 0.18 % on synthetic, in-domain and out-domain test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--705 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.705" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.705/>Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset</a></strong><br><a href=/people/t/tianqing-fang/>Tianqing Fang</a>
|
<a href=/people/w/weiqi-wang/>Weiqi Wang</a>
|
<a href=/people/s/sehyun-choi/>Sehyun Choi</a>
|
<a href=/people/s/shibo-hao/>Shibo Hao</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/b/bin-he/>Bin He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--705><div class="card-body p-3 small">Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a high-quality human-annotated evaluation set to probe neural models&#8217; commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. Experimental results show that generalizing <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> on unseen assertions is inherently a hard task. Models achieving high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> during training perform poorly on the evaluation set, with a large gap between human performance. We will make the data publicly available for future contributions. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.706.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.706/>Enhancing the Context Representation in Similarity-based Word Sense Disambiguation</a></strong><br><a href=/people/m/ming-wang/>Ming Wang</a>
|
<a href=/people/j/jianzhang-zhang/>Jianzhang Zhang</a>
|
<a href=/people/y/yinglin-wang/>Yinglin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--706><div class="card-body p-3 small">In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its <a href=https://en.wikipedia.org/wiki/Context_(language_use)>global context</a>. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system&#8217;s performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--710 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.710" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.710/>Cross-Domain Label-Adaptive Stance Detection</a></strong><br><a href=/people/m/momchil-hardalov/>Momchil Hardalov</a>
|
<a href=/people/a/arnav-arora/>Arnav Arora</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--710><div class="card-body p-3 small">Stance detection concerns the classification of a writer&#8217;s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--711 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.711" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.711/>Text AutoAugment : Learning Compositional Augmentation Policy for Text Classification<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>A</span>ugment: Learning Compositional Augmentation Policy for Text Classification</a></strong><br><a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--711><div class="card-body p-3 small">Data augmentation aims to enrich training samples for alleviating the <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting issue</a> in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best <a href=https://en.wikipedia.org/wiki/Policy>policy</a>, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> in low-resource and class-imbalanced regimes by an average of 8.8 % and 9.7 %, respectively, outperforming strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.712" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.712/>Distilling Relation Embeddings from Pretrained Language Models</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--712><div class="card-body p-3 small">Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embeddings</a>, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--713 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.713" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.713/>Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</a></strong><br><a href=/people/p/prasetya-utama/>Prasetya Utama</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--713><div class="card-body p-3 small">Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>inference heuristics</a> based on <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical overlap</a>, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> can be destructive to useful knowledge learned during the pretraining. We then show that adding a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--717 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.717" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.717/>NB-MLM : Efficient Domain Adaptation of Masked Language Models for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>NB</span>-<span class=acl-fixed-case>MLM</span>: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis</a></strong><br><a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/d/dmitrii-kharchev/>Dmitrii Kharchev</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--717><div class="card-body p-3 small">While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> that focuses on predicting words with large weights of the <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes classifier</a> trained for the task at hand, which are likely more relevant than the most frequent words. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides faster adaptation and better final performance for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> compared to the standard approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--720 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.720/>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</a></strong><br><a href=/people/x/xiaobao-guo/>Xiaobao Guo</a>
|
<a href=/people/a/adams-kong/>Adams Kong</a>
|
<a href=/people/h/huan-zhou/>Huan Zhou</a>
|
<a href=/people/x/xianfeng-wang/>Xianfeng Wang</a>
|
<a href=/people/m/min-wang/>Min Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--720><div class="card-body p-3 small">Effective <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representation</a> and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representations</a>, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a crossmodal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.728/>Towards Label-Agnostic Emotion Embeddings</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/l/luise-modersohn/>Luise Modersohn</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--728><div class="card-body p-3 small">Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic levels</a> (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural languages and text genres</a> (e.g., product reviews, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural languages</a>, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired <a href=https://en.wikipedia.org/wiki/Interoperability>interoperability</a> without penalizing prediction quality. Code and data are archived under DOI 10.5281 / zenodo.5466068.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.729.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--729 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.729 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.729" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.729/>Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer</a></strong><br><a href=/people/y/yun-ma/>Yun Ma</a>
|
<a href=/people/y/yangbin-chen/>Yangbin Chen</a>
|
<a href=/people/x/xudong-mao/>Xudong Mao</a>
|
<a href=/people/q/qing-li/>Qing Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--729><div class="card-body p-3 small">Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> is regularized by knowledge from its peer which has a different <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition process</a>. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and under-transfer problems. Experimental results on two benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves strong empirical results on both style compatibility and content preservation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.730.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--730 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.730 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.730" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.730/>Exploring Non-Autoregressive Text Style Transfer</a></strong><br><a href=/people/y/yun-ma/>Yun Ma</a>
|
<a href=/people/q/qing-li/>Qing Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--730><div class="card-body p-3 small">In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>AR model</a>, this NAR model sacrifices its transfer performance due to the lack of <a href=https://en.wikipedia.org/wiki/Conditional_dependence>conditional dependence</a> between output tokens. To this end, we investigate three <a href=https://en.wikipedia.org/wiki/Methodology>techniques</a>, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.733.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--733 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.733 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.733/>Progressively Guide to Attend : An Iterative Alignment Framework for Temporal Sentence Grounding</a></strong><br><a href=/people/d/daizong-liu/>Daizong Liu</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/p/pan-zhou/>Pan Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--733><div class="card-body p-3 small">A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.741.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--741 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.741 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.741" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.741/>ARMAN : Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization<span class=acl-fixed-case>ARMAN</span>: <span class=acl-fixed-case>P</span>re-training with <span class=acl-fixed-case>S</span>emantically <span class=acl-fixed-case>S</span>electing and <span class=acl-fixed-case>R</span>eordering of <span class=acl-fixed-case>S</span>entences for <span class=acl-fixed-case>P</span>ersian <span class=acl-fixed-case>A</span>bstractive <span class=acl-fixed-case>S</span>ummarization</a></strong><br><a href=/people/a/alireza-salemi/>Alireza Salemi</a>
|
<a href=/people/e/emad-kebriaei/>Emad Kebriaei</a>
|
<a href=/people/g/ghazal-neisi-minaei/>Ghazal Neisi Minaei</a>
|
<a href=/people/a/azadeh-shakery/>Azadeh Shakery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--741><div class="card-body p-3 small">Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In <a href=https://en.wikipedia.org/wiki/ARMAN>ARMAN</a>, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>, question paraphrasing, and <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice question answering</a>. Finally, we established a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> and show that using the semantic score significantly improves summarization results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.745.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--745 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.745 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.745" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.745/>Revisiting Tri-training of Dependency Parsers</a></strong><br><a href=/people/j/joachim-wagner/>Joachim Wagner</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--745><div class="card-body p-3 small">We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> can be expected to have the most impact here. Based on treebank size and available ELMo models, we select <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, Uyghur (a zero-shot language for mBERT) and <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Furthermore, we include <a href=https://en.wikipedia.org/wiki/English_language>English</a> in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.746.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--746 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.746 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.746" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.746/>Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion : Re-explore Zero-Shot Learning for Slot Filling</a></strong><br><a href=/people/l/liwen-wang/>Liwen Wang</a>
|
<a href=/people/x/xuefeng-li/>Xuefeng Li</a>
|
<a href=/people/j/jiachi-liu/>Jiachi Liu</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--746><div class="card-body p-3 small">Zero-shot cross-domain slot filling alleviates the <a href=https://en.wikipedia.org/wiki/Data_dependence>data dependence</a> in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.747.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--747 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.747 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.747/>Neuralizing Regular Expressions for Slot Filling</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zijian-jin/>Zijian Jin</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--747><div class="card-body p-3 small">Neural models and <a href=https://en.wikipedia.org/wiki/Mathematical_logic>symbolic rules</a> such as <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Specifically, we first convert <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.755.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--755 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.755 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.755/>Case-based Reasoning for Natural Language Queries over Knowledge Bases</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/e/ethan-perez/>Ethan Perez</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/l/lizhen-tan/>Lizhen Tan</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--755><div class="card-body p-3 small">It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions a <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a <a href=https://en.wikipedia.org/wiki/Parametric_model>parametric model</a> that can generate a <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11 % on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training : by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.<i>without</i> any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.756.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--756 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.756 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.756" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.756/>Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation</a></strong><br><a href=/people/c/chen-zhao/>Chen Zhao</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--756><div class="card-body p-3 small">Open-domain question answering answers a question based on evidence retrieved from a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> that rely on them can not transfer to the more common setting, where only questionanswer pairs are available. This paper investigates whether <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.760.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--760 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.760 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.760/>Set Generation Networks for End-to-End Knowledge Base Population</a></strong><br><a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/c/chenhao-wang/>Chenhao Wang</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--760><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base population (KBP)</a> aims to discover facts about entities from texts and expand a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a>, we also design a <a href=https://en.wikipedia.org/wiki/Loss_function>set-based loss</a> that forces unique predictions via <a href=https://en.wikipedia.org/wiki/Bipartite_matching>bipartite matching</a>. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.762.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--762 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.762 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.762" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.762/>Progressive Adversarial Learning for Bootstrapping : A Case Study on Entity Set Expansion</a></strong><br><a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--762><div class="card-body p-3 small">Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> which jointly models the <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping process</a> and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks ; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.765.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--765 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.765 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.765" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.765/>A Relation-Oriented Clustering Method for Open Relation Extraction</a></strong><br><a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--765><div class="card-body p-3 small">The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters can not explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn to cluster <a href=https://en.wikipedia.org/wiki/Relational_model>relational data</a>, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> reduces the <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> by 29.2 % and 15.7 %, on two datasets respectively, compared with current SOTA methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.768.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--768 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.768 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.768/>Meta Distant Transfer Learning for Pre-trained Language Models</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/h/haojie-pan/>Haojie Pan</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/f/fei-yang/>Fei Yang</a>
|
<a href=/people/y/yin-zhang/>Yin Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--768><div class="card-body p-3 small">With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from <a href=https://en.wikipedia.org/wiki/Negative_transfer>negative transfer</a>. Inspired by <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.769.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--769 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.769 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.769/>UniKER : A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>KER</span>: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference</a></strong><br><a href=/people/k/kewei-cheng/>Kewei Cheng</a>
|
<a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--769><div class="card-body p-3 small">Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional <a href=https://en.wikipedia.org/wiki/Logical_consequence>logical rule reasoning</a> and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use <a href=https://en.wikipedia.org/wiki/Probabilistic_model>probabilistic model</a> to approximate the exact <a href=https://en.wikipedia.org/wiki/Logical_inference>logical inference</a> (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> to be definite Horn rules, which can fully exploit the knowledge in <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.771.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--771 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.771 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.771/>Jointly Learning to Repair Code and Generate Commit Message</a></strong><br><a href=/people/j/jiaqi-bai/>Jiaqi Bai</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/a/ambrosio-blanco/>Ambrosio Blanco</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--771><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for <a href=https://en.wikipedia.org/wiki/Software_development>software development</a>. However, existing work usually performs the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> independently. We construct a multilingual triple dataset including <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a>, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the <a href=https://en.wikipedia.org/wiki/Source_code>program code</a> and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.773.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--773 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.773 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.773/>On Pursuit of Designing Multi-modal Transformer for Video Grounding</a></strong><br><a href=/people/m/meng-cao/>Meng Cao</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/m/mike-zheng-shou/>Mike Zheng Shou</a>
|
<a href=/people/c/can-zhang/>Can Zhang</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--773><div class="card-body p-3 small">Video grounding aims to localize the temporal segment corresponding to a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence query</a> from an <a href=https://en.wikipedia.org/wiki/Video_editing>untrimmed video</a>. Almost all existing video grounding methods fall into two frameworks : 1) <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>Top-down model</a> : It predefines a set of segment candidates and then conducts segment classification and <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a>. 2) Bottom-up model : It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, we design a new Multi-head Cross-Modal Attention. The whole <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GTR</a> is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, with several times faster <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.775.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--775 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.775 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.775" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.775/>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</a></strong><br><a href=/people/s/stella-frank/>Stella Frank</a>
|
<a href=/people/e/emanuele-bugliarello/>Emanuele Bugliarello</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--775><div class="card-body p-3 small">Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are not symmetrically cross-modal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.778.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--778 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.778 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.778" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.778/>QA-Align : Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions<span class=acl-fixed-case>QA</span>-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions</a></strong><br><a href=/people/d/daniela-brook-weiss/>Daniela Brook Weiss</a>
|
<a href=/people/p/paul-roit/>Paul Roit</a>
|
<a href=/people/a/ayal-klein/>Ayal Klein</a>
|
<a href=/people/o/ori-ernst/>Ori Ernst</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--778><div class="card-body p-3 small">Multi-text applications, such as <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>, are typically required to model redundancies across related texts. Current methods confronting <a href=https://en.wikipedia.org/wiki/Consolidation_(business)>consolidation</a> struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Analyses show that our new task is semantically challenging, capturing content overlap beyond <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.780.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--780 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.780 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.780" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.780/>Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings<span class=acl-fixed-case>T</span>witter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings</a></strong><br><a href=/people/m/marco-di-giovanni/>Marco Di Giovanni</a>
|
<a href=/people/m/marco-brambilla/>Marco Brambilla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--780><div class="card-body p-3 small">Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter&#8217;s intrinsic powerful signals of relatedness : replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2 M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities. Code available at https://github.com/marco-digio/Twitter4SSE</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.781.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--781 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.781 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.781/>Guilt by Association : Emotion Intensities in Lexical Representations</a></strong><br><a href=/people/s/shahab-raji/>Shahab Raji</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--781><div class="card-body p-3 small">What do linguistic models reveal about the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.782.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--782 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.782 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.782/>Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender</a></strong><br><a href=/people/s/sky-ch-wang/>Sky CH-Wang</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--782><div class="card-body p-3 small">Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these <a href=https://en.wikipedia.org/wiki/Choice>choices</a> in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study <a href=https://en.wikipedia.org/wiki/Word_choice>word choice</a> within a sociolinguistic lexical variablealternate words used to express the same conceptin order to test for change in the United States towards <a href=https://en.wikipedia.org/wiki/Human_sexuality>sexuality</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a>. We examine two variables : i) referents to significant others, such as the word partner and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and <a href=https://en.wikipedia.org/wiki/Gender_equality>gender equality</a>, respectively. In longitudinal analyses across <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> over 87 M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of <a href=https://en.wikipedia.org/wiki/Linguistic_change>linguistic change</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.785.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--785 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.785 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.785/>Assessing the Reliability of Word Embedding Gender Bias Measures</a></strong><br><a href=/people/y/yupei-du/>Yupei Du</a>
|
<a href=/people/q/qixiang-fang/>Qixiang Fang</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--785><div class="card-body p-3 small">Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these <a href=https://en.wikipedia.org/wiki/Measurement>measures</a> can suffer from <a href=https://en.wikipedia.org/wiki/Observational_error>measurement error</a>. One indication of measurement quality is <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a>, concerning the extent to which a <a href=https://en.wikipedia.org/wiki/Measurement>measure</a> produces consistent results. In this paper, we assess three types of <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a>, scoring rules and <a href=https://en.wikipedia.org/wiki/Word_formation>words</a>. Furthermore, we analyse the effects of various factors on these <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a>&#8217; <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability scores</a>. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.788.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--788 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.788 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.788" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.788/>SWEAT : Scoring Polarization of Topics across Different Corpora<span class=acl-fixed-case>SWEAT</span>: Scoring Polarization of Topics across Different Corpora</a></strong><br><a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/m/marco-marelli/>Marco Marelli</a>
|
<a href=/people/p/paolo-nicoli/>Paolo Nicoli</a>
|
<a href=/people/m/matteo-palmonari/>Matteo Palmonari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--788><div class="card-body p-3 small">Understanding differences of viewpoints across corpora is a fundamental task for <a href=https://en.wikipedia.org/wiki/Computational_social_sciences>computational social sciences</a>. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, <a href=https://en.wikipedia.org/wiki/SWEAT>SWEAT</a> uses two additional wordsets, deemed to have opposite valence, to represent two different <a href=https://en.wikipedia.org/wiki/Zeros_and_poles>poles</a>. We validate our approach and illustrate a case study to show the usefulness of the introduced <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.791.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--791 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.791 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.791" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.791/>PAUSE : Positive and Annealed Unlabeled Sentence Embedding<span class=acl-fixed-case>PAUSE</span>: Positive and Annealed Unlabeled Sentence Embedding</a></strong><br><a href=/people/l/lele-cao/>Lele Cao</a>
|
<a href=/people/e/emil-larsson/>Emil Larsson</a>
|
<a href=/people/v/vilhelm-von-ehrenheim/>Vilhelm von Ehrenheim</a>
|
<a href=/people/d/dhiana-deva-cavalcanti-rocha/>Dhiana Deva Cavalcanti Rocha</a>
|
<a href=/people/a/anna-martin/>Anna Martin</a>
|
<a href=/people/s/sonja-horn/>Sonja Horn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--791><div class="card-body p-3 small">Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) applications</a>. The majority of these techniques are either <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised</a> or unsupervised. Compared to the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, the supervised ones make less assumptions about <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization objectives</a> and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> without the burden of extensive manual annotation work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.793.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--793 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.793 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.793" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.793/>An Information-Theoretic Characterization of Morphological Fusion</a></strong><br><a href=/people/n/neil-rathi/>Neil Rathi</a>
|
<a href=/people/m/michael-hahn/>Michael Hahn</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--793><div class="card-body p-3 small">Linguistic typology generally divides <a href=https://en.wikipedia.org/wiki/Synthetic_language>synthetic languages</a> into groups based on their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological fusion</a>. However, this <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called <a href=https://en.wikipedia.org/wiki/Information_fusion>informational fusion</a>, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that <a href=https://en.wikipedia.org/wiki/Language>languages</a> have characteristic levels of fusion ; rather, the degree of fusion varies across part-of-speech within languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.794.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--794 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.794 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.794/>The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning</a></strong><br><a href=/people/y/yuchen-lian/>Yuchen Lian</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/t/tessa-verhoef/>Tessa Verhoef</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--794><div class="card-body p-3 small">Natural languages display a trade-off among different strategies to convey <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>, such as <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> or <a href=https://en.wikipedia.org/wiki/Inflection>inflection</a>. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field : (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, instead of developing a more efficient or systematic language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--800 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.800 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.800" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.800/>UNKs Everywhere : Adapting Multilingual Language Models to New Scripts<span class=acl-fixed-case>UNK</span>s Everywhere: <span class=acl-fixed-case>A</span>dapting Multilingual Language Models to New Scripts</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--800><div class="card-body p-3 small">Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model&#8217;s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT&#8217;s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--805 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.805" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.805/>Discretized Integrated Gradients for Explaining Language Models</a></strong><br><a href=/people/s/soumya-sanyal/>Soumya Sanyal</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--805><div class="card-body p-3 small">As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a>, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--814 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.814/>XLEnt : Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment<span class=acl-fixed-case>XLE</span>nt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/a/adithya-renduchintala/>Adithya Renduchintala</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--814><div class="card-body p-3 small">Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and cross-lingual wikification. While <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> contain a large number of entities in high-resource languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--816 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.816/>Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction<span class=acl-fixed-case>R</span>elation <span class=acl-fixed-case>E</span>xtraction</a></strong><br><a href=/people/b/bruno-taille/>Bruno Taillé</a>
|
<a href=/people/v/vincent-guigue/>Vincent Guigue</a>
|
<a href=/people/g/geoffrey-scoutheeten/>Geoffrey Scoutheeten</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--816><div class="card-body p-3 small">State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> : the mere retention of training relation triples. In this paper we propose two experiments confirming that retention of known facts is a key factor of performance on standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Furthermore, one experiment suggests that a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a> able to use intermediate type representations is less prone to over-rely on retention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--817 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.817" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.817/>Automatic Text Evaluation through the Lens of Wasserstein Barycenters<span class=acl-fixed-case>W</span>asserstein Barycenters</a></strong><br><a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/g/guillaume-staerman/>Guillaume Staerman</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a>
|
<a href=/people/p/pablo-piantanida/>Pablo Piantanida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--817><div class="card-body p-3 small">A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is motivated by a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> relying on optimal transport tools, i.e., <a href=https://en.wikipedia.org/wiki/Wasserstein_distance>Wasserstein distance</a> and <a href=https://en.wikipedia.org/wiki/Barycenter>barycenter</a>. By modelling the layer output of deep contextualized embeddings as a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> rather than by a vector embedding ; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> provides theoretical grounds to our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> and offers an alternative to available solutions (e.g., MoverScore and BertScore). Numerical evaluation is performed on four different tasks : <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>.<i>e.g.</i>, BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, <i>i.e.</i>, Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (<i>e.g.</i>, MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.820.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--820 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.820 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.820/>Robustness Evaluation of Entity Disambiguation Using Prior Probes : the Case of Entity Overshadowing</a></strong><br><a href=/people/v/vera-provatorova/>Vera Provatorova</a>
|
<a href=/people/s/samarth-bhargav/>Samarth Bhargav</a>
|
<a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--820><div class="card-body p-3 small">Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, that propagate the prior probability bias of the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity distribution</a> towards more frequently occurring entities. It was shown that the performance of the EL systems on such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16 K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.821.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--821 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.821 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.821" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.821/>IndoNLI : A Natural Language Inference Dataset for Indonesian<span class=acl-fixed-case>I</span>ndo<span class=acl-fixed-case>NLI</span>: A Natural Language Inference Dataset for <span class=acl-fixed-case>I</span>ndonesian</a></strong><br><a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/f/fahrurrozi-rahman/>Fahrurrozi Rahman</a>
|
<a href=/people/c/clara-vania/>Clara Vania</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--821><div class="card-body p-3 small">We present IndoNLI, the first human-elicited NLI dataset for <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>. We adapt the data collection protocol for MNLI and collect ~18 K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pre-trained models in our data. The best performance on the expert-annotated data is still far below <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> (13.4 % accuracy gap), suggesting that this <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can help accelerate progress in Indonesian NLP research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.824.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--824 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.824 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.824" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.824/>Efficient Sampling of Dependency Structure</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--824><div class="card-body p-3 small">Probabilistic distributions over spanning trees in <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a> are a fundamental model of dependency structure in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, syntactic dependency trees. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, dependency trees often have an additional root constraint : only one edge may emanate from the root. However, no <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling algorithm</a> has been presented in the literature to account for this additional <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a>. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> subject to the root constraint. Wilson (1996 (&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(H) where H is the mean hitting time of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Colbourn (1996)&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(N3), which is often greater than the mean hitting time of a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>. Additionally, we build upon Colbourn&#8217;s algorithm and present a novel extension that can sample K trees without replacement in O(K N3 + K2 N) time. To the best of our knowledge, no <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> has been given for sampling spanning trees without replacement from a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--825 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.825" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.825/>Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--825><div class="card-body p-3 small">Discontinuous constituent parsers have always lagged behind continuous approaches in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>, as the presence of constituents with discontinuous yield introduces extra <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a <a href=https://en.wikipedia.org/wiki/Bijection>bijective function</a> to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.826.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--826 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.826 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.826" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.826/>A New Representation for Span-based CCG Parsing<span class=acl-fixed-case>CCG</span> Parsing</a></strong><br><a href=/people/y/yoshihide-kato/>Yoshihide Kato</a>
|
<a href=/people/s/shigeki-matsubara/>Shigeki Matsubara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--826><div class="card-body p-3 small">This paper proposes a new <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--828 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.828" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.828/>PermuteFormer : Efficient Relative Position Encoding for Long Sequences<span class=acl-fixed-case>P</span>ermute<span class=acl-fixed-case>F</span>ormer: Efficient Relative Position Encoding for Long Sequences</a></strong><br><a href=/people/p/peng-chen/>Peng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--828><div class="card-body p-3 small">A recent variation of <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a>, <a href=https://en.wikipedia.org/wiki/Performer>Performer</a>, scales <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to <a href=https://en.wikipedia.org/wiki/Performer_(disambiguation)>Performer</a>. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> by design that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> and outperforms vanilla Transformer on most of the tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.829.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--829 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.829 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.829" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.829/>Block Pruning For Faster Transformers</a></strong><br><a href=/people/f/francois-lagunas/>François Lagunas</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--829><div class="card-body p-3 small">Pre-training has improved <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a> for both classification and generation tasks at the cost of introducing much larger and slower <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74 % smaller BERT on SQuAD v1, with a 1 % drop on F1, competitive both with distilled models in speed and pruned models in size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.831.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--831 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.831 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.831" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.831/>How to Train BERT with an Academic Budget<span class=acl-fixed-case>BERT</span> with an Academic Budget</a></strong><br><a href=/people/p/peter-izsak/>Peter Izsak</a>
|
<a href=/people/m/moshe-berchansky/>Moshe Berchansky</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--831><div class="card-body p-3 small">While large language models a la BERT are used ubiquitously in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such <a href=https://en.wikipedia.org/wiki/Physical_model>models</a> with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.834.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--834 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.834 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.834" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.834/>Pushing on Text Readability Assessment : A Transformer Meets Handcrafted Linguistic Features</a></strong><br><a href=/people/b/bruce-w-lee/>Bruce W. Lee</a>
|
<a href=/people/y/yoo-sung-jang/>Yoo Sung Jang</a>
|
<a href=/people/j/jason-lee/>Jason Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--834><div class="card-body p-3 small">We report two essential improvements in readability assessment : 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> (e.g. RoBERTa) to augment <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance. First, we explore suitable <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99 %, a 20.3 % increase from the previous SOTA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.837.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--837 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.837 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.837" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.837/>MTAdam : Automatic Balancing of Multiple Training Loss Terms<span class=acl-fixed-case>MTA</span>dam: Automatic Balancing of Multiple Training Loss Terms</a></strong><br><a href=/people/i/itzik-malkiel/>Itzik Malkiel</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--837><div class="card-body p-3 small">When training neural models, it is common to combine multiple loss terms. The balancing of these <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms</a> requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the <a href=https://en.wikipedia.org/wiki/Loss_function>loss terms</a> can change as training progresses, e.g., for <a href=https://en.wikipedia.org/wiki/Loss_function>adversarial terms</a>. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the <a href=https://en.wikipedia.org/wiki/Derivative>derivative</a> of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.839.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--839 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.839 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.839" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.839/>Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</a></strong><br><a href=/people/x/xinghua-zhang/>Xinghua Zhang</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/z/zhenyu-zhang/>Zhenyu Zhang</a>
|
<a href=/people/j/jiawei-sheng/>Jiawei Sheng</a>
|
<a href=/people/x/xue-mengge/>Xue Mengge</a>
|
<a href=/people/h/hongbo-xu/>Hongbo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--839><div class="card-body p-3 small">Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.842.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--842 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.842 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.842.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.842" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.842/>VeeAlign : Multifaceted Context Representation Using Dual Attention for Ontology Alignment<span class=acl-fixed-case>V</span>ee<span class=acl-fixed-case>A</span>lign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment</a></strong><br><a href=/people/v/vivek-iyer/>Vivek Iyer</a>
|
<a href=/people/a/arvind-agarwal/>Arvind Agarwal</a>
|
<a href=/people/h/harshit-kumar/>Harshit Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--842><div class="card-body p-3 small">Ontology Alignment is an important research problem applied to various fields such as <a href=https://en.wikipedia.org/wiki/Data_integration>data integration</a>, <a href=https://en.wikipedia.org/wiki/Data_transmission>data transfer</a>, <a href=https://en.wikipedia.org/wiki/Data_preparation>data preparation</a>, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a>, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on four different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.844.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--844 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.844 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.844" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.844/>GeneSis : A Generative Approach to Substitutes in Context<span class=acl-fixed-case>G</span>ene<span class=acl-fixed-case>S</span>is: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>G</span>enerative <span class=acl-fixed-case>A</span>pproach to <span class=acl-fixed-case>S</span>ubstitutes in <span class=acl-fixed-case>C</span>ontext</a></strong><br><a href=/people/c/caterina-lacerra/>Caterina Lacerra</a>
|
<a href=/people/r/rocco-tripodi/>Rocco Tripodi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--844><div class="card-body p-3 small">The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Furthermore, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at https://github.com/SapienzaNLP/genesis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.847.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--847 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.847 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.847/>Detecting Contact-Induced Semantic Shifts : What Can Embedding-Based Methods Do in Practice?<span class=acl-fixed-case>W</span>hat Can Embedding-Based Methods Do in Practice?</a></strong><br><a href=/people/f/filip-miletic/>Filip Miletic</a>
|
<a href=/people/a/anne-przewozny-desriaux/>Anne Przewozny-Desriaux</a>
|
<a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--847><div class="card-body p-3 small">This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in <a href=https://en.wikipedia.org/wiki/Quebec_English>Quebec English</a>. We contrast synchronic data from different regions in order to identify the meanings that are specific to <a href=https://en.wikipedia.org/wiki/Quebec>Quebec</a> and potentially related to <a href=https://en.wikipedia.org/wiki/Language_contact>language contact</a>. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>synchrony</a>, obtaining results comparable to the state of the art on similar tasks in <a href=https://en.wikipedia.org/wiki/Diachrony>diachrony</a>. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.</div></div></div><hr><div id=2021emnlp-demo><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.emnlp-demo/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.0/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.12/>LMdiff : A Visual Diff Tool to Compare <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a><span class=acl-fixed-case>LM</span>diff: A Visual Diff Tool to Compare Language Models</a></strong><br><a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/b/benjamin-hoover/>Benjamin Hoover</a>
|
<a href=/people/a/arvind-satyanaryan/>Arvind Satyanaryan</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--12><div class="card-body p-3 small">While different language models are ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distributions</a> of two <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that differ, e.g., through <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>, <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a>, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMdiff for hypothesis generation across multiple case studies. A demo is available at http://lmdiff.net.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.14/>Beyond <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Accuracy</a> : A Consolidated Tool for Visual Question Answering Benchmarking</a></strong><br><a href=/people/d/dirk-vath/>Dirk Väth</a>
|
<a href=/people/p/pascal-tilli/>Pascal Tilli</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--14><div class="card-body p-3 small">On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across multiple datasets, evaluating not just <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, but also performance in more realistic real-world scenarios such as <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to input noise. Additionally, we include <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that measure <a href=https://en.wikipedia.org/wiki/Bias>biases</a> and <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the <a href=https://en.wikipedia.org/wiki/Sample_(statistics)>data sample level</a>. As proof of concept, we perform a case study on four <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they can not recognize text in images. Our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> allow us to quantify which image and question embeddings provide most robustness to a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. All code s publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.15/>Athena 2.0 : Contextualized Dialogue Management for an Alexa Prize SocialBot<span class=acl-fixed-case>A</span>lexa <span class=acl-fixed-case>P</span>rize <span class=acl-fixed-case>S</span>ocial<span class=acl-fixed-case>B</span>ot</a></strong><br><a href=/people/j/juraj-juraska/>Juraj Juraska</a>
|
<a href=/people/k/kevin-bowden/>Kevin Bowden</a>
|
<a href=/people/l/lena-reed/>Lena Reed</a>
|
<a href=/people/v/vrindavan-harrison/>Vrindavan Harrison</a>
|
<a href=/people/w/wen-cui/>Wen Cui</a>
|
<a href=/people/o/omkar-patil/>Omkar Patil</a>
|
<a href=/people/r/rishi-rajasekaran/>Rishi Rajasekaran</a>
|
<a href=/people/a/angela-ramirez/>Angela Ramirez</a>
|
<a href=/people/c/cecilia-li/>Cecilia Li</a>
|
<a href=/people/e/eduardo-zamora/>Eduardo Zamora</a>
|
<a href=/people/p/phillip-lee/>Phillip Lee</a>
|
<a href=/people/j/jeshwanth-bheemanpally/>Jeshwanth Bheemanpally</a>
|
<a href=/people/r/rohan-pandey/>Rohan Pandey</a>
|
<a href=/people/a/adwait-ratnaparkhi/>Adwait Ratnaparkhi</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--15><div class="card-body p-3 small">Athena 2.0 is an <a href=https://en.wikipedia.org/wiki/Alexa_Internet>Alexa Prize SocialBot</a> that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena&#8217;s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe <a href=https://en.wikipedia.org/wiki/Athena>Athena</a>&#8217;s system design and performance in the <a href=https://en.wikipedia.org/wiki/Alexa_Internet>Alexa Prize</a> during the 20/21 competition. A live demo of <a href=https://en.wikipedia.org/wiki/Athena>Athena</a> as well as video recordings will provoke discussion on the state of the art in conversational AI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.19/>UMR-Writer : A Web Application for Annotating Uniform Meaning Representations<span class=acl-fixed-case>UMR</span>-Writer: A Web Application for Annotating Uniform Meaning Representations</a></strong><br><a href=/people/j/jin-zhao/>Jin Zhao</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/jens-van-gysel/>Jens Van Gysel</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--19><div class="card-body p-3 small">We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a <a href=https://en.wikipedia.org/wiki/Tool>tool</a> and how they are addressed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.22/>Summary Explorer : Visualizing the State of the Art in Text Summarization</a></strong><br><a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/t/tariq-yousef/>Tariq Yousef</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/s/stefan-janicke/>Stefan Jänicke</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--22><div class="card-body p-3 small">This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, and position bias), encapsulated in a guided assessment based on tailored visualizations. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> complements existing approaches for locally debugging summarization models and improves upon them. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> is available at https://tldr.webis.de/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.23/>MeetDot : <a href=https://en.wikipedia.org/wiki/Videotelephony>Videoconferencing</a> with Live Translation Captions<span class=acl-fixed-case>M</span>eet<span class=acl-fixed-case>D</span>ot: Videoconferencing with Live Translation Captions</a></strong><br><a href=/people/a/arkady-arkhangorodsky/>Arkady Arkhangorodsky</a>
|
<a href=/people/c/christopher-chu/>Christopher Chu</a>
|
<a href=/people/s/scot-fang/>Scot Fang</a>
|
<a href=/people/y/yiqi-huang/>Yiqi Huang</a>
|
<a href=/people/d/denglin-jiang/>Denglin Jiang</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--23><div class="card-body p-3 small">We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The <a href=https://en.wikipedia.org/wiki/System>system</a> aims to facilitate conversation between people who speak different languages, thereby reducing <a href=https://en.wikipedia.org/wiki/Language_barrier>communication barriers</a> between multilingual participants. Currently, our system supports <a href=https://en.wikipedia.org/wiki/Speech>speech</a> and captions in 4 languages and combines <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition (ASR)</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our <a href=https://en.wikipedia.org/wiki/System>system</a> has very strict latency requirements to have acceptable call quality. We implement several features to enhance <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> and reduce their <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a>, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.25/>LexiClean : An annotation tool for rapid multi-task lexical normalisation<span class=acl-fixed-case>L</span>exi<span class=acl-fixed-case>C</span>lean: An annotation tool for rapid multi-task lexical normalisation</a></strong><br><a href=/people/t/tyler-bikaun/>Tyler Bikaun</a>
|
<a href=/people/t/tim-french/>Tim French</a>
|
<a href=/people/m/melinda-hodkiewicz/>Melinda Hodkiewicz</a>
|
<a href=/people/m/michael-stewart/>Michael Stewart</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--25><div class="card-body p-3 small">NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, but currently lacks suitable tools to acquire high-quality annotated data to support <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based approaches</a>. In this paper, we present LexiClean, the first open-source web-based annotation tool for multi-task lexical normalisation. LexiClean&#8217;s main contribution is support for simultaneous in situ token-level modification and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> that can be rapidly applied corpus wide. We demonstrate the usefulness of our <a href=https://en.wikipedia.org/wiki/Tool>tool</a> through a case study on two sets of noisy corpora derived from the specialised-domain of industrial mining. We show that LexiClean allows for the rapid and efficient development of high-quality parallel corpora. A demo of our <a href=https://en.wikipedia.org/wiki/System>system</a> is available at : https://youtu.be/P7_ooKrQPDU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.26/>T3-Vis : visual analytic for Training and fine-Tuning Transformers in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/r/raymond-li/>Raymond Li</a>
|
<a href=/people/w/wen-xiao/>Wen Xiao</a>
|
<a href=/people/l/lanjun-wang/>Lanjun Wang</a>
|
<a href=/people/h/hyeju-jang/>Hyeju Jang</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--26><div class="card-body p-3 small">Transformers are the dominant architecture in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s intrinsic properties and behaviours. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> offers an intuitive overview that allows the user to explore different facets of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (e.g., hidden states, attention) through <a href=https://en.wikipedia.org/wiki/Interactive_visualization>interactive visualization</a>, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is useful, and suggest several improvements. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is available at : https://github.com/raymondzmc/T3-Vis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.28/>OpenFraming : Open-sourced Tool for Computational Framing Analysis of Multilingual Data<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>F</span>raming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data</a></strong><br><a href=/people/v/vibhu-bhatia/>Vibhu Bhatia</a>
|
<a href=/people/v/vidya-prasad-akavoor/>Vidya Prasad Akavoor</a>
|
<a href=/people/s/sejin-paik/>Sejin Paik</a>
|
<a href=/people/l/lei-guo/>Lei Guo</a>
|
<a href=/people/m/mona-jalal/>Mona Jalal</a>
|
<a href=/people/a/alyssa-smith/>Alyssa Smith</a>
|
<a href=/people/d/david-assefa-tofu/>David Assefa Tofu</a>
|
<a href=/people/e/edward-edberg-halim/>Edward Edberg Halim</a>
|
<a href=/people/y/yimeng-sun/>Yimeng Sun</a>
|
<a href=/people/m/margrit-betke/>Margrit Betke</a>
|
<a href=/people/p/prakash-ishwar/>Prakash Ishwar</a>
|
<a href=/people/d/derry-tanti-wijaya/>Derry Tanti Wijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--28><div class="card-body p-3 small">When journalists cover a news story, they can cover the story from multiple angles or perspectives. These <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a> are called frames, and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a <a href=https://en.wikipedia.org/wiki/Web_application>web-based system</a> for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>media framing theory</a> in <a href=https://en.wikipedia.org/wiki/Communication_studies>communication research</a>. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user&#8217;s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs/, or through our GitHub page https://github.com/vibss2397/openFraming.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.32/>CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset<span class=acl-fixed-case>C</span>ro<span class=acl-fixed-case>A</span>no : A Crowd Annotation Platform for Improving Label Consistency of <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span> Dataset</a></strong><br><a href=/people/b/baoli-zhang/>Baoli Zhang</a>
|
<a href=/people/z/zhucong-li/>Zhucong Li</a>
|
<a href=/people/z/zhen-gan/>Zhen Gan</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/j/jing-wan/>Jing Wan</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/y/yafei-shi/>Yafei Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--32><div class="card-body p-3 small">In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and <a href=https://en.wikipedia.org/wiki/Data_management>data management</a>, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator : CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector : CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer : We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error system</a> to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96 % and +2.57 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> respectively in model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.35/>SeqAttack : On Adversarial Attacks for Named Entity Recognition<span class=acl-fixed-case>S</span>eq<span class=acl-fixed-case>A</span>ttack: <span class=acl-fixed-case>O</span>n Adversarial Attacks for Named Entity Recognition</a></strong><br><a href=/people/w/walter-simoncini/>Walter Simoncini</a>
|
<a href=/people/g/gerasimos-spanakis/>Gerasimos Spanakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--35><div class="card-body p-3 small">Named Entity Recognition is a fundamental task in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and the ability of adversarial training to counteract these attacks. We find that character-level and word-level attacks are the most effective, but adversarial training can grant significant protection at little to no expense of standard performance. Alongside our results, we also release SeqAttack, a framework to conduct adversarial attacks against token classification models (used in this work for named entity recognition) and a companion web application to inspect and cherry pick adversarial examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-demo.40.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-demo.40" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.40/>DRIFT : A Toolkit for Diachronic Analysis of Scientific Literature<span class=acl-fixed-case>DRIFT</span>: A Toolkit for Diachronic Analysis of Scientific Literature</a></strong><br><a href=/people/a/abheesht-sharma/>Abheesht Sharma</a>
|
<a href=/people/g/gunjan-chhablani/>Gunjan Chhablani</a>
|
<a href=/people/h/harshit-pandey/>Harshit Pandey</a>
|
<a href=/people/r/rajaswa-patil/>Rajaswa Patil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--40><div class="card-body p-3 small">In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The analysis methods are collated from well-cited research works, with a few of our own methods added for good measure. Succinctly put, some of the analysis methods are : keyword extraction, word clouds, predicting declining / stagnant / growing trends using Productivity, tracking bi-grams using Acceleration plots, finding the Semantic Drift of words, tracking trends using similarity, etc. To demonstrate the utility and efficacy of our tool, we perform a case study on the cs. CL corpus of the arXiv repository and draw inferences from the analysis methods. The toolkit and the associated code are available here : https://github.com/rajaswa/DRIFT.</div></div></div><hr><div id=2021emnlp-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.emnlp-tutorials/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-tutorials.0/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></strong><br><a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-tutorials.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-tutorials--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-tutorials.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-tutorials.2/>Financial Opinion Mining</a></strong><br><a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-tutorials--2><div class="card-body p-3 small">In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers&#8217; opinions toward <a href=https://en.wikipedia.org/wiki/Financial_services>financial services</a> in the <a href=https://en.wikipedia.org/wiki/Financial_technology>FinTech industry</a>, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-tutorials.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-tutorials--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-tutorials.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-tutorials.5/>Robustness and Adversarial Examples in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-tutorials--5><div class="card-body p-3 small">Recent studies show that many NLP systems are sensitive and vulnerable to a small perturbation of inputs and do not generalize well across different datasets. This lack of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> derails the use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> in real-world applications. This tutorial aims at bringing awareness of practical concerns about NLP robustness. It targets NLP researchers and practitioners who are interested in building reliable NLP systems. In particular, we will review recent studies on analyzing the weakness of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> when facing adversarial inputs and data with a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distribution shift</a>. We will provide the audience with a holistic view of 1) how to use adversarial examples to examine the weakness of NLP models and facilitate <a href=https://en.wikipedia.org/wiki/Debugging>debugging</a> ; 2) how to enhance the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of existing NLP models and defense against adversarial inputs ; and 3) how the consideration of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> affects the real-world NLP applications used in our daily lives. We will conclude the tutorial by outlining future research directions in this area.</div></div></div><hr><div id=2021argmining-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.argmining-1/>Proceedings of the 8th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.0/>Proceedings of the 8th Workshop on Argument Mining</a></strong><br><a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.1/>Argument Mining on Twitter : A Case Study on the Planned Parenthood Debate<span class=acl-fixed-case>T</span>witter: A Case Study on the Planned Parenthood Debate</a></strong><br><a href=/people/m/muhammad-mahad-afzal-bhatti/>Muhammad Mahad Afzal Bhatti</a>
|
<a href=/people/a/ahsan-suheer-ahmad/>Ahsan Suheer Ahmad</a>
|
<a href=/people/j/joonsuk-park/>Joonsuk Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--1><div class="card-body p-3 small">Twitter is a popular platform to share opinions and claims, which may be accompanied by the underlying rationale. Such information can be invaluable to policy makers, marketers and social scientists, to name a few. However, the effort to mine arguments on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has been limited, mainly because a tweet is typically too short to contain an argument both a claim and a premise. In this paper, we propose a novel problem formulation to mine arguments from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> : We formulate <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> as a text classification task to identify tweets that serve as premises for a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> that represents a claim of interest. To demonstrate the efficacy of this formulation, we mine arguments for and against funding Planned Parenthood expressed in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We first present a new dataset of 24,100 tweets containing <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> # StandWithPP or # DefundPP, manually labeled as SUPPORT WITH REASON, SUPPORT WITHOUT REASON, and NO EXPLICIT SUPPORT. We then train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to determine the types of tweets, achieving the best performance of 71 % F1. Our results manifest claim-specific keywords as the most informative features, which in turn reveal prominent arguments for and against funding Planned Parenthood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.argmining-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.3/>Explainable Unsupervised Argument Similarity Rating with Abstract Meaning Representation and Conclusion Generation<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation and Conclusion Generation</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/p/philipp-heinisch/>Philipp Heinisch</a>
|
<a href=/people/p/philipp-wiesenbach/>Philipp Wiesenbach</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--3><div class="card-body p-3 small">When assessing the similarity of arguments, researchers typically use approaches that do not provide interpretable evidence or justifications for their ratings. Hence, the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> that determine argument similarity remain elusive. We address this issue by introducing novel argument similarity metrics that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that similar premises often lead to similar conclusionsand extend an approach for AMR-based argument similarity rating by estimating, in addition, the similarity of conclusions that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more interpretable and may even support argument quality judgements. Our approach provides significant performance improvements over strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in a fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations.<i>novel argument similarity metrics</i> that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that <i>similar premises</i> often lead to <i>similar conclusions</i>&#8212;and extend an approach for <i>AMR-based argument similarity rating</i> by estimating, in addition, the similarity of <i>conclusions</i> that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more <i>interpretable</i> and may even support <i>argument quality judgements</i>. Our approach provides significant performance improvements over strong baselines in a <i>fully unsupervised</i> setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.argmining-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.7/>Assessing the Sufficiency of Arguments through Conclusion Generation</a></strong><br><a href=/people/t/timon-gurcke/>Timon Gurcke</a>
|
<a href=/people/m/milad-alshomary/>Milad Alshomary</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--7><div class="card-body p-3 small">The premises of an argument give evidence or other reasons to support a conclusion. However, the amount of support required depends on the generality of a conclusion, the nature of the individual premises, and similar. An argument whose premises make its conclusion rationally worthy to be drawn is called sufficient in argument quality research. Previous work tackled sufficiency assessment as a standard text classification problem, not modeling the inherent relation of premises and conclusion. In this paper, we hypothesize that the conclusion of a sufficient argument can be generated from its premises. To study this hypothesis, we explore the potential of assessing sufficiency based on the output of large-scale pre-trained language models. Our best model variant achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of.885, outperforming the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> and being on par with <a href=https://en.wikipedia.org/wiki/Expert>human experts</a>. While manual evaluation reveals the quality of the generated conclusions, their impact remains low ultimately.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.11/>Bayesian Argumentation-Scheme Networks : A Probabilistic Model of Argument Validity Facilitated by Argumentation Schemes<span class=acl-fixed-case>B</span>ayesian Argumentation-Scheme Networks: <span class=acl-fixed-case>A</span> Probabilistic Model of Argument Validity Facilitated by Argumentation Schemes</a></strong><br><a href=/people/t/takahiro-kondo/>Takahiro Kondo</a>
|
<a href=/people/k/koki-washio/>Koki Washio</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--11><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for representing the reasoning structure of arguments using <a href=https://en.wikipedia.org/wiki/Bayesian_network>Bayesian networks</a> and <a href=https://en.wikipedia.org/wiki/First-order_logic>predicate logic</a> facilitated by <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation schemes</a>. We express the meaning of text segments using <a href=https://en.wikipedia.org/wiki/First-order_logic>predicate logic</a> and map the boolean values of <a href=https://en.wikipedia.org/wiki/First-order_logic>predicate logic expressions</a> to nodes in a <a href=https://en.wikipedia.org/wiki/Bayesian_network>Bayesian network</a>. The reasoning structure among text segments is described with a <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>directed acyclic graph</a>. While our <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> is highly expressive and capable of describing the informal logic of human arguments, it is too open-ended to actually build a network for an argument. It is not at all obvious which segment of argumentative text should be considered as a node in a <a href=https://en.wikipedia.org/wiki/Bayesian_network>Bayesian network</a>, and how to decide the dependencies among nodes. To alleviate the difficulty, we provide abstract network fragments, called <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a>, which represent typical argument justification patterns derived from <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation schemes</a>. The network construction process is decomposed into idiom selection, idiom instantiation, and idiom combination. We define 17 <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> in total by referring to <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation schemes</a> as well as analyzing actual arguments and fitting <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> to them. We also create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of pairs of an argumentative text and a corresponding <a href=https://en.wikipedia.org/wiki/Bayesian_network>Bayesian network</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains about 2,400 pairs, which is large in the research area of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation schemes</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.13/>Predicting Moderation of Deliberative Arguments : Is Argument Quality the Key?</a></strong><br><a href=/people/n/neele-falk/>Neele Falk</a>
|
<a href=/people/i/iman-jundi/>Iman Jundi</a>
|
<a href=/people/e/eva-maria-vecchi/>Eva Maria Vecchi</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--13><div class="card-body p-3 small">Human moderation is commonly employed in <a href=https://en.wikipedia.org/wiki/Deliberation>deliberative contexts</a> (argumentation and discussion targeting a shared decision on an issue relevant to a group, e.g., citizens arguing on how to employ a shared budget). As the scale of discussion enlarges in online settings, the overall discussion quality risks to drop and <a href=https://en.wikipedia.org/wiki/Moderation>moderation</a> becomes more important to assist participants in having a cooperative and productive interaction. The scale also makes it more important to employ <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP methods</a> for(semi-)automatic moderation, e.g. to prioritize when <a href=https://en.wikipedia.org/wiki/Moderation>moderation</a> is most needed. In this work, we make the first steps towards (semi-)automatic moderation by using state-of-the-art classification models to predict which posts require <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a>, showing that while the task is undoubtedly difficult, performance is significantly above baseline. We further investigate whether argument quality is a key indicator of the need for <a href=https://en.wikipedia.org/wiki/Moderation>moderation</a>, showing that surprisingly, high quality arguments also trigger <a href=https://en.wikipedia.org/wiki/Moderation>moderation</a>. We make our code and data publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.argmining-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--argmining-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.argmining-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.argmining-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.argmining-1.17/>Matching The Statements : A Simple and Accurate Model for Key Point Analysis</a></strong><br><a href=/people/h/hoang-phan/>Hoang Phan</a>
|
<a href=/people/l/long-nguyen/>Long Nguyen</a>
|
<a href=/people/l/long-nguyen/>Long Nguyen</a>
|
<a href=/people/k/khanh-doan/>Khanh Doan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--argmining-1--17><div class="card-body p-3 small">Key Point Analysis (KPA) is one of the most essential tasks in building an Opinion Summarization system, which is capable of generating key points for a collection of arguments toward a particular topic. Furthermore, KPA allows quantifying the coverage of each summary by counting its matched arguments. With the aim of creating high-quality summaries, it is necessary to have an in-depth understanding of each individual argument as well as its universal semantic in a specified context. In this paper, we introduce a promising model, named Matching the Statements (MTS) that incorporates the discussed topic information into arguments / key points comprehension to fully understand their meanings, thus accurately performing ranking and retrieving best-match key points for an input argument. Our approach has achieved the 4th place in Track 1 of the Quantitative Summarization Key Point Analysis Shared Task by IBM, yielding a competitive performance of 0.8956 (3rd) and 0.9632 (7th) strict and relaxed mean Average Precision, respectively.</div></div></div><hr><div id=2021blackboxnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.blackboxnlp-1/>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.0/>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.3/>Does <a href=https://en.wikipedia.org/wiki/Knowledge>External Knowledge</a> Help Explainable <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>? Automatic Evaluation vs. Human Ratings</a></strong><br><a href=/people/h/hendrik-schuff/>Hendrik Schuff</a>
|
<a href=/people/h/hsiu-yu-yang/>Hsiu-Yu Yang</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--3><div class="card-body p-3 small">Natural language inference (NLI) requires models to learn and apply commonsense knowledge. These reasoning abilities are particularly important for explainable NLI systems that generate a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanation</a> in addition to their label prediction. The integration of external knowledge has been shown to improve NLI systems, here we investigate whether it can also improve their explanation capabilities. For this, we investigate different sources of external knowledge and evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on in-domain data as well as on special transfer datasets that are designed to assess fine-grained reasoning capabilities. We find that different sources of knowledge have a different effect on reasoning abilities, for example, <a href=https://en.wikipedia.org/wiki/Implicit_knowledge>implicit knowledge</a> stored in language models can hinder reasoning on numbers and <a href=https://en.wikipedia.org/wiki/Negation>negations</a>. Finally, we conduct the largest and most fine-grained explainable NLI crowdsourcing study to date. It reveals that even large differences in automatic performance scores do neither reflect in human ratings of label, explanation, commonsense nor <a href=https://en.wikipedia.org/wiki/Grammar>grammar correctness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.5/>On the Limits of <a href=https://en.wikipedia.org/wiki/Minimal_pairs>Minimal Pairs</a> in Contrastive Evaluation</a></strong><br><a href=/people/j/jannis-vamvas/>Jannis Vamvas</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--5><div class="card-body p-3 small">Minimal sentence pairs are frequently used to analyze the behavior of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. It is often assumed that model behavior on contrastive pairs is predictive of <a href=https://en.wikipedia.org/wiki/Behavioral_model>model behavior</a> at large. We argue that two conditions are necessary for this assumption to hold : First, a tested hypothesis should be well-motivated, since experiments show that contrastive evaluation can lead to false positives. Secondly, test data should be chosen such as to minimize distributional discrepancy between evaluation time and deployment time. For a good approximation of deployment-time decoding, we recommend that <a href=https://en.wikipedia.org/wiki/Minimal_pairs>minimal pairs</a> are created based on machine-generated text, as opposed to human-written references. We present a contrastive evaluation suite for EnglishGerman MT that implements this recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.6/>What Models Know About Their Attackers : Deriving Attacker Information From Latent Representations</a></strong><br><a href=/people/z/zhouhang-xie/>Zhouhang Xie</a>
|
<a href=/people/j/jonathan-brophy/>Jonathan Brophy</a>
|
<a href=/people/a/adam-noack/>Adam Noack</a>
|
<a href=/people/w/wencong-you/>Wencong You</a>
|
<a href=/people/k/kalyani-asthana/>Kalyani Asthana</a>
|
<a href=/people/c/carter-perkins/>Carter Perkins</a>
|
<a href=/people/s/sabrina-reis/>Sabrina Reis</a>
|
<a href=/people/z/zayd-hammoudeh/>Zayd Hammoudeh</a>
|
<a href=/people/d/daniel-lowd/>Daniel Lowd</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--6><div class="card-body p-3 small">Adversarial attacks curated against NLP models are increasingly becoming practical threats. Although various methods have been developed to detect adversarial attacks, securing learning-based NLP systems in practice would require more than identifying and evading perturbed instances. To address these issues, we propose a new set of adversary identification tasks, Attacker Attribute Classification via Textual Analysis (AACTA), that attempts to obtain more detailed information about the attackers from adversarial texts. Specifically, given a piece of adversarial text, we hope to accomplish tasks such as localizing perturbed tokens, identifying the attacker&#8217;s access level to the target model, determining the evasion mechanism imposed, and specifying the perturbation type employed by the attacking algorithm. Our contributions are as follows : we formalize the task of classifying attacker attributes, and create a benchmark on various target models from sentiment classification and abuse detection domains. We show that signals from BERT models and target models can be used to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that reveal the properties of the attacking algorithms. We demonstrate that adversarial attacks leave interpretable traces in both feature spaces of pre-trained language models and target models, making AACTA a promising direction towards more trustworthy NLP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.8/>ProSPer : Probing Human and Neural Network Language Model Understanding of Spatial Perspective<span class=acl-fixed-case>P</span>ro<span class=acl-fixed-case>SP</span>er: Probing Human and Neural Network Language Model Understanding of Spatial Perspective</a></strong><br><a href=/people/t/tessa-masis/>Tessa Masis</a>
|
<a href=/people/c/carolyn-anderson/>Carolyn Anderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--8><div class="card-body p-3 small">Understanding perspectival language is important for applications like <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> and <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a>. We propose a probe task that explores how well <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> understand <a href=https://en.wikipedia.org/wiki/Perspective_(graphical)>spatial perspective</a>. We present a dataset for evaluating perspective inference in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, ProSPer, and use it to explore how humans and Transformer-based language models infer perspective. Although the best bidirectional model performs similarly to humans, they display different strengths : <a href=https://en.wikipedia.org/wiki/Human>humans</a> outperform <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> in conversational contexts, while RoBERTa excels at written genres.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.10/>Transferring Knowledge from Vision to Language : How to Achieve it and how to Measure it?</a></strong><br><a href=/people/t/tobias-norlund/>Tobias Norlund</a>
|
<a href=/people/l/lovisa-hagstrom/>Lovisa Hagström</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--10><div class="card-body p-3 small">Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is based on two steps, 1) a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> querying for <a href=https://en.wikipedia.org/wiki/Memory_color>knowledge of memory colors</a>, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a <a href=https://en.wikipedia.org/wiki/Modeling_language>model architecture</a> that involves a visual imagination step and evaluate it with our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.13/>A howling success or a working sea? Testing what BERT knows about metaphors<span class=acl-fixed-case>BERT</span> knows about metaphors</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/e/eliana-di-palma/>Eliana Di Palma</a>
|
<a href=/people/l/ludovica-cerini/>Ludovica Cerini</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--13><div class="card-body p-3 small">Metaphor is a widespread linguistic and cognitive phenomenon that is ruled by mechanisms which have received attention in the literature. Transformer Language Models such as BERT have brought improvements in metaphor-related tasks. However, they have been used only in application contexts, while their knowledge of the phenomenon has not been analyzed. To test what BERT knows about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>, we challenge it on a new dataset that we designed to test various aspects of this phenomenon such as variations in linguistic structure, variations in <a href=https://en.wikipedia.org/wiki/Convention_(norm)>conventionality</a>, the boundaries of the plausibility of a metaphor and the interpretations that we attribute to metaphoric expressions. Results bring out some tendencies that suggest that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can reproduce some human intuitions about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.14/>How Length Prediction Influence the Performance of Non-Autoregressive Translation?</a></strong><br><a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/s/su-chang/>Su Chang</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--14><div class="card-body p-3 small">Length prediction is a special task in a series of NAT models where target length has to be determined before generation. However, the performance of length prediction and its influence on translation quality has seldom been discussed. In this paper, we present comprehensive analyses on length prediction task of NAT, aiming to find the factors that influence performance, as well as how it associates with translation quality. We mainly perform experiments based on Conditional Masked Language Model (CMLM) (Ghazvininejad et al., 2019), a representative NAT model, and evaluate it on two language pairs, En-De and En-Ro. We draw two conclusions : 1) The performance of length prediction is mainly influenced by properties of language pairs such as alignment pattern, <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.15/>On the Language-specificity of Multilingual BERT and the Impact of <a href=https://en.wikipedia.org/wiki/Fine-tuning>Fine-tuning</a><span class=acl-fixed-case>BERT</span> and the Impact of Fine-tuning</a></strong><br><a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a>
|
<a href=/people/c/claudia-borg/>Claudia Borg</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--15><div class="card-body p-3 small">Recent work has shown evidence that the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> acquired by multilingual BERT (mBERT) has two components : a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks POS tagging and natural language inference which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, a result that is supported by evidence from <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> experiments. However, further experiments on &#8216;unlearning&#8217; language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. The results presented here suggest that the process of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> causes a reorganisation of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s limited representational capacity, enhancing language-independent representations at the expense of language-specific ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.18/>Variation and generality in encoding of syntactic anomaly information in sentence embeddings</a></strong><br><a href=/people/q/qinxuan-wu/>Qinxuan Wu</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--18><div class="card-body p-3 small">While sentence anomalies have been applied periodically for testing in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, we have yet to establish a picture of the precise status of anomaly information in representations from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine-grained differences in anomaly encoding by designing probing tasks that vary the hierarchical level at which anomalies occur in a sentence. Second, we test not only models&#8217; ability to detect a given <a href=https://en.wikipedia.org/wiki/Anomaly_(physics)>anomaly</a>, but also the generality of the detected anomaly signal, by examining transfer between distinct anomaly types. Results suggest that all <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> encode some information supporting <a href=https://en.wikipedia.org/wiki/Anomaly_detection>anomaly detection</a>, but detection performance varies between anomalies, and only representations from more re- cent transformer models show signs of generalized knowledge of anomalies. Follow-up analyses support the notion that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position information is likely also a contributor to the observed <a href=https://en.wikipedia.org/wiki/Anomaly_detection>anomaly detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.19/>Enhancing Interpretable Clauses Semantically using Pretrained Word Representation</a></strong><br><a href=/people/r/rohan-kumar-yadav/>Rohan Kumar Yadav</a>
|
<a href=/people/l/lei-jiao/>Lei Jiao</a>
|
<a href=/people/o/ole-christoffer-granmo/>Ole-Christoffer Granmo</a>
|
<a href=/people/m/morten-goodwin/>Morten Goodwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--19><div class="card-body p-3 small">Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based on <a href=https://en.wikipedia.org/wiki/Propositional_calculus>propositional logic</a>, which has demonstrated competitive performance in many Natural Language Processing (NLP) tasks, including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, text classification, and <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. To obtain human-level interpretability, legacy TM employs Boolean input features such as bag-of-words (BOW). However, the BOW representation makes it difficult to use any pre-trained information, for instance, <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe word representations. This restriction has constrained the performance of TM compared to deep neural networks (DNNs) in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. To reduce the performance gap, in this paper, we propose a novel way of using pre-trained word representations for TM. The approach significantly enhances the performance and interpretability of TM. We achieve this by extracting semantically related words from pre-trained word representations as input features to the TM. Our experiments show that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the proposed approach is significantly higher than the previous BOW-based TM, reaching the level of DNN-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.21/>An in-depth look at Euclidean disk embeddings for structure preserving parsing<span class=acl-fixed-case>E</span>uclidean disk embeddings for structure preserving parsing</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/l/lan-xiao/>Lan Xiao</a>
|
<a href=/people/a/allan-jepson/>Allan Jepson</a>
|
<a href=/people/a/afsaneh-fazly/>Afsaneh Fazly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--21><div class="card-body p-3 small">Preserving the structural properties of trees or graphs when embedding them into a <a href=https://en.wikipedia.org/wiki/Metric_space>metric space</a> allows for a high degree of <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, and has been shown beneficial for <a href=https://en.wikipedia.org/wiki/Downstream_(computer_science)>downstream tasks</a> (e.g., hypernym detection, natural language inference, multimodal retrieval). However, whereas the majority of prior work looks at using structure-preserving embeddings when encoding a <a href=https://en.wikipedia.org/wiki/Structure>structure</a> given as input, e.g., <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> (Fellbaum, 1998), there is little exploration on how to use such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> when predicting one. We address this gap for two structure generation tasks, namely dependency and semantic parsing. We test the applicability of disk embeddings (Suzuki et al., 2019) that has been proposed for embedding Directed Acyclic Graphs (DAGs) but has not been tested on tasks that generate such structures. Our experimental results show that for both tasks the original disk embedding formulation leads to much worse performance when compared to non-structure-preserving baselines. We propose enhancements to this formulation and show that they almost close the performance gap for dependency parsing. However, the gap still remains notable for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> due to the complexity of meaning representation graphs, suggesting a challenge for generating interpretable semantic parse representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.26/>Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference<span class=acl-fixed-case>J</span>apanese Adversarial Natural Language Inference</a></strong><br><a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--26><div class="card-body p-3 small">Despite the success of multilingual pre-trained language models, it remains unclear to what extent these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have human-like generalization capacity across languages. The aim of this study is to investigate the out-of-distribution generalization of pre-trained language models through Natural Language Inference (NLI) in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the typological properties of which are different from those of <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We introduce a synthetically generated Japanese NLI dataset, called the Japanese Adversarial NLI (JaNLI) dataset, which is inspired by the English HANS dataset and is designed to require understanding of Japanese linguistic phenomena and illuminate the vulnerabilities of models. Through a series of experiments to evaluate the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance of both Japanese and multilingual BERT models, we demonstrate that there is much room to improve current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on Japanese NLI tasks. Furthermore, a comparison of human performance and model performance on the different types of garden-path sentences in the JaNLI dataset shows that structural phenomena that ease interpretation of garden-path sentences for human readers do not help <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> in the same way, highlighting a difference between human readers and the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.27/>Investigating Negation in Pre-trained Vision-and-language Models</a></strong><br><a href=/people/r/radina-dobreva/>Radina Dobreva</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--27><div class="card-body p-3 small">Pre-trained vision-and-language models have achieved impressive results on a variety of tasks, including ones that require complex reasoning beyond <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object recognition</a>. However, little is known about how they achieve these results or what their limitations are. In this paper, we focus on a particular linguistic capability, namely the understanding of negation. We borrow techniques from the analysis of language models to investigate the ability of pre-trained vision-and-language models to handle <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. We find that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> severely underperform in the presence of <a href=https://en.wikipedia.org/wiki/Negation>negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.30/>Learning Mathematical Properties of Integers</a></strong><br><a href=/people/m/maria-ryskina/>Maria Ryskina</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--30><div class="card-body p-3 small">Embedding words in <a href=https://en.wikipedia.org/wiki/High-dimensional_space>high-dimensional vector spaces</a> has proven valuable in many <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language applications</a>. In this work, we investigate whether similarly-trained embeddings of integers can capture concepts that are useful for <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>mathematical applications</a>. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.34/>An Investigation of Language Model Interpretability via Sentence Editing</a></strong><br><a href=/people/s/samuel-stevens/>Samuel Stevens</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--34><div class="card-body p-3 small">Pre-trained language models (PLMs) like BERT are being used for almost all language-related tasks, but interpreting their behavior still remains a significant challenge and many important questions remain largely unanswered. In this work, we re-purpose a sentence editing dataset, where faithful high-quality human rationales can be automatically extracted and compared with extracted model rationales, as a new testbed for interpretability. This enables us to conduct a systematic investigation on an array of questions regarding PLMs&#8217; interpretability, including the role of pre-training procedure, comparison of rationale extraction methods, and different layers in the PLM. The investigation generates new insights, for example, contrary to the common understanding, we find that attention weights correlate well with <a href=https://en.wikipedia.org/wiki/Rationality>human rationales</a> and work better than gradient-based saliency in extracting model rationales. Both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and code will be released to facilitate future interpretability research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.37/>Controlled tasks for model analysis : Retrieving discrete information from sequences</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--37><div class="card-body p-3 small">In recent years, the NLP community has shown increasing interest in analysing how <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> work. Given that large models trained on complex <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are difficult to inspect, some of this work has focused on controlled tasks that emulate specific aspects of <a href=https://en.wikipedia.org/wiki/Language>language</a>. We propose a new set of such controlled tasks to explore a crucial aspect of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that has not received enough attention : the need to retrieve discrete information from sequences. We also study model behavior on the tasks with simple instantiations of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> and <a href=https://en.wikipedia.org/wiki/Light-emitting_diode>LSTMs</a>. Our results highlight the beneficial role of decoder attention and its sometimes unexpected interaction with other components. Moreover, we show that, for most of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, these simple models still show significant difficulties. We hope that the community will take up the analysis possibilities that our tasks afford, and that a clearer understanding of model behavior on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> will lead to better and more transparent models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.40/>Do <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> Know the Way to Rome?<span class=acl-fixed-case>R</span>ome?</a></strong><br><a href=/people/b/bastien-lietard/>Bastien Liétard</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--40><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Global_geometry>global geometry</a> of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in <a href=https://en.wikipedia.org/wiki/Geography>geography</a>, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which <a href=https://en.wikipedia.org/wiki/Language_model>language model representations</a> of city and country names are isomorphic to real-world geography, e.g., if you tell a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> where <a href=https://en.wikipedia.org/wiki/Paris>Paris</a> and <a href=https://en.wikipedia.org/wiki/Berlin>Berlin</a> are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> performing the best, suggesting that geographic knowledge can be induced from higher-order co-occurrence statistics.<i>can</i> be induced from higher-order co-occurrence statistics.</div></div></div><hr><div id=2021cinlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.cinlp-1/>Proceedings of the First Workshop on Causal Inference and NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.0/>Proceedings of the First Workshop on Causal Inference and NLP</a></strong><br><a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/e/emaad-manzoor/>Emaad Manzoor</a>
|
<a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/d/dhanya-sridhar/>Dhanya Sridhar</a>
|
<a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/j/justin-grimmer/>Justin Grimmer</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/m/molly-roberts/>Molly Roberts</a>
|
<a href=/people/u/uri-shalit/>Uri Shalit</a>
|
<a href=/people/b/brandon-m-stewart/>Brandon Stewart</a>
|
<a href=/people/v/victor-veitch/>Victor Veitch</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cinlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cinlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.2/>Text as Causal Mediators : Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects</a></strong><br><a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/d/douglas-rice/>Douglas Rice</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cinlp-1--2><div class="card-body p-3 small">Using observed language to understand <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>interpersonal interactions</a> is important in <a href=https://en.wikipedia.org/wiki/Decision-making>high-stakes decision making</a>. We propose a causal research design for observational (non-experimental) data to estimate the natural direct and indirect effects of social group signals (e.g. race or gender) on speakers&#8217; responses with separate aspects of <a href=https://en.wikipedia.org/wiki/Language>language</a> as causal mediators. We illustrate the promises and challenges of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> via a theoretical case study of the effect of an advocate&#8217;s gender on interruptions from justices during <a href=https://en.wikipedia.org/wiki/Supreme_Court_of_the_United_States>U.S. Supreme Court oral arguments</a>. We also discuss challenges conceptualizing and operationalizing causal variables such as gender and <a href=https://en.wikipedia.org/wiki/Language>language</a> that comprise of many components, and we articulate technical open challenges such as temporal dependence between <a href=https://en.wikipedia.org/wiki/Language>language mediators</a> in conversational settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cinlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cinlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cinlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.3/>Enhancing Model Robustness and Fairness with <a href=https://en.wikipedia.org/wiki/Causality>Causality</a> : A Regularization Approach</a></strong><br><a href=/people/z/zhao-wang/>Zhao Wang</a>
|
<a href=/people/k/kai-shu/>Kai Shu</a>
|
<a href=/people/a/aron-culotta/>Aron Culotta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cinlp-1--3><div class="card-body p-3 small">Recent work has raised concerns on the risk of spurious correlations and unintended biases in statistical machine learning models that threaten <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>model robustness</a> and <a href=https://en.wikipedia.org/wiki/Fair_division>fairness</a>. In this paper, we propose a simple and intuitive regularization approach to integrate causal knowledge during model training and build a robust and fair model by emphasizing causal features and de-emphasizing spurious features. Specifically, we first manually identify causal and spurious features with principles inspired from the counterfactual framework of causal inference. Then, we propose a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization approach</a> to penalize causal and spurious features separately. By adjusting the strength of the penalty for each type of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a>, we build a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> that relies more on causal features and less on non-causal features. We conduct experiments to evaluate model robustness and fairness on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with multiple metrics. Empirical results show that the new models built with causal awareness significantly improve model robustness with respect to counterfactual texts and model fairness with respect to sensitive attributes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cinlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cinlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cinlp-1.5.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.5/>Sensitivity Analysis for Causal Mediation through Text : an Application to Political Polarization</a></strong><br><a href=/people/g/graham-tierney/>Graham Tierney</a>
|
<a href=/people/a/alexander-volfovsky/>Alexander Volfovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cinlp-1--5><div class="card-body p-3 small">We introduce a procedure to examine a text-as-mediator problem from a novel <a href=https://en.wikipedia.org/wiki/Randomized_experiment>randomized experiment</a> that studied the effect of <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> on <a href=https://en.wikipedia.org/wiki/Political_polarization>political polarization</a>. In this <a href=https://en.wikipedia.org/wiki/Randomized_experiment>randomized experiment</a>, Americans from the Democratic and Republican parties were either randomly paired with one-another to have an anonymous conversation about politics or alternatively not assigned to a conversation change in <a href=https://en.wikipedia.org/wiki/Political_polarization>political polarization</a> over time was measured for all participants. This paper analyzes the text of the conversations to identify potential mediators of <a href=https://en.wikipedia.org/wiki/Depolarization>depolarization</a> and is faced with a unique challenge, necessitated by the primary research hypothesis, that individuals in the control condition do not have conversations and so lack observed text data. We highlight the importance of using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to perform <a href=https://en.wikipedia.org/wiki/Dimensionality_reduction>dimension reduction</a> on the <a href=https://en.wikipedia.org/wiki/Writing>text data</a>, and describe a procedure to characterize indirect effects via <a href=https://en.wikipedia.org/wiki/Writing>text</a> when the text is only observed in one arm of the experiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cinlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cinlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.8/>It’s quality and quantity : the effect of the amount of comments on online suicidal posts</a></strong><br><a href=/people/d/daniel-low/>Daniel Low</a>
|
<a href=/people/k/kelly-zuromski/>Kelly Zuromski</a>
|
<a href=/people/d/daniel-kessler/>Daniel Kessler</a>
|
<a href=/people/s/satrajit-s-ghosh/>Satrajit S. Ghosh</a>
|
<a href=/people/m/matthew-k-nock/>Matthew K. Nock</a>
|
<a href=/people/w/walter-dempsey/>Walter Dempsey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cinlp-1--8><div class="card-body p-3 small">Every day, individuals post suicide notes on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> asking for support, resources, and reasons to live. Some posts receive few comments while others receive many. While prior studies have analyzed whether specific responses are more or less helpful, it is not clear if the quantity of comments received is beneficial in reducing symptoms or in keeping the user engaged with the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> and hence with life. In the present study, we create a large dataset of users&#8217; first r / SuicideWatch (SW) posts from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> (N=21,274), collect the comments as well as the user&#8217;s subsequent posts (N=1,615,699) to determine whether they post in SW again in the future. We use propensity score stratification, a causal inference method for observational data, and estimate whether the amount of comments as a measure of <a href=https://en.wikipedia.org/wiki/Social_support>social support</a> increases or decreases the likelihood of posting again on SW. One hypothesis is that receiving more comments may decrease the likelihood of the user posting in SW in the future, either by reducing symptoms or because comments from untrained peers may be harmful. On the contrary, we find that receiving more comments increases the likelihood a user will post in SW again. We discuss how receiving more comments is helpful, not by permanently relieving symptoms since users make another SW post and their second posts have similar mentions of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a>, but rather by reinforcing users to seek support and remain engaged with the platform.<i>decrease</i> the likelihood of the user posting in SW in the future, either by reducing symptoms or because comments from untrained peers may be harmful. On the contrary, we find that receiving more comments <i>increases</i> the likelihood a user will post in SW again. We discuss how receiving more comments is helpful, not by permanently relieving symptoms since users make another SW post and their second posts have similar mentions of suicidal ideation, but rather by reinforcing users to seek support and remain engaged with the platform. Furthermore, since receiving only 1 comment &#8212;the most common case&#8212; decreases the likelihood of posting again by 14% on average depending on the time window, it is important to develop systems that encourage more commenting.</div></div></div><hr><div id=2021codi-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.codi-main/>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.0/>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</a></strong><br><a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.2/>Developing Conversational Data and Detection of Conversational Humor in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a><span class=acl-fixed-case>T</span>elugu</a></strong><br><a href=/people/v/vaishnavi-pamulapati/>Vaishnavi Pamulapati</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--2><div class="card-body p-3 small">In the field of humor research, there has been a recent surge of interest in the sub-domain of Conversational Humor (CH). This study has two main objectives. (a) develop a conversational (humorous and non-humorous) dataset in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>. (b) detect <a href=https://en.wikipedia.org/wiki/Methylene_bridge>CH</a> in the compiled <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In this paper, the challenges faced while collecting the data and experiments carried out are elucidated. Transfer learning and non-transfer learning techniques are implemented by utilizing pre-trained models such as FastText word embeddings, BERT language models and Text GCN, which learns the word and document embeddings simultaneously of the corpus given. State-of-the-art results are observed with a 99.3 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and a 98.5 % f1 score achieved by <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.9/>Comparison of methods for explicit discourse connective identification across various domains</a></strong><br><a href=/people/m/merel-scholman/>Merel Scholman</a>
|
<a href=/people/t/tianai-dong/>Tianai Dong</a>
|
<a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--9><div class="card-body p-3 small">Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014 ; the winner of CONLL2015, Wang et al., 2015 ; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other <a href=https://en.wikipedia.org/wiki/Parsing>parse methods</a> in all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. However, performance drops significantly from the <a href=https://en.wikipedia.org/wiki/PDTB>PDTB</a> to all other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.10/>Revisiting Shallow Discourse Parsing in the PDTB-3 : Handling Intra-sentential Implicits<span class=acl-fixed-case>PDTB</span>-3: Handling Intra-sentential Implicits</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--10><div class="card-body p-3 small">In the PDTB-3, several thousand implicit discourse relations were newly annotated within individual sentences, adding to the over 15,000 implicit relations annotated across adjacent sentences in the PDTB-2. Given that the position of the arguments to these intra-sentential implicits is no longer as well-defined as with inter-sentential implicits, a discourse parser must identify both their location and their sense. That is the focus of the current work. The paper provides a comprehensive analysis of our results, showcasing <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance under different scenarios, pointing out limitations and noting future directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.codi-main.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.12/>discopy : A Neural System for Shallow Discourse Parsing</a></strong><br><a href=/people/r/rene-knaebel/>René Knaebel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--12><div class="card-body p-3 small">This paper demonstrates discopy, a novel framework that makes it easy to design components for end-to-end shallow discourse parsing. For the purpose of demonstration, we implement recent neural approaches and integrate contextualized word embeddings to predict explicit and non-explicit discourse relations. Our proposed neural feature-free system performs competitively to systems presented at the latest Shared Task on Shallow Discourse Parsing. Finally, a <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>web front end</a> is shown that simplifies the inspection of annotated documents. The source code, documentation, and pretrained models are publicly accessible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-main.14/>Capturing document context inside sentence-level neural machine translation models with self-training</a></strong><br><a href=/people/e/elman-mansimov/>Elman Mansimov</a>
|
<a href=/people/g/gabor-melis/>Gábor Melis</a>
|
<a href=/people/l/lei-yu/>Lei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-main--14><div class="card-body p-3 small">Neural machine translation (NMT) has arguably achieved human level parity when trained and evaluated at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level</a>. Document-level neural machine translation has received less attention and lags behind its sentence-level counterpart. The majority of the proposed document-level approaches investigate ways of conditioning the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on several source or target sentences to capture <a href=https://en.wikipedia.org/wiki/Context_(language_use)>document context</a>. These approaches require training a specialized NMT model from scratch on parallel document-level corpora. We propose an approach that does n&#8217;t require training a specialized <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on parallel document-level corpora and is applied to a trained sentence-level NMT model at decoding time. We process the document from left to right multiple times and self-train the sentence-level model on pairs of source sentences and generated translations. Our approach reinforces the choices made by the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, thus making it more likely that the same choices will be made in other sentences in the document. We evaluate our approach on three document-level datasets : NIST Chinese-English, WMT19 Chinese-English and OpenSubtitles English-Russian. We demonstrate that our approach has higher BLEU score and higher human preference than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Qualitative analysis of our approach shows that choices made by model are consistent across the document.</div></div></div><hr><div id=2021codi-sharedtask><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.codi-sharedtask/>Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-sharedtask.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-sharedtask.0/>Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</a></strong><br><a href=/people/s/sopan-khosla/>Sopan Khosla</a>
|
<a href=/people/r/ramesh-manuvinakurike/>Ramesh Manuvinakurike</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rosé</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-sharedtask.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-sharedtask--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-sharedtask.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-sharedtask.3/>Anaphora Resolution in Dialogue : Description of the DFKI-TalkingRobots System for the CODI-CRAC 2021 Shared-Task<span class=acl-fixed-case>DFKI</span>-<span class=acl-fixed-case>T</span>alking<span class=acl-fixed-case>R</span>obots System for the <span class=acl-fixed-case>CODI</span>-<span class=acl-fixed-case>CRAC</span> 2021 Shared-Task</a></strong><br><a href=/people/t/tatiana-anikina/>Tatiana Anikina</a>
|
<a href=/people/c/cennet-oguz/>Cennet Oguz</a>
|
<a href=/people/n/natalia-skachkova/>Natalia Skachkova</a>
|
<a href=/people/s/siyu-tao/>Siyu Tao</a>
|
<a href=/people/s/sharmila-upadhyaya/>Sharmila Upadhyaya</a>
|
<a href=/people/i/ivana-kruijff-korbayova/>Ivana Kruijff-Korbayova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-sharedtask--3><div class="card-body p-3 small">We describe the system developed by the DFKI-TalkingRobots Team for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. Our system consists of three subsystems : (1) the Workspace Coreference System (WCS) incrementally clusters mentions using semantic similarity based on embeddings combined with lexical feature heuristics ; (2) the Mention-to-Mention (M2 M) coreference resolution system pairs same entity mentions ; (3) the Discourse Deixis Resolution (DDR) system employs a Siamese Network to detect discourse anaphor-antecedent pairs. WCS achieved F1-score of 55.6 % averaged across the evaluation test sets, <a href=https://en.wikipedia.org/wiki/Wildlife_Conservation_Society>M2 M</a> achieved 57.2 % and <a href=https://en.wikipedia.org/wiki/Wildlife_Conservation_Society>DDR</a> achieved 21.5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.codi-sharedtask.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--codi-sharedtask--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.codi-sharedtask.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.codi-sharedtask.8/>The CODI-CRAC 2021 Shared Task on <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>Anaphora</a>, Bridging, and Discourse Deixis Resolution in Dialogue : A Cross-Team Analysis<span class=acl-fixed-case>CODI</span>-<span class=acl-fixed-case>CRAC</span> 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis Resolution in Dialogue: A Cross-Team Analysis</a></strong><br><a href=/people/s/shengjie-li/>Shengjie Li</a>
|
<a href=/people/h/hideo-kobayashi/>Hideo Kobayashi</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--codi-sharedtask--8><div class="card-body p-3 small">The CODI-CRAC 2021 shared task is the first shared task that focuses exclusively on <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a> in dialogue and provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis resolution. We perform a cross-task analysis of the <a href=https://en.wikipedia.org/wiki/System>systems</a> that participated in the shared task in each of these tracks.</div></div></div><hr><div id=2021conll-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.conll-1/>Proceedings of the 25th Conference on Computational Natural Language Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.0/>Proceedings of the 25th Conference on Computational Natural Language Learning</a></strong><br><a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.1/>It’s our fault ! : Insights Into Users’ Understanding and Interaction With an Explanatory Collaborative Dialog System</a></strong><br><a href=/people/k/katharina-weitz/>Katharina Weitz</a>
|
<a href=/people/l/lindsey-vanderlyn/>Lindsey Vanderlyn</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/e/elisabeth-andre/>Elisabeth André</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--1><div class="card-body p-3 small">Human-AI collaboration, a long standing goal in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>, refers to a partnership where a human and artificial intelligence work together towards a shared goal. Collaborative dialog allows human-AI teams to communicate and leverage strengths from both partners. To design collaborative dialog systems, it is important to understand what mental models users form about their AI-dialog partners, however, how users perceive these <a href=https://en.wikipedia.org/wiki/System>systems</a> is not fully understood. In this study, we designed a novel, collaborative, communication-based puzzle game and explanatory dialog system. We created a public corpus from 117 conversations and post-surveys and used this to analyze what <a href=https://en.wikipedia.org/wiki/Mental_model>mental models</a> users formed. Key takeaways include : Even when users were not engaged in the <a href=https://en.wikipedia.org/wiki/Game>game</a>, they perceived the AI-dialog partner as intelligent and likeable, implying they saw it as a partner separate from the game. This was further supported by users often overestimating the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s abilities and projecting human-like attributes which led to miscommunications. We conclude that creating shared mental models between users and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a> is important to achieving successful dialogs. We propose that our insights on mental models and miscommunication, the <a href=https://en.wikipedia.org/wiki/Game>game</a>, and our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> provide useful tools for designing collaborative dialog systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.5/>On <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Creoles</a></strong><br><a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/e/emanuele-bugliarello/>Emanuele Bugliarello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--5><div class="card-body p-3 small">Creole languages such as <a href=https://en.wikipedia.org/wiki/Nigerian_Pidgin_English>Nigerian Pidgin English</a> and <a href=https://en.wikipedia.org/wiki/Haitian_Creole>Haitian Creole</a> are under-resourced and largely ignored in the NLP literature. Creoles typically result from the fusion of a foreign language with multiple local languages, and what grammatical and lexical features are transferred to the <a href=https://en.wikipedia.org/wiki/Creole_language>creole</a> is a complex process. While <a href=https://en.wikipedia.org/wiki/Creole_language>creoles</a> are generally stable, the prominence of some features may be much stronger with certain demographics or in some linguistic situations. This paper makes several contributions : We collect existing corpora and release models for <a href=https://en.wikipedia.org/wiki/Haitian_Creole>Haitian Creole</a>, <a href=https://en.wikipedia.org/wiki/Nigerian_Pidgin_English>Nigerian Pidgin English</a>, and <a href=https://en.wikipedia.org/wiki/Singaporean_English>Singaporean Colloquial English</a>. We evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on intrinsic and extrinsic tasks. Motivated by the above literature, we compare standard language models with distributionally robust ones and find that, somewhat surprisingly, the standard language models are superior to the distributionally robust ones. We investigate whether this is an effect of <a href=https://en.wikipedia.org/wiki/Parameterized_complexity>over-parameterization</a> or relative distributional stability, and find that the difference persists in the absence of <a href=https://en.wikipedia.org/wiki/Parameterized_complexity>over-parameterization</a>, and that drift is limited, confirming the relative stability of <a href=https://en.wikipedia.org/wiki/Creole_language>creole languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.11/>Enriching Language Models with Visually-grounded Word Vectors and the Lancaster Sensorimotor Norms<span class=acl-fixed-case>L</span>ancaster Sensorimotor Norms</a></strong><br><a href=/people/c/casey-kennington/>Casey Kennington</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--11><div class="card-body p-3 small">Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete, denoting physical entities and embodied states. To enrich language models with some of this missing experience, we leverage two sources of information : (1) the Lancaster Sensorimotor norms, which provide ratings (means and standard deviations) for over 40,000 English words along several dimensions of embodiment, and which capture the extent to which something is experienced across 11 different sensory modalities, and (2) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary. We pre-trained the ELECTRA model and fine-tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark. We find that enriching <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> with the Lancaster norms and image vectors improves results in both tasks, with some implications for robust <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that capture holistic linguistic meaning in a language learning context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.15/>Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction</a></strong><br><a href=/people/s/shauli-ravfogel/>Shauli Ravfogel</a>
|
<a href=/people/g/grusha-prasad/>Grusha Prasad</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--15><div class="card-body p-3 small">When language models process syntactically complex sentences, do they use their representations of syntax in a manner that is consistent with the grammar of the language? We propose AlterRep, an intervention-based method to address this question. For any linguistic feature of a given sentence, AlterRep generates counterfactual representations by altering how the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>feature</a> is encoded, while leaving in- tact all other aspects of the original <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. By measuring the change in a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s word prediction behavior when these counterfactual representations are substituted for the original ones, we can draw conclusions about the causal effect of the linguistic feature in question on the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s behavior. We apply this method to study how BERT models of different sizes process relative clauses (RCs). We find that BERT variants use RC boundary information during <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a> in a manner that is consistent with the rules of English grammar ; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.16/>Who’s on First? : Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</a></strong><br><a href=/people/d/david-demeter/>David Demeter</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--16><div class="card-body p-3 small">The capabilities of today&#8217;s <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> are typically evaluated using large datasets of curated questions and answers. While these are critical benchmarks of progress, they also suffer from weakness due to <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>artificial distributions</a> and <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>incomplete knowledge</a>. Artifacts arising from artificial distributions can overstate <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance, while incomplete knowledge limits fine-grained analysis. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT). SPLATs are corpora of language encodings of activity in some closed domain (we study traces from chess and baseball games in this work). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. We show that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> of three different architectures can answer questions about <a href=https://en.wikipedia.org/wiki/State_(polity)>world states</a> using only verb-like encodings of activity. Our approach is extensible to new <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and additional question-answering tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.17/>Data Augmentation of Incorporating Real Error Patterns and Linguistic Knowledge for Grammatical Error Correction</a></strong><br><a href=/people/x/xia-li/>Xia Li</a>
|
<a href=/people/j/junyi-he/>Junyi He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--17><div class="card-body p-3 small">Data augmentation aims at expanding training data with clean text using noising schemes to improve the performance of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction (GEC)</a>. In practice, there are a great number of real error patterns in the manually annotated training data. We argue that these real error patterns can be introduced into clean text to effectively generate more real and high quality synthetic data, which is not fully explored by previous studies. Moreover, we also find that linguistic knowledge can be incorporated into <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for generating more representative and more diverse <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a>. In this paper, we propose a novel data augmentation method that fully considers the real error patterns and the linguistic knowledge for the GEC task. We conduct extensive experiments on public data sets and the experimental results show that our method outperforms several strong baselines with far less external unlabeled clean text data, highlighting its extraordinary effectiveness in the GEC task that lacks large-scale labeled training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.19/>A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/l/lukas-nielsen/>Lukas Nielsen</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--19><div class="card-body p-3 small">Negation is one of the most fundamental concepts in human cognition and language, and several natural language inference (NLI) probes have been designed to investigate pretrained language models&#8217; ability to detect and reason with <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>. However, the existing probing datasets are limited to English only, and do not enable controlled probing of performance in the absence or presence of <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. In response, we present a multilingual (English, Bulgarian, German, French and Chinese) benchmark collection of NLI examples that are grammatical and correctly labeled, as a result of manual inspection and reformulation. We use the benchmark to probe the negation-awareness of multilingual language models and find that models that correctly predict examples with negation cues, often fail to correctly predict their counter-examples without negation cues, even when the cues are irrelevant for semantic inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.23/>A Coarse-to-Fine Labeling Framework for Joint Word Segmentation, POS Tagging, and Constituent Parsing<span class=acl-fixed-case>POS</span> Tagging, and Constituent Parsing</a></strong><br><a href=/people/y/yang-hou/>Yang Hou</a>
|
<a href=/people/h/houquan-zhou/>Houquan Zhou</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/z/zhefeng-wang/>Zhefeng Wang</a>
|
<a href=/people/b/baoxing-huai/>Baoxing Huai</a>
|
<a href=/people/n/nicholas-jing-yuan/>Nicholas Jing Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--23><div class="card-body p-3 small">The most straightforward approach to joint word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing is converting a word-level tree into a char-level tree, which, however, leads to two severe challenges. First, a larger label set (e.g., 600) and longer inputs both increase computational costs. Second, it is difficult to rule out illegal trees containing conflicting production rules, which is important for reliable model evaluation. If a POS tag (like VV) is above a phrase tag (like VP) in the output tree, it becomes quite complex to decide word boundaries. To deal with both challenges, this work proposes a two-stage coarse-to-fine labeling framework for joint WS-POS-PAR. In the coarse labeling stage, the joint model outputs a bracketed tree, in which each node corresponds to one of four labels (i.e., phrase, subphrase, word, subword). The <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> is guaranteed to be legal via constrained CKY decoding. In the fine labeling stage, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> expands each coarse label into a final label (such as VP, VP *, VV, VV *). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline approach</a> on both settings of <a href=https://en.wikipedia.org/wiki/W/o>w/o</a> and <a href=https://en.wikipedia.org/wiki/BERT>w/ BERT</a>, and achieves new state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.24/>Understanding the Extent to which Content Quality Metrics Measure the Information Quality of Summaries</a></strong><br><a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--24><div class="card-body p-3 small">Reference-based metrics such as <a href=https://en.wikipedia.org/wiki/ROUGE_(metric)>ROUGE</a> or BERTScore evaluate the content quality of a summary by comparing the summary to a reference. Ideally, this comparison should measure the summary&#8217;s information quality by calculating how much information the summaries have in common. In this work, we analyze the token alignments used by ROUGE and BERTScore to compare summaries and argue that their scores largely can not be interpreted as measuring information overlap. Rather, they are better estimates of the extent to which the summaries discuss the same topics. Further, we provide evidence that this result holds true for many other summarization evaluation metrics. The consequence of this result is that the most frequently used summarization evaluation metrics do not align with the community&#8217;s research goal, to generate summaries with high-quality information. However, we conclude by demonstrating that a recently proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, QAEval, which scores summaries using <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a>, appears to better capture <a href=https://en.wikipedia.org/wiki/Information_quality>information quality</a> than current evaluations, highlighting a direction for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.25/>Summary-Source Proposition-level Alignment : Task, Datasets and Supervised Baseline</a></strong><br><a href=/people/o/ori-ernst/>Ori Ernst</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/michael-lepioshkin/>Michael Lepioshkin</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--25><div class="card-body p-3 small">Aligning sentences in a reference summary with their counterparts in source documents was shown as a useful auxiliary summarization task, notably for generating training data for <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salience detection</a>. Despite its assessed utility, the alignment step was mostly approached with heuristic unsupervised methods, typically ROUGE-based, and was never independently optimized or evaluated. In this paper, we propose establishing summary-source alignment as an explicit task, while introducing two major novelties : (1) applying it at the more accurate proposition span level, and (2) approaching it as a supervised classification task. To that end, we created a novel <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training dataset</a> for proposition-level alignment, derived automatically from available summarization evaluation data. In addition, we crowdsourced dev and test datasets, enabling model development and proper evaluation. Utilizing these data, we present a supervised proposition alignment baseline model, showing improved alignment-quality over the unsupervised approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.27/>Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning</a></strong><br><a href=/people/c/christos-theodoropoulos/>Christos Theodoropoulos</a>
|
<a href=/people/j/james-henderson/>James Henderson</a>
|
<a href=/people/a/andrei-catalin-coman/>Andrei Catalin Coman</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--27><div class="card-body p-3 small">Though language model text embeddings have revolutionized NLP research, their ability to capture high-level semantic information, such as relations between entities in text, is limited. In this paper, we propose a novel contrastive learning framework that trains <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> to encode the relations in a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. Given a sentence (unstructured text) and its <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we use contrastive learning to impose relation-related structure on the token level representations of the sentence obtained with a CharacterBERT (El Boukkouri et al., 2020) model. The resulting relation-aware sentence embeddings achieve state-of-the-art results on the relation extraction task using only a simple KNN classifier, thereby demonstrating the success of the proposed method. Additional visualization by a tSNE analysis shows the effectiveness of the learned <a href=https://en.wikipedia.org/wiki/Representation_space>representation space</a> compared to baselines. Furthermore, we show that we can learn a different space for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, again using a contrastive learning objective, and demonstrate how to successfully combine both representation spaces in an entity-relation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.29/>Pragmatic competence of pre-trained language models through the lens of discourse connectives</a></strong><br><a href=/people/l/lalchand-pandia/>Lalchand Pandia</a>
|
<a href=/people/y/yan-cong/>Yan Cong</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--29><div class="card-body p-3 small">As pre-trained language models (LMs) continue to dominate <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, it is increasingly important that we understand the depth of language capabilities in these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we target pre-trained LMs&#8217; competence in <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a>, with a focus on <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>. We focus on testing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>&#8217; ability to use pragmatic cues to predict discourse connectives, <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>&#8217; ability to understand implicatures relating to connectives, and the extent to which <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> show humanlike preferences regarding temporal dynamics of connectives. We find that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the findings suggest that at present, dominant pre-training paradigms do not result in substantial pragmatic competence in our models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.32/>Scaffolded input promotes atomic organization in the recurrent neural network language model</a></strong><br><a href=/people/p/philip-a-huebner/>Philip A. Huebner</a>
|
<a href=/people/j/jon-a-willits/>Jon A. Willits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--32><div class="card-body p-3 small">The recurrent neural network (RNN) language model is a powerful tool for learning arbitrary sequential dependencies in language data. Despite its enormous success in representing <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical sequences</a>, little is known about the quality of the <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical representations</a> that it acquires. In this work, we conjecture that it is straightforward to extract <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical representations</a> (i.e. static word embeddings) from an RNN, but that the amount of semantic information that is encoded is limited when lexical items in the training data provide redundant semantic information. We conceptualize this limitation of the RNN as a failure to learn atomic internal states-states which capture information relevant to single word types without being influenced by redundant information provided by words with which they co-occur. Using a corpus of artificial language, we verify that redundancy in the training data yields non-atomic internal states, and propose a novel method for inducing atomic internal states. We show that 1) our method successfully induces atomic internal organization in controlled experiments, and 2) under more realistic conditions in which the training consists of child-directed language, application of our method improves the performance of lexical representations on a downstream semantic categorization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.35/>Relation-aware Bidirectional Path Reasoning for Commonsense Question Answering</a></strong><br><a href=/people/j/junxing-wang/>Junxing Wang</a>
|
<a href=/people/x/xinyi-li/>Xinyi Li</a>
|
<a href=/people/z/zhen-tan/>Zhen Tan</a>
|
<a href=/people/x/xiang-zhao/>Xiang Zhao</a>
|
<a href=/people/w/weidong-xiao/>Weidong Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--35><div class="card-body p-3 small">Commonsense Question Answering is an important natural language processing (NLP) task that aims to predict the correct answer to a question through <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. Previous studies utilize pre-trained models on large-scale corpora such as BERT, or perform <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> on <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. However, these methods do not explicitly model the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> that connect entities, which are informational and can be used to enhance <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. To address this issue, we propose a relation-aware reasoning method. Our method uses a relation-aware graph neural network to capture the rich contextual information from both entities and relations. Compared with methods that use fixed relation embeddings from pre-trained models, our model dynamically updates relations with contextual information from a multi-source subgraph, built from multiple external knowledge sources. The enhanced representations of relations are then fed to a bidirectional reasoning module. A bidirectional attention mechanism is applied between the question sequence and the paths that connect entities, which provides us with transparent interpretability. Experimental results on the CommonsenseQA dataset illustrate that our method results in significant improvements over the baselines while also providing clear reasoning paths.<i>relations</i> that connect entities, which are informational and can be used to enhance reasoning. To address this issue, we propose a relation-aware reasoning method. Our method uses a relation-aware graph neural network to capture the rich contextual information from both entities and relations. Compared with methods that use fixed relation embeddings from pre-trained models, our model dynamically updates relations with contextual information from a multi-source subgraph, built from multiple external knowledge sources. The enhanced representations of relations are then fed to a bidirectional reasoning module. A bidirectional attention mechanism is applied between the question sequence and the paths that connect entities, which provides us with transparent interpretability. Experimental results on the CommonsenseQA dataset illustrate that our method results in significant improvements over the baselines while also providing clear reasoning paths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.38/>Commonsense Knowledge in <a href=https://en.wikipedia.org/wiki/Word_association>Word Associations</a> and ConceptNet<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--38><div class="card-body p-3 small">Humans use countless basic, shared facts about the world to efficiently navigate in their environment. This <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> is rarely communicated explicitly, however, understanding how <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> is represented in different paradigms is important for (a) a deeper understanding of human cognition and (b) augmenting automatic reasoning systems. This paper presents an in-depth comparison of two large-scale resources of general knowledge : <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a>, an engineered relational database, and SWOW, a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> derived from crowd-sourced word associations. We examine the structure, overlap and differences between the two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, as well as the extent of situational commonsense knowledge present in the two <a href=https://en.wikipedia.org/wiki/Factors_of_production>resources</a>. We finally show empirically that both resources improve downstream task performance on commonsense reasoning benchmarks over text-only baselines, suggesting that large-scale word association data, which have been obtained for several languages through crowd-sourcing, can be a valuable complement to curated knowledge graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.39/>Cross-document Event Identity via Dense Annotation</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/k/kimihiro-hasegawa/>Kimihiro Hasegawa</a>
|
<a href=/people/l/linwei-li/>Linwei Li</a>
|
<a href=/people/y/yukari-yamakawa/>Yukari Yamakawa</a>
|
<a href=/people/s/shikun-zhang/>Shikun Zhang</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--39><div class="card-body p-3 small">In this paper, we study the identity of textual events from different documents. While the complex nature of event identity is previously studied (Hovy et al., 2013), the case of events across documents is unclear. Prior work on cross-document event coreference has two main drawbacks. First, they restrict the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to a limited set of event types. Second, they insufficiently tackle the concept of event identity. Such annotation setup reduces the pool of event mentions and prevents one from considering the possibility of quasi-identity relations. We propose a dense annotation approach for cross-document event coreference, comprising a rich source of event mentions and a dense annotation effort between related document pairs. To this end, we design a new annotation workflow with careful quality control and an easy-to-use annotation interface. In addition to the links, we further collect overlapping event contexts, including time, location, and participants, to shed some light on the relation between <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>identity decisions</a> and context. We present an open-access dataset for cross-document event coreference, CDEC-WN, collected from English Wikinews and open-source our annotation toolkit to encourage further research on cross-document tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.41" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.41/>Negation-Instance Based Evaluation of End-to-End Negation Resolution</a></strong><br><a href=/people/e/elizaveta-sineva/>Elizaveta Sineva</a>
|
<a href=/people/s/stefan-grunewald/>Stefan Grünewald</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--41><div class="card-body p-3 small">In this paper, we revisit the task of negation resolution, which includes the subtasks of cue detection (e.g. not, never) and scope resolution. In the context of previous shared tasks, a variety of evaluation metrics have been proposed. Subsequent works usually use different subsets of these, including variations and custom implementations, rendering meaningful comparisons between systems difficult. Examining the problem both from a linguistic perspective and from a downstream viewpoint, we here argue for a negation-instance based approach to evaluating negation resolution. Our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> correspond to expectations over per-instance scores and hence are intuitively interpretable. To render research comparable and to foster future work, we provide results for a set of current state-of-the-art systems for negation resolution on three English corpora, and make our implementation of the evaluation scripts publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.42/>Controlling Prosody in End-to-End TTS : A Case Study on Contrastive Focus Generation<span class=acl-fixed-case>TTS</span>: A Case Study on Contrastive Focus Generation</a></strong><br><a href=/people/s/siddique-latif/>Siddique Latif</a>
|
<a href=/people/i/inyoung-kim/>Inyoung Kim</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--42><div class="card-body p-3 small">While End-2-End Text-to-Speech (TTS) has made significant progresses over the past few years, these systems still lack intuitive user controls over <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a>. For instance, generating <a href=https://en.wikipedia.org/wiki/Speech>speech</a> with fine-grained prosody control (prosodic prominence, contextually appropriate emotions) is still an open challenge. In this paper, we investigate whether we can control <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> directly from the input text, in order to code information related to contrastive focus which emphasizes a specific word that is contrary to the presuppositions of the interlocutor. We build and share a specific <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this purpose and show that it allows to train a TTS system were this fine-grained prosodic feature can be correctly conveyed using control tokens. Our evaluation compares synthetic and natural utterances and shows that prosodic patterns of contrastive focus (variations of Fo, Intensity and Duration) can be learnt accurately. Such a milestone is important to allow, for example, <a href=https://en.wikipedia.org/wiki/Smart_speaker>smart speakers</a> to be programmatically controlled in terms of output prosody.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.conll-1.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.43/>A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/h/hoyun-song/>Hoyun Song</a>
|
<a href=/people/s/soo-hyun-ryu/>Soo Hyun Ryu</a>
|
<a href=/people/h/huije-lee/>Huije Lee</a>
|
<a href=/people/j/jong-c-park/>Jong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--43><div class="card-body p-3 small">As users in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> suffer from severe side effects of <a href=https://en.wikipedia.org/wiki/Abuse>abusive language</a>, many researchers attempted to detect abusive texts from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, presenting several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for such detection. However, none of them contain both comprehensive labels and contextual information, which are essential for thoroughly detecting all kinds of abusiveness from texts, since datasets with such fine-grained features demand a significant amount of <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, leading to much increased <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. In this paper, we propose a Comprehensive Abusiveness Detection Dataset (CADD), collected from the English Reddit posts, with multifaceted labels and contexts. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is annotated hierarchically for an efficient <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> on a large-scale. We also empirically explore the characteristics of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and provide a detailed analysis for novel insights. The results of our experiments with strong pre-trained natural language understanding models on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> gives rise to meaningful performance, assuring its practicality for abusive language detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.conll-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--conll-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.conll-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.conll-1.51/>Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models</a></strong><br><a href=/people/j/juliette-millet/>Juliette Millet</a>
|
<a href=/people/i/ioana-chitoran/>Ioana Chitoran</a>
|
<a href=/people/e/ewan-dunbar/>Ewan Dunbar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--conll-1--51><div class="card-body p-3 small">Our <a href=https://en.wikipedia.org/wiki/First_language>native language</a> influences the way we perceive <a href=https://en.wikipedia.org/wiki/Phone_(phonetics)>speech sounds</a>, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the <a href=https://en.wikipedia.org/wiki/First_language>native language</a> on <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a> : the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalise this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants&#8217; speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of <a href=https://en.wikipedia.org/wiki/First_language>native language</a> on <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a>, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a>.</div></div></div><hr><div id=2021crac-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.crac-1/>Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.crac-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.crac-1.0/>Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.crac-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--crac-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.crac-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.crac-1.4/>DramaCoref : A Hybrid Coreference Resolution System for German Theater Plays<span class=acl-fixed-case>D</span>rama<span class=acl-fixed-case>C</span>oref: A Hybrid Coreference Resolution System for <span class=acl-fixed-case>G</span>erman Theater Plays</a></strong><br><a href=/people/j/janis-pagel/>Janis Pagel</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--crac-1--4><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for resolving <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> on <a href=https://en.wikipedia.org/wiki/Play_(theatre)>theater plays</a>, DramaCoref. The <a href=https://en.wikipedia.org/wiki/System>system</a> uses <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network techniques</a> to provide a list of potential mentions. These mentions are assigned to common entities using generic and domain-specific rules. We find that DramaCoref works well on the <a href=https://en.wikipedia.org/wiki/Play_(theatre)>theater plays</a> when compared to corpora from other domains and profits from the inclusion of information specific to theater plays. On the best-performing setup, it achieves a CoNLL score of 32 % when using automatically detected mentions and 55 % when using gold mentions. Single rules achieve high precision scores ; however, <a href=https://en.wikipedia.org/wiki/Game_theory>rules</a> designed on other domains are often not applicable or yield unsatisfactory results. Error analysis shows that the mention detection is the main weakness of the <a href=https://en.wikipedia.org/wiki/System>system</a>, providing directions for future improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.crac-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--crac-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.crac-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.crac-1.6/>Lazy Low-Resource Coreference Resolution : a Study on Leveraging Black-Box Translation Tools</a></strong><br><a href=/people/s/semere-kiros-bitew/>Semere Kiros Bitew</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--crac-1--6><div class="card-body p-3 small">Large annotated corpora for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> are available for few languages. For <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, however, strong black-box systems exist for many languages. We empirically explore the appealing idea of leveraging such translation tools for bootstrapping <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in languages with limited resources. Two scenarios are analyzed, in which a large coreference corpus in a high-resource language is used for coreference predictions in a smaller language, i.e., by <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translating</a> either the training corpus or the test data. In our empirical evaluation of <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> using the two scenarios on several medium-resource languages, we find no improvement over monolingual baseline models. Our analysis of the various sources of error inherent to the studied scenarios, reveals that in fact the quality of contemporary machine translation tools is the main limiting factor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.crac-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--crac-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.crac-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.crac-1.8.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.crac-1.8/>CoreLM : Coreference-aware Language Model Fine-Tuning<span class=acl-fixed-case>C</span>ore<span class=acl-fixed-case>LM</span>: Coreference-aware Language Model Fine-Tuning</a></strong><br><a href=/people/n/nikolaos-stylianou/>Nikolaos Stylianou</a>
|
<a href=/people/i/ioannis-vlahavas/>Ioannis Vlahavas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--crac-1--8><div class="card-body p-3 small">Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> very effective across many NLP task, leading to significant advancements in the field. However, <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> come with a big <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, which results in a better <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> for a fraction of the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower <a href=https://en.wikipedia.org/wiki/Perplexity>Perplexity</a> in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models&#8217; performance in terms of Accuracy in LAMBADA and Children&#8217;s Book Test, with and without the use of model-created coreference annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.crac-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--crac-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.crac-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.crac-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.crac-1.12/>On Generalization in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/s/shubham-toshniwal/>Shubham Toshniwal</a>
|
<a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/s/sam-wiseman/>Sam Wiseman</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--crac-1--12><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is defined independently of dataset domain, most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for performing <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> do not transfer well to unseen domains. We consolidate a set of 8 coreference resolution datasets targeting different domains to evaluate the off-the-shelf performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We then mix three datasets for training ; even though their domain, annotation guidelines, and <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> to account for annotation differences and <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a> to balance the data quantities. We find that in a zero-shot setting, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models. This work contributes a new <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for robust coreference resolution and multiple new state-of-the-art results.</div></div></div><hr><div id=2021disrpt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.disrpt-1/>Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.disrpt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.disrpt-1.0/>Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021)</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/y/yang-janet-liu/>Yang Janet Liu</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/s/sonia-badene/>Sonia Badene</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.disrpt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--disrpt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.disrpt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.disrpt-1.1/>The DISRPT 2021 Shared Task on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification<span class=acl-fixed-case>DISRPT</span> 2021 Shared Task on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/y/yang-janet-liu/>Yang Janet Liu</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/s/sonia-badene/>Sonia Badene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--disrpt-1--1><div class="card-body p-3 small">In 2021, we organized the second iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms : the DISRPT Shared Task (Discourse Relation Parsing and Treebanking). Adding to the 2019 tasks on Elementary Discourse Unit Segmentation and Connective Detection, this iteration of the Shared Task included for the first time a track on discourse relation classification across three formalisms : RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.disrpt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--disrpt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.disrpt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.disrpt-1.3.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.disrpt-1.3/>Multi-lingual Discourse Segmentation and Connective Identification : MELODI at Disrpt2021<span class=acl-fixed-case>MELODI</span> at Disrpt2021</a></strong><br><a href=/people/m/morteza-kamaladdini-ezzabady/>Morteza Kamaladdini Ezzabady</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--disrpt-1--3><div class="card-body p-3 small">We present an approach for discourse segmentation and discourse connective identification, both at the sentence and document level, within the Disrpt 2021 shared task, a multi-lingual and multi-formalism evaluation campaign. Building on the most successful architecture from the 2019 similar shared task, we leverage datasets in the same or similar languages to augment training data and improve on the best systems from the previous campaign on 3 out of 4 subtasks, with a mean improvement on all 16 datasets of 0.85 %. Within the Disrpt 21 campaign the <a href=https://en.wikipedia.org/wiki/Role-playing_game_system>system</a> ranks 3rd overall, very close to the 2nd system, but with a significant gap with respect to the best <a href=https://en.wikipedia.org/wiki/Role-playing_game_system>system</a>, which uses a rich set of additional features. The system is nonetheless the best on languages that benefited from crosslingual training on sentence internal segmentation (German and Spanish).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.disrpt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--disrpt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.disrpt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.disrpt-1.4/>Delexicalised Multilingual Discourse Segmentation for DISRPT 2021 and Tense, Mood, Voice and Modality Tagging for 11 Languages<span class=acl-fixed-case>DISRPT</span> 2021 and Tense, Mood, Voice and Modality Tagging for 11 Languages</a></strong><br><a href=/people/t/tillmann-donicke/>Tillmann Dönicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--disrpt-1--4><div class="card-body p-3 small">This paper describes our participating system for the Shared Task on Discourse Segmentation and Connective Identification across Formalisms and Languages. Key features of the presented approach are the formulation as a clause-level classification task, a language-independent feature inventory based on Universal Dependencies grammar, and composite-verb-form analysis. The achieved F1 is 92 % for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> and lower for other languages. The paper also presents a clause-level tagger for <a href=https://en.wikipedia.org/wiki/Grammatical_tense>grammatical tense</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_mood>mood</a>, voice and <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a> in 11 languages.</div></div></div><hr><div id=2021eancs-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eancs-1/>The First Workshop on Evaluations and Assessments of Neural Conversation Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eancs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eancs-1.0/>The First Workshop on Evaluations and Assessments of Neural Conversation Systems</a></strong><br><a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/b/bo-dai/>Bo Dai</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/y/y-lan-boureau/>Y-Lan Boureau</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/a/alborz-geramifard/>Alborz Geramifard</a>
|
<a href=/people/a/aman-ahuja/>Aman Ahuja</a>
|
<a href=/people/h/haoming-jiang/>Haoming Jiang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eancs-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eancs-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eancs-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eancs-1.2/>GCDF1 : A Goal- and Context- Driven F-Score for Evaluating User Models<span class=acl-fixed-case>GCDF</span>1: A Goal- and Context- Driven <span class=acl-fixed-case>F</span>-Score for Evaluating User Models</a></strong><br><a href=/people/a/alexandru-coca/>Alexandru Coca</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eancs-1--2><div class="card-body p-3 small">The evaluation of dialogue systems in interaction with simulated users has been proposed to improve turn-level, corpus-based metrics which can only evaluate test cases encountered in a corpus and can not measure <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s ability to sustain multi-turn interactions. Recently, little emphasis was put on automatically assessing the quality of the <a href=https://en.wikipedia.org/wiki/User_model>user model</a> itself, so unless correlations with human studies are measured, the reliability of <a href=https://en.wikipedia.org/wiki/User_model>user model based evaluation</a> is unknown. We propose GCDF1, a simple but effective measure of the quality of semantic-level conversations between a goal-driven user agent and a <a href=https://en.wikipedia.org/wiki/System_agent>system agent</a>. In contrast with previous approaches we measure the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> at dialogue level and consider user and system behaviours to improve recall and precision estimation. We facilitate scores interpretation by providing a rich hierarchical structure with information about conversational patterns present in the test data and tools to efficiently query the conversations generated. We apply our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to assess the performance and weaknesses of a Convlab2 user model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eancs-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eancs-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eancs-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eancs-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eancs-1.3/>A Comprehensive Assessment of Dialog Evaluation Metrics</a></strong><br><a href=/people/y/yi-ting-yeh/>Yi-Ting Yeh</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a>
|
<a href=/people/s/shikib-mehri/>Shikib Mehri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eancs-1--3><div class="card-body p-3 small">Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Furthermore, the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are assessed in different settings, to better qualify their respective strengths and weaknesses. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess <a href=https://en.wikipedia.org/wiki/Performance_metric>evaluation metrics</a> and indicates promising directions for future work.</div></div></div><hr><div id=2021econlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.econlp-1/>Proceedings of the Third Workshop on Economics and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.0/>Proceedings of the Third Workshop on Economics and Natural Language Processing</a></strong><br><a href=/people/u/udo-hahn/>Udo Hahn</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--econlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.econlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.1/>A Fine-Grained Annotated Corpus for Target-Based Opinion Analysis of Economic and Financial Narratives</a></strong><br><a href=/people/j/jiahui-hu/>Jiahui Hu</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--econlp-1--1><div class="card-body p-3 small">In this paper about aspect-based sentiment analysis (ABSA), we present the first version of a fine-grained annotated corpus for target-based opinion analysis (TBOA) to analyze <a href=https://en.wikipedia.org/wiki/Economy>economic activities</a> or <a href=https://en.wikipedia.org/wiki/Financial_market>financial markets</a>. We have annotated, at an intra-sentential level, a corpus of sentences extracted from documents representative of financial analysts&#8217; most-read materials by considering how financial actors communicate about the evolution of event trends and analyze related publications (news, official communications, etc.). Since we focus on identifying the expressions of opinions related to the <a href=https://en.wikipedia.org/wiki/Economy>economy</a> and financial markets, we annotated the sentences that contain at least one subjective expression about a domain-specific term. Candidate sentences for annotations were randomly chosen from texts of specialized press and professional information channels over a period ranging from 1986 to 2021. Our annotation scheme relies on various linguistic markers like domain-specific vocabulary, <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a>, and rhetorical relations to explicitly describe the author&#8217;s subjective stance. We investigated and evaluated the recourse to automatic pre-annotation with existing <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing technologies</a> to alleviate the annotation workload. Our aim is to propose a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> usable on the one hand as training material for the automatic detection of the opinions expressed on an extensive range of domain-specific aspects and on the other hand as a gold standard for evaluation TBOA. In this paper, we present our pre-annotation models and evaluations of their performance, introduce our annotation scheme and report on the main characteristics of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--econlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.econlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.econlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.2/>EDGAR-CORPUS : Billions of Tokens Make The World Go Round<span class=acl-fixed-case>EDGAR</span>-<span class=acl-fixed-case>CORPUS</span>: Billions of Tokens Make The World Go Round</a></strong><br><a href=/people/l/lefteris-loukas/>Lefteris Loukas</a>
|
<a href=/people/m/manos-fergadiotis/>Manos Fergadiotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--econlp-1--2><div class="card-body p-3 small">We release EDGAR-CORPUS, a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprising annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP corpus available to date. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use JSON format. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC embeddings for the financial domain. We employ these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in a battery of financial NLP tasks and showcase their superiority over generic GloVe embeddings and other existing financial word embeddings. We also open-source EDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--econlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.econlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.3/>The Global Banking Standards QA Dataset (GBS-QA)<span class=acl-fixed-case>QA</span> Dataset (<span class=acl-fixed-case>GBS</span>-<span class=acl-fixed-case>QA</span>)</a></strong><br><a href=/people/k/kyunghwan-sohn/>Kyunghwan Sohn</a>
|
<a href=/people/s/sunjae-kwon/>Sunjae Kwon</a>
|
<a href=/people/j/jaesik-choi/>Jaesik Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--econlp-1--3><div class="card-body p-3 small">A domain specific question answering (QA) dataset dramatically improves the machine comprehension performance. This paper presents a new Global Banking Standards QA dataset (GBS-QA) in the banking regulation domain. The GBS-QA has three values. First, it contains actual questions from market players and answers from global rule setter, the Basel Committee on Banking Supervision (BCBS) in the middle of creating and revising banking regulations. Second, financial regulation experts analyze and verify pairs of questions and answers in the annotation process. Lastly, the GBS-QA is a totally different dataset with existing datasets in <a href=https://en.wikipedia.org/wiki/Finance>finance</a> and is applicable to stimulate transfer learning research in the banking regulation domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--econlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.econlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.4/>Corporate Bankruptcy Prediction with Domain-Adapted BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/alex-gunwoo-kim/>Alex Gunwoo Kim</a>
|
<a href=/people/s/sangwon-yoon/>Sangwon Yoon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--econlp-1--4><div class="card-body p-3 small">This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on <a href=https://en.wikipedia.org/wiki/Bankruptcy_prediction>bankruptcy prediction</a> mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply <a href=https://en.wikipedia.org/wiki/Autodidacticism>self-learning</a> with confidence-based filtering to corporate disclosure data (10-K). We achieve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy rate</a> of 91.56 % and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.econlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--econlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.econlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.econlp-1.9/>To What Extent Can English-as-a-Second Language Learners Read Economic News Texts?<span class=acl-fixed-case>E</span>nglish-as-a-Second Language Learners Read Economic News Texts?</a></strong><br><a href=/people/y/yo-ehara/>Yo Ehara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--econlp-1--9><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a> in the <a href=https://en.wikipedia.org/wiki/Economy>economic field</a>, an especially important requirement is to rapidly understand news to absorb ever-changing economic situations. Given that most economic news is written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the ability to read such information without waiting for a translation is particularly valuable in <a href=https://en.wikipedia.org/wiki/Economics>economics</a> in contrast to other fields. In consideration of this issue, this research investigated the extent to which <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native English speakers</a> are able to read economic news to make decisions accordingly an issue that has been rarely addressed in previous studies. Using an existing standard <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as training data, we created a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that automatically evaluates the <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of text with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for English learners. Our assessment of the <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of an economic news corpus revealed that most news texts can be read by <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>intermediate English learners</a>. We also found that in some cases, <a href=https://en.wikipedia.org/wiki/Readability>readability</a> varies considerably depending on the knowledge of certain words specific to the <a href=https://en.wikipedia.org/wiki/Economics>economic field</a>.</div></div></div><hr><div id=2021eval4nlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eval4nlp-1/>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.0/>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></strong><br><a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.1/>Differential Evaluation : a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing</a></strong><br><a href=/people/l/lucie-gianola/>Lucie Gianola</a>
|
<a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/c/cyril-grouin/>Cyril Grouin</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--1><div class="card-body p-3 small">Most of the time, when dealing with a particular Natural Language Processing task, systems are compared on the basis of global statistics such as <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, F1-score, etc. While such scores provide a general idea of the behavior of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, they ignore a key piece of information that can be useful for assessing progress and discerning remaining challenges : the relative difficulty of test instances. To address this shortcoming, we introduce the notion of differential evaluation which effectively defines a pragmatic partition of instances into gradually more difficult bins by leveraging the predictions made by a set of systems. Comparing systems along these difficulty bins enables us to produce a finer-grained analysis of their relative merits, which we illustrate on two use-cases : a comparison of systems participating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.2/>Validating Label Consistency in NER Data Annotation<span class=acl-fixed-case>NER</span> Data Annotation</a></strong><br><a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/m/mengxia-yu/>Mengxia Yu</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/t/tianwen-jiang/>Tianwen Jiang</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--2><div class="card-body p-3 small">Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an <a href=https://en.wikipedia.org/wiki/Empirical_research>empirical method</a> to explore the relationship between label (in-)consistency and <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a> performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7 % and 5.4 % label mistakes). It validated the consistency in the corrected version of both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.4/>StoryDB : Broad Multi-language Narrative Dataset<span class=acl-fixed-case>S</span>tory<span class=acl-fixed-case>DB</span>: Broad Multi-language Narrative Dataset</a></strong><br><a href=/people/a/alexey-tikhonov/>Alexey Tikhonov</a>
|
<a href=/people/i/igor-samenko/>Igor Samenko</a>
|
<a href=/people/i/ivan-yamshchikov/>Ivan Yamshchikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--4><div class="card-body p-3 small">This paper presents StoryDB a broad multi-language dataset of <a href=https://en.wikipedia.org/wiki/Narrative>narratives</a>. StoryDB is a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of texts</a> that includes stories in 42 different languages. Every language includes 500 + stories. Some of the languages include more than 20 000 stories. Every story is indexed across languages and labeled with tags such as a genre or a topic. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> shows rich topical and language variation and can serve as a resource for the study of the role of <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> across various languages including low resource ones. We also demonstrate how the dataset could be used to benchmark three modern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.5/>SeqScore : Addressing Barriers to Reproducible Named Entity Recognition Evaluation<span class=acl-fixed-case>S</span>eq<span class=acl-fixed-case>S</span>core: Addressing Barriers to Reproducible Named Entity Recognition Evaluation</a></strong><br><a href=/people/c/chester-palen-michel/>Chester Palen-Michel</a>
|
<a href=/people/n/nolan-holley/>Nolan Holley</a>
|
<a href=/people/c/constantine-lignos/>Constantine Lignos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--5><div class="card-body p-3 small">To address a looming crisis of unreproducible evaluation for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, we propose guidelines and introduce SeqScore, a software package to improve <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility</a>. The guidelines we propose are extremely simple and center around transparency regarding how chunks are encoded and scored. We demonstrate that despite the apparent simplicity of NER evaluation, unreported differences in the scoring procedure can result in changes to scores that are both of noticeable magnitude and statistically significant. We describe SeqScore, which addresses many of the issues that cause <a href=https://en.wikipedia.org/wiki/Replication_(computing)>replication failures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.6/>Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator</a></strong><br><a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/l/luc-lamontagne/>Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--6><div class="card-body p-3 small">In this paper, we introduce a new embedding-based metric relying on trainable ranking models to evaluate the semantic accuracy of neural data-to-text generators. This <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is especially well suited to semantically and factually assess the performance of a <a href=https://en.wikipedia.org/wiki/Text_generator>text generator</a> when tables can be associated with multiple references and table values contain textual utterances. We first present how one can implement and further specialize the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> by training the underlying ranking models on a legal Data-to-Text dataset. We show how it may provide a more robust evaluation than other evaluation schemes in challenging settings using a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprising <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> between the table values and their respective references. Finally, we evaluate its generalization capabilities on a well-known dataset, WebNLG, by comparing it with human evaluation and a recently introduced <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> based on natural language inference. We then illustrate how it naturally characterizes, both quantitatively and qualitatively, omissions and <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.7/>Evaluation of Unsupervised Automatic Readability Assessors Using Rank Correlations</a></strong><br><a href=/people/y/yo-ehara/>Yo Ehara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--7><div class="card-body p-3 small">Automatic readability assessment (ARA) is the task of automatically assessing readability with little or no <a href=https://en.wikipedia.org/wiki/Supervisor>human supervision</a>. ARA is essential for many second language acquisition applications to reduce the workload of annotators, who are usually language teachers. Previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> manually searched textual features that correlated well with <a href=https://en.wikipedia.org/wiki/Readability>readability labels</a>, such as perplexity scores of large language models. This paper argues that, to evaluate an assessors&#8217; performance, rank-correlation coefficients should be used instead of Pearson&#8217;s correlation coefficient (). In the experiments, we show that its performance can be easily underestimated using Pearson&#8217;s, which is significantly affected by the <a href=https://en.wikipedia.org/wiki/Linearity>linearity</a> of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson&#8217;s among all unsupervised assessors compared.<tex-math>\\rho</tex-math>). In the experiments, we show that its performance can be easily underestimated using Pearson&#8217;s <tex-math>\\rho</tex-math>, which is significantly affected by the linearity of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson&#8217;s <tex-math>\\rho</tex-math> among all unsupervised assessors compared.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.8/>Testing Cross-Database Semantic Parsers With Canonical Utterances</a></strong><br><a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--8><div class="card-body p-3 small">The benchmark performance of cross-database semantic parsing has climbed steadily in recent years, catalyzed by the wide adoption of pre-trained language models. Yet existing work have shown that state-of-the-art cross-database semantic parsers struggle to generalize to novel user utterances, databases and query structures. To obtain transparent details on the strengths and limitation of these models, we propose a diagnostic testing approach based on controlled synthesis of canonical natural language and SQL pairs. Inspired by the CheckList, we characterize a set of essential capabilities for cross-database semantic parsing models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using the proposed approach, and identified several non-obvious weaknesses across <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/BehaviorCheckingSemPar.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eval4nlp-1.9.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.9/>Writing Style Author Embedding Evaluation</a></strong><br><a href=/people/e/enzo-terreau/>Enzo Terreau</a>
|
<a href=/people/a/antoine-gourru/>Antoine Gourru</a>
|
<a href=/people/j/julien-velcin/>Julien Velcin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--9><div class="card-body p-3 small">Learning authors representations from their textual productions is now widely used to solve multiple downstream tasks, such as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, link prediction or <a href=https://en.wikipedia.org/wiki/Recommender_system>user recommendation</a>. Author embedding methods are often built on top of either Doc2Vec (Mikolov et al. 2014) or the Transformer architecture (Devlin et al. Evaluating the quality of these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> and what they capture is a difficult task. Most articles use either <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> or <a href=https://en.wikipedia.org/wiki/Attribution_(copyright)>authorship attribution</a>, which does not clearly measure the quality of the <a href=https://en.wikipedia.org/wiki/Representation_space>representation space</a>, if it really captures what it has been built for. In this paper, we propose a novel evaluation framework of author embedding methods based on the <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. It allows to quantify if the embedding space effectively captures a set of <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic features</a>, chosen to be the best proxy of an author writing style. This approach gives less importance to the topics conveyed by the documents. It turns out that recent <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are mostly driven by the inner semantic of authors&#8217; production. They are outperformed by simple baselines, based on state-of-the-art pretrained sentence embedding models, on several linguistic axes. These baselines can grasp complex linguistic phenomena and <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a> more efficiently, paving the way for designing new style-driven author embedding models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.11/>Statistically Significant Detection of Semantic Shifts using Contextual Word Embeddings</a></strong><br><a href=/people/y/yang-liu-Helsinki/>Yang Liu</a>
|
<a href=/people/a/alan-medlar/>Alan Medlar</a>
|
<a href=/people/d/dorota-glowacka/>Dorota Glowacka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--11><div class="card-body p-3 small">Detecting lexical semantic change in smaller data sets, e.g. in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a> and <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, is challenging due to a lack of <a href=https://en.wikipedia.org/wiki/Statistical_power>statistical power</a>. This issue is exacerbated by non-contextual embedding models that produce one embedding per word and, therefore, mask the variability present in the data. In this article, we propose an approach to estimate <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a> by combining contextual word embeddings with permutation-based statistical tests. We use the false discovery rate procedure to address the large number of <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis tests</a> being conducted simultaneously. We demonstrate the performance of this approach in simulation where it achieves consistently high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> by suppressing false positives. We additionally analyze real-world data from SemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by taking sample variation into account, we can improve the robustness of individual semantic shift estimates without degrading overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.12/>Referenceless Parsing-Based Evaluation of AMR-to-English Generation<span class=acl-fixed-case>AMR</span>-to-<span class=acl-fixed-case>E</span>nglish Generation</a></strong><br><a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--12><div class="card-body p-3 small">Reference-based automatic evaluation metrics are notoriously limited for NLG due to their inability to fully capture the range of possible outputs. We examine a referenceless alternative : evaluating the adequacy of English sentences generated from Abstract Meaning Representation (AMR) graphs by parsing into AMR and comparing the parse directly to the input. We find that the errors introduced by automatic AMR parsing substantially limit the effectiveness of this approach, but a manual editing study indicates that as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> improves, parsing-based evaluation has the potential to outperform most reference-based metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.14/>IST-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task</a></strong><br><a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/n/nuno-m-guerreiro/>Nuno M. Guerreiro</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--14><div class="card-body p-3 small">We present the joint contribution of Instituto Superior Tcnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks : constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the <a href=https://en.wikipedia.org/wiki/Norm_(mathematics)>norm of value vectors</a> yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.21/>What is SemEval evaluating? A Systematic Analysis of Evaluation Campaigns in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val evaluating? A Systematic Analysis of Evaluation Campaigns in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/o/oskar-wysocki/>Oskar Wysocki</a>
|
<a href=/people/m/malina-florea/>Malina Florea</a>
|
<a href=/people/d/donal-landers/>Dónal Landers</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--21><div class="card-body p-3 small">SemEval is the primary venue in the NLP community for the proposal of new challenges and for the systematic empirical evaluation of NLP systems. This paper provides a systematic quantitative analysis of <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a> aiming to evidence the patterns of the contributions behind <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>. By understanding the distribution of task types, <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a>, <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a>, participation and <a href=https://en.wikipedia.org/wiki/Citation>citations</a> over time we aim to answer the question on what is being evaluated by <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.22/>The UMD Submission to the Explainable MT Quality Estimation Shared Task : Combining Explanation Models with Sequence Labeling<span class=acl-fixed-case>UMD</span> Submission to the Explainable <span class=acl-fixed-case>MT</span> Quality Estimation Shared Task: Combining Explanation Models with Sequence Labeling</a></strong><br><a href=/people/t/tasnim-kabir/>Tasnim Kabir</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--22><div class="card-body p-3 small">This paper describes the UMD submission to the Explainable Quality Estimation Shared Task at the EMNLP 2021 Workshop on Evaluation & Comparison of NLP Systems. We participated in the word-level and sentence-level MT Quality Estimation (QE) constrained tasks for all language pairs : Estonian-English, Romanian-English, German-Chinese, and Russian-German. Our approach combines the predictions of a word-level explainer model on top of a sentence-level QE model and a sequence labeler trained on synthetic data. These <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are based on pre-trained multilingual language models and do not require any word-level annotations for training, making them well suited to zero-shot settings. Our best-performing system improves over the best baseline across all metrics and language pairs, with an average gain of 0.1 in AUC, <a href=https://en.wikipedia.org/wiki/Average_precision>Average Precision</a>, and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> at Top-K score.</div></div></div><hr><div id=2021fever-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.fever-1/>Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.0/>Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER)</a></strong><br><a href=/people/r/rami-aly/>Rami Aly</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a>
|
<a href=/people/m/michael-schlichtkrull/>Michael Schlichtkrull</a>
|
<a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.1/>The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task<span class=acl-fixed-case>VER</span>ification Over Unstructured and Structured information (<span class=acl-fixed-case>FEVEROUS</span>) Shared Task</a></strong><br><a href=/people/r/rami-aly/>Rami Aly</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/m/michael-sejr-schlichtkrull/>Michael Sejr Schlichtkrull</a>
|
<a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--1><div class="card-body p-3 small">The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) shared task, asks participating systems to determine whether human-authored claims are Supported or Refuted based on evidence retrieved from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> (or NotEnoughInfo if the claim can not be verified). Compared to the FEVER 2018 shared task, the main challenge is the addition of structured data (tables and lists) as a source of evidence. The claims in the FEVEROUS dataset can be verified using only structured evidence, only unstructured evidence, or a mixture of both. Submissions are evaluated using the FEVEROUS score that combines label accuracy and evidence retrieval. Unlike FEVER 2018, FEVEROUS requires partial evidence to be returned for NotEnoughInfo claims, and the claims are longer and thus more complex. The shared task received 13 entries, six of which were able to beat the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>. The winning team was Bust a move !, achieving a FEVEROUS score of 27 % (+9 % compared to the baseline). In this paper we describe the shared task, present the full results and highlight commonalities and innovations among the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.4/>FaBULOUS : Fact-checking Based on Understanding of Language Over Unstructured and Structured information<span class=acl-fixed-case>F</span>a<span class=acl-fixed-case>BULOUS</span>: Fact-checking Based on Understanding of Language Over Unstructured and Structured information</a></strong><br><a href=/people/m/mostafa-bouziane/>Mostafa Bouziane</a>
|
<a href=/people/h/hugo-perrin/>Hugo Perrin</a>
|
<a href=/people/a/amine-sadeq/>Amine Sadeq</a>
|
<a href=/people/t/thanh-nguyen/>Thanh Nguyen</a>
|
<a href=/people/a/aurelien-cluzeau/>Aurélien Cluzeau</a>
|
<a href=/people/j/julien-mardas/>Julien Mardas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--4><div class="card-body p-3 small">As part of the FEVEROUS shared task, we developed a robust and finely tuned architecture to handle the joint retrieval and entailment on text data as well as structured data like tables. We proposed two <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training schemes</a> to tackle the hurdles inherent to multi-hop multi-modal datasets. The first one allows having a robust retrieval of full evidence sets, while the second one enables <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> to take full advantage of noisy evidence inputs. In addition, our work has revealed important insights and potential avenue of research for future improvement on this kind of <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In preliminary evaluation on the FEVEROUS shared task test set, our system achieves 0.271 FEVEROUS score, with 0.4258 evidence recall and 0.5607 entailment accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.5/>Team Papelo at FEVEROUS : Multi-hop Evidence Pursuit<span class=acl-fixed-case>FEVEROUS</span>: Multi-hop Evidence Pursuit</a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--5><div class="card-body p-3 small">We develop a system for the FEVEROUS fact extraction and verification task that ranks an initial set of potential evidence and then pursues missing evidence in subsequent hops by trying to generate it, with a next hop prediction module whose output is matched against page elements in a predicted article. Seeking evidence with the next hop prediction module continues to improve FEVEROUS score for up to seven hops. Label classification is trained on possibly incomplete extracted evidence chains, utilizing hints that facilitate <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical comparison</a>. The system achieves.281 FEVEROUS score and.658 label accuracy on the development set, and finishes in second place with.259 FEVEROUS score and.576 label accuracy on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.8/>Stance Detection in German News Articles<span class=acl-fixed-case>G</span>erman News Articles</a></strong><br><a href=/people/l/laura-mascarell/>Laura Mascarell</a>
|
<a href=/people/t/tatyana-ruzsics/>Tatyana Ruzsics</a>
|
<a href=/people/c/christian-schneebeli/>Christian Schneebeli</a>
|
<a href=/people/p/philippe-schlattner/>Philippe Schlattner</a>
|
<a href=/people/l/luca-campanella/>Luca Campanella</a>
|
<a href=/people/s/severin-klingler/>Severin Klingler</a>
|
<a href=/people/c/cristina-kadar/>Cristina Kadar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--8><div class="card-body p-3 small">The widespread use of the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> and the rapid dissemination of information poses the challenge of identifying the veracity of its content. Stance detection, which is the task of predicting the position of a text in regard to a specific target (e.g. claim or debate question), has been used to determine the veracity of information in tasks such as rumor classification and fake news detection. While most of the work and available datasets for stance detection address short texts snippets extracted from textual dialogues, <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, or <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> with a strong focus on the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, there is a lack of resources targeting long texts in other languages. Our contribution in this paper is twofold. First, we present a German dataset of debate questions and <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> that is manually annotated for stance and emotion detection. Second, we leverage the dataset to tackle the supervised task of classifying the stance of a news article with regards to a debate question and provide baseline models as a reference for future work on stance detection in German news articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.fever-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.9/>FANG-COVID : A New Large-Scale Benchmark Dataset for Fake News Detection in German<span class=acl-fixed-case>FANG</span>-<span class=acl-fixed-case>COVID</span>: A New Large-Scale Benchmark Dataset for Fake News Detection in <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/j/justus-mattern/>Justus Mattern</a>
|
<a href=/people/y/yu-qiao/>Yu Qiao</a>
|
<a href=/people/e/elma-kerz/>Elma Kerz</a>
|
<a href=/people/d/daniel-wiechmann/>Daniel Wiechmann</a>
|
<a href=/people/m/markus-strohmaier/>Markus Strohmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--9><div class="card-body p-3 small">As the world continues to fight the COVID-19 pandemic, it is simultaneously fighting an &#8216;infodemic&#8217; a flood of <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation</a> and spread of <a href=https://en.wikipedia.org/wiki/Conspiracy_theory>conspiracy theories</a> leading to health threats and the division of society. To combat this infodemic, there is an urgent need for <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark datasets</a> that can help researchers develop and evaluate <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> geared towards automatic detection of disinformation. While there are increasing efforts to create adequate, open-source benchmark datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, comparable resources are virtually unavailable for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, leaving research for the <a href=https://en.wikipedia.org/wiki/German_language>German language</a> lagging significantly behind. In this paper, we introduce the new benchmark dataset FANG-COVID consisting of 28,056 real and 13,186 fake German news articles related to the COVID-19 pandemic as well as data on their propagation on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Furthermore, we propose an explainable textual- and social context-based model for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a>, compare its performance to black-box models and perform feature ablation to assess the relative importance of human-interpretable features in distinguishing <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> from authentic news.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.11/>Automatic Fact-Checking with Document-level Annotations using BERT and Multiple Instance Learning<span class=acl-fixed-case>BERT</span> and Multiple Instance Learning</a></strong><br><a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/j/joonsuk-park/>Joonsuk Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--11><div class="card-body p-3 small">Automatic fact-checking is crucial for recognizing misinformation spreading on the internet. Most existing fact-checkers break down the process into several subtasks, one of which determines candidate evidence sentences that can potentially support or refute the claim to be verified ; typically, evidence sentences with gold-standard labels are needed for this. In a more realistic setting, however, such <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level annotations</a> are not available. In this paper, we tackle the natural language inference (NLI) subtaskgiven a document and a (sentence) claim, determine whether the document supports or refutes the claimonly using document-level annotations. Using fine-tuned BERT and <a href=https://en.wikipedia.org/wiki/Multiple_instance_learning>multiple instance learning</a>, we achieve 81.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, significantly outperforming the existing results on the WikiFactCheck-English dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.12/>Neural Re-rankers for Evidence Retrieval in the FEVEROUS Task<span class=acl-fixed-case>FEVEROUS</span> Task</a></strong><br><a href=/people/m/mohammed-saeed/>Mohammed Saeed</a>
|
<a href=/people/g/giulio-alfarano/>Giulio Alfarano</a>
|
<a href=/people/k/khai-nguyen/>Khai Nguyen</a>
|
<a href=/people/d/duc-pham/>Duc Pham</a>
|
<a href=/people/r/raphael-troncy/>Raphael Troncy</a>
|
<a href=/people/p/paolo-papotti/>Paolo Papotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--12><div class="card-body p-3 small">Computational fact-checking has gained a lot of traction in the machine learning and natural language processing communities. A plethora of solutions have been developed, but methods which leverage both structured and unstructured information to detect <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> are of particular relevance. In this paper, we tackle the FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) challenge which consists of an open source baseline system together with a benchmark dataset containing 87,026 verified claims. We extend this baseline model by improving the evidence retrieval module yielding the best evidence F1 score among the competitors in the challenge leaderboard while obtaining an overall FEVEROUS score of 0.20 (5th best ranked system).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.fever-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--fever-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.fever-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.fever-1.13/>A Fact Checking and Verification System for FEVEROUS Using a Zero-Shot Learning Approach<span class=acl-fixed-case>FEVEROUS</span> Using a Zero-Shot Learning Approach</a></strong><br><a href=/people/o/orkun-temiz/>Orkun Temiz</a>
|
<a href=/people/o/ozgun-ozan-kilic/>Özgün Ozan Kılıç</a>
|
<a href=/people/a/arif-ozan-kizildag/>Arif Ozan Kızıldağ</a>
|
<a href=/people/t/tugba-taskaya-temizel/>Tuğba Taşkaya Temizel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--fever-1--13><div class="card-body p-3 small">In this paper, we propose a novel fact checking and verification system to check claims against <a href=https://en.wikipedia.org/wiki/Content_(media)>Wikipedia content</a>. Our system retrieves relevant Wikipedia pages using <a href=https://en.wikipedia.org/wiki/Anserini>Anserini</a>, uses BERT-large-cased question answering model to select correct evidence, and verifies claims using XLNET natural language inference model by comparing it with the evidence. Table cell evidence is obtained through looking for entity-matching cell values and TAPAS table question answering model. The <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> utilizes zero-shot capabilities of existing <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> and all the <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> used in the <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> requires no additional training. Our <a href=https://en.wikipedia.org/wiki/System>system</a> got a <a href=https://en.wikipedia.org/wiki/Extreme_value>FEVEROUS score</a> of 0.06 and a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>label accuracy</a> of 0.39 in <a href=https://en.wikipedia.org/wiki/Extreme_value>FEVEROUS challenge</a>.</div></div></div><hr><div id=2021insights-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.insights-1/>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.0/>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.1/>Corrected CBOW Performs as well as <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a><span class=acl-fixed-case>CBOW</span> Performs as well as Skip-gram</a></strong><br><a href=/people/o/ozan-irsoy/>Ozan İrsoy</a>
|
<a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--1><div class="card-body p-3 small">Mikolov et al. (2013a) observed that continuous bag-of-words (CBOW) word embeddings tend to underperform Skip-gram (SG) embeddings, and this finding has been reported in subsequent works. We find that these observations are driven not by fundamental differences in their training objectives, but more likely on faulty negative sampling CBOW implementations in popular <a href=https://en.wikipedia.org/wiki/Library_(computing)>libraries</a> such as the official implementation, word2vec.c, and <a href=https://en.wikipedia.org/wiki/Gensim>Gensim</a>. We show that after correcting a bug in the CBOW gradient update, one can learn CBOW word embeddings that are fully competitive with SG on various intrinsic and extrinsic tasks, while being many times faster to train.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.3/>BERT Can not Align Characters<span class=acl-fixed-case>BERT</span> Cannot Align Characters</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--3><div class="card-body p-3 small">In previous work, it has been shown that BERT can adequately align cross-lingual sentences on the <a href=https://en.wikipedia.org/wiki/Syntax>word level</a>. Here we investigate whether BERT can also operate as a char-level aligner. The languages examined are <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Fake <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We show that the closer two languages are, the better BERT can align them on the character level. BERT indeed works well in <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Fake English alignment, but this does not generalize to natural languages to the same extent. Nevertheless, the proximity of two languages does seem to be a factor. English is more related to <a href=https://en.wikipedia.org/wiki/German_language>German</a> than to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> and this is reflected in how well BERT aligns them ; <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> is better than <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We examine multiple setups and show that the <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrices</a> for <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> show weaker relations the further apart two languages are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.4/>Two Heads are Better than One? Verification of Ensemble Effect in Neural Machine Translation</a></strong><br><a href=/people/c/chanjun-park/>Chanjun Park</a>
|
<a href=/people/s/sungjin-park/>Sungjin Park</a>
|
<a href=/people/s/seolhwa-lee/>Seolhwa Lee</a>
|
<a href=/people/t/taesun-whang/>Taesun Whang</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--4><div class="card-body p-3 small">In the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensembles</a> are broadly known to be effective in improving performance. This paper analyzes how ensemble of neural machine translation (NMT) models affect performance improvement by designing various experimental setups (i.e., intra-, inter-ensemble, and non-convergence ensemble). To an in-depth examination, we analyze each ensemble method with respect to several aspects such as different attention models and vocab strategies. Experimental results show that ensembling is not always resulting in performance increases and give noteworthy negative findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.8/>Comparing Euclidean and Hyperbolic Embeddings on the <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> Nouns Hypernymy Graph<span class=acl-fixed-case>E</span>uclidean and Hyperbolic Embeddings on the <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Nouns Hypernymy Graph</a></strong><br><a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/a/adrian-benton/>Adrian Benton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--8><div class="card-body p-3 small">Nickel and Kiela (2017) present a new method for embedding tree nodes in the Poincare ball, and suggest that these hyperbolic embeddings are far more effective than Euclidean embeddings at embedding nodes in large, hierarchically structured graphs like the WordNet nouns hypernymy tree. This is especially true in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek to reproduce their experiments on embedding and reconstructing the WordNet nouns hypernymy graph. Counter to what they report, we find that Euclidean embeddings are able to represent this <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> at least as well as Poincare embeddings, when allowed at least 50 dimensions. We note that this does not diminish the significance of their work given the impressive performance of hyperbolic embeddings in very low-dimensional settings. However, given the wide influence of their work, our aim here is to present an updated and more accurate comparison between the Euclidean and hyperbolic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.12/>The Highs and Lows of Simple Lexical Domain Adaptation Approaches for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--12><div class="card-body p-3 small">Machine translation systems are vulnerable to domain mismatch, especially in a low-resource scenario. Out-of-domain translations are often of poor quality and prone to <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a>, due to <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> and the <a href=https://en.wikipedia.org/wiki/Translator_(computing)>decoder</a> acting as a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. We adopt two approaches to alleviate this problem : lexical shortlisting restricted by IBM statistical alignments, and hypothesis reranking based on <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a>. The <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are computationally cheap and show success on low-resource out-of-domain test sets. However, the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> lose advantage when there is sufficient data or too great domain mismatch. This is due to both the IBM model losing its advantage over the implicitly learned neural alignment, and issues with subword segmentation of unseen words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.13/>Backtranslation in Neural Morphological Inflection</a></strong><br><a href=/people/l/ling-liu/>Ling Liu</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--13><div class="card-body p-3 small">Backtranslation is a common technique for leveraging unlabeled data in low-resource scenarios in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The method is directly applicable to morphological inflection generation if unlabeled word forms are available. This paper evaluates the potential of backtranslation for morphological inflection using <a href=https://en.wikipedia.org/wiki/Data>data</a> from six languages with labeled data drawn from the SIGMORPHON shared task resource and unlabeled data from different sources. Our core finding is that backtranslation can offer modest improvements in low-resource scenarios, but only if the unlabeled data is very clean and has been filtered by the same annotation standards as the labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.19/>Challenging the Semi-Supervised VAE Framework for Text Classification<span class=acl-fixed-case>VAE</span> Framework for Text Classification</a></strong><br><a href=/people/g/ghazi-felhi/>Ghazi Felhi</a>
|
<a href=/people/j/joseph-le-roux/>Joseph Le Roux</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--19><div class="card-body p-3 small">Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for data efficient learning. In this paper, we question the adequacy of the standard design of sequence SSVAEs for the task of text classification as we exhibit two sources of overcomplexity for which we provide simplifications. These simplifications to SSVAEs preserve their theoretical soundness while providing a number of practical advantages in the semi-supervised setup where the result of training is a text classifier. These simplifications are the removal of (i) the Kullback-Liebler divergence from its objective and (ii) the fully unobserved latent variable from its probabilistic model. These changes relieve users from choosing a prior for their <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>, make the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> smaller and faster, and allow for a better flow of information into the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. We compare the simplified versions to standard SSVAEs on 4 text classification tasks. On top of the above-mentioned simplification, experiments show a speed-up of 26 %, while keeping equivalent classification scores. The code to reproduce our experiments is public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.insights-1.20.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.20/>Active Learning for Argument Strength Estimation</a></strong><br><a href=/people/n/nataliia-kees/>Nataliia Kees</a>
|
<a href=/people/m/michael-fromm/>Michael Fromm</a>
|
<a href=/people/e/evgeniy-faerman/>Evgeniy Faerman</a>
|
<a href=/people/t/thomas-seidl/>Thomas Seidl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--20><div class="card-body p-3 small">High-quality arguments are an essential part of <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making</a>. Automatically predicting the quality of an argument is a complex task that recently got much attention in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. However, the annotation effort for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is exceptionally high. Therefore, we test uncertainty-based active learning (AL) methods on two popular argument-strength data sets to estimate whether sample-efficient learning can be enabled. Our extensive empirical evaluation shows that uncertainty-based acquisition functions can not surpass the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> reached with the <a href=https://en.wikipedia.org/wiki/Random_variable>random acquisition</a> on these <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>.</div></div></div><hr><div id=2021latechclfl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.latechclfl-1/>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.0/>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.2/>FrameNet-like Annotation of Olfactory Information in Texts<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et-like Annotation of Olfactory Information in Texts</a></strong><br><a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/s/stefano-menini/>Stefano Menini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--2><div class="card-body p-3 small">Although olfactory references play a crucial role in our <a href=https://en.wikipedia.org/wiki/Cultural_memory>cultural memory</a>, only few works in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have tried to capture them from a computational perspective. Currently, the main challenge is not much the development of technological components for olfactory information extraction, given recent advances in <a href=https://en.wikipedia.org/wiki/Semantic_processing>semantic processing</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, but rather the lack of a theoretical framework to capture this <a href=https://en.wikipedia.org/wiki/Information>information</a> from a linguistic point of view, as a preliminary step towards the development of automated systems. Therefore, in this work we present the annotation guidelines, developed with the help of history scholars and domain experts, aimed at capturing all the relevant elements involved in olfactory situations or events described in texts. These guidelines have been inspired by FrameNet annotation, but underwent some adaptations, which are detailed in this paper. Furthermore, we present a case study concerning the annotation of olfactory situations in English historical travel writings describing trips to <a href=https://en.wikipedia.org/wiki/Italy>Italy</a>. An analysis of the most frequent role fillers show that olfactory descriptions pertain to some typical domains such as <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, <a href=https://en.wikipedia.org/wiki/Food>food</a>, <a href=https://en.wikipedia.org/wiki/Nature>nature</a>, ancient past, poor sanitation, all supporting the creation of a stereotypical imagery related to <a href=https://en.wikipedia.org/wiki/Italy>Italy</a>. On the other hand, positive feelings triggered by smells are prevalent, and contribute to framing travels to Italy as an exciting experience involving all senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.3/>Batavia asked for advice. Pretrained language models for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> in historical texts.</a></strong><br><a href=/people/s/sophie-i-arnoult/>Sophie I. Arnoult</a>
|
<a href=/people/l/lodewijk-petram/>Lodewijk Petram</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--3><div class="card-body p-3 small">Pretrained language models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> enriched with editorial notes : how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> based on 17th and 18th century United East India Company (VOC) reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can leverage mixed-variant data. In particular, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> successfully incorporate <a href=https://en.wikipedia.org/wiki/Note_(typography)>notes</a> for the prediction of entities in <a href=https://en.wikipedia.org/wiki/History>historical texts</a>. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand : multilingual models lose their advantage when confronted with more semantical tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.8/>Emotion Classification in <a href=https://en.wikipedia.org/wiki/German_language>German</a> Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language<span class=acl-fixed-case>G</span>erman Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language</a></strong><br><a href=/people/t/thomas-schmidt/>Thomas Schmidt</a>
|
<a href=/people/k/katrin-dennerlein/>Katrin Dennerlein</a>
|
<a href=/people/c/christian-wolff/>Christian Wolff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--8><div class="card-body p-3 small">We present results of a project on emotion classification on historical German plays of <a href=https://en.wikipedia.org/wiki/Age_of_Enlightenment>Enlightenment</a>, <a href=https://en.wikipedia.org/wiki/Storm_and_Stress>Storm and Stress</a>, and <a href=https://en.wikipedia.org/wiki/German_Classicism>German Classicism</a>. We have developed a hierarchical annotation scheme consisting of 13 sub-emotions like <a href=https://en.wikipedia.org/wiki/Suffering>suffering</a>, love and joy that sum up to 6 main and 2 polarity classes (positive / negative). We have conducted <a href=https://en.wikipedia.org/wiki/Annotation>textual annotations</a> on 11 German plays and have acquired over 13,000 <a href=https://en.wikipedia.org/wiki/Annotation>emotion annotations</a> by two annotators per play. We have evaluated multiple traditional machine learning approaches as well as transformer-based models pretrained on historical and contemporary language for a single-label text sequence emotion classification for the different <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion categories</a>. The evaluation is carried out on three different instances of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> : (1) taking all annotations, (2) filtering overlapping annotations by annotators, (3) applying a <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> for speech-based analysis. Best results are achieved on the filtered corpus with the best <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> being large transformer-based models pretrained on contemporary German language. For the polarity classification accuracies of up to 90 % are achieved. The accuracies become lower for settings with a higher number of classes, achieving 66 % for 13 sub-emotions. Further pretraining of a historical model with a corpus of dramatic texts led to no improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.9/>Automating the Detection of Poetic Features : The Limerick as Model Organism</a></strong><br><a href=/people/a/almas-abdibayev/>Almas Abdibayev</a>
|
<a href=/people/y/yohei-igarashi/>Yohei Igarashi</a>
|
<a href=/people/a/allen-riddell/>Allen Riddell</a>
|
<a href=/people/d/daniel-rockmore/>Daniel Rockmore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--9><div class="card-body p-3 small">In this paper we take up the problem of limerick detection and describe a <a href=https://en.wikipedia.org/wiki/System>system</a> to identify five-line poems as <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a> or not. This turns out to be a surprisingly difficult challenge with many subtleties. More precisely, we produce an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> which focuses on the structural aspects of the <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limerick rhyme scheme</a> and <a href=https://en.wikipedia.org/wiki/Rhythm>rhythm</a> (i.e., stress patterns) and when tested on a a culled data set of 98,454 publicly available <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a>, our limerick filter accepts 67 % as <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a>. The primary failure of our <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filter</a> is on the detection of non-standard rhymes, which we highlight as an outstanding challenge in computational poetics. Our accent detection algorithm proves to be very robust. Our main contributions are (1) a novel rhyme detection algorithm that works on English words including rare proper nouns and made-up words (and thus, words not in the widely used CMUDict database) ; (2) a novel rhythm-identifying heuristic that is robust to language noise at moderate levels and comparable in accuracy to state-of-the-art scansion algorithms. As a third significant contribution (3) we make publicly available a large corpus of <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a> that includes tags of limerick or not-limerick as determined by our identification software, thereby providing a benchmark for the community. The poetic tasks that we have identified as challenges for machines suggest that the <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limerick</a> is a useful model organism for the study of machine capabilities in <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a> and more broadly literature and language. We include a list of open challenges as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.12/>Translationese in Russian Literary Texts<span class=acl-fixed-case>R</span>ussian Literary Texts</a></strong><br><a href=/people/m/maria-kunilovskaya/>Maria Kunilovskaya</a>
|
<a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--12><div class="card-body p-3 small">The paper reports the results of a translationese study of <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a> based on translated and non-translated Russian. We aim to find out if <a href=https://en.wikipedia.org/wiki/Translation>translations</a> deviate from non-translated literary texts, and if the established differences can be attributed to <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological relations</a> between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Our results show that non-translated fiction is different from <a href=https://en.wikipedia.org/wiki/Translation>translations</a> to the degree that these two <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> can be automatically classified. As expected, <a href=https://en.wikipedia.org/wiki/Linguistic_typology>language typology</a> is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.15/>A Pilot Study for BERT Language Modelling and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphological Analysis</a> for Ancient and Medieval Greek<span class=acl-fixed-case>BERT</span> Language Modelling and Morphological Analysis for Ancient and Medieval <span class=acl-fixed-case>G</span>reek</a></strong><br><a href=/people/p/pranaydeep-singh/>Pranaydeep Singh</a>
|
<a href=/people/g/gorik-rutten/>Gorik Rutten</a>
|
<a href=/people/e/els-lefever/>Els Lefever</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--15><div class="card-body p-3 small">This paper presents a pilot study to automatic linguistic preprocessing of <a href=https://en.wikipedia.org/wiki/Ancient_Greek>Ancient and Byzantine Greek</a>, and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> more specifically. To this end, a novel subword-based BERT language model was trained on the basis of a varied corpus of Modern, Ancient and Post-classical Greek texts. Consequently, the obtained BERT embeddings were incorporated to train a fine-grained Part-of-Speech tagger for Ancient and Byzantine Greek. In addition, a corpus of Greek Epigrams was manually annotated and the resulting gold standard was used to evaluate the performance of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyser</a> on <a href=https://en.wikipedia.org/wiki/Medieval_Greek>Byzantine Greek</a>. The experimental results show very good perplexity scores (4.9) for the BERT language model and state-of-the-art performance for the fine-grained Part-of-Speech tagger for in-domain data (treebanks containing a mixture of Classical and Medieval Greek), as well as for the newly created Byzantine Greek gold standard data set. The language models and associated code are made available for use at https://github.com/pranaydeeps/Ancient-Greek-BERT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.16/>Zero-Shot Information Extraction to Enhance a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graph</a> Describing Silk Textiles</a></strong><br><a href=/people/t/thomas-schleider/>Thomas Schleider</a>
|
<a href=/people/r/raphael-troncy/>Raphael Troncy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--16><div class="card-body p-3 small">The knowledge of the European silk textile production is a typical case for which the information collected is heterogeneous, spread across many museums and sparse since rarely complete. Knowledge Graphs for this cultural heritage domain, when being developed with appropriate <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> and vocabularies, enable to integrate and reconcile this diverse information. However, many of these original <a href=https://en.wikipedia.org/wiki/Collection_(artwork)>museum records</a> still have some metadata gaps. In this paper, we present a zero-shot learning approach that leverages the ConceptNet common sense knowledge graph to predict categorical metadata informing about the silk objects production. We compared the performance of our approach with traditional supervised deep learning-based methods that do require <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. We demonstrate promising and competitive performance for similar datasets and circumstances and the ability to predict sometimes more fine-grained information. Our results can be reproduced using the code and datasets published at https://github.com/silknow/ZSL-KG-silk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.19/>Period Classification in Chinese Historical Texts<span class=acl-fixed-case>C</span>hinese Historical Texts</a></strong><br><a href=/people/z/zuoyu-tian/>Zuoyu Tian</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--19><div class="card-body p-3 small">In this study, we study <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> in Chinese Biji by using a classification task : classifying Ancient Chinese texts by time periods. Specifically, we focus on a unique genre in <a href=https://en.wikipedia.org/wiki/Classical_Chinese_literature>classical Chinese literature</a> : <a href=https://en.wikipedia.org/wiki/Biji_(Chinese_literature)>Biji</a> (literally notebook or brush notes), i.e., collections of anecdotes, quotations, etc., anything authors consider noteworthy, <a href=https://en.wikipedia.org/wiki/Biji_(Chinese_literature)>Biji</a> span hundreds of years across many dynasties and conserve informal language in written form. For these reasons, they are regarded as a good resource for investigating <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> (Fang, 2010). In this paper, we create a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 108 Biji across four dynasties. Based on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we first introduce a time period classification task for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Then we investigate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature representation methods</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. The results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using contextualized embeddings perform best. An analysis of the top <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> chosen by the word n-gram model (after bleaching proper nouns) confirms that these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> are informative and correspond to observations and assumptions made by historical linguists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.21/>Stylometric Literariness Classification : the Case of Stephen King</a></strong><br><a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/e/erik-ketzan/>Erik Ketzan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--21><div class="card-body p-3 small">This paper applies <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> to quantify the literariness of 73 novels and novellas by American author Stephen King, chosen as an extraordinary case of a writer who has been dubbed both high and low in literariness in critical reception. We operationalize <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a> using a measure of stylistic distance (Cosine Delta) based on the 1000 most frequent words in two bespoke comparison corpora used as proxies for <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a> : one of popular genre fiction, another of National Book Award-winning authors. We report that a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> is highly effective in distinguishing the two categories, with 94.6 % macro average in a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>. We define two subsets of texts by Kinghigh and low literariness works as suggested by critics and ourselvesand find that a predictive model does identify King&#8217;s Dark Tower series and novels such as Dolores Claiborne as among his most literary texts, consistent with critical reception, which has also ascribed postmodern qualities to the Dark Tower novels. Our results demonstrate the efficacy of Cosine Delta-based stylometry in quantifying the literariness of texts, while also highlighting the methodological challenges of <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a>, especially in the case of <a href=https://en.wikipedia.org/wiki/Stephen_King>Stephen King</a>. The code and data to reproduce our results are available at https://github.com/andreasvc/kinglit</div></div></div><hr><div id=2021law-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.law-1/>Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.0/>Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop</a></strong><br><a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--law-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.law-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.3/>Representing Implicit Positive Meaning of Negated Statements in AMR<span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/k/katharina-stein/>Katharina Stein</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--law-1--3><div class="card-body p-3 small">Abstract Meaning Representation (AMR) has become popular for representing the meaning of natural language in <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structures</a>. However, AMR does not represent scope information, posing a problem for its overall expressivity and specifically for drawing inferences from negated statements. This is the case with so-called positive interpretations of negated statements, in which implicit positive meaning is identified by inferring the opposite of the negation&#8217;s focus. In this work, we investigate how potential positive interpretations (PPIs) can be represented in AMR. We propose a logically motivated AMR structure for PPIs that makes the focus of negation explicit and sketch an initial proposal for a systematic methodology to generate this more expressive structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--law-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.law-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.5/>Can predicate-argument relationships be extracted from UD trees?<span class=acl-fixed-case>UD</span> trees?</a></strong><br><a href=/people/a/adam-ek/>Adam Ek</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--law-1--5><div class="card-body p-3 small">In this paper we investigate the possibility of extracting predicate-argument relations from UD trees (and enhanced UD graphs). Con- cretely, we apply UD parsers on an En- glish question answering / semantic-role label- ing data set (FitzGerald et al., 2018) and check if the annotations reflect the relations in the resulting parse trees, using a small number of rules to extract this information. We find that 79.1 % of the <a href=https://en.wikipedia.org/wiki/Argument_of_a_function>argument-predicate pairs</a> can be found in this way, on the basis of Ud- ify (Kondratyuk and Straka, 2019). Error anal- ysis reveals that half of the error cases are at- tributable to shortcomings in the dataset. The remaining errors are mostly due to predicate- argument relations not being extractible algo- rithmically from the UD trees (requiring se- mantic reasoning to be resolved). The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> itself is only responsible for a small portion of errors. Our analysis suggests a number of improvements to the UD annotation schema : we propose to enhance the <a href=https://en.wikipedia.org/wiki/Database_schema>schema</a> in four ways, in order to capture argument-predicate relations. Additionally, we propose improve- ments regarding <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> for question answering / semantic-role labeling data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--law-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.law-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.law-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.6/>Classifying Divergences in Cross-lingual AMR Pairs<span class=acl-fixed-case>AMR</span> Pairs</a></strong><br><a href=/people/s/shira-wein/>Shira Wein</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--law-1--6><div class="card-body p-3 small">Translation divergences are varied and widespread, challenging approaches that rely on <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel text</a>. To annotate translation divergences, we propose a schema grounded in the Abstract Meaning Representation (AMR), a sentence-level semantic framework instantiated for a number of languages. By comparing parallel AMR graphs, we can identify specific points of divergence. Each <a href=https://en.wikipedia.org/wiki/Divergence>divergence</a> is labeled with both a type and a cause. We release a small corpus of annotated English-Spanish data, and analyze the annotations in our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--law-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.law-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.10/>Subcategorizing Adverbials in Universal Conceptual Cognitive Annotation<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>C</span>onceptual <span class=acl-fixed-case>C</span>ognitive <span class=acl-fixed-case>A</span>nnotation</a></strong><br><a href=/people/z/zhuxin-wang/>Zhuxin Wang</a>
|
<a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--law-1--10><div class="card-body p-3 small">Universal Conceptual Cognitive Annotation (UCCA) is a semantic annotation scheme that organizes texts into coarse predicate-argument structure, offering broad coverage of semantic phenomena. At the same time, there is still need for a finer-grained treatment of many of the <a href=https://en.wikipedia.org/wiki/Categorization>categories</a>. The Adverbial category is of special interest, as it covers a wide range of fundamentally different meanings such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Causality>causation</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, and event quantification. In this paper we introduce a refinement annotation scheme for UCCA&#8217;s Adverbial category, showing that UCCA Adverbials can indeed be subcategorized into at least 7 semantic types, and doing so can help clarify and disambiguate the otherwise coarse-grained labels. We provide a preliminary set of annotation guidelines, as well as pilot annotation experiments with high inter-annotator agreement, confirming the validity of the scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.law-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--law-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.law-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.law-1.18/>WikiGUM : Exhaustive Entity Linking for <a href=https://en.wikipedia.org/wiki/Wikification>Wikification</a> in 12 Genres<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>GUM</span>: Exhaustive Entity Linking for Wikification in 12 Genres</a></strong><br><a href=/people/j/jessica-lin/>Jessica Lin</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--law-1--18><div class="card-body p-3 small">Previous work on <a href=https://en.wikipedia.org/wiki/Entity_Linking>Entity Linking</a> has focused on resources targeting non-nested proper named entity mentions, often in data from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, i.e. Wikification. In this paper, we present and evaluate WikiGUM, a fully wikified dataset, covering all mentions of named entities, including their non-named and pronominal mentions, as well as mentions nested within other mentions. The dataset covers a broad range of 12 written and spoken genres, most of which have not been included in Entity Linking efforts to date, leading to poor performance by a pretrained SOTA system in our evaluation. The availability of a variety of other annotations for the same data also enables further research on entities in context.</div></div></div><hr><div id=2021mrl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.mrl-1/>Proceedings of the 1st Workshop on Multilingual Representation Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.0/>Proceedings of the 1st Workshop on Multilingual Representation Learning</a></strong><br><a href=/people/d/duygu-ataman/>Duygu Ataman</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/g/gozde-gul-sahin/>Gozde Gul Sahin</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrl-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.2/>Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora</a></strong><br><a href=/people/t/takashi-wada/>Takashi Wada</a>
|
<a href=/people/t/tomoharu-iwata/>Tomoharu Iwata</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--2><div class="card-body p-3 small">We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model parameters</a> among different languages, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly trains the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely <a href=https://en.wikipedia.org/wiki/Yongning_Na_language>Yongning Na</a>, Shipibo-Konibo, and <a href=https://en.wikipedia.org/wiki/Griko_language>Griko</a>. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.4/>Do not neglect related languages : The case of low-resource Occitan cross-lingual word embeddings<span class=acl-fixed-case>O</span>ccitan cross-lingual word embeddings</a></strong><br><a href=/people/l/lisa-woller/>Lisa Woller</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--4><div class="card-body p-3 small">Cross-lingual word embeddings (CLWEs) have proven indispensable for various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, e.g., bilingual lexicon induction (BLI). However, the lack of data often impairs the quality of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus. We therefore claim that it is necessary to explore further <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to improve CLWEs in low-resource setups. In this paper we propose to incorporate data of related high-resource languages. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In our experiments we focus on <a href=https://en.wikipedia.org/wiki/Occitan_language>Occitan</a>, a low-resource Romance language which is often neglected due to lack of resources. We leverage data from <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a> for training and evaluate on the Occitan-English BLI task. By incorporating supporting languages our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> outperforms previous approaches by a large margin. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.7/>Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training</a></strong><br><a href=/people/v/vikram-gupta/>Vikram Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--7><div class="card-body p-3 small">Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of <a href=https://en.wikipedia.org/wiki/Value-added_tax>VAT</a> for multilingual and multilabel emotion recognition has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2 % (Arabic), 3.8 % (Spanish) and 1.8 % (English) over <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> with same amount of labelled data (10 % of training data). We also improve the existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 7 %, 4.5 % and 1 % (Jaccard Index) for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> respectively and perform probing experiments for understanding the impact of different layers of the contextual models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrl-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.8/>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</a></strong><br><a href=/people/k/karthikeyan-k/>Karthikeyan K</a>
|
<a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/s/somak-aditya/>Somak Aditya</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--8><div class="card-body p-3 small">Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Certain types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrl-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrl-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrl-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrl-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrl-1.17/>Shaking Syntactic Trees on the Sesame Street : Multilingual Probing with Controllable Perturbations</a></strong><br><a href=/people/e/ekaterina-taktasheva/>Ekaterina Taktasheva</a>
|
<a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrl-1--17><div class="card-body p-3 small">Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a> with a varying degree of word order flexibility : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>sensitivity</a> grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.</div></div></div><hr><div id=2021mrqa-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.mrqa-1/>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.0/>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></strong><br><a href=/people/a/adam-fisch/>Adam Fisch</a>
|
<a href=/people/a/alon-talmor/>Alon Talmor</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrqa-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.1/>MFAQ : a Multilingual FAQ Dataset<span class=acl-fixed-case>MFAQ</span>: a Multilingual <span class=acl-fixed-case>FAQ</span> Dataset</a></strong><br><a href=/people/m/maxime-de-bruyn/>Maxime De Bruyn</a>
|
<a href=/people/e/ehsan-lotfi/>Ehsan Lotfi</a>
|
<a href=/people/j/jeska-buhmann/>Jeska Buhmann</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--1><div class="card-body p-3 small">In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6 M FAQ pairs from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges : duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa achieves the best results, except for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on simple word changes. We publicly release our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, and training script.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.6/>Can Question Generation Debias <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering Models</a>? A Case Study on QuestionContext Lexical Overlap</a></strong><br><a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--6><div class="card-body p-3 small">Question answering (QA) models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have been demonstrated to exploit unintended dataset biases such as questioncontext lexical overlap. This hinders QA models from generalizing to <a href=https://en.wikipedia.org/wiki/Underrepresented_group>under-represented samples</a> such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, while improving that on questions with high <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrqa-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.9/>Eliciting Bias in Question Answering Models through Ambiguity</a></strong><br><a href=/people/a/andrew-mao/>Andrew Mao</a>
|
<a href=/people/n/naveen-raman/>Naveen Raman</a>
|
<a href=/people/m/matthew-shu/>Matthew Shu</a>
|
<a href=/people/e/eric-li/>Eric Li</a>
|
<a href=/people/f/franklin-yang/>Franklin Yang</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--9><div class="card-body p-3 small">Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> can inform methods to mitigate <a href=https://en.wikipedia.org/wiki/Equity_(economics)>inequity</a>. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. We feed three deep-learning-based QA systems with our question sets and evaluate responses for <a href=https://en.wikipedia.org/wiki/Bias>bias</a> via the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater <a href=https://en.wikipedia.org/wiki/Freedom_of_choice>freedom of choice</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.10/>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</a></strong><br><a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/w/wentao-ma/>Wentao Ma</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/j/jiani-ye/>Jiani Ye</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--10><div class="card-body p-3 small">Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that <a href=https://en.wikipedia.org/wiki/WEAM>WEAM</a> can significantly improve the zero-shot performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.14/>Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering</a></strong><br><a href=/people/f/fahim-faisal/>Fahim Faisal</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--14><div class="card-body p-3 small">Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.15/>Semantic Answer Similarity for Evaluating Question Answering Models</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/t/timo-moller/>Timo Möller</a>
|
<a href=/people/j/julian-gutsch/>Julian Gutsch</a>
|
<a href=/people/m/malte-pietsch/>Malte Pietsch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--15><div class="card-body p-3 small">The evaluation of question answering models compares <a href=https://en.wikipedia.org/wiki/Ground_truth>ground-truth annotations</a> with <a href=https://en.wikipedia.org/wiki/Prediction>model predictions</a>. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> hinders user acceptance in applications and complicates a fair comparison of different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Therefore, there is a need for an evaluation metric that is based on <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> instead of pure <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>. In this short paper, we present SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, and compare it to seven existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. To this end, we create an English and a German three-way annotated evaluation dataset containing pairs of answers along with human judgment of their <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, which we release along with an implementation of the SAS metric and the experiments. We find that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on our two newly created datasets and one dataset from related work.</div></div></div><hr><div id=2021newsum-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.newsum-1/>Proceedings of the Third Workshop on New Frontiers in Summarization</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.0/>Proceedings of the Third Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.newsum-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.1/>Sentence-level Planning for Especially Abstractive Summarization</a></strong><br><a href=/people/a/andreas-marfurt/>Andreas Marfurt</a>
|
<a href=/people/j/james-henderson/>James Henderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--1><div class="card-body p-3 small">Abstractive summarization models heavily rely on copy mechanisms, such as the pointer network or <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, to achieve good performance, measured by textual overlap with reference summaries. As a result, the generated summaries stay close to the formulations in the source document. We propose the * sentence planner * model to generate more abstractive summaries. It includes a hierarchical decoder that first generates a <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> for the next summary sentence, and then conditions the word generator on this <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. Our generated summaries are more abstractive and at the same time achieve high ROUGE scores when compared to human reference summaries. We verify the effectiveness of our design decisions with extensive evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.2/>Template-aware Attention Model for Earnings Call Report Generation</a></strong><br><a href=/people/y/yangchen-huang/>Yangchen Huang</a>
|
<a href=/people/p/prashant-k-dhingra/>Prashant K. Dhingra</a>
|
<a href=/people/s/seyed-danial-mohseni-taheri/>Seyed Danial Mohseni Taheri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--2><div class="card-body p-3 small">Earning calls are among important resources for investors and analysts for updating their price targets. Firms usually publish corresponding transcripts soon after earnings events. However, raw transcripts are often too long and miss the coherent structure. To enhance the clarity, analysts write well-structured reports for some important earnings call events by analyzing them, requiring time and effort. In this paper, we propose TATSum (Template-Aware aTtention model for Summarization), a generalized neural summarization approach for structured report generation, and evaluate its performance in the earnings call domain. We build a large corpus with thousands of transcripts and reports using historical earnings events. We first generate a candidate set of reports from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as potential soft templates which do not impose actual rules on the output. Then, we employ an encoder model with margin-ranking loss to rank the candidate set and select the best quality template. Finally, the transcript and the selected soft template are used as input in a seq2seq framework for report generation. Empirical results on the earnings call dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms state-of-the-art models in terms of informativeness and structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.6/>Evaluation of Summarization Systems across Gender, Age, and Race</a></strong><br><a href=/people/a/anna-jorgensen/>Anna Jørgensen</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--6><div class="card-body p-3 small">Summarization systems are ultimately evaluated by human annotators and raters. Usually, annotators and raters do not reflect the demographics of end users, but are recruited through student populations or crowdsourcing platforms with skewed demographics. For two different evaluation scenarios evaluation against gold summaries and system output ratings we show that summary evaluation is sensitive to protected attributes. This can severely bias system development and evaluation, leading us to build models that cater for some groups rather than others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.8/>Capturing Speaker Incorrectness : Speaker-Focused Post-Correction for Abstractive Dialogue Summarization</a></strong><br><a href=/people/d/dongyub-lee/>Dongyub Lee</a>
|
<a href=/people/j/jungwoo-lim/>Jungwoo Lim</a>
|
<a href=/people/t/taesun-whang/>Taesun Whang</a>
|
<a href=/people/c/chanhee-lee/>Chanhee Lee</a>
|
<a href=/people/s/seungwoo-cho/>Seungwoo Cho</a>
|
<a href=/people/m/mingun-park/>Mingun Park</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--8><div class="card-body p-3 small">In this paper, we focus on improving the quality of the summary generated by neural abstractive dialogue summarization systems. Even though pre-trained language models generate well-constructed and promising results, it is still challenging to summarize the conversation of multiple participants since the summary should include a description of the overall situation and the actions of each speaker. This paper proposes self-supervised strategies for speaker-focused post-correction in abstractive dialogue summarization. Specifically, our model first discriminates which type of speaker correction is required in a draft summary and then generates a revised summary according to the required type. Experimental results show that our proposed method adequately corrects the draft summaries, and the revised summaries are significantly improved in both quantitative and qualitative evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.11/>Context or No Context? A preliminary exploration of human-in-the-loop approach for Incremental Temporal Summarization in meetings</a></strong><br><a href=/people/n/nicole-beckage/>Nicole Beckage</a>
|
<a href=/people/s/shachi-h-kumar/>Shachi H Kumar</a>
|
<a href=/people/s/saurav-sahay/>Saurav Sahay</a>
|
<a href=/people/r/ramesh-manuvinakurike/>Ramesh Manuvinakurike</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--11><div class="card-body p-3 small">Incremental meeting temporal summarization, summarizing relevant information of partial multi-party meeting dialogue, is emerging as the next challenge in summarization research. Here we examine the extent to which human abstractive summaries of the preceding increments (context) can be combined with extractive meeting dialogue to generate abstractive summaries. We find that previous context improves ROUGE scores. Our findings further suggest that contexts begin to outweigh the dialogue. Using <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a> and semantic role labeling (SRL), we find that SRL captures relevant information without overwhelming the the model architecture. By compressing the previous contexts by ~70 %, we achieve better ROUGE scores over our baseline models. Collectively, these results suggest that context matters, as does the way in which context is presented to the model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.12/>Are We Summarizing the Right Way? A Survey of Dialogue Summarization Data Sets</a></strong><br><a href=/people/d/don-tuggener/>Don Tuggener</a>
|
<a href=/people/m/margot-mieskes/>Margot Mieskes</a>
|
<a href=/people/j/jan-milan-deriu/>Jan Deriu</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--12><div class="card-body p-3 small">Dialogue summarization is a long-standing task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and several data sets with <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> and associated human-written summaries of different styles exist. However, it is unclear for which type of dialogue which type of summary is most appropriate. For this reason, we apply a linguistic model of dialogue types to derive matching summary items and NLP tasks. This allows us to map existing dialogue summarization data sets into this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and identify gaps and potential directions for future work. As part of this process, we also provide an extensive overview of existing dialogue summarization data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.newsum-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.13/>Modeling Endorsement for Multi-Document Abstractive Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/b/bingqing-wang/>Bingqing Wang</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--13><div class="card-body p-3 small">A crucial difference between single- and multi-document summarization is how <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient content</a> manifests itself in the document(s). While such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a> may appear at the beginning of a single document, essential information is frequently reiterated in a set of documents related to a particular topic, resulting in an endorsement effect that increases information salience. In this paper, we model the cross-document endorsement effect and its utilization in multiple document summarization. Our method generates a synopsis from each document, which serves as an endorser to identify salient content from other documents. Strongly endorsed text segments are used to enrich a neural encoder-decoder model to consolidate them into an abstractive summary. The method has a great potential to learn from fewer examples to identify salient content, which alleviates the need for costly retraining when the set of documents is dynamically adjusted. Through extensive experiments on benchmark multi-document summarization datasets, we demonstrate the effectiveness of our proposed method over strong published baselines. Finally, we shed light on future research directions and discuss broader challenges of this task using a case study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.14/>SUBSUME : A Dataset for Subjective Summary Extraction from Wikipedia Documents<span class=acl-fixed-case>SUBSUME</span>: A Dataset for Subjective Summary Extraction from <span class=acl-fixed-case>W</span>ikipedia Documents</a></strong><br><a href=/people/n/nishant-yadav/>Nishant Yadav</a>
|
<a href=/people/m/matteo-brucato/>Matteo Brucato</a>
|
<a href=/people/a/anna-fariha/>Anna Fariha</a>
|
<a href=/people/o/oscar-youngquist/>Oscar Youngquist</a>
|
<a href=/people/j/julian-killingback/>Julian Killingback</a>
|
<a href=/people/a/alexandra-meliou/>Alexandra Meliou</a>
|
<a href=/people/p/peter-haas/>Peter Haas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--14><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> require generation of summaries tailored to the user&#8217;s information needs, i.e., their intent. Methods that express intent via explicit user queries fall short when query interpretation is subjective. Several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> exist for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> with objective intents where, for each document and intent (e.g., weather), a single summary suffices for all users. No <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> exist, however, for <a href=https://en.wikipedia.org/wiki/Intention_(philosophy)>subjective intents</a> (e.g., interesting places) where different users will provide different summaries. We present <a href=https://en.wikipedia.org/wiki/Subset>SUBSUME</a>, the first <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for evaluation of SUBjective SUMmary Extraction systems. SUBSUME contains 2,200 (document, intent, summary) triplets over 48 Wikipedia pages, with ten intents of varying subjectivity, provided by 103 individuals over Mechanical Turk. We demonstrate statistically that the intents in <a href=https://en.wikipedia.org/wiki/Subspecies>SUBSUME</a> vary systematically in <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity</a>. To indicate SUBSUME&#8217;s usefulness, we explore a collection of baseline algorithms for subjective extractive summarization and show that (i) as expected, example-based approaches better capture subjective intents than query-based ones, and (ii) there is ample scope for improving upon the baseline algorithms, thereby motivating further research on this challenging problem.</div></div></div><hr><div id=2021nllp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.nllp-1/>Proceedings of the Natural Legal Language Processing Workshop 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.0/>Proceedings of the Natural Legal Language Processing Workshop 2021</a></strong><br><a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a>
|
<a href=/people/l/leslie-barrett/>Leslie Barrett</a>
|
<a href=/people/c/catalina-goanta/>Catalina Goanta</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preotiuc-Pietro</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.5/>A Multilingual Approach to Identify and Classify Exceptional Measures against COVID-19<span class=acl-fixed-case>COVID</span>-19</a></strong><br><a href=/people/g/georgios-tziafas/>Georgios Tziafas</a>
|
<a href=/people/e/eugenie-de-saint-phalle/>Eugenie de Saint-Phalle</a>
|
<a href=/people/w/wietse-de-vries/>Wietse de Vries</a>
|
<a href=/people/c/clara-egger/>Clara Egger</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--5><div class="card-body p-3 small">The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and com- pare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are im- plemented across these countries. We evalu- ated multiple multi-label classifiers on a manu- ally annotated corpus at sentence level. The XLM-RoBERTa model achieves highest per- formance on this multilingual multi-label clas- sification task, with a <a href=https://en.wikipedia.org/wiki/Proportionality_(mathematics)>macro-average F1 score</a> of 59.8 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.9/>JuriBERT : A Masked-Language Model Adaptation for French Legal Text<span class=acl-fixed-case>J</span>uri<span class=acl-fixed-case>BERT</span>: A Masked-Language Model Adaptation for <span class=acl-fixed-case>F</span>rench Legal Text</a></strong><br><a href=/people/s/stella-douka/>Stella Douka</a>
|
<a href=/people/h/hadi-abdine/>Hadi Abdine</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a>
|
<a href=/people/r/rajaa-el-hamdani/>Rajaa El Hamdani</a>
|
<a href=/people/d/david-restrepo-amariles/>David Restrepo Amariles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--9><div class="card-body p-3 small">Language models have proven to be very useful when adapted to specific domains. Nonetheless, little research has been done on the adaptation of domain-specific BERT models in the <a href=https://en.wikipedia.org/wiki/French_language>French language</a>. In this paper, we focus on creating a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> adapted to <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal text</a> with the goal of helping law professionals. We conclude that some specific tasks do not benefit from generic language models pre-trained on large amounts of data. We explore the use of smaller <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> in domain-specific sub-languages and their benefits for <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal text</a>. We prove that domain-specific pre-trained models can perform better than their equivalent generalised ones in the legal domain. Finally, we release JuriBERT, a new set of BERT models adapted to the <a href=https://en.wikipedia.org/wiki/Law_of_France>French legal domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.11/>A Free Format Legal Question Answering System</a></strong><br><a href=/people/s/soha-khazaeli/>Soha Khazaeli</a>
|
<a href=/people/j/janardhana-punuru/>Janardhana Punuru</a>
|
<a href=/people/c/chad-morris/>Chad Morris</a>
|
<a href=/people/s/sanjay-sharma/>Sanjay Sharma</a>
|
<a href=/people/b/bert-staub/>Bert Staub</a>
|
<a href=/people/m/michael-cole/>Michael Cole</a>
|
<a href=/people/s/sunny-chiu-webster/>Sunny Chiu-Webster</a>
|
<a href=/people/d/dhruv-sakalley/>Dhruv Sakalley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--11><div class="card-body p-3 small">We present an information retrieval-based question answer system to answer legal questions. The system is not limited to a predefined set of questions or patterns and uses both sparse vector search and <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for input to a BERT-based answer re-ranking system. A combination of general domain and legal domain data is used for training. This natural question answering system is in production and is used commercially.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.16/>Legal Terminology Extraction with the Termolator</a></strong><br><a href=/people/n/nhi-pham/>Nhi Pham</a>
|
<a href=/people/l/lachlan-pham/>Lachlan Pham</a>
|
<a href=/people/a/adam-l-meyers/>Adam L. Meyers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--16><div class="card-body p-3 small">Domain-specific terminology is ubiquitous in <a href=https://en.wikipedia.org/wiki/Legal_instrument>legal documents</a>. Despite potential utility in populating <a href=https://en.wikipedia.org/wiki/Glossary>glossaries</a> and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> or as arguments in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and document classification tasks, there has been limited work done for legal terminology extraction. This paper describes some work to remedy this omission. In the described research, we make some modifications to the Termolator, a high-performing, open-source terminology extractor which has been tuned to scientific articles. Our changes are designed to improve the Termolator&#8217;s results when applied to United States Supreme Court decisions. Unaltered and using the recommended settings, the original Termolator provides a list of terminology with a precision of 23 % and 25 % for the categories of economic activity (development set) and criminal procedures (test set) respectively. These were the most frequently occurring broad issues in Washington University in St. Louis Database corpus, a database of Supreme Court decisions that have been manually classified by topic. Our contribution includes the introduction of several legal domain-specific filtration steps and changes to the web search relevance score ; each incrementally improved precision culminating in a combined <a href=https://en.wikipedia.org/wiki/Significant_figures>precision</a> of 63 % and 65 %. We also evaluated the baseline version of the Termolator on more specific subcategories and on broad issues with fewer cases. Our results show that a narrowed scope as well as smaller document numbers significantly lower the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. In both cases, the modifications to the Termolator improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.18/>Named Entity Recognition in Historic Legal Text : A Transformer and State Machine Ensemble Method</a></strong><br><a href=/people/f/fernando-trias/>Fernando Trias</a>
|
<a href=/people/h/hongming-wang/>Hongming Wang</a>
|
<a href=/people/s/sylvain-jaume/>Sylvain Jaume</a>
|
<a href=/people/s/stratos-idreos/>Stratos Idreos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--18><div class="card-body p-3 small">Older legal texts are often scanned and digitized via Optical Character Recognition (OCR), which results in numerous errors. Although <a href=https://en.wikipedia.org/wiki/Spell_checker>spelling</a> and <a href=https://en.wikipedia.org/wiki/Spell_checker>grammar checkers</a> can correct much of the scanned text automatically, Named Entity Recognition (NER) is challenging, making correction of names difficult. To solve this, we developed an ensemble language model using a transformer neural network architecture combined with a <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite state machine</a> to extract names from English-language legal text. We use the US-based English language Harvard Caselaw Access Project for training and testing. Then, the extracted <a href=https://en.wikipedia.org/wiki/Name>names</a> are subjected to heuristic textual analysis to identify errors, make corrections, and quantify the extent of problems. With this <a href=https://en.wikipedia.org/wiki/System>system</a>, we are able to extract most <a href=https://en.wikipedia.org/wiki/Name>names</a>, automatically correct numerous errors and identify potential mistakes that can later be reviewed for manual correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nllp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nllp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nllp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nllp-1.23/>Semi-automatic Triage of Requests for Free Legal Assistance</a></strong><br><a href=/people/m/meladel-mistica/>Meladel Mistica</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/b/brayden-merrifield/>Brayden Merrifield</a>
|
<a href=/people/k/kate-fazio/>Kate Fazio</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nllp-1--23><div class="card-body p-3 small">Free legal assistance is critically under-resourced, and many of those who seek legal help have their needs unmet. A major bottleneck in the provision of free legal assistance to those most in need is the determination of the precise nature of the legal problem. This paper describes a collaboration with a major provider of free legal assistance, and the deployment of natural language processing models to assign area-of-law categories to real-world requests for <a href=https://en.wikipedia.org/wiki/Legal_aid>legal assistance</a>. In particular, we focus on an investigation of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to generate efficiencies in the triage process, but also the risks associated with naive use of model predictions, including fairness across different user demographics.</div></div></div><hr><div id=2021nlp4convai-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.nlp4convai-1/>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.0/>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/elnaz-nouri/>Elnaz Nouri</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.6/>Not So Fast, Classifier Accuracy and Entropy Reduction in Incremental Intent Classification</a></strong><br><a href=/people/l/lianna-hrycyk/>Lianna Hrycyk</a>
|
<a href=/people/a/alessandra-zarcone/>Alessandra Zarcone</a>
|
<a href=/people/l/luzian-hahn/>Luzian Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--6><div class="card-body p-3 small">Incremental intent classification requires the assignment of intent labels to partial utterances. However, partial utterances do not necessarily contain enough information to be mapped to the intent class of their complete utterance (correctly and with a certain degree of confidence). Using the final interpretation as the ground truth to measure a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier&#8217;s accuracy</a> during intent classification of partial utterances is thus problematic. We release inCLINC, a dataset of partial and full utterances with human annotations of plausible intent labels for different portions of each utterance, as an upper (human) baseline for incremental intent classification. We analyse the incremental annotations and propose entropy reduction as a measure of human annotators&#8217; convergence on an interpretation (i.e. intent label). We argue that, when the annotators do not converge to one or a few possible interpretations and yet the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> already identifies the final intent class early on, it is a sign of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> that can be ascribed to artefacts in the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.8/>Amendable Generation for Dialogue State Tracking</a></strong><br><a href=/people/x/xin-tian/>Xin Tian</a>
|
<a href=/people/l/liankai-huang/>Liankai Huang</a>
|
<a href=/people/y/yingzhan-lin/>Yingzhan Lin</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/y/yunyi-yang/>Yunyi Yang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/s/shuqi-sun/>Shuqi Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--8><div class="card-body p-3 small">In task-oriented dialogue systems, recent dialogue state tracking methods tend to perform one-pass generation of the dialogue state based on the previous dialogue state. The mistakes of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> made at the current turn are prone to be carried over to the next turn, causing <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a novel Amendable Generation for Dialogue State Tracking (AG-DST), which contains a two-pass generation process : (1) generating a primitive dialogue state based on the dialogue of the current turn and the previous dialogue state, and (2) amending the primitive dialogue state from the first pass. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state, which plays the role of reviser in the double-checking process and alleviates unnecessary error propagation. Experimental results show that AG-DST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.9/>What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts</a></strong><br><a href=/people/j/james-d-finch/>James D. Finch</a>
|
<a href=/people/s/sarah-e-finch/>Sarah E. Finch</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--9><div class="card-body p-3 small">Improving user experience of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies <a href=https://en.wikipedia.org/wiki/Interaction>interactions</a> that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.12/>Semi-supervised Intent Discovery with Contrastive Learning</a></strong><br><a href=/people/x/xiang-shen/>Xiang Shen</a>
|
<a href=/people/y/yinge-sun/>Yinge Sun</a>
|
<a href=/people/y/yao-zhang/>Yao Zhang</a>
|
<a href=/people/m/mani-najmabadi/>Mani Najmabadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--12><div class="card-body p-3 small">User intent discovery is a key step in developing a Natural Language Understanding (NLU) module at the core of any modern Conversational AI system. Typically, human experts review a representative sample of user input data to discover new intents, which is subjective, costly, and error-prone. In this work, we aim to assist the NLU developers by presenting a novel method for discovering new intents at scale given a corpus of utterances. Our method utilizes supervised contrastive learning to leverage information from a domain-relevant, already labeled dataset and identifies new intents in the corpus at hand using unsupervised K-means clustering. Our method outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a large margin up to 2 % and 13 % on two benchmark datasets, measured by clustering accuracy. Furthermore, we apply our method on a large dataset from the travel domain to demonstrate its effectiveness on a real-world use case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.13/>CS-BERT : a pretrained model for customer service dialogues<span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>BERT</span>: a pretrained model for customer service dialogues</a></strong><br><a href=/people/p/peiyao-wang/>Peiyao Wang</a>
|
<a href=/people/j/joyce-fang/>Joyce Fang</a>
|
<a href=/people/j/julia-reinspach/>Julia Reinspach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--13><div class="card-body p-3 small">Large-scale pretrained transformer models have demonstrated state-of-the-art (SOTA) performance in a variety of NLP tasks. Nowadays, numerous pretrained models are available in different model flavors and different languages, and can be easily adapted to one&#8217;s downstream task. However, only a limited number of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are available for dialogue tasks, and in particular, goal-oriented dialogue tasks. In addition, the available pretrained models are trained on general domain language, creating a mismatch between the pretraining language and the downstream domain launguage. In this contribution, we present CS-BERT, a BERT model pretrained on millions of dialogues in the customer service domain. We evaluate CS-BERT on several downstream customer service dialogue tasks, and demonstrate that our in-domain pretraining is advantageous compared to other pretrained models in both zero-shot experiments as well as in finetuning experiments, especially in a low-resource data setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.14/>PLATO-KAG : Unsupervised Knowledge-Grounded Conversation via Joint Modeling<span class=acl-fixed-case>PLATO</span>-<span class=acl-fixed-case>KAG</span>: Unsupervised Knowledge-Grounded Conversation via Joint Modeling</a></strong><br><a href=/people/x/xinxian-huang/>Xinxian Huang</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--14><div class="card-body p-3 small">Large-scale conversation models are turning to leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a> to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>factual accuracy</a> in response generation. Considering the infeasibility to annotate the external knowledge for large-scale dialogue corpora, it is desirable to learn the knowledge selection and response generation in an unsupervised manner. In this paper, we propose PLATO-KAG (Knowledge-Augmented Generation), an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling. For each dialogue context, the top-k relevant knowledge elements are selected and then employed in knowledge-grounded response generation. The two components of knowledge selection and response generation are optimized jointly and effectively under a balanced objective. Experimental results on two publicly available datasets validate the superiority of PLATO-KAG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.17/>Personalized Search-based Query Rewrite System for Conversational AI<span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/z/zheng-chen/>Zheng Chen</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--17><div class="card-body p-3 small">Query rewrite (QR) is an emerging component in conversational AI systems, reducing user defect. User defect is caused by various reasons, such as errors in the spoken dialogue system, users&#8217; slips of the tongue or their abridged language. Many of the user defects stem from personalized factors, such as user&#8217;s speech pattern, <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a>, or preferences. In this work, we propose a personalized search-based QR framework, which focuses on automatic reduction of user defect. We build a personalized index for each user, which encompasses diverse affinity layers to reflect personal preferences for each user in the conversational AI. Our personalized QR system contains retrieval and ranking layers. Supported by user feedback based learning, training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> does not require hand-annotated data. Experiments on personalized test set showed that our personalized QR system is able to correct systematic and user errors by utilizing phonetic and semantic inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.19/>AuGPT : Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models<span class=acl-fixed-case>AuGPT</span>: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models</a></strong><br><a href=/people/j/jonas-kulhanek/>Jonáš Kulhánek</a>
|
<a href=/people/v/vojtech-hudecek/>Vojtěch Hudeček</a>
|
<a href=/people/t/tomas-nekvinda/>Tomáš Nekvinda</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--19><div class="card-body p-3 small">Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We carefully evaluate our <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>contributions</a> with both <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>human and automatic methods</a>. Our model substantially outperforms the baseline on the MultiWOZ data and shows competitive performance with state of the art in both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.22/>Using Pause Information for More Accurate Entity Recognition</a></strong><br><a href=/people/s/sahas-dendukuri/>Sahas Dendukuri</a>
|
<a href=/people/p/pooja-chitkara/>Pooja Chitkara</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/m/manos-tsagkias/>Manos Tsagkias</a>
|
<a href=/people/s/stephen-pulman/>Stephen Pulman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--22><div class="card-body p-3 small">Entity tags in human-machine dialog are integral to natural language understanding (NLU) tasks in conversational assistants. However, current systems struggle to accurately parse spoken queries with the typical use of text input alone, and often fail to understand the user intent. Previous work in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> has identified a cross-language tendency for longer speech pauses surrounding <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> as compared to <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>. We demonstrate that the linguistic observation on pauses can be used to improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in machine-learnt language understanding tasks. Analysis of pauses in French and English utterances from a commercial voice assistant shows the statistically significant difference in pause duration around multi-token entity span boundaries compared to within entity spans. Additionally, in contrast to text-based NLU, we apply pause duration to enrich contextual embeddings to improve shallow parsing of entities. Results show that our proposed novel embeddings improve the relative error rate by up to 8 % consistently across three domains for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, without any added annotation or alignment costs to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.24/>Teach Me What to Say and I Will Learn What to Pick : Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models<span class=acl-fixed-case>I</span> Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models</a></strong><br><a href=/people/e/ehsan-lotfi/>Ehsan Lotfi</a>
|
<a href=/people/m/maxime-de-bruyn/>Maxime De Bruyn</a>
|
<a href=/people/j/jeska-buhmann/>Jeska Buhmann</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--24><div class="card-body p-3 small">Knowledge Grounded Conversation Models are usually based on a selection / retrieval module and a generation module, trained separately or simultaneously, with or without having access to a &#8216;gold&#8217; knowledge option. With the introduction of large pre-trained generative models, the selection and generation part have become more and more entangled, shifting the focus towards enhancing knowledge incorporation (from multiple sources) instead of trying to pick the best knowledge option. These approaches however depend on knowledge labels and/or a separate dense retriever for their best performance. In this work we study the unsupervised selection abilities of pre-trained generative models (e.g. BART) and show that by adding a score-and-aggregate module between <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, they are capable of learning to pick the proper knowledge through minimising the language modelling loss (i.e. without having access to knowledge labels). Trained as such, our model-K-Mine-shows competitive selection and generation performance against models that benefit from knowledge labels and/or separate dense retriever.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.25/>Influence of user personality on dialogue task performance : A case study using a rule-based dialogue system</a></strong><br><a href=/people/a/ao-guo/>Ao Guo</a>
|
<a href=/people/a/atsumoto-ohashi/>Atsumoto Ohashi</a>
|
<a href=/people/r/ryu-hirai/>Ryu Hirai</a>
|
<a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/y/yuiko-tsunomori/>Yuiko Tsunomori</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--25><div class="card-body p-3 small">Endowing a task-oriented dialogue system with adaptiveness to <a href=https://en.wikipedia.org/wiki/User_(computing)>user personality</a> can greatly help improve the performance of a dialogue task. However, such a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> can be practically challenging to implement, because it is unclear how <a href=https://en.wikipedia.org/wiki/User_(computing)>user personality</a> influences dialogue task performance. To explore the relationship between user personality and dialogue task performance, we enrolled participants via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to first answer specified personality questionnaires and then chat with a dialogue system to accomplish assigned tasks. A rule-based dialogue system on the prevalent Multi-Domain Wizard-of-Oz (MultiWOZ) task was used. A total of 211 participants&#8217; personalities and their 633 dialogues were collected and analyzed. The results revealed that sociable and extroverted people tended to fail the task, whereas neurotic people were more likely to succeed. We extracted <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> related to user dialogue behaviors and performed further analysis to determine which kind of <a href=https://en.wikipedia.org/wiki/Behavior>behavior</a> influences task performance. As a result, we identified that average utterance length and slots per utterance are the key features of dialogue behavior that are highly correlated with both task performance and user personality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.27/>Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--27><div class="card-body p-3 small">Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. This work focuses on identifying such <a href=https://en.wikipedia.org/wiki/User_(computing)>user requests</a>. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a>. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3 K parameters. We demonstrate <a href=https://en.wikipedia.org/wiki/Rede>REDE</a>&#8217;s competitive performance on DSTC9 data and our newly collected test set.</div></div></div><hr><div id=2021sustainlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.sustainlp-1/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.0/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/a/ana-marasovic/>Ana Marasović</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.sustainlp-1.1.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.1/>Low Resource Quadratic Forms for Knowledge Graph Embeddings</a></strong><br><a href=/people/z/zachary-zhou/>Zachary Zhou</a>
|
<a href=/people/j/jeffery-kline/>Jeffery Kline</a>
|
<a href=/people/d/devin-conathan/>Devin Conathan</a>
|
<a href=/people/g/glenn-fung/>Glenn Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--1><div class="card-body p-3 small">We address the problem of link prediction between entities and relations of knowledge graphs. State of the art techniques that address this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a <a href=https://en.wikipedia.org/wiki/Quadratic_form>quadratic form</a> that is used as a <a href=https://en.wikipedia.org/wiki/Ranking_function>ranking function</a>. The structure of our <a href=https://en.wikipedia.org/wiki/Convex_optimization>convex program</a> is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model&#8217;s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.<i>link prediction</i> between entities and relations of knowledge graphs. State of the art techniques that address this problem, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a quadratic form that is used as a ranking function. The structure of our convex program is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model&#8217;s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.3/>Limitations of Knowledge Distillation for Zero-shot Transfer Learning</a></strong><br><a href=/people/s/saleh-soltan/>Saleh Soltan</a>
|
<a href=/people/h/haidar-khan/>Haidar Khan</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--3><div class="card-body p-3 small">Pretrained transformer-based encoders such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have been demonstrated to achieve state-of-the-art performance on numerous NLP tasks. Despite their success, BERT style encoders are large in size and have high latency during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> (especially on CPU machines) which make them unappealing for many <a href=https://en.wikipedia.org/wiki/Online_application>online applications</a>. Recently introduced compression and distillation methods have provided effective ways to alleviate this shortcoming. However, the focus of these works has been mainly on monolingual encoders. Motivated by recent successes in zero-shot cross-lingual transfer learning using multilingual pretrained encoders such as mBERT, we evaluate the effectiveness of Knowledge Distillation (KD) both during pretraining stage and during fine-tuning stage on multilingual BERT models. We demonstrate that in contradiction to the previous observation in the case of monolingual distillation, in multilingual settings, <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during pretraining is more effective than <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during fine-tuning for zero-shot transfer learning. Moreover, we observe that <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a> may hurt zero-shot cross-lingual performance. Finally, we demonstrate that distilling a larger model (BERT Large) results in the strongest distilled model that performs best both on the source language as well as target languages in zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sustainlp-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.7/>Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering</a></strong><br><a href=/people/g/georgios-sidiropoulos/>Georgios Sidiropoulos</a>
|
<a href=/people/n/nikos-voskarides/>Nikos Voskarides</a>
|
<a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--7><div class="card-body p-3 small">In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.8/>Learning to Rank in the Age of <a href=https://en.wikipedia.org/wiki/The_Muppets>Muppets</a> : EffectivenessEfficiency Tradeoffs in Multi-Stage Ranking</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/c/chengcheng-hu/>ChengCheng Hu</a>
|
<a href=/people/y/yuqi-liu/>Yuqi Liu</a>
|
<a href=/people/h/hui-fang/>Hui Fang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--8><div class="card-body p-3 small">It is well known that rerankers built on pretrained transformer models such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have dramatically improved retrieval effectiveness in many tasks. However, these gains have come at substantial costs in terms of <a href=https://en.wikipedia.org/wiki/Economic_efficiency>efficiency</a>, as noted by many researchers. In this work, we show that it is possible to retain the benefits of transformer-based rerankers in a multi-stage reranking pipeline by first using feature-based learning-to-rank techniques to reduce the number of candidate documents under consideration without adversely affecting their quality in terms of recall. Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a>, but with up to 18 increase in <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for traditional LTR techniques, and that we should not forget history.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.13/>Distiller : A Systematic Study of Model Distillation Methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/h/haoyu-he/>Haoyu He</a>
|
<a href=/people/x/xingjian-shi/>Xingjian Shi</a>
|
<a href=/people/j/jonas-mueller/>Jonas Mueller</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/g/george-karypis/>George Karypis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--13><div class="card-body p-3 small">Knowledge Distillation (KD) offers a natural way to reduce the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and memory / energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets / tasks, such as the data augmentation policy, the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component&#8217;s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MI-objective functions with better bias / variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following : 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.14/>Shrinking Bigfoot : Reducing wav2vec 2.0 footprint</a></strong><br><a href=/people/z/zilun-peng/>Zilun Peng</a>
|
<a href=/people/a/akshay-budhkar/>Akshay Budhkar</a>
|
<a href=/people/i/ilana-tuil/>Ilana Tuil</a>
|
<a href=/people/j/jason-levy/>Jason Levy</a>
|
<a href=/people/p/parinaz-sobhani/>Parinaz Sobhani</a>
|
<a href=/people/r/raphael-cohen/>Raphael Cohen</a>
|
<a href=/people/j/jumana-nassour/>Jumana Nassour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--14><div class="card-body p-3 small">Wav2vec 2.0 is a state-of-the-art <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition model</a> which maps speech audio waveforms into latent representations. The largest version of wav2vec 2.0 contains 317 million parameters. Hence, the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>inference latency</a> of wav2vec 2.0 will be a bottleneck in production, leading to high costs and a significant environmental footprint. To improve wav2vec&#8217;s applicability to a production setting, we explore multiple model compression methods borrowed from the domain of large language models. Using a teacher-student approach, we distilled the knowledge from the original wav2vec 2.0 model into a student model, which is 2 times faster, 4.8 times smaller than the original model. More importantly, the student model is 2 times more energy efficient than the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in terms of <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO2 emission</a>. This increase in performance is accomplished with only a 7 % degradation in word error rate (WER). Our quantized model is 3.6 times smaller than the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, with only a 0.1 % degradation in <a href=https://en.wikipedia.org/wiki/White_blood_cell>WER</a>. To the best of our knowledge, this is the first work that compresses wav2vec 2.0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sustainlp-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.17/>Unsupervised Contextualized Document Representation</a></strong><br><a href=/people/a/ankur-gupta/>Ankur Gupta</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--17><div class="card-body p-3 small">Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</div></div></div><hr><div id=2021wmt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.wmt-1/>Proceedings of the Sixth Conference on Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.0/>Proceedings of the Sixth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loic Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondrej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.3/>GTCOM Neural Machine Translation Systems for WMT21<span class=acl-fixed-case>GTCOM</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--3><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT21 shared news translation task. We participate in six directions : <a href=https://en.wikipedia.org/wiki/English_language>English</a> to / from <a href=https://en.wikipedia.org/wiki/Hausa_language>Hausa</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to / from Bengali and <a href=https://en.wikipedia.org/wiki/Zulu_language>Zulu</a> to / from <a href=https://en.wikipedia.org/wiki/Xhosa_language>Xhosa</a>. Our submitted systems are unconstrained and focus on multilingual translation odel, backtranslation and forward-translation. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.6/>The TALP-UPC Participation in WMT21 News Translation Task : an mBART-based NMT Approach<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Participation in <span class=acl-fixed-case>WMT</span>21 News Translation Task: an m<span class=acl-fixed-case>BART</span>-based <span class=acl-fixed-case>NMT</span> Approach</a></strong><br><a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/i/ioannis-tsiamas/>Ioannis Tsiamas</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/j/javier-ferrando/>Javier Ferrando</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--6><div class="card-body p-3 small">This paper describes the submission to the WMT 2021 news translation shared task by the UPC Machine Translation group. The goal of the task is to translate German to French (De-Fr) and French to German (Fr-De). Our submission focuses on fine-tuning a pre-trained model to take advantage of <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>. We fine-tune mBART50 using the filtered data, and additionally, we train a Transformer model on the same <a href=https://en.wikipedia.org/wiki/Data>data</a> from scratch. In the experiments, we show that fine-tuning mBART50 results in 31.69 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> for De-Fr and 23.63 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> for Fr-De, which increases 2.71 and 1.90 BLEU accordingly, as compared to the model we train from scratch. Our final submission is an ensemble of these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, further increasing 0.3 <a href=https://en.wikipedia.org/wiki/Bijection>BLEU</a> for Fr-De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.9/>Mieind’s WMT 2021 Submission<span class=acl-fixed-case>WMT</span> 2021 Submission</a></strong><br><a href=/people/h/haukur-barri-simonarson/>Haukur Barri Símonarson</a>
|
<a href=/people/v/vesteinn-snaebjarnarson/>Vésteinn Snæbjarnarson</a>
|
<a href=/people/p/petur-orri-ragnarson/>Pétur Orri Ragnarson</a>
|
<a href=/people/h/haukur-jonsson/>Haukur Jónsson</a>
|
<a href=/people/v/vilhjalmur-thorsteinsson/>Vilhjalmur Thorsteinsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--9><div class="card-body p-3 small">We present Mieind&#8217;s submission for the EnglishIcelandic and IcelandicEnglish subsets of the 2021 WMT news translation task. Transformer-base models are trained for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to generate backtranslations teratively. A pretrained mBART-25 model is then adapted for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> using parallel data as well as the last backtranslation iteration. This adapted pretrained model is then used to re-generate backtranslations, and the training of the adapted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is continued.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.10/>Allegro.eu Submission to WMT21 News Translation Task<span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/m/mikolaj-koszowski/>Mikołaj Koszowski</a>
|
<a href=/people/k/karol-grzegorczyk/>Karol Grzegorczyk</a>
|
<a href=/people/t/tsimur-hadeliya/>Tsimur Hadeliya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--10><div class="card-body p-3 small">We submitted two uni-directional models, one for EnglishIcelandic direction and other for IcelandicEnglish direction. Our news translation system is based on the transformer-big architecture, it makes use of corpora filtering, <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and forward translation applied to parallel and monolingual data alike</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.11/>Illinois Japanese English News Translation for WMT 2021<span class=acl-fixed-case>I</span>llinois <span class=acl-fixed-case>J</span>apanese <span class=tex-math>↔</span> <span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>N</span>ews <span class=acl-fixed-case>T</span>ranslation for <span class=acl-fixed-case>WMT</span> 2021</a></strong><br><a href=/people/g/giang-le/>Giang Le</a>
|
<a href=/people/s/shinka-mori/>Shinka Mori</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--11><div class="card-body p-3 small">This system paper describes an end-to-end NMT pipeline for the Japanese English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.<tex-math>\\leftrightarrow</tex-math> English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.13/>The Fujitsu DMATH Submissions for WMT21 News Translation and Biomedical Translation Tasks<span class=acl-fixed-case>DMATH</span> Submissions for <span class=acl-fixed-case>WMT</span>21 News Translation and Biomedical Translation Tasks</a></strong><br><a href=/people/a/ander-martinez/>Ander Martinez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--13><div class="card-body p-3 small">This paper describes the Fujitsu DMATH systems used for WMT 2021 News Translation and Biomedical Translation tasks. We focused on low-resource pairs, using a simple <a href=https://en.wikipedia.org/wiki/System>system</a>. We conducted experiments on <a href=https://en.wikipedia.org/wiki/Hausa_language>English-Hausa</a>, <a href=https://en.wikipedia.org/wiki/Xhosa_language>Xhosa-Zulu</a> and <a href=https://en.wikipedia.org/wiki/Basque_language>English-Basque</a>, and submitted the results for XhosaZulu in the News Translation Task, and EnglishBasque in the Biomedical Translation Task, abstract and terminology translation subtasks. Our system combines BPE dropout, sub-subword features and back-translation with a Transformer (base) model, achieving good results on the evaluation sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.16/>The University of Edinburgh’s Bengali-Hindi Submissions to the WMT21 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s <span class=acl-fixed-case>B</span>engali-<span class=acl-fixed-case>H</span>indi Submissions to the <span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/p/proyag-pal/>Proyag Pal</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/sukanta-sen/>Sukanta Sen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--16><div class="card-body p-3 small">We describe the University of Edinburgh&#8217;s BengaliHindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.<tex-math>\\leftrightarrow</tex-math>Hindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.17/>The Volctrans GLAT System : Non-autoregressive Translation Meets WMT21<span class=acl-fixed-case>GLAT</span> System: Non-autoregressive Translation Meets <span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/l/lihua-qian/>Lihua Qian</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/y/yaoming-zhu/>Yaoming Zhu</a>
|
<a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/s/shanbo-cheng/>Shanbo Cheng</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--17><div class="card-body p-3 small">This paper describes the Volctrans&#8217; submission to the WMT21 news translation shared task for German-English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-English translation task, outperforming all strong <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive counterparts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.20/>Tencent Translation System for the WMT21 News Translation Task<span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/f/fangxu-liu/>Fangxu Liu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/j/jiali-zeng/>Jiali Zeng</a>
|
<a href=/people/w/wen-zhang/>Wen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--20><div class="card-body p-3 small">This paper describes Tencent Translation systems for the WMT21 shared task. We participate in the news translation task on three language pairs : Chinese-English, English-Chinese and German-English. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are built on various Transformer models with novel techniques adapted from our recent research work. First, we combine different data augmentation methods including <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, forward-translation and right-to-left training to enlarge the training data. We also apply language coverage bias, data rejuvenation and uncertainty-based sampling approaches to select content-relevant and high-quality data from large parallel and monolingual corpora. Expect for in-domain fine-tuning, we also propose a fine-grained one model one domain approach to model characteristics of different news genres at fine-tuning and decoding stages. Besides, we use greed-based ensemble algorithm and transductive ensemble method to further boost our systems. Based on our success in the last WMT, we continuously employed advanced techniques such as large batch training, data selection and data filtering. Finally, our constrained Chinese-English system achieves 33.4 case-sensitive BLEU score, which is the highest among all submissions. The German-English system is ranked at second place accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.21/>HW-TSC’s Participation in the WMT 2021 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--21><div class="card-body p-3 small">This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT 2021 News Translation Shared Task. We participate in 7 language pairs, including Zh / En, De / En, Ja / En, Ha / En, Is / En, Hi / Bn, and Xh / Zu in both directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Multilingual Translation, Ensemble Knowledge Distillation, etc. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.24/>Small Model and In-Domain Data Are All You Need</a></strong><br><a href=/people/h/hui-zeng/>Hui Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--24><div class="card-body p-3 small">I participated in the WMT shared news translation task and focus on one high resource language pair : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Chinese (two directions, Chinese to English and English to Chinese). The submitted systems (ZengHuiMT) focus on <a href=https://en.wikipedia.org/wiki/Data_cleansing>data cleaning</a>, data selection, <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> and model ensemble. The techniques I used for data filtering and selection include filtering by rules, <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. I used a base translation model trained on initial corpus to obtain the target versions of the WMT21 test sets, then I used language models to find out the monolingual data that is most similar to the target version of test set, such monolingual data was then used to do back translation. On the test set, my best submitted systems achieve 35.9 and 32.2 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for English to Chinese and Chinese to English directions respectively, which are quite high for a small model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.25/>The Mininglamp Machine Translation System for WMT21<span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/m/minghui-wu/>Minghui Wu</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--25><div class="card-body p-3 small">This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> to / from English, Hausa to / from English, <a href=https://en.wikipedia.org/wiki/German_language>German</a> to / from English and <a href=https://en.wikipedia.org/wiki/French_language>French</a> to / from German. Our fundamental system was based on Transformer architecture, with wider or smaller construction for different news translation tasks. We mainly utilized the method of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> to boost single model, while the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> was used to combine single models. Our final submission has ranked first for the English to / from Hausa task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.27/>Improving Similar Language Translation With <a href=https://en.wikipedia.org/wiki/Transfer_of_learning>Transfer Learning</a></a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--27><div class="card-body p-3 small">We investigate <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on pre-trained neural machine translation models to translate between (low-resource) similar languages. This work is part of our contribution to the WMT 2021 Similar Languages Translation Shared Task where we submitted models for different language pairs, including French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for Catalan-Spanish (82.79 BLEU)and Portuguese-Spanish (87.11 BLEU) rank top 1 in the official shared task evaluation, and we are the only team to submit <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Bambara_language>French-Bambara pairs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.28/>T4 T Solution : WMT21 Similar Language Task for the Spanish-Catalan and Spanish-Portuguese Language Pair<span class=acl-fixed-case>T</span>4<span class=acl-fixed-case>T</span> Solution: <span class=acl-fixed-case>WMT</span>21 Similar Language Task for the <span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>C</span>atalan and <span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>P</span>ortuguese Language Pair</a></strong><br><a href=/people/m/miguel-canals/>Miguel Canals</a>
|
<a href=/people/m/marc-raventos-tato/>Marc Raventós Tato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--28><div class="card-body p-3 small">The main idea of this <a href=https://en.wikipedia.org/wiki/Solution>solution</a> has been to focus on corpus cleaning and preparation and after that, use an out of box solution (OpenNMT) with its default published transformer model. To prepare the corpus, we have used set of standard tools (as Moses scripts or python packages), but also, among other python scripts, a python custom tokenizer with the ability to replace numbers for variables, solve the upper / lower case issue of the vocabulary and provide good segmentation for most of the punctuation. We also have started a line to clean corpus based on statistical probability estimation of source-target corpus, with unclear results. Also, we have run some tests with syllabical word segmentation, again with unclear results, so at the end, after word sentence tokenization we have used BPE SentencePiece for subword units to feed OpenNMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.31/>Similar Language Translation for <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> Using Marian NMT<span class=acl-fixed-case>C</span>atalan, <span class=acl-fixed-case>P</span>ortuguese and <span class=acl-fixed-case>S</span>panish Using <span class=acl-fixed-case>M</span>arian <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/r/reinhard-rapp/>Reinhard Rapp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--31><div class="card-body p-3 small">This paper describes the SEBAMAT contribution to the 2021 WMT Similar Language Translation shared task. Using the Marian neural machine translation toolkit, translation systems based on Google&#8217;s transformer architecture were built in both directions of CatalanSpanish and PortugueseSpanish. The systems were trained in two contrastive parameter settings (different vocabulary sizes for byte pair encoding) using only the parallel but not the comparable corpora provided by the shared task organizers. According to their official evaluation results, the SEBAMAT system turned out to be competitive with rankings among the top teams and BLEU scores between 38 and 47 for the language pairs involving <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and between 76 and 80 for the language pairs involving <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.35/>Adapting <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> for Automatic Post-Editing</a></strong><br><a href=/people/a/abhishek-sharma/>Abhishek Sharma</a>
|
<a href=/people/p/prabhakar-gupta/>Prabhakar Gupta</a>
|
<a href=/people/a/anil-nelakanti/>Anil Nelakanti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--35><div class="card-body p-3 small">Automatic post-editing (APE) models are usedto correct machine translation (MT) system outputs by learning from human post-editing patterns. We present the system used in our submission to the WMT&#8217;21 Automatic Post-Editing (APE) English-German (En-De) shared task. We leverage the state-of-the-art MT system (Ng et al., 2019) for this task. For further improvements, we adapt the MT model to the task domain by using WikiMatrix (Schwenket al., 2021) followed by fine-tuning with additional APE samples from previous editions of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a> (WMT-16,17,18) and ensembling the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our systems beat the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on TER scores on the WMT&#8217;21 test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.37/>HW-TSC’s Participation in the WMT 2021 Triangular MT Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Triangular <span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--37><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Service Center (HW-TSC) to WMT 2021 Triangular MT Shared Task. We participate in the Russian-to-Chinese task under the constrained condition. We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes. We perform detailed data pre-processing and filtering on the provided large-scale bilingual data. Several strategies are used to train our models, such as Multilingual Translation, Back Translation, Forward Translation, Data Denoising, Average Checkpoint, Ensemble, Fine-tuning, etc. Our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains 32.5 <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>BLEU</a> on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>dev set</a> and 27.7 BLEU on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>test set</a>, the highest score among all submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.43/>Transfer Learning with Shallow Decoders : BSC at WMT2021’s Multilingual Low-Resource Translation for Indo-European Languages Shared Task<span class=acl-fixed-case>BSC</span> at <span class=acl-fixed-case>WMT</span>2021’s Multilingual Low-Resource Translation for <span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>E</span>uropean Languages Shared Task</a></strong><br><a href=/people/k/ksenia-kharitonova/>Ksenia Kharitonova</a>
|
<a href=/people/o/ona-de-gibert-bonet/>Ona de Gibert Bonet</a>
|
<a href=/people/j/jordi-armengol-estape/>Jordi Armengol-Estapé</a>
|
<a href=/people/m/mar-rodriguez-i-alvarez/>Mar Rodriguez i Alvarez</a>
|
<a href=/people/m/maite-melero/>Maite Melero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--43><div class="card-body p-3 small">This paper describes the participation of the BSC team in the WMT2021&#8217;s Multilingual Low-Resource Translation for Indo-European Languages Shared Task. The system aims to solve the Subtask 2 : Wikipedia cultural heritage articles, which involves translation in four <a href=https://en.wikipedia.org/wiki/Romance_languages>Romance languages</a> : <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Occitan_language>Occitan</a> and <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>. The submitted <a href=https://en.wikipedia.org/wiki/System>system</a> is a multilingual semi-supervised machine translation model. It is based on a pre-trained language model, namely XLM-RoBERTa, that is later fine-tuned with parallel data obtained mostly from <a href=https://en.wikipedia.org/wiki/OPUS>OPUS</a>. Unlike other works, we only use XLM to initialize the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and randomly initialize a shallow decoder. The reported results are robust and perform well for all tested languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.50/>Back-translation for Large-Scale Multilingual Machine Translation</a></strong><br><a href=/people/b/baohao-liao/>Baohao Liao</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/s/sanjika-hewavitharana/>Sanjika Hewavitharana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--50><div class="card-body p-3 small">This paper illustrates our approach to the shared task on large-scale multilingual machine translation in the sixth conference on machine translation (WMT-21). In this work, we aim to build a single multilingual translation system with a hypothesis that a universal cross-language representation leads to better multilingual translation performance. We extend the exploration of different back-translation methods from bilingual translation to multilingual translation. Better performance is obtained by the constrained sampling method, which is different from the finding of the bilingual translation. Besides, we also explore the effect of <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabularies</a> and the amount of <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a>. Surprisingly, the smaller size of vocabularies perform better, and the extensive monolingual English data offers a modest improvement. We submitted to both the small tasks and achieve the second place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.51/>Maastricht University’s Large-Scale Multilingual Machine Translation System for WMT 2021<span class=acl-fixed-case>WMT</span> 2021</a></strong><br><a href=/people/d/danni-liu/>Danni Liu</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--51><div class="card-body p-3 small">We present our development of the multilingual machine translation system for the large-scale multilingual machine translation task at WMT 2021. Starting form the provided <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>, we investigated several techniques to improve the translation quality on the target subset of languages. We were able to significantly improve the translation quality by adapting the <a href=https://en.wikipedia.org/wiki/System>system</a> towards the target subset of languages and by generating synthetic data using the initial <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Techniques successfully applied in zero-shot multilingual machine translation (e.g. similarity regularizer) only had a minor effect on the final translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.54/>Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task<span class=acl-fixed-case>M</span>icrosoft for <span class=acl-fixed-case>WMT</span>21 Shared Task</a></strong><br><a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/a/alexandre-muzio/>Alexandre Muzio</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--54><div class="card-body p-3 small">This report describes Microsoft&#8217;s <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative back-translation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.55/>HW-TSC’s Participation in the WMT 2021 Large-Scale Multilingual Translation Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Large-Scale Multilingual Translation Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--55><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Services Center (HW-TSC) to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Samll Track # 2, including 6 languages : <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese (Jv)</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian (I d)</a>, <a href=https://en.wikipedia.org/wiki/Malay_language>Malay (Ms)</a>, <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog (Tl)</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil (Ta)</a> and <a href=https://en.wikipedia.org/wiki/English_language>English (En)</a> with 30 directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We train a single multilingual model to translate all the 30 directions. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Ensemble Knowledge Distillation, Adapter Fine-tuning. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains competitive results in the end.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.58" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.58/>Just Ask ! Evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> by Asking and Answering Questions</a></strong><br><a href=/people/m/mateusz-krubinski/>Mateusz Krubiński</a>
|
<a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--58><div class="card-body p-3 small">In this paper, we show that automatically-generated questions and answers can be used to evaluate the quality of Machine Translation (MT) systems. Building on recent work on the evaluation of abstractive text summarization, we propose a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for system-level MT evaluation, compare it with other state-of-the-art solutions, and show its robustness by conducting experiments for various MT directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.60/>Evaluating Multiway Multilingual NMT in the <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic Languages</a><span class=acl-fixed-case>NMT</span> in the <span class=acl-fixed-case>T</span>urkic Languages</a></strong><br><a href=/people/j/jamshidbek-mirzakhalov/>Jamshidbek Mirzakhalov</a>
|
<a href=/people/a/anoop-babu/>Anoop Babu</a>
|
<a href=/people/a/aigiz-kunafin/>Aigiz Kunafin</a>
|
<a href=/people/a/ahsan-wahab/>Ahsan Wahab</a>
|
<a href=/people/b/bekhzodbek-moydinboyev/>Bekhzodbek Moydinboyev</a>
|
<a href=/people/s/sardana-ivanova/>Sardana Ivanova</a>
|
<a href=/people/m/mokhiyakhon-uzokova/>Mokhiyakhon Uzokova</a>
|
<a href=/people/s/shaxnoza-pulatova/>Shaxnoza Pulatova</a>
|
<a href=/people/d/duygu-ataman/>Duygu Ataman</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/j/john-licato/>John Licato</a>
|
<a href=/people/s/sriram-chellappan/>Sriram Chellappan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--60><div class="card-body p-3 small">Despite the increasing number of large and comprehensive machine translation (MT) systems, evaluation of these methods in various languages has been restrained by the lack of high-quality parallel corpora as well as engagement with the people that speak these languages. In this study, we present an evaluation of state-of-the-art approaches to training and evaluating MT systems in 22 languages from the <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic language family</a>, most of which being extremely under-explored. First, we adopt the TIL Corpus with a few key improvements to the training and the evaluation sets. Then, we train 26 bilingual baselines as well as a multi-way neural MT (MNMT) model using the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and perform an extensive analysis using automatic metrics as well as human evaluations. We find that the MNMT model outperforms almost all bilingual baselines in the out-of-domain test sets and finetuning the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a downstream task of a single pair also results in a huge performance boost in both low- and high-resource scenarios. Our attentive analysis of evaluation criteria for MT models in <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic languages</a> also points to the necessity for further research in this direction. We release the corpus splits, test sets as well as <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to the public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.63/>DELA Corpus-A Document-Level Corpus Annotated with Context-Related Issues<span class=acl-fixed-case>DELA</span> Corpus - A Document-Level Corpus Annotated with Context-Related Issues</a></strong><br><a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/j/joao-lucas-cavalheiro-camargo/>João Lucas Cavalheiro Camargo</a>
|
<a href=/people/m/miguel-menezes/>Miguel Menezes</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--63><div class="card-body p-3 small">Recently, the Machine Translation (MT) community has become more interested in document-level evaluation especially in light of reactions to claims of human parity, since examining the quality at the level of the document rather than at the sentence level allows for the assessment of suprasentential context, providing a more reliable evaluation. This paper presents a document-level corpus annotated in English with context-aware issues that arise when translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>, namely ellipsis, gender, lexical ambiguity, number, reference, and terminology, with six different domains. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can be used as a challenge test set for evaluation and as a training / testing corpus for MT as well as for deep linguistic analysis of context issues. To the best of our knowledge, this is the first corpus of its kind.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.66/>Improving <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> of Rare and Unseen Word Senses</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/q/qianchu-liu/>Qianchu Liu</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--66><div class="card-body p-3 small">The performance of NMT systems has improved drastically in the past few years but the translation of multi-sense words still poses a challenge. Since word senses are not represented uniformly in the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used for training, there is an excessive use of the most frequent sense in MT output. In this work, we propose CmBT (Contextually-mined Back-Translation), an approach for improving multi-sense word translation leveraging pre-trained cross-lingual contextual word representations (CCWRs). Because of their contextual sensitivity and their large <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training data</a>, CCWRs can easily capture <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> that are missing or very rare in <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used to train MT. Specifically, CmBT applies bilingual lexicon induction on CCWRs to mine sense-specific target sentences from a monolingual dataset, and then back-translates these sentences to generate a pseudo parallel corpus as additional training data for an MT system. We test the translation quality of ambiguous words on the MuCoW test suite, which was built to test the <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> effectiveness of MT systems. We show that our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on the translation of difficult unseen and low frequency word senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.71/>Findings of the WMT 2021 Shared Task on Quality Estimation<span class=acl-fixed-case>WMT</span> 2021 Shared Task on Quality Estimation</a></strong><br><a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--71><div class="card-body p-3 small">We report the results of the WMT 2021 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels. This edition focused on two main novel additions : (i) <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> for unseen languages, i.e. zero-shot settings, and (ii) prediction of sentences with catastrophic errors. In addition, new <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a> was released for a number of languages, especially post-edited data. Participating teams from 19 institutions submitted altogether 1263 <a href=https://en.wikipedia.org/wiki/System>systems</a> to different task variants and language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.74/>Efficient <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> with Model Pruning and Quantization</a></strong><br><a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/g/graeme-nail/>Graeme Nail</a>
|
<a href=/people/q/qianqian-zhu/>Qianqian Zhu</a>
|
<a href=/people/s/svetlana-tchistiakova/>Svetlana Tchistiakova</a>
|
<a href=/people/j/jelmer-van-der-linde/>Jelmer van der Linde</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/sidharth-kashyap/>Sidharth Kashyap</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--74><div class="card-body p-3 small">We participated in all tracks of the WMT 2021 efficient machine translation task : <a href=https://en.wikipedia.org/wiki/Single-core>single-core CPU</a>, <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core CPU</a>, and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> with throughput and latency conditions. Our submissions combine several efficiency strategies : knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, lexical shortlists, smaller numerical formats, and pruning. For the CPU track, we used <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantized 8-bit models</a>. For the GPU track, we experimented with <a href=https://en.wikipedia.org/wiki/FP16>FP16</a> and 8-bit integers in tensorcores. Some of our submissions optimize for size via 4-bit log quantization and omitting a <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical shortlist</a>. We have extended pruning to more parts of the network, emphasizing component- and block-level pruning that actually improves speed unlike coefficient-wise pruning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.78/>Lingua Custodia’s Participation at the WMT 2021 Machine Translation Using Terminologies Shared Task<span class=acl-fixed-case>WMT</span> 2021 Machine Translation Using Terminologies Shared Task</a></strong><br><a href=/people/m/melissa-ailem/>Melissa Ailem</a>
|
<a href=/people/j/jingshu-liu/>Jingshu Liu</a>
|
<a href=/people/r/raheel-qader/>Raheel Qader</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--78><div class="card-body p-3 small">This paper describes Lingua Custodia&#8217;s submission to the WMT21 shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> using <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>. We consider three directions, namely <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We rely on a Transformer-based architecture as a building block, and we explore a method which introduces two main changes to the standard procedure to handle terminologies. The first one consists in augmenting the training data in such a way as to encourage the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to learn a copy behavior when it encounters terminology constraint terms. The second change is constraint token masking, whose purpose is to ease copy behavior learning and to improve model generalization. Empirical results show that our method satisfies most terminology constraints while maintaining high translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.79/>Kakao Enterprise’s WMT21 Machine Translation Using Terminologies Task Submission<span class=acl-fixed-case>WMT</span>21 Machine Translation Using Terminologies Task Submission</a></strong><br><a href=/people/y/yunju-bak/>Yunju Bak</a>
|
<a href=/people/j/jimin-sun/>Jimin Sun</a>
|
<a href=/people/j/jay-kim/>Jay Kim</a>
|
<a href=/people/s/sungwon-lyu/>Sungwon Lyu</a>
|
<a href=/people/c/changmin-lee/>Changmin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--79><div class="card-body p-3 small">This paper describes Kakao Enterprise&#8217;s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the EnFr language direction. Furthermore, we explore various methods such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, explicitly training <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a> as additional parallel data, and in-domain data selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.80/>The SPECTRANS System Description for the WMT21 Terminology Task<span class=acl-fixed-case>SPECTRANS</span> System Description for the <span class=acl-fixed-case>WMT</span>21 Terminology Task</a></strong><br><a href=/people/n/nicolas-ballier/>Nicolas Ballier</a>
|
<a href=/people/d/dahn-cho/>Dahn Cho</a>
|
<a href=/people/b/bilal-faye/>Bilal Faye</a>
|
<a href=/people/z/zong-you-ke/>Zong-You Ke</a>
|
<a href=/people/h/hanna-martikainen/>Hanna Martikainen</a>
|
<a href=/people/m/mojca-pecman/>Mojca Pecman</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/j/jean-baptiste-yunes/>Jean-Baptiste Yunès</a>
|
<a href=/people/l/lichao-zhu/>Lichao Zhu</a>
|
<a href=/people/m/maria-zimina-poirot/>Maria Zimina-Poirot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--80><div class="card-body p-3 small">This paper discusses the WMT 2021 terminology shared task from a meta perspective. We present the results of our experiments using the terminology dataset and the OpenNMT (Klein et al., 2017) and JoeyNMT (Kreutzer et al., 2019) toolkits for the language direction English to French. Our experiment 1 compares the predictions of the two <a href=https://en.wikipedia.org/wiki/Widget_toolkit>toolkits</a>. Experiment 2 uses OpenNMT to fine-tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We report our results for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with the evaluation script but mostly discuss the linguistic properties of the terminology dataset provided for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We provide evidence of the importance of text genres across scores, having replicated the evaluation scripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.81/>Dynamic Terminology Integration for COVID-19 and Other Emerging Domains<span class=acl-fixed-case>COVID</span>-19 and Other Emerging Domains</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--81><div class="card-body p-3 small">The majority of <a href=https://en.wikipedia.org/wiki/Domain_of_discourse>language domains</a> require prudent use of terminology to ensure clarity and adequacy of information conveyed. While the correct use of terminology for some languages and domains can be achieved by adapting general-purpose MT systems on large volumes of in-domain parallel data, such quantities of domain-specific data are seldom available for less-resourced languages and niche domains. Furthermore, as exemplified by COVID-19 recently, no domain-specific parallel data is readily available for emerging domains. However, the gravity of this recent calamity created a high demand for reliable translation of critical information regarding pandemic and infection prevention. This work is part of WMT2021 Shared Task : <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> using Terminologies, where we describe Tilde MT systems that are capable of dynamic terminology integration at the time of translation. Our systems achieve up to 94 % COVID-19 term use accuracy on the test set of the EN-FR language pair without having access to any form of in-domain information during system training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.82/>CUNI Systems for WMT21 : Terminology Translation Shared Task<span class=acl-fixed-case>CUNI</span> Systems for <span class=acl-fixed-case>WMT</span>21: Terminology Translation Shared Task</a></strong><br><a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/j/joao-paulo-aires/>João Paulo Aires</a>
|
<a href=/people/d/dusan-varis/>Dusan Varis</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--82><div class="card-body p-3 small">This paper describes Charles University sub-mission for Terminology translation Shared Task at WMT21. The objective of this task is to design a <a href=https://en.wikipedia.org/wiki/System>system</a> which translates certain terms based on a provided <a href=https://en.wikipedia.org/wiki/Terminology_database>terminology database</a>, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to use these provided terms. We lemmatize the terms both during the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the <a href=https://en.wikipedia.org/wiki/Terminology_database>terminology database</a>. Our submission ranked second in Exact Match metric which evaluates the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to produce desired terms in the translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.83/>PROMT Systems for WMT21 Terminology Translation Task<span class=acl-fixed-case>PROMT</span> Systems for <span class=acl-fixed-case>WMT</span>21 Terminology Translation Task</a></strong><br><a href=/people/a/alexander-molchanov/>Alexander Molchanov</a>
|
<a href=/people/v/vladislav-kovalenko/>Vladislav Kovalenko</a>
|
<a href=/people/f/fedor-bykov/>Fedor Bykov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--83><div class="card-body p-3 small">This paper describes the PROMT submissions for the WMT21 Terminology Translation Task. We participate in two directions : <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Our final submissions are MarianNMT-based neural systems. We present two <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> for terminology translation : a modification of the Dinu et al. (2019) soft-constrained approach and our own approach called PROMT Smart Neural Dictionary (SmartND). We achieve good results in both directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.84/>SYSTRAN @ WMT 2021 : Terminology Task<span class=acl-fixed-case>SYSTRAN</span> @ <span class=acl-fixed-case>WMT</span> 2021: Terminology Task</a></strong><br><a href=/people/m/minh-quang-pham/>Minh Quang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/a/antoine-senellart/>Antoine Senellart</a>
|
<a href=/people/d/dan-berrebbi/>Dan Berrebbi</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--84><div class="card-body p-3 small">This paper describes SYSTRAN submissions to the WMT 2021 terminology shared task. We participate in the English-to-French translation direction with a standard Transformer neural machine translation network that we enhance with the ability to dynamically include terminology constraints, a very common industrial practice. Two state-of-the-art terminology insertion methods are evaluated based (i) on the use of placeholders complemented with morphosyntactic annotation and (ii) on the use of target constraints injected in the source stream. Results show the suitability of the presented approaches in the evaluated scenario where <a href=https://en.wikipedia.org/wiki/Terminology>terminology</a> is used in a <a href=https://en.wikipedia.org/wiki/System>system</a> trained on generic data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.85/>TermMind : Alibaba’s WMT21 Machine Translation Using Terminologies Task Submission<span class=acl-fixed-case>T</span>erm<span class=acl-fixed-case>M</span>ind: <span class=acl-fixed-case>A</span>libaba’s <span class=acl-fixed-case>WMT</span>21 Machine Translation Using Terminologies Task Submission</a></strong><br><a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/s/shuqin-gu/>Shuqin Gu</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/y/yuqi-zhang/>Yuqi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--85><div class="card-body p-3 small">This paper describes our work in the WMT 2021 Machine Translation using Terminologies Shared Task. We participate in the shared translation terminologies task in English to Chinese language pair. To satisfy terminology constraints on <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, we use a terminology data augmentation strategy based on Transformer model. We used <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> to mark and add the term translations into the matched sentences. We created synthetic terms using phrase tables extracted from bilingual corpus to increase the proportion of term translations in training data. Detailed pre-processing and filtering on data, in-domain finetuning and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> are used in our system. Our submission obtains competitive results in the terminology-targeted evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.86/>FJWU Participation for the WMT21 Biomedical Translation Task<span class=acl-fixed-case>FJWU</span> Participation for the <span class=acl-fixed-case>WMT</span>21 Biomedical Translation Task</a></strong><br><a href=/people/s/sumbal-naz/>Sumbal Naz</a>
|
<a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/s/sami-ul-haq/>Sami Ul Haq</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--86><div class="card-body p-3 small">In this paper we present the FJWU&#8217;s system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>) with <a href=https://en.wikipedia.org/wiki/English_language>English</a> as target language. Our NMT systems based on Transformer architecture, were trained on combination of in-domain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.88/>Huawei AARC’s Submissions to the WMT21 Biomedical Translation Task : Domain Adaption from a Practical Perspective<span class=acl-fixed-case>AARC</span>’s Submissions to the <span class=acl-fixed-case>WMT</span>21 Biomedical Translation Task: Domain Adaption from a Practical Perspective</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/x/xupeng-meng/>Xupeng Meng</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--88><div class="card-body p-3 small">This paper describes Huawei Artificial Intelligence Application Research Center&#8217;s neural machine translation systems and submissions to the WMT21 biomedical translation shared task. Four of the submissions achieve state-of-the-art BLEU scores based on the official-released automatic evaluation results (EN-FR, EN-IT and ZH-EN). We perform experiments to unveil the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and under-translation are also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.92/>HW-TSC’s Participation at WMT 2021 Quality Estimation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation at <span class=acl-fixed-case>WMT</span> 2021 Quality Estimation Shared Task</a></strong><br><a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/c/chang-su/>Chang Su</a>
|
<a href=/people/y/yingtao-zhang/>Yingtao Zhang</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/x/xiang-geng/>Xiang Geng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/w/wang-minghan/>Wang Minghan</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/y/yujia-liu/>Yujia Liu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--92><div class="card-body p-3 small">This paper presents our work in WMT 2021 Quality Estimation (QE) Shared Task. We participated in all of the three sub-tasks, including Sentence-Level Direct Assessment (DA) task, Word and Sentence-Level Post-editing Effort task and Critical Error Detection task, in all language pairs. Our systems employ the framework of Predictor-Estimator, concretely with a pre-trained XLM-Roberta as Predictor and task-specific classifier or regressor as Estimator. For all tasks, we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> or encoding it with predictors directly. Moreover, in zero-shot setting, our data augmentation strategy based on Monte-Carlo Dropout brings up significant improvement on DA sub-task. Notably, our submissions achieve remarkable results over all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.94/>The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task<span class=acl-fixed-case>JHU</span>-<span class=acl-fixed-case>M</span>icrosoft Submission for <span class=acl-fixed-case>WMT</span>21 Quality Estimation Shared Task</a></strong><br><a href=/people/s/shuoyang-ding/>Shuoyang Ding</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--94><div class="card-body p-3 small">This paper presents the JHU-Microsoft joint submission for WMT 2021 quality estimation shared task. We only participate in Task 2 (post-editing effort estimation) of the shared task, focusing on the target-side word-level quality estimation. The techniques we experimented with include Levenshtein Transformer training and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> with a combination of forward, backward, round-trip translation, and pseudo post-editing of the MT output. We demonstrate the competitiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the widely adopted OpenKiwi-XLM baseline. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is also the top-ranking system on the MT MCC metric for the English-German language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.98/>Papago’s Submission for the WMT21 Quality Estimation Shared Task<span class=acl-fixed-case>WMT</span>21 Quality Estimation Shared Task</a></strong><br><a href=/people/s/seunghyun-lim/>Seunghyun Lim</a>
|
<a href=/people/h/hantae-kim/>Hantae Kim</a>
|
<a href=/people/h/hyunjoong-kim/>Hyunjoong Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--98><div class="card-body p-3 small">This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1 : Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with gold (labeled) data. We then compress our <a href=https://en.wikipedia.org/wiki/System>system</a> via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.99/>NICT Kyoto Submission for the WMT’21 Quality Estimation Task : Multimetric Multilingual Pretraining for Critical Error Detection<span class=acl-fixed-case>NICT</span> <span class=acl-fixed-case>K</span>yoto Submission for the <span class=acl-fixed-case>WMT</span>’21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection</a></strong><br><a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/b/benjamin-marie/>Benjamin Marie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--99><div class="card-body p-3 small">This paper presents the NICT Kyoto submission for the WMT&#8217;21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of <a href=https://en.wikipedia.org/wiki/Correlation_coefficient>correlation coefficient</a> and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.101/>Direct Exploitation of Attention Weights for Translation Quality Estimation</a></strong><br><a href=/people/l/lisa-yankovskaya/>Lisa Yankovskaya</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--101><div class="card-body p-3 small">The paper presents our submission to the WMT2021 Shared Task on Quality Estimation (QE). We participate in sentence-level predictions of human judgments and post-editing effort. We propose a glass-box approach based on attention weights extracted from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. In contrast to the previous works, we directly explore attention weight matrices without replacing them with general metrics (like entropy). We show that some of our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can be trained with a small amount of a high-cost labelled data. In the absence of training data our approach still demonstrates a moderate linear correlation, when trained with synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.102/>IST-Unbabel 2021 Submission for the Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Quality Estimation Shared Task</a></strong><br><a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/j/jose-g-c-de-souza/>José G. C. de Souza</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--102><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks : Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.103/>The IICT-Yverdon System for the WMT 2021 Unsupervised MT and Very Low Resource Supervised MT Task<span class=acl-fixed-case>IICT</span>-Yverdon System for the <span class=acl-fixed-case>WMT</span> 2021 Unsupervised <span class=acl-fixed-case>MT</span> and Very Low Resource Supervised <span class=acl-fixed-case>MT</span> Task</a></strong><br><a href=/people/a/alex-r-atrio/>Àlex R. Atrio</a>
|
<a href=/people/g/gabriel-luthier/>Gabriel Luthier</a>
|
<a href=/people/a/axel-fahy/>Axel Fahy</a>
|
<a href=/people/g/giorgos-vernikos/>Giorgos Vernikos</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/l/ljiljana-dolamic/>Ljiljana Dolamic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--103><div class="card-body p-3 small">In this paper, we present the systems submitted by our team from the Institute of ICT (HEIG-VD / HES-SO) to the Unsupervised MT and Very Low Resource Supervised MT task. We first study the improvements brought to a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> by techniques such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and initialization from a parent model. We find that both techniques are beneficial and suffice to reach performance that compares with more sophisticated <a href=https://en.wikipedia.org/wiki/System>systems</a> from the 2020 task. We then present the application of this system to the 2021 task for low-resource supervised Upper Sorbian (HSB) to German translation, in both directions. Finally, we present a contrastive system for HSB-DE in both directions, and for unsupervised German to Lower Sorbian (DSB) translation, which uses multi-task training with various training schedules to improve over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.105/>The <a href=https://en.wikipedia.org/wiki/Ludwig_Maximilian_University_of_Munich>LMU Munich Systems</a> for the WMT21 Unsupervised and Very Low-Resource Translation Task<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Systems for the <span class=acl-fixed-case>WMT</span>21 Unsupervised and Very Low-Resource Translation Task</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--105><div class="card-body p-3 small">We present our submissions to the WMT21 shared task in Unsupervised and Very Low Resource machine translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian, <a href=https://en.wikipedia.org/wiki/German_language>German and Lower Sorbian</a>, and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Chuvash_language>Chuvash</a>. Our low-resource systems (GermanUpper Sorbian, RussianChuvash) are pre-trained on high-resource pairs of related languages. We fine-tune those systems using the available authentic parallel data and improve by iterated back-translation. The unsupervised GermanLower Sorbian system is initialized by the best Upper Sorbian system and improved by iterated back-translation using monolingual data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wmt-1.109.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.109/>cushLEPOR : customising hLEPOR metric using Optuna for higher agreement with human judgments or pre-trained language model LaBSE<span class=acl-fixed-case>LEPOR</span>: customising h<span class=acl-fixed-case>LEPOR</span> metric using Optuna for higher agreement with human judgments or pre-trained language model <span class=acl-fixed-case>L</span>a<span class=acl-fixed-case>BSE</span></a></strong><br><a href=/people/l/lifeng-han/>Lifeng Han</a>
|
<a href=/people/i/irina-sorokina/>Irina Sorokina</a>
|
<a href=/people/g/gleb-erofeev/>Gleb Erofeev</a>
|
<a href=/people/s/serge-gladkoff/>Serge Gladkoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--109><div class="card-body p-3 small">Human evaluation has always been expensive while researchers struggle to trust the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. To address this, we propose to customise traditional <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> by taking advantages of the pre-trained language models (PLMs) and the limited available human labelled scores. We first re-introduce the hLEPOR metric factors, followed by the Python version we developed (ported) which achieved the automatic tuning of the weighting parameters in hLEPOR metric. Then we present the customised hLEPOR (cushLEPOR) which uses Optuna hyper-parameter optimisation framework to fine-tune hLEPOR weighting parameters towards better agreement to pre-trained language models (using LaBSE) regarding the exact MT language pairs that cushLEPOR is deployed to. We also optimise cushLEPOR towards professional human evaluation data based on MQM and pSQM framework on English-German and Chinese-English language pairs. The experimental investigations show cushLEPOR boosts hLEPOR performances towards better agreements to PLMs like LABSE with much lower cost, and better agreements to human evaluations including MQM and pSQM scores, and yields much better performances than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. Official results show that our submissions win three language pairs including English-German and Chinese-English on News domain via cushLEPOR(LM) and English-Russian on TED domain via hLEPOR. (data available at https://github.com/poethan/cushLEPOR)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.110/>MTEQA at WMT21 Metrics Shared Task<span class=acl-fixed-case>MTEQA</span> at <span class=acl-fixed-case>WMT</span>21 Metrics Shared Task</a></strong><br><a href=/people/m/mateusz-krubinski/>Mateusz Krubiński</a>
|
<a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--110><div class="card-body p-3 small">In this paper, we describe our submission to the WMT 2021 Metrics Shared Task. We use the automatically-generated questions and answers to evaluate the quality of Machine Translation (MT) systems. Our submission builds upon the recently proposed MTEQA framework. Experiments on WMT20 evaluation datasets show that at the system-level the MTEQA metric achieves performance comparable with other state-of-the-art solutions, while considering only a certain amount of information from the whole translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.111/>Are References Really Needed? Unbabel-IST 2021 Submission for the Metrics Shared Task<span class=acl-fixed-case>IST</span> 2021 Submission for the Metrics Shared Task</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--111><div class="card-body p-3 small">In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year&#8217;s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year&#8217;s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> than the original model, while also achieving state-of-the-art correlations with <a href=https://en.wikipedia.org/wiki/MQM>MQM</a>. Finally, in the <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> as a metric track, we also participated with a <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE model</a> trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.120/>Simultaneous Neural Machine Translation with Constituent Label Prediction</a></strong><br><a href=/people/y/yasumasa-kano/>Yasumasa Kano</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--120><div class="card-body p-3 small">Simultaneous translation is a task in which <a href=https://en.wikipedia.org/wiki/Translation>translation</a> begins before the speaker has finished speaking, so it is important to decide when to start the <a href=https://en.wikipedia.org/wiki/Translation>translation process</a>. However, deciding whether to read more input words or start to translate is difficult for language pairs with different word orders such as <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. Motivated by the concept of pre-reordering, we propose a couple of simple decision rules using the label of the next constituent predicted by incremental constituent label prediction. In experiments on English-to-Japanese simultaneous translation, the proposed method outperformed <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>quality-latency trade-off</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.121/>Contrastive Learning for Context-aware Neural Machine Translation Using Coreference Information</a></strong><br><a href=/people/y/yongkeun-hwang/>Yongkeun Hwang</a>
|
<a href=/people/h/hyeongu-yun/>Hyeongu Yun</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--121><div class="card-body p-3 small">Context-aware neural machine translation (NMT) incorporates contextual information of surrounding texts, that can improve the translation quality of document-level machine translation. Many existing works on context-aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results. However, most of existing works rely on cross-entropy loss, resulting in limited use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. In this paper, we propose CorefCL, a novel data augmentation and contrastive learning scheme based on <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> between the source and contextual sentences. By corrupting automatically detected coreference mentions in the contextual sentence, CorefCL can train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be sensitive to coreference inconsistency. We experimented with our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> on common context-aware NMT models and two document-level translation tasks. In the experiments, our method consistently improved BLEU of compared models on <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-German and English-Korean tasks</a>. We also show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly improves <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in the English-German contrastive test suite.</div></div></div><hr><div id=2021wnut-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.wnut-1/>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.0/>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.1/>Text Simplification for Comprehension-based Question-Answering</a></strong><br><a href=/people/t/tanvi-dadu/>Tanvi Dadu</a>
|
<a href=/people/k/kartikey-pant/>Kartikey Pant</a>
|
<a href=/people/s/seema-nagar/>Seema Nagar</a>
|
<a href=/people/f/ferdous-barbhuiya/>Ferdous Barbhuiya</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--1><div class="card-body p-3 small">Text simplification is the process of splitting and rephrasing a sentence to a sequence of sentences making it easier to read and understand while preserving the content and approximating the original meaning. Text simplification has been exploited in NLP applications like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, opening a broad avenue for its exploitation in comprehension-based question-answering downstream tasks. In this work, we investigate the effect of <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> in the task of <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a> using a <a href=https://en.wikipedia.org/wiki/Context_(language_use)>comprehension context</a>. We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset. Firstly, we outline each step in the dataset creation pipeline, including style transfer, thresholding of sentences showing correct transfer, and offset finding for each answer. Secondly, we verify the quality of the transferred sentences through various <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> involving both automated and human evaluation. Thirdly, we benchmark the newly created <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and perform an ablation study for examining the effect of the simplification process in the SQuAD-based question answering task. Our experiments show that simplification leads to up to 2.04 % and 1.74 % increase in <a href=https://en.wikipedia.org/wiki/Exact_Match>Exact Match</a> and F1, respectively. Finally, we conclude with an analysis of the transfer process, investigating the types of edits made by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and the effect of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> on the transfer model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wnut-1.4.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.wnut-1.4/>Keyphrase Extraction with Incomplete Annotated Training Data</a></strong><br><a href=/people/y/yanfei-lei/>Yanfei Lei</a>
|
<a href=/people/c/chunming-hu/>Chunming Hu</a>
|
<a href=/people/g/guanghui-ma/>Guanghui Ma</a>
|
<a href=/people/r/richong-zhang/>Richong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--4><div class="card-body p-3 small">Extracting keyphrases that summarize the main points of a document is a fundamental task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated. However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios. To this end, we propose a more robust <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training method</a> that learns to mitigate the misguidance brought by unlabeled keyphrases. We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios. Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines. Extensive experiments on five scientific domain datasets of different scales demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is competitive with the state-of-the-art method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.5/>Fine-grained Temporal Relation Extraction with Ordered-Neuron LSTM and Graph Convolutional Networks<span class=acl-fixed-case>LSTM</span> and Graph Convolutional Networks</a></strong><br><a href=/people/m/minh-tran-phu/>Minh Tran Phu</a>
|
<a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--5><div class="card-body p-3 small">Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> of the input sentences to enrich the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vectors</a>. In this work, we propose to fill this gap by introducing novel methods to integrate the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> into the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for FineTempRel. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> focuses on two types of syntactic information from the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>dependency trees</a>, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a> for the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>event mentions</a>. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.9/>A Text Editing Approach to Joint Japanese Word Segmentation, <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS Tagging</a>, and Lexical Normalization<span class=acl-fixed-case>J</span>apanese Word Segmentation, <span class=acl-fixed-case>POS</span> Tagging, and Lexical Normalization</a></strong><br><a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--9><div class="card-body p-3 small">Lexical normalization, in addition to <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieved better <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> performance when trained on more diverse pseudo-labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.10/>Intrinsic evaluation of language models for <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a></a></strong><br><a href=/people/s/sik-feng-cheong/>Sik Feng Cheong</a>
|
<a href=/people/h/hai-leong-chieu/>Hai Leong Chieu</a>
|
<a href=/people/j/jing-lim/>Jing Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--10><div class="card-body p-3 small">Language models used in <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> are often either evaluated intrinsically using <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> on test data, or extrinsically with an automatic speech recognition (ASR) system. The former <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> does not always correlate well with <a href=https://en.wikipedia.org/wiki/Signaling_(telecommunications)>ASR</a> performance, while the latter could be specific to particular <a href=https://en.wikipedia.org/wiki/Signaling_(telecommunications)>ASR systems</a>. Recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer. Underlying such an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is the assumption that the generated sentences are linguistically incorrect. In this paper, we first put this assumption into question, and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit. Secondly, we showed that by using multi-lingual BERT, we can achieve better performance than previous work on two code-switching data sets. Our implementation is publicly available on Github at https://github.com/sikfeng/language-modelling-for-code-switching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.12/>Perceived and Intended Sarcasm Detection with Graph Attention Networks</a></strong><br><a href=/people/j/joan-plepi/>Joan Plepi</a>
|
<a href=/people/l/lucie-flek/>Lucie Flek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--12><div class="card-body p-3 small">Existing sarcasm detection systems focus on exploiting <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a>, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user&#8217;s conversational neighborhood in an interaction graph, to contextualize the interpretation of the post. We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with dense user history representations. Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30 K labeled tweets, adding 10 M unlabeled tweets as context, our results indicate that the model contributes to interpreting the sarcastic intentions of an author more than to predicting the sarcasm perception by others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.18/>Comparing Grammatical Theories of Code-Mixing</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--18><div class="card-body p-3 small">Code-mixed text generation systems have found applications in many downstream tasks, including <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these <a href=https://en.wikipedia.org/wiki/Theory>theories</a>. We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two <a href=https://en.wikipedia.org/wiki/Grammatical_theory>grammatical theories</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.21/>Mitigation of Diachronic Bias in Fake News Detection Dataset</a></strong><br><a href=/people/t/taichi-murayama/>Taichi Murayama</a>
|
<a href=/people/s/shoko-wakamiya/>Shoko Wakamiya</a>
|
<a href=/people/e/eiji-aramaki/>Eiji Aramaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--21><div class="card-body p-3 small">Fake news causes significant damage to society. To deal with these <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> generated by political changes and social changes ; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, especially <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a> including <a href=https://en.wikipedia.org/wiki/Personal_name>person names</a>, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.24/>Changes in Twitter geolocations : Insights and suggestions for future usage<span class=acl-fixed-case>T</span>witter geolocations: Insights and suggestions for future usage</a></strong><br><a href=/people/a/anna-kruspe/>Anna Kruspe</a>
|
<a href=/people/m/matthias-haberle/>Matthias Häberle</a>
|
<a href=/people/e/eike-j-hoffmann/>Eike J. Hoffmann</a>
|
<a href=/people/s/samyo-rode-hasinger/>Samyo Rode-Hasinger</a>
|
<a href=/people/k/karam-abdulahhad/>Karam Abdulahhad</a>
|
<a href=/people/x/xiao-xiang-zhu/>Xiao Xiang Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--24><div class="card-body p-3 small">Twitter data has become established as a valuable source of data for various application scenarios in the past years. For many such <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, it is necessary to know where Twitter posts (tweets) were sent from or what location they refer to. Researchers have frequently used exact coordinates provided in a small percentage of tweets, but <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> removed the option to share these <a href=https://en.wikipedia.org/wiki/Coordinate_system>coordinates</a> in mid-2019. Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to <a href=https://en.wikipedia.org/wiki/Global_Positioning_System>GPS coordinates</a> of the user even before that. In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data. Finally, we make suggestions for future research requiring geolocated tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.32/>Coping with Noisy Training Data Labels in Paraphrase Detection</a></strong><br><a href=/people/t/teemu-vahtola/>Teemu Vahtola</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a>
|
<a href=/people/e/eetu-sjoblom/>Eetu Sjöblom</a>
|
<a href=/people/s/sami-itkonen/>Sami Itkonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--32><div class="card-body p-3 small">We present new state-of-the-art benchmarks for <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> on all six languages in the Opusparcus sentential paraphrase corpus : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We reach these <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by fine-tuning BERT. The best results are achieved on smaller and cleaner subsets of the training sets than was observed in previous research. Additionally, we study a translation-based approach that is competitive for the languages with more limited and noisier training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.35/>Detecting Cross-Geographic Biases in Toxicity Modeling on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/d/dylan-baker/>Dylan Baker</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--35><div class="card-body p-3 small">Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to <a href=https://en.wikipedia.org/wiki/Social_exclusion>marginalized groups</a>, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations / lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with ground truth labels to better understand these biases, and present preliminary mitigation experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.36/>Detection of Puffery on the <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a><span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/a/amanda-bertsch/>Amanda Bertsch</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--36><div class="card-body p-3 small">On <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia&#8217;s editorial policies. Wikipedia&#8217;s policy on maintaining a neutral point of view has inspired recent research on bias detection, including <a href=https://en.wikipedia.org/wiki/Weasel_word>weasel words</a> and <a href=https://en.wikipedia.org/wiki/Hedge_(finance)>hedges</a>. Yet to date, little work has been done on identifying <a href=https://en.wikipedia.org/wiki/Puffery>puffery</a>, phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by combining <a href=https://en.wikipedia.org/wiki/Wikipedia_community>Wikipedia editorial annotations</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval techniques</a>. We compare several approaches to predicting puffery, and achieve 0.963 <a href=https://en.wikipedia.org/wiki/F-number>f1 score</a> by incorporating <a href=https://en.wikipedia.org/wiki/Citation>citation features</a> into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia&#8217;s public infrastructure to give back to the Wikipedia editor community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.37/>Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text<span class=acl-fixed-case>BERT</span> Models Predicting <span class=acl-fixed-case>A</span>lzheimer’s Disease from Text</a></strong><br><a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--37><div class="card-body p-3 small">Understanding robustness and <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>sensitivity</a> of BERT models predicting Alzheimer&#8217;s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.39/>CIDEr-R : Robust Consensus-based Image Description Evaluation<span class=acl-fixed-case>CIDE</span>r-<span class=acl-fixed-case>R</span>: Robust Consensus-based Image Description Evaluation</a></strong><br><a href=/people/g/gabriel-oliveira-dos-santos/>Gabriel Oliveira dos Santos</a>
|
<a href=/people/e/esther-luna-colombini/>Esther Luna Colombini</a>
|
<a href=/people/s/sandra-avila/>Sandra Avila</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--39><div class="card-body p-3 small">This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work properly on datasets where the number of words in the sentence is significantly greater than those in the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of multiple reference sentences and high variance of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>. To bypass this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with high sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human judgment than CIDEr-D ; CIDEr-R is more robust regarding the number of available references. Our results reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized, the generated captions&#8217; length tends to be similar to the reference length. However, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> also repeat several times the same word to increase the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.42/>Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction</a></strong><br><a href=/people/s/shubhanshu-mishra/>Shubhanshu Mishra</a>
|
<a href=/people/a/aria-haghighi/>Aria Haghighi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--42><div class="card-body p-3 small">We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on source language task data and evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in the target language. In particular, we focus on language pairs where <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is difficult for mBERT : those where source and target languages are different in script, <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, and <a href=https://en.wikipedia.org/wiki/Linguistic_typology>linguistic typology</a>. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> on two social media tasks : NER (a 37 % average relative improvement in F1 across target languages) and sentiment classification (12 % relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7 % relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at : https://github.com/twitter-research/multilingual-alignment-tpp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.46/>Character Transformations for Non-Autoregressive GEC Tagging<span class=acl-fixed-case>GEC</span> Tagging</a></strong><br><a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jakub-naplava/>Jakub Náplava</a>
|
<a href=/people/j/jana-strakova/>Jana Straková</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--46><div class="card-body p-3 small">We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for <a href=https://en.wikipedia.org/wiki/Spelling>spelling</a>, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021_character_transformations_gec.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.47/>Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?</a></strong><br><a href=/people/a/arij-riabi/>Arij Riabi</a>
|
<a href=/people/b/benoit-sagot/>Benoît Sagot</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--47><div class="card-body p-3 small">Recent impressive improvements in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the <a href=https://en.wikipedia.org/wiki/Latin_script>Latin script</a>, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> in low-resource and high language variability set- tings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.48/>Something Something Hota Hai ! An Explainable Approach towards <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> on Indian Code-Mixed Data<span class=acl-fixed-case>I</span>ndian Code-Mixed Data</a></strong><br><a href=/people/a/aman-priyanshu/>Aman Priyanshu</a>
|
<a href=/people/a/aleti-vardhan/>Aleti Vardhan</a>
|
<a href=/people/s/sudarshan-sivakumar/>Sudarshan Sivakumar</a>
|
<a href=/people/s/supriti-vijay/>Supriti Vijay</a>
|
<a href=/people/n/nipuna-chhabra/>Nipuna Chhabra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--48><div class="card-body p-3 small">The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this <a href=https://en.wikipedia.org/wiki/Data>data</a> can provide integral insights into people&#8217;s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions allows us to determine the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> against different forms of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis models</a> we evaluate how well the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to adapt to the implicit noises present in code-mixed data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.49" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.49/>BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets<span class=acl-fixed-case>BERT</span>weet<span class=acl-fixed-case>FR</span> : Domain Adaptation of Pre-Trained Language Models for <span class=acl-fixed-case>F</span>rench Tweets</a></strong><br><a href=/people/y/yanzhu-guo/>Yanzhu Guo</a>
|
<a href=/people/v/virgile-rennard/>Virgile Rennard</a>
|
<a href=/people/c/christos-xypolopoulos/>Christos Xypolopoulos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--49><div class="card-body p-3 small">We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.50/>To What Extent Does Lexical Normalization Help English-as-a-Second Language Learners to Read Noisy English Texts?<span class=acl-fixed-case>E</span>nglish-as-a-Second Language Learners to Read Noisy <span class=acl-fixed-case>E</span>nglish Texts?</a></strong><br><a href=/people/y/yo-ehara/>Yo Ehara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--50><div class="card-body p-3 small">How difficult is it for English-as-a-second language (ESL) learners to read noisy English texts? Do <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL learners</a> need lexical normalization to read noisy English texts? These questions may also affect community formation on <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking sites</a> where differences can be attributed to <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL learners</a> and <a href=https://en.wikipedia.org/wiki/First_language>native English speakers</a>. However, few studies have addressed these questions. To this end, we built highly accurate readability assessors to evaluate the readability of texts for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL learners</a>. We then applied these assessors to noisy English texts to further assess the readability of the texts. The experimental results showed that although intermediate-level ESL learners can read most noisy English texts in the first place, lexical normalization significantly improves the readability of noisy English texts for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL learners</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.51/>Multilingual Sequence Labeling Approach to solve Lexical Normalization</a></strong><br><a href=/people/d/divesh-kubal/>Divesh Kubal</a>
|
<a href=/people/a/apurva-nagvenkar/>Apurva Nagvenkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--51><div class="card-body p-3 small">The task of converting a <a href=https://en.wikipedia.org/wiki/Nonstandard_dialect>nonstandard text</a> to a standard and readable text is known as lexical normalization. Almost all the Natural Language Processing (NLP) applications require the text data in normalized form to build quality task-specific models. Hence, lexical normalization has been proven to improve the performance of numerous natural language processing tasks on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This study aims to solve the problem of Lexical Normalization by formulating the Lexical Normalization task as a Sequence Labeling problem. This paper proposes a sequence labeling approach to solve the problem of Lexical Normalization in combination with the word-alignment technique. The goal is to use a single model to normalize text in various languages namely <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian-English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a>, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish-German</a>. This is a shared task in 2021 The 7th Workshop on Noisy User-generated Text (W-NUT) in which the participants are expected to create a system / model that performs lexical normalization, which is the translation of non-canonical texts into their canonical equivalents, comprising data from over 12 languages. The proposed single multilingual model achieves an overall ERR score of 43.75 on intrinsic evaluation and an overall Labeled Attachment Score (LAS) score of 63.12 on extrinsic evaluation. Further, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves the highest Error Reduction Rate (ERR) score of 61.33 among the participants in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.52/>Sesame Street to Mount Sinai : BERT-constrained character-level Moses models for multilingual lexical normalization<span class=acl-fixed-case>BERT</span>-constrained character-level <span class=acl-fixed-case>M</span>oses models for multilingual lexical normalization</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--52><div class="card-body p-3 small">This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.53/>Sequence-to-Sequence Lexical Normalization with Multilingual Transformers</a></strong><br><a href=/people/a/ana-maria-bucur/>Ana-Maria Bucur</a>
|
<a href=/people/a/adrian-cosma/>Adrian Cosma</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--53><div class="card-body p-3 small">Current benchmark tasks for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> contain text that is qualitatively different from the text used in informal day to day digital communication. This discrepancy has led to severe performance degradation of state-of-the-art NLP models when fine-tuned on real-world data. One way to resolve this issue is through lexical normalization, which is the process of transforming non-standard text, usually from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, into a more standardized form. In this work, we propose a sentence-level sequence-to-sequence model based on mBART, which frames the problem as a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation problem</a>. As the noisy text is a pervasive problem across languages, not just <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we leverage the multi-lingual pre-training of mBART to fine-tune it to our data. While current approaches mainly operate at the word or subword level, we argue that this approach is straightforward from a technical standpoint and builds upon existing pre-trained transformer networks. Our results show that while word-level, intrinsic, performance evaluation is behind other methods, our model improves performance on extrinsic, downstream tasks through <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> compared to models operating on raw, unprocessed, social media text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wnut-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wnut-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wnut-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wnut-1.54" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wnut-1.54/>FAL at MultiLexNorm 2021 : Improving Multilingual Lexical Normalization by Fine-tuning ByT5<span class=acl-fixed-case>ÚFAL</span> at <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ex<span class=acl-fixed-case>N</span>orm 2021: Improving Multilingual Lexical Normalization by Fine-tuning <span class=acl-fixed-case>B</span>y<span class=acl-fixed-case>T</span>5</a></strong><br><a href=/people/d/david-samuel/>David Samuel</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wnut-1--54><div class="card-body p-3 small">We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>