<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Joint Conference on Lexical and Computational Semantics (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Joint Conference on Lexical and Computational Semantics (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021starsem-1>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li></ul></div></div><div id=2021starsem-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.starsem-1/>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.0/>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.2/>Can Transformer Language Models Predict Psychometric Properties?</a></strong><br><a href=/people/a/antonio-laverghetta-jr/>Antonio Laverghetta Jr.</a>
|
<a href=/people/a/animesh-nighojkar/>Animesh Nighojkar</a>
|
<a href=/people/j/jamshidbek-mirzakhalov/>Jamshidbek Mirzakhalov</a>
|
<a href=/people/j/john-licato/>John Licato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--2><div class="card-body p-3 small">Transformer-based language models (LMs) continue to advance state-of-the-art performance on NLP benchmark tasks, including tasks designed to mimic human-inspired commonsense competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts of the field of <a href=https://en.wikipedia.org/wiki/Psychometrics>psychometrics</a>. But to what extent can the benefits flow in the other direction? I.e., can <a href=https://en.wikipedia.org/wiki/Longitudinal_study>LMs</a> be of use in predicting what the <a href=https://en.wikipedia.org/wiki/Psychometrics>psychometric properties</a> of test items will be when those items are given to human participants? We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the responses to calculate standard psychometric properties of the items in the <a href=https://en.wikipedia.org/wiki/Medical_test>diagnostic test</a>, using the human responses and the LM responses separately. We then determine how well these two sets of predictions match. We find cases in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.4.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.starsem-1.4/>A Study on Using Semantic Word Associations to Predict the Success of a Novel</a></strong><br><a href=/people/s/syeda-jannatus-saba/>Syeda Jannatus Saba</a>
|
<a href=/people/b/biddut-sarker-bijoy/>Biddut Sarker Bijoy</a>
|
<a href=/people/h/henry-gorelick/>Henry Gorelick</a>
|
<a href=/people/s/sabir-ismail/>Sabir Ismail</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/m/mohammad-ruhul-amin/>Mohammad Ruhul Amin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--4><div class="card-body p-3 small">Many new books get published every year, and only a fraction of them become popular among the readers. So the prediction of a book success can be a very useful parameter for publishers to make a reliable decision. This article presents the study of semantic word associations using the word embedding of book content for a set of Roget&#8217;s thesaurus concepts for book success prediction. In this work, we discuss the method to represent a book as a spectrum of concepts based on the association score between its content embedding and a global embedding (i.e. fastText) for a set of semantically linked word clusters. We show that the semantic word associations outperform the previous methods for book success prediction. In addition, we present that semantic word associations also provide better results than using features like the frequency of word groups in Roget&#8217;s thesaurus, LIWC (a popular tool for linguistic inquiry and word count), NRC (word association emotion lexicon), and part of speech (PoS). Our study reports that concept associations based on <a href=https://en.wikipedia.org/wiki/Roget&#8217;s_Thesaurus>Roget&#8217;s Thesaurus</a> using word embedding of individual novel resulted in the state-of-the-art performance of 0.89 average weighted F1-score for book success prediction. Finally, we present a set of dominant themes that contribute towards the popularity of a book for a specific genre.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.9.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.9/>ParsFEVER : a Dataset for Farsi Fact Extraction and Verification<span class=acl-fixed-case>P</span>ars<span class=acl-fixed-case>FEVER</span>: a Dataset for <span class=acl-fixed-case>F</span>arsi Fact Extraction and Verification</a></strong><br><a href=/people/m/majid-zarharan/>Majid Zarharan</a>
|
<a href=/people/m/mahsa-ghaderan/>Mahsa Ghaderan</a>
|
<a href=/people/a/amin-pourdabiri/>Amin Pourdabiri</a>
|
<a href=/people/z/zahra-sayedi/>Zahra Sayedi</a>
|
<a href=/people/b/behrouz-minaei-bidgoli/>Behrouz Minaei-Bidgoli</a>
|
<a href=/people/s/sauleh-eetemadi/>Sauleh Eetemadi</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--9><div class="card-body p-3 small">Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER : the first publicly available Farsi dataset for fact extraction and verification. We adopt the <a href=https://en.wikipedia.org/wiki/Formal_grammar>construction procedure</a> of the standard English dataset for the task, i.e., <a href=https://en.wikipedia.org/wiki/Fever>FEVER</a>, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprises nearly 23 K manually-annotated claims. Over 65 % of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13 % of the claims in <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a> are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on ParsFEVER attains similar downstream performance, indicating the quality of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.10/>BiQuAD : Towards <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> based on deeper text understanding<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>AD</span>: Towards <span class=acl-fixed-case>QA</span> based on deeper text understanding</a></strong><br><a href=/people/f/frank-grimm/>Frank Grimm</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--10><div class="card-body p-3 small">Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these <a href=https://en.wikipedia.org/wiki/System>systems</a> are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consist of 4,190 closed-domain texts and a total of 99,149 <a href=https://en.wikipedia.org/wiki/Question_answering>question-answer pairs</a>. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a (logical) model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-the-art models achieve F1-scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a> and aid model introspection when approaching complex text understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.11/>Evaluating Universal Dependency Parser Recovery of Predicate Argument Structure via CompChain Analysis<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parser Recovery of Predicate Argument Structure via <span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>C</span>hain Analysis</a></strong><br><a href=/people/s/sagar-indurkhya/>Sagar Indurkhya</a>
|
<a href=/people/b/beracah-yankama/>Beracah Yankama</a>
|
<a href=/people/r/robert-c-berwick/>Robert C. Berwick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--11><div class="card-body p-3 small">Accurate recovery of predicate-argument structure from a Universal Dependency (UD) parse is central to downstream tasks such as extraction of semantic roles or event representations. This study introduces compchains, a categorization of the hierarchy of predicate dependency relations present within a UD parse. Accuracy of compchain classification serves as a proxy for measuring accurate recovery of predicate-argument structure from sentences with embedding. We analyzed the distribution of compchains in three UD English treebanks, EWT, GUM and LinES, revealing that these treebanks are sparse with respect to sentences with predicate-argument structure that includes predicate-argument embedding. We evaluated the CoNLL 2018 Shared Task UDPipe (v1.2) baseline (dependency parsing) models as compchain classifiers for the EWT, GUMS and LinES UD treebanks. Our results indicate that these three baseline models exhibit poorer performance on sentences with predicate-argument structure with more than one level of embedding ; we used compchains to characterize the errors made by these parsers and present examples of erroneous parses produced by the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> that were identified using compchains. We also analyzed the distribution of compchains in 58 non-English UD treebanks and then used compchains to evaluate the CoNLL&#8217;18 Shared Task baseline model for each of these <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. Our analysis shows that performance with respect to compchain classification is only weakly correlated with the official evaluation metrics (LAS, MLAS and BLEX). We identify gaps in the distribution of compchains in several of the UD treebanks, thus providing a roadmap for how these <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> may be supplemented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.14/>Disentangling Online Chats with DAG-structured LSTMs<span class=acl-fixed-case>DAG</span>-structured <span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/d/duccio-pappadopulo/>Duccio Pappadopulo</a>
|
<a href=/people/l/lisa-bauer/>Lisa Bauer</a>
|
<a href=/people/m/marco-farina/>Marco Farina</a>
|
<a href=/people/o/ozan-irsoy/>Ozan İrsoy</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--14><div class="card-body p-3 small">Many modern <a href=https://en.wikipedia.org/wiki/Message-oriented_middleware>messaging systems</a> allow fast and synchronous textual communication among many users. The resulting sequence of messages hides a more complicated structure in which independent sub-conversations are interwoven with one another. This poses a challenge for any task aiming to understand the content of the <a href=https://en.wikipedia.org/wiki/Chat_log>chat logs</a> or gather information from them. The ability to disentangle these <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> is then tantamount to the success of many downstream <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Structured information accompanying the text such as user turn, user mentions, timestamps, is used as a cue by the participants themselves who need to follow the conversation and has been shown to be important for disentanglement. DAG-LSTMs, a generalization of Tree-LSTMs that can handle directed acyclic dependencies, are a natural way to incorporate such information and its non-sequential nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement task. We perform our experiments on the Ubuntu IRC dataset. We show that the novel model we propose achieves state of the art status on the task of recovering reply-to relations and it is competitive on other disentanglement metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.15/>Toward Diverse Precondition Generation</a></strong><br><a href=/people/h/heeyoung-kwon/>Heeyoung Kwon</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--15><div class="card-body p-3 small">A typical goal for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem : given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> and <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering</a>. Unlike other conditional generation systems, DiP automatically generates <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more <a href=https://en.wikipedia.org/wiki/Precursor_(chemistry)>preconditions</a> than a strong nucleus sampling baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.19/>Incorporating EDS Graph for AMR Parsing<span class=acl-fixed-case>EDS</span> Graph for <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/z/ziyi-shou/>Ziyi Shou</a>
|
<a href=/people/f/fangzhen-lin/>Fangzhen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--19><div class="card-body p-3 small">AMR (Abstract Meaning Representation) and EDS (Elementary Dependency Structures) are two popular meaning representations in NLP / NLU. AMR is more abstract and conceptual, while EDS is more low level, closer to the lexical structures of the given sentences. It is thus not surprising that EDS parsing is easier than AMR parsing. In this work, we consider using information from EDS parsing to help improve the performance of AMR parsing. We adopt a transition-based parser and propose to add EDS graphs as additional semantic features using a graph encoder composed of LSTM layer and GCN layer. Our experimental results show that the additional information from EDS parsing indeed gives a boost to the performance of the base AMR parser used in our experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.21/>Neural Metaphor Detection with Visibility Embeddings</a></strong><br><a href=/people/g/gitit-kehat/>Gitit Kehat</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--21><div class="card-body p-3 small">We present new results for the problem of sequence metaphor labeling, using the recently developed Visibility Embeddings. We show that concatenating such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to the input of a BiLSTM obtains consistent and significant improvements at almost no cost, and we present further improved results when visibility embeddings are combined with BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.22.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.22/>Inducing Language-Agnostic Multilingual Representations</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--22><div class="card-body p-3 small">Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this : (i) re-aligning the vector spaces of target languages (all together) to a pivot source language ; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product ; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approachesunlike <a href=https://en.wikipedia.org/wiki/Vector_normalization>vector normalization</a>, vector space re-alignment and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> do not achieve consistent gains across encoders and languages. Due to the approaches&#8217; additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.25/>Spurious Correlations in Cross-Topic Argument Mining</a></strong><br><a href=/people/t/terne-sasha-thorn-jakobsen/>Terne Sasha Thorn Jakobsen</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--25><div class="card-body p-3 small">Recent work in cross-topic argument mining attempts to learn <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that generalise across topics rather than merely relying on within-topic spurious correlations. We examine the effectiveness of this approach by analysing the output of single-task and multi-task models for cross-topic argument mining, through a combination of linear approximations of their decision boundaries, manual feature grouping, challenge examples, and ablations across the input vocabulary. Surprisingly, we show that cross-topic models still rely mostly on spurious correlations and only generalise within closely related topics, e.g., a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained only on closed-class words and a few common open-class words outperforms a state-of-the-art cross-topic model on distant target topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.27/>Overcoming Poor Word Embeddings with Word Definitions</a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--27><div class="card-body p-3 small">Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could learn to use <a href=https://en.wikipedia.org/wiki/Definition>definitions</a>, provided in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a>, to overcome this handicap. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s understanding of a <a href=https://en.wikipedia.org/wiki/Definition>definition</a> is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>