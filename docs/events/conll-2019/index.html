<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Computational Natural Language Learning (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Computational Natural Language Learning (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#k19-1>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a>
<span class="badge badge-info align-middle ml-1">55&nbsp;papers</span></li><li><a class=align-middle href=#k19-2>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=k19-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/K19-1/>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1000/>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></strong><br><a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1004/>Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings with Token-Level Evaluation</a></strong><br><a href=/people/q/qianchu-liu/>Qianchu Liu</a>
|
<a href=/people/d/diana-mccarthy/>Diana McCarthy</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1004><div class="card-body p-3 small">In this paper, we present a thorough investigation on methods that align pre-trained contextualized embeddings into shared cross-lingual context-aware embedding space, providing strong reference benchmarks for future context-aware crosslingual models. We propose a novel and challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, Bilingual Token-level Sense Retrieval (BTSR). It specifically evaluates the accurate alignment of words with the same meaning in cross-lingual non-parallel contexts, currently not evaluated by existing tasks such as Bilingual Contextual Word Similarity and Sentence Retrieval. We show how the proposed BTSR task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1007/>Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models</a></strong><br><a href=/people/g/grusha-prasad/>Grusha Prasad</a>
|
<a href=/people/m/marten-van-schijndel/>Marten van Schijndel</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1007><div class="card-body p-3 small">Neural language models (LMs) perform well on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that require sensitivity to <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. Drawing on the syntactic priming paradigm from <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, we propose a novel technique to analyze the representations that enable such success. By establishing a gradient similarity metric between structures, this technique allows us to reconstruct the organization of the LMs&#8217; syntactic representational space. We use this technique to demonstrate that LSTM LMs&#8217; representations of different types of sentences with relative clauses are organized hierarchically in a linguistically interpretable manner, suggesting that the LMs track abstract properties of the sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1009.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1009/>Compositional Generalization in Image Captioning</a></strong><br><a href=/people/m/mitja-nikolaus/>Mitja Nikolaus</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/m/matthew-lamm/>Matthew Lamm</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1009><div class="card-body p-3 small">Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> composes unseen combinations of concepts when describing <a href=https://en.wikipedia.org/wiki/Image>images</a>. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and imagesentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1010/>Representing Movie Characters in Dialogues</a></strong><br><a href=/people/m/mahmoud-azab/>Mahmoud Azab</a>
|
<a href=/people/n/noriyuki-kojima/>Noriyuki Kojima</a>
|
<a href=/people/j/jia-deng/>Jia Deng</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1010><div class="card-body p-3 small">We introduce a new embedding model to represent movie characters and their interactions in a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> by encoding in the same representation the language used by these <a href=https://en.wikipedia.org/wiki/Character_(arts)>characters</a> as well as information about the other participants in the dialogue. We evaluate the performance of these new character embeddings on two tasks : (1) character relatedness, using a dataset we introduce consisting of a dense character interaction matrix for 4,378 unique character pairs over 22 hours of dialogue from eighteen movies ; and (2) character relation classification, for fine- and coarse-grained relations, as well as sentiment relations. Our experiments show that our model significantly outperforms the traditional Word2Vec continuous bag-of-words and skip-gram models, demonstrating the effectiveness of the character embeddings we introduce. We further show how these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can be used in conjunction with a visual question answering system to improve over previous results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1014/>Weird Inflects but OK : Making Sense of Morphological Generation Errors<span class=acl-fixed-case>OK</span>: Making Sense of Morphological Generation Errors</a></strong><br><a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/magdalena-markowska/>Magdalena Markowska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1014><div class="card-body p-3 small">We conduct a manual error analysis of the CoNLL-SIGMORPHON Shared Task on Morphological Reinflection. This task involves <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> : systems are given a word in citation form (e.g., hug) and asked to produce the corresponding <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>inflected form</a> (e.g., the simple past hugged). We propose an error taxonomy and use it to annotate errors made by the top two <a href=https://en.wikipedia.org/wiki/List_of_systems_of_plant_taxonomy>systems</a> across twelve languages. Many of the observed errors are related to inflectional patterns sensitive to inherent linguistic properties such as <a href=https://en.wikipedia.org/wiki/Animacy>animacy</a> or <a href=https://en.wikipedia.org/wiki/Affect_(linguistics)>affect</a> ; many others are failures to predict truly unpredictable inflectional behaviors. We also find nearly one quarter of the residual errors reflect errors in the gold data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1015/>Learning to Represent Bilingual Dictionaries</a></strong><br><a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/y/yingtao-tian/>Yingtao Tian</a>
|
<a href=/people/h/haochen-chen/>Haochen Chen</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/s/steven-skiena/>Steven Skiena</a>
|
<a href=/people/c/carlo-zaniolo/>Carlo Zaniolo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1015><div class="card-body p-3 small">Bilingual word embeddings have been widely used to capture the correspondence of lexical semantics in different <a href=https://en.wikipedia.org/wiki/Language>human languages</a>. However, the cross-lingual correspondence between sentences and words is less studied, despite that this correspondence can significantly benefit many applications such as crosslingual semantic search and textual inference. To bridge this gap, we propose a neural embedding model that leverages <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionaries</a>. The proposed model is trained to map the lexical definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. We conduct experiments on two new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In the cross-lingual reverse dictionary retrieval task, we demonstrate that our model is capable of comprehending bilingual concepts based on descriptions, and the proposed learning strategies are effective. In the bilingual paraphrase identification task, we show that our model effectively associates sentences in different languages via a shared embedding space, and outperforms existing approaches in identifying bilingual paraphrases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1016/>Improving <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> by Reverse Mapping Bytepair Encoding</a></strong><br><a href=/people/c/chaodong-tong/>Chaodong Tong</a>
|
<a href=/people/h/huailiang-peng/>Huailiang Peng</a>
|
<a href=/people/q/qiong-dai/>Qiong Dai</a>
|
<a href=/people/l/lei-jiang/>Lei Jiang</a>
|
<a href=/people/j/jianghua-huang/>Jianghua Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1016><div class="card-body p-3 small">We propose a method called reverse mapping bytepair encoding, which maps named-entity information and other word-level linguistic features back to subwords during the encoding procedure of bytepair encoding (BPE). We employ this method to the Generative Pre-trained Transformer (OpenAI GPT) by adding a weighted linear layer after the embedding layer. We also propose a new model architecture named as the multi-channel separate transformer to employ a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a> without parameter-sharing. Evaluation on Stories Cloze, RTE, SciTail and SST-2 datasets demonstrates the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1017.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1017.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1017/>Made for Each Other : Broad-Coverage Semantic Structures Meet Preposition Supersenses</a></strong><br><a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1017><div class="card-body p-3 small">Universal Conceptual Cognitive Annotation (UCCA ; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositions</a>, as formulated by Schneider et al. (2018), is complementary and suitable for integration within <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a>. We show empirically for <a href=https://en.wikipedia.org/wiki/English_language>English</a> that the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>schemes</a>, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1018.Supplementary_Material.zip data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1018/>Generating Timelines by Modeling Semantic Change</a></strong><br><a href=/people/g/guy-d-rosin/>Guy D. Rosin</a>
|
<a href=/people/k/kira-radinsky/>Kira Radinsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1018><div class="card-body p-3 small">Though <a href=https://en.wikipedia.org/wiki/Language>languages</a> can evolve slowly, they can also react strongly to <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>dramatic world events</a>. By studying the connection between words and events, it is possible to identify which events change our vocabulary and in what way. In this work, we tackle the task of creating timelines-records of historical turning points, represented by either words or events, to understand the dynamics of a target word. Our approach identifies these points by leveraging both static and time-varying word embeddings to measure the influence of words and events. In addition to quantifying changes, we show how our technique can help isolate semantic changes. Our qualitative and quantitative evaluations show that we are able to capture this <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> and event influence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1019/>Diversify Your Datasets : Analyzing Generalization via Controlled Variance in Adversarial Datasets</a></strong><br><a href=/people/o/ohad-rozen/>Ohad Rozen</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1019><div class="card-body p-3 small">Phenomenon-specific adversarial datasets have been recently designed to perform targeted stress-tests for particular inference types. Recent work (Liu et al., 2019a) proposed that such datasets can be utilized for training NLI and other types of models, often allowing to learn the phenomenon in focus and improve on the challenge dataset, indicating a blind spot in the original training data. Yet, although a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can improve in such a training process, it might still be vulnerable to other challenge datasets targeting the same phenomenon but drawn from a different distribution, such as having a different syntactic complexity level. In this work, we extend this method to drive conclusions about a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s ability to learn and generalize a target phenomenon rather than to learn a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1020/>Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based on BERT for Identifying Parallel Data<span class=acl-fixed-case>BERT</span> for Identifying Parallel Data</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1020><div class="card-body p-3 small">We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT Bidirectional Encoder Representations from Transformers (Devlin et al., 2019). The goal of crosslingual STS is to measure to what degree two segments of text in different languages express the same meaning. Not only is it a key task in crosslingual natural language understanding (XLU), it is also particularly useful for identifying parallel resources for training and evaluating downstream multilingual natural language processing (NLP) applications, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Most previous crosslingual STS methods relied heavily on existing <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel resources</a>, thus leading to a circular dependency problem. With the advent of massively multilingual context representation models such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1021/>On the Importance of Subword Information for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphological Tasks</a> in Truly Low-Resource Languages</a></strong><br><a href=/people/y/yi-zhu/>Yi Zhu</a>
|
<a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1021><div class="card-body p-3 small">Recent work has validated the importance of subword information for word representation learning. Since <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks : fine-grained entity typing, morphological tagging, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We conduct a systematic study that spans several dimensions of comparison : 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, or both ; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu) ; 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1022/>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a></strong><br><a href=/people/a/austin-matthews/>Austin Matthews</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1022><div class="card-body p-3 small">Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Both models use <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural nets</a> to avoid making explicit <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independence assumptions</a>, but they differ in the order used to construct the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> : one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. While both <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> improve <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance over a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative baseline</a>, they are significantly less effective than non-syntactic LSTM language models. Surprisingly, little difference between the construction orders is observed for either <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> or <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1023/>Representation Learning and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>Dynamic Programming</a> for Arc-Hybrid Parsing</a></strong><br><a href=/people/j/joseph-le-roux/>Joseph Le Roux</a>
|
<a href=/people/a/antoine-rozenknop/>Antoine Rozenknop</a>
|
<a href=/people/m/mathieu-lacroix/>Mathieu Lacroix</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1023><div class="card-body p-3 small">We present a new method for transition-based parsing where a solution is a pair made of a dependency tree and a derivation graph describing the construction of the former. From this representation we are able to derive an efficient <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithm</a> and design a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that learns <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertex representations</a> and arc scores. Experimentally, although we only train via local classifiers, our approach improves over previous arc-hybrid systems and reach state-of-the-art parsing accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1025/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> by Achieving <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>Knowledge Transfer</a> with Sentence Alignment Learning</a></strong><br><a href=/people/x/xuewen-shi/>Xuewen Shi</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/w/wenguan-wang/>Wenguan Wang</a>
|
<a href=/people/p/ping-jian/>Ping Jian</a>
|
<a href=/people/y/yi-kun-tang/>Yi-Kun Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1025><div class="card-body p-3 small">Neural Machine Translation (NMT) optimized by Maximum Likelihood Estimation (MLE) lacks the guarantee of translation adequacy. To alleviate this problem, we propose an NMT approach that heightens the adequacy in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> by transferring the semantic knowledge learned from bilingual sentence alignment. Specifically, we first design a discriminator that learns to estimate sentence aligning score over translation candidates, and then the learned semantic knowledge is transfered to the NMT model under an adversarial learning framework. We also propose a gated self-attention based encoder for <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>. Furthermore, an N-pair training loss is introduced in our framework to aid the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> in better capturing lexical evidence in translation candidates. Experimental results show that our proposed method outperforms baseline NMT models on Chinese-to-English and English-to-German translation tasks. Further analysis also indicates the detailed semantic knowledge transfered from the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> to the NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1026.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1026/>Code-Switched Language Models Using Neural Based Synthetic Data from Parallel Sentences</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1026><div class="card-body p-3 small">Training code-switched language models is difficult due to lack of data and complexity in the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical structure</a>. Linguistic constraint theories have been used for decades to generate artificial code-switching sentences to cope with this issue. However, this require external word alignments or <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituency parsers</a> that create erroneous results on distant languages. We propose a sequence-to-sequence model using a copy mechanism to generate code-switching data by leveraging parallel monolingual translations from a limited source of code-switching data. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns how to combine words from parallel sentences and identifies when to switch one language to the other. Moreover, it captures code-switching constraints by attending and aligning the words in inputs, without requiring any external knowledge. Based on experimental results, the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> trained with the generated sentences achieves state-of-the-art performance and improves end-to-end automatic speech recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1027/>Unsupervised Neural Machine Translation with Future Rewarding</a></strong><br><a href=/people/x/xiangpeng-wei/>Xiangpeng Wei</a>
|
<a href=/people/y/yue-hu/>Yue Hu</a>
|
<a href=/people/l/luxi-xing/>Luxi Xing</a>
|
<a href=/people/l/li-gao/>Li Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1027><div class="card-body p-3 small">In this paper, we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework, which aims to optimize the global word predictions for unsupervised neural machine translation. To this end, we design a novel <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> to characterize high-quality translations from two aspects : n-gram matching and semantic adequacy. The n-gram matching is defined as an alternative for the discrete BLEU metric, and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. During <a href=https://en.wikipedia.org/wiki/Training>training</a>, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. Besides, a variational inference network (VIN) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. On the widely used WMT&#8217;14 English-French, WMT&#8217;16 English-German and NIST Chinese-to-English benchmarks, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> respectively obtain 27.59/27.15, 19.65/23.42 and 22.40 BLEU points without using any labeled data, demonstrating consistent improvements over previous unsupervised NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1028.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1028/>Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural Machine Translation</a></strong><br><a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1028><div class="card-body p-3 small">We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We therefore propose an automatic approach for extracting challenge sets rich with long-distance dependencies, and argue that evaluation using this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> provides a complementary perspective on system performance. To support our claim, we compile challenge sets for <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/German_language>German-English</a>, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1030/>Improving Pre-Trained Multilingual Model with Vocabulary Expansion</a></strong><br><a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1030><div class="card-body p-3 small">Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> for each language in such a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> designed for monolingual settings, we investigate two <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1031/>On the Relation between Position Information and <a href=https://en.wikipedia.org/wiki/Sentence_length>Sentence Length</a> in Neural Machine Translation</a></strong><br><a href=/people/m/masato-neishi/>Masato Neishi</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1031><div class="card-body p-3 small">Long sentences have been one of the major challenges in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. Although some approaches such as the attention mechanism have partially remedied the problem, we found that the current standard NMT model, Transformer, has difficulty in translating long sentences compared to the former standard, Recurrent Neural Network (RNN)-based model. One of the key differences of these NMT models is how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> handles position information which is essential to process sequential data. In this study, we focus on the position information type of NMT models, and hypothesize that relative position is better than absolute position. To examine the hypothesis, we propose RNN-Transformer which replaces positional encoding layer of Transformer by RNN, and then compare RNN-based model and four variants of Transformer. Experiments on ASPEC English-to-Japanese and WMT2014 English-to-German translation tasks demonstrate that relative position helps translating sentences longer than those in the training data. Further experiments on length-controlled training data reveal that absolute position actually causes <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> to the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1032.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1032/>Word Recognition, Competition, and Activation in a Model of Visually Grounded Speech</a></strong><br><a href=/people/w/william-n-havard/>William N. Havard</a>
|
<a href=/people/j/jean-pierre-chevrot/>Jean-Pierre Chevrot</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1032><div class="card-body p-3 small">In this paper, we study how word-like units are represented and activated in a recurrent neural model of visually grounded speech. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used in our experiments is trained to project an <a href=https://en.wikipedia.org/wiki/Image>image</a> and its spoken description in a common representation space. We show that a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent model</a> trained on spoken sentences implicitly segments its input into word-like units and reliably maps them to their correct visual referents. We introduce a methodology originating from <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> to analyse the representation learned by neural networks the gating paradigm and show that the correct representation of a word is only activated if the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> has access to first phoneme of the target word, suggesting that the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> does not rely on a global acoustic pattern. Furthermore, we find out that not all speech frames (MFCC vectors in our case) play an equal role in the final encoded representation of a given word, but that some frames have a crucial effect on it. Finally we suggest that word representation could be activated through a process of lexical competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1034/>Linguistic Analysis Improves Neural Metaphor Detection</a></strong><br><a href=/people/k/kevin-stowe/>Kevin Stowe</a>
|
<a href=/people/s/sarah-moeller/>Sarah Moeller</a>
|
<a href=/people/l/laura-michaelis/>Laura Michaelis</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1034><div class="card-body p-3 small">In the field of metaphor detection, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a> are the ubiquitous and achieve strong performance on many tasks. However, due to the complicated procedures for manually identifying metaphors, the datasets available are relatively small and fraught with complications. We show that using syntactic features and lexical resources can automatically provide additional high-quality training data for metaphoric language, and this <a href=https://en.wikipedia.org/wiki/Data>data</a> can cover gaps and inconsistencies in metaphor annotation, improving state-of-the-art word-level metaphor identification. This novel application of automatically improving training data improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> across numerous tasks, and reconfirms the necessity of high-quality data for deep learning frameworks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1036/>A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification</a></strong><br><a href=/people/r/ruizhe-li/>Ruizhe Li</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/m/matthew-collinson/>Matthew Collinson</a>
|
<a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/g/guanyi-chen/>Guanyi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1036><div class="card-body p-3 small">Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dual-attention hierarchical recurrent neural network for DA classification. Our model is partially inspired by the observation that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between DAs and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both DAs and topics, as well as information about the interactions between them. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification, yielding better or comparable performance to the state-of-the-art method on three public datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1037.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1037/>Mimic and Rephrase : Reflective Listening in Open-Ended Dialogue</a></strong><br><a href=/people/j/justin-dieter/>Justin Dieter</a>
|
<a href=/people/t/tian-wang/>Tian Wang</a>
|
<a href=/people/a/arun-tejasvi-chaganty/>Arun Tejasvi Chaganty</a>
|
<a href=/people/g/gabor-angeli/>Gabor Angeli</a>
|
<a href=/people/a/angel-chang/>Angel X. Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1037><div class="card-body p-3 small">Reflective listeningdemonstrating that you have heard your conversational partneris key to effective <a href=https://en.wikipedia.org/wiki/Communication>communication</a>. Expert human communicators often mimic and rephrase their conversational partner, e.g., when responding to sentimental stories or to questions they do n&#8217;t know the answer to. We introduce a new task and an associated <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> wherein dialogue agents similarly mimic and rephrase a user&#8217;s request to communicate sympathy (I&#8217;m sorry to hear that) or lack of knowledge (I do not know that). We study what makes a rephrasal response good against a set of qualitative metrics. We then evaluate three models for generating responses : a syntax-aware rule-based system, a seq2seq LSTM neural models with attention (S2SA), and the same neural model augmented with a copy mechanism (S2SA+C). In a human evaluation, we find that S2SA+C and the <a href=https://en.wikipedia.org/wiki/Rule-based_system>rule-based system</a> are comparable and approach human-generated response quality. In addition, experiences with a live deployment of S2SA+C in a customer support setting suggest that this generation task is a practical contribution to real world conversational agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1040.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1040/>Leveraging Past References for Robust Language Grounding</a></strong><br><a href=/people/s/subhro-roy/>Subhro Roy</a>
|
<a href=/people/m/michael-noseworthy/>Michael Noseworthy</a>
|
<a href=/people/r/rohan-paul/>Rohan Paul</a>
|
<a href=/people/d/daehyung-park/>Daehyung Park</a>
|
<a href=/people/n/nicholas-roy/>Nicholas Roy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1040><div class="card-body p-3 small">Grounding referring expressions to objects in an environment has traditionally been considered a one-off, ahistorical task. However, in realistic applications of grounding, multiple users will repeatedly refer to the same set of objects. As a result, past referring expressions for objects can provide strong signals for grounding subsequent <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expressions</a>. We therefore reframe the <a href=https://en.wikipedia.org/wiki/Grounding_problem>grounding problem</a> from the perspective of coreference detection and propose a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that detects when two expressions are referring to the same object. The <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> combines information from <a href=https://en.wikipedia.org/wiki/Computer_vision>vision</a> and past referring expressions to resolve which object is being referred to. Our experiments show that detecting referring expression coreference is an effective way to ground objects described by subtle visual properties, which standard visual grounding models have difficulty capturing. We also show the ability to detect object coreference allows the grounding model to perform well even when it encounters object categories not seen in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1041/>Procedural Reasoning Networks for Understanding Multimodal Procedures</a></strong><br><a href=/people/m/mustafa-sercan-amac/>Mustafa Sercan Amac</a>
|
<a href=/people/s/semih-yagcioglu/>Semih Yagcioglu</a>
|
<a href=/people/a/aykut-erdem/>Aykut Erdem</a>
|
<a href=/people/e/erkut-erdem/>Erkut Erdem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1041><div class="card-body p-3 small">This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> and explore the question of how <a href=https://en.wikipedia.org/wiki/Multimodality>multimodality</a> can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns effective dynamic representations of entities even though we do not use any <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> at the level of entity states.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1042/>On the Limits of Learning to Actively Learn Semantic Representations</a></strong><br><a href=/people/o/omri-koshorek/>Omri Koshorek</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/y/yichu-zhou/>Yichu Zhou</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1042><div class="card-body p-3 small">One of the goals of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> is to develop <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that map sentences into meaning representations. However, training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> requires expensive annotation of complex structures, which hinders their adoption. Learning to actively-learn(LTAL) is a recent paradigm for reducing the amount of labeled data by learning a policy that selects which samples should be labeled. In this work, we examine <a href=https://en.wikipedia.org/wiki/LTAL>LTAL</a> for learning semantic representations, such as QA-SRL. We show that even an oracle policy that is allowed to pick examples that maximize performance on the test set (and constitutes an upper bound on the potential of LTAL), does not substantially improve performance compared to a random policy. We investigate factors that could explain this finding and show that a distinguishing characteristic of successful applications of LTAL is the interaction between <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> and the oracle policy selection process. In successful applications of LTAL, the examples selected by the oracle policy do not substantially depend on the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a>, while in our setup the stochastic nature of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> in learning semantic meaning representations is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1043.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1043/>How Does <a href=https://en.wikipedia.org/wiki/Grammatical_gender>Grammatical Gender</a> Affect Noun Representations in Gender-Marking Languages?</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1043><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> assign <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a> also to inanimate nouns in the language. In such <a href=https://en.wikipedia.org/wiki/Language>languages</a>, words that relate to the gender-marked nouns are inflected to agree with the noun&#8217;s gender. We show that this affects the word representations of inanimate nouns, resulting in <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> with the same gender being closer to each other than nouns with different gender. While embedding debiasing methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words&#8217; context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1047/>Detecting Frames in <a href=https://en.wikipedia.org/wiki/Headline>News Headlines</a> and Its Application to Analyzing News Framing Trends Surrounding U.S. Gun Violence<span class=acl-fixed-case>U</span>.<span class=acl-fixed-case>S</span>. Gun Violence</a></strong><br><a href=/people/s/siyi-liu/>Siyi Liu</a>
|
<a href=/people/l/lei-guo/>Lei Guo</a>
|
<a href=/people/k/kate-mays/>Kate Mays</a>
|
<a href=/people/m/margrit-betke/>Margrit Betke</a>
|
<a href=/people/d/derry-tanti-wijaya/>Derry Tanti Wijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1047><div class="card-body p-3 small">Different news articles about the same topic often offer a variety of perspectives : an article written about <a href=https://en.wikipedia.org/wiki/Gun_violence>gun violence</a> might emphasize <a href=https://en.wikipedia.org/wiki/Gun_control>gun control</a>, while another might promote <a href=https://en.wikipedia.org/wiki/Second_Amendment_to_the_United_States_Constitution>2nd Amendment rights</a>, and yet a third might focus on <a href=https://en.wikipedia.org/wiki/Mental_health>mental health issues</a>. In <a href=https://en.wikipedia.org/wiki/Communication_studies>communication research</a>, these different perspectives are known as <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a>, which, when used in <a href=https://en.wikipedia.org/wiki/News_media>news media</a> will influence the opinion of their readers in multiple ways. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for effectively detecting <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> in <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a>. Our training and performance evaluation is based on a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> related to the issue of <a href=https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States>gun violence</a> in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. Our proposed approach sets a new state-of-the-art performance for multiclass news frame detection, significantly outperforming a recent <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 35.9 % absolute difference in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We apply our frame detection approach in a large scale study of 88k <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> about the coverage of <a href=https://en.wikipedia.org/wiki/Gun_violence>gun violence</a> in the U.S. between 2016 and 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1049/>Learning Dense Representations for Entity Retrieval</a></strong><br><a href=/people/d/dan-gillick/>Daniel Gillick</a>
|
<a href=/people/s/sayali-kulkarni/>Sayali Kulkarni</a>
|
<a href=/people/l/larry-lansing/>Larry Lansing</a>
|
<a href=/people/a/alessandro-presta/>Alessandro Presta</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/e/eugene-ie/>Eugene Ie</a>
|
<a href=/people/d/diego-garcia-olano/>Diego Garcia-Olano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1049><div class="card-body p-3 small">We show that it is feasible to perform <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can retrieve candidates extremely fast, and generalizes well to a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> derived from <a href=https://en.wikipedia.org/wiki/Wikinews>Wikinews</a>. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1051/>KnowSemLM : A Knowledge Infused Semantic Language Model<span class=acl-fixed-case>K</span>now<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>LM</span>: A Knowledge Infused Semantic Language Model</a></strong><br><a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1051><div class="card-body p-3 small">Story understanding requires developing expectations of what events come next in text. Prior knowledge both statistical and declarative is essential in guiding such expectations. While existing semantic language models (SemLM) capture event co-occurrence information by modeling event sequences as <a href=https://en.wikipedia.org/wiki/Semantic_frame>semantic frames</a>, entities, and other semantic units, this paper aims at augmenting them with causal knowledge (i.e., one event is likely to lead to another). Such <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> is modeled at the frame and entity level, and can be obtained either statistically from text or stated declaratively. The proposed method, KnowSemLM, infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story / referent prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1052" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1052/>Neural Attentive Bag-of-Entities Model for Text Classification</a></strong><br><a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1052><div class="card-body p-3 small">This study proposes a Neural Attentive Bag-of-Entities model, which is a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that performs text classification using entities in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Entities provide unambiguous and relevant semantic signals that are beneficial for text classification. We combine simple high-recall entity detection based on a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using two standard <a href=https://en.wikipedia.org/wiki/Text_classification>text classification datasets</a> (i.e., the 20 <a href=https://en.wikipedia.org/wiki/Usenet_newsgroup>Newsgroups</a> and R8 datasets) and a popular factoid question answering dataset based on a <a href=https://en.wikipedia.org/wiki/Quiz>trivia quiz game</a>. As a result, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1055/>MrMep : Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet Attention<span class=acl-fixed-case>M</span>r<span class=acl-fixed-case>M</span>ep: Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet Attention</a></strong><br><a href=/people/j/jiayu-chen/>Jiayu Chen</a>
|
<a href=/people/c/caixia-yuan/>Caixia Yuan</a>
|
<a href=/people/x/xiaojie-wang/>Xiaojie Wang</a>
|
<a href=/people/z/ziwei-bai/>Ziwei Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1055><div class="card-body p-3 small">This paper focuses on how to extract multiple relational facts from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Neural encoder-decoder models have provided a viable new approach for jointly extracting relations and entity pairs. However, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> either fail to deal with entity overlapping among relational facts, or neglect to produce the whole entity pairs. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> that augments the encoder and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> in two elegant ways. First, we apply a binary CNN classifier for each relation, which identifies all possible relations maintained in the text, while retaining the target relation representation to aid <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair recognition</a>. Second, we perform a multi-head attention over the text and a triplet attention with the target relation interacting with every token of the text to precisely produce all possible entity pairs in a sequential manner. Experiments on three benchmark datasets show that our proposed method successfully addresses the multiple relations and multiple entity pairs even with complex overlapping and significantly outperforms the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1056/>Effective Attention Modeling for Neural Relation Extraction</a></strong><br><a href=/people/t/tapas-nayak/>Tapas Nayak</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1056><div class="card-body p-3 small">Relation extraction is the task of determining the relation between two entities in a sentence. Distantly-supervised models are popular for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, sentences can be long and two entities can be located far from each other in a sentence. The pieces of evidence supporting the presence of a relation between two entities may not be very direct, since the entities may be connected via some indirect links such as a third entity or via <a href=https://en.wikipedia.org/wiki/Co-reference>co-reference</a>. Relation extraction in such scenarios becomes more challenging as we need to capture the long-distance interactions among the entities and other words in the sentence. Also, the words in a sentence do not contribute equally in identifying the relation between the two entities. To address this issue, we propose a novel and effective attention model which incorporates syntactic information of the sentence and a multi-factor attention mechanism. Experiments on the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times corpus</a> show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms prior state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1057/>Exploiting the Entity Type Sequence to Benefit Event Detection</a></strong><br><a href=/people/y/yuze-ji/>Yuze Ji</a>
|
<a href=/people/y/youfang-lin/>Youfang Lin</a>
|
<a href=/people/j/jianwei-gao/>Jianwei Gao</a>
|
<a href=/people/h/huaiyu-wan/>Huaiyu Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1057><div class="card-body p-3 small">Event Detection (ED) is one of the most important task in the field of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. The goal of ED is to find triggers in sentences and classify them into different <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event types</a>. In previous works, the information of entity types are commonly utilized to benefit event detection. However, the sequential features of entity types have not been well utilized yet in the existing ED methods. In this paper, we propose a novel ED approach which learns sequential features from word sequences and entity type sequences separately, and combines these two types of sequential features with the help of a trigger-entity interaction learning module. The experimental results demonstrate that our proposed approach outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1060/>Named Entity Recognition with Partially Annotated Training Data</a></strong><br><a href=/people/s/stephen-mayhew/>Stephen Mayhew</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/c/chen-tse-tsai/>Chen-Tse Tsai</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1060><div class="card-body p-3 small">Supervised machine learning assumes the availability of fully-labeled data, but in many cases, such as low-resource languages, the only data available is partially annotated. We study the problem of Named Entity Recognition (NER) with partially annotated training data in which a fraction of the named entities are labeled, and all other tokens, entities or otherwise, are labeled as non-entity by default. In order to train on this <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy dataset</a>, we need to distinguish between the true and false negatives. To this end, we introduce a constraint-driven iterative algorithm that learns to detect false negatives in the noisy set and downweigh them, resulting in a weighted training set. With this <a href=https://en.wikipedia.org/wiki/Set_(mathematics)>set</a>, we train a weighted NER model. We evaluate our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> with weighted variants of neural and non-neural NER models on data in 8 languages from several language and script families, showing strong ability to learn from partial data. Finally, to show real-world efficacy, we evaluate on a Bengali NER corpus annotated by non-speakers, outperforming the prior <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by over 5 points <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1062" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1062/>Deep Structured Neural Network for Event Temporal Relation Extraction</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/i/i-hung-hsu/>I-Hung Hsu</a>
|
<a href=/people/m/mu-yang/>Mu Yang</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/r/ralph-weischedel/>Ralph Weischedel</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1062><div class="card-body p-3 small">We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> to learn scoring functions for pair-wise relations, and 2) a <a href=https://en.wikipedia.org/wiki/Structured_support_vector_machine>structured support vector machine (SSVM)</a> to make joint predictions. The <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> combines the benefits of both <a href=https://en.wikipedia.org/wiki/Data-driven_learning>data-driven learning</a> and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1068/>Memory Graph Networks for Explainable Memory-grounded Question Answering</a></strong><br><a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1068><div class="card-body p-3 small">We introduce Episodic Memory QA, the task of answering personal user questions grounded on memory graph (MG), where <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memories</a> and related entity nodes are connected via relational edges. We create a new benchmark dataset first by generating synthetic memory graphs with simulated attributes, and by composing 100 K QA pairs for the generated MG with bootstrapped scripts. To address the unique challenges for the proposed task, we propose Memory Graph Networks (MGN), a novel extension of memory networks to enable dynamic expansion of memory slots through graph traversals, thus able to answer queries in which contexts from multiple linked episodes and external knowledge are required. We then propose the Episodic Memory QA Net with multiple module networks to effectively handle various question types. Empirical results show improvement over the QA baselines in top-k answer prediction accuracy in the proposed task. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also generates a graph walk path and attention vectors for each predicted answer, providing a natural way to explain its QA reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1073.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1073/>TILM : Neural Language Models with Evolving Topical Influence<span class=acl-fixed-case>TILM</span>: Neural Language Models with Evolving Topical Influence</a></strong><br><a href=/people/s/shubhra-kanti-karmaker-santu/>Shubhra Kanti Karmaker Santu</a>
|
<a href=/people/k/kalyan-veeramachaneni/>Kalyan Veeramachaneni</a>
|
<a href=/people/c/chengxiang-zhai/>Chengxiang Zhai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1073><div class="card-body p-3 small">Content of text data are often influenced by <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual factors</a> which often evolve over time (e.g., content of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> are often influenced by topics covered in the major news streams). Existing <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> do not consider the influence of such related evolving topics, and thus are not optimal. In this paper, we propose to incorporate such topical-influence into a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to both improve its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and enable cross-stream analysis of topical influences. Specifically, we propose a novel <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> called Topical Influence Language Model (TILM), which is a novel extension of a neural language model to capture the influences on the contents in one text stream by the evolving topics in another related (or possibly same) text stream. Experimental results on six different text stream data comprised of conference paper titles show that the incorporation of evolving topical influence into a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> is beneficial and TILM outperforms multiple baselines in a challenging task of text forecasting. In addition to serving as a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, TILM further enables interesting analysis of topical influence among multiple text streams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1074/>Pretraining-Based Natural Language Generation for Text Summarization</a></strong><br><a href=/people/h/haoyu-zhang/>Haoyu Zhang</a>
|
<a href=/people/j/jingjing-cai/>Jingjing Cai</a>
|
<a href=/people/j/jianjun-xu/>Jianjun Xu</a>
|
<a href=/people/j/ji-wang/>Ji Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1074><div class="card-body p-3 small">In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we encode the input sequence into context representations using BERT. For the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on the text summarization task. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on both CNN / Daily Mail and New York Times datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1075.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1075/>Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue Generation</a></strong><br><a href=/people/y/yi-an-lai/>Yi-An Lai</a>
|
<a href=/people/a/arshit-gupta/>Arshit Gupta</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1075><div class="card-body p-3 small">Hierarchical neural networks are often used to model inherent structures within dialogues. For goal-oriented dialogues, these models miss a mechanism adhering to the goals and neglect the distinct conversational patterns between two interlocutors. In this work, we propose Goal-Embedded Dual Hierarchical Attentional Encoder-Decoder (G-DuHA) able to center around goals and capture interlocutor-level disparity while modeling goal-oriented dialogues. Experiments on dialogue generation, response generation, and human evaluations demonstrate that the proposed model successfully generates higher-quality, more diverse and goal-centric dialogues. Moreover, we apply <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> via goal-oriented dialogue generation for task-oriented dialog systems with better performance achieved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1077.Supplementary_Material.zip data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1077" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1077/>In Conclusion Not Repetition : Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/n/natalia-vanetik/>Natalia Vanetik</a>
|
<a href=/people/z/zuying-huang/>Zuying Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1077><div class="card-body p-3 small">Various Seq2Seq learning models designed for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> were applied for abstractive summarization task recently. Despite these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provide high ROUGE scores, they are limited to generate comprehensive summaries with a high level of abstraction due to its degenerated <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution</a>. We introduce Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) using Determinantal Point Processes methods(Micro DPPs and Macro DPPs) to produce attention distribution considering both quality and diversity. Without breaking the <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end to end architecture</a>, DivCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla models and strong baselines. All the reproducible codes and datasets are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1081.Supplementary_Material.zip data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1081/>BIOfid Dataset : Publishing a German Gold Standard for Named Entity Recognition in Historical Biodiversity Literature<span class=acl-fixed-case>BIO</span>fid Dataset: Publishing a <span class=acl-fixed-case>G</span>erman Gold Standard for Named Entity Recognition in Historical Biodiversity Literature</a></strong><br><a href=/people/s/sajawel-ahmed/>Sajawel Ahmed</a>
|
<a href=/people/m/manuel-stoeckel/>Manuel Stoeckel</a>
|
<a href=/people/c/christine-driller/>Christine Driller</a>
|
<a href=/people/a/adrian-pachzelt/>Adrian Pachzelt</a>
|
<a href=/people/a/alexander-mehler/>Alexander Mehler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1081><div class="card-body p-3 small">The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> from historical scientific literature on the <a href=https://en.wikipedia.org/wiki/Biodiversity>biodiversity</a> of plants, birds, <a href=https://en.wikipedia.org/wiki/Moth>moths</a> and <a href=https://en.wikipedia.org/wiki/Butterfly>butterflies</a>. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this <a href=https://en.wikipedia.org/wiki/Data>data</a> to train a number of leading <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning tools</a> and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1083" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1083/>Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes</a></strong><br><a href=/people/n/noemien-kocher/>Noémien Kocher</a>
|
<a href=/people/c/christian-scuito/>Christian Scuito</a>
|
<a href=/people/l/lorenzo-tarantino/>Lorenzo Tarantino</a>
|
<a href=/people/a/alexandros-lazaridis/>Alexandros Lazaridis</a>
|
<a href=/people/a/andreas-fischer/>Andreas Fischer</a>
|
<a href=/people/c/claudiu-musat/>Claudiu Musat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1083><div class="card-body p-3 small">In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order informationAlleviated TOIby iteratively overlapping the token composition of data points. For recurrent networks, we use <a href=https://en.wikipedia.org/wiki/Prime_number>prime numbers</a> for the <a href=https://en.wikipedia.org/wiki/Batch_processing>batch size</a> to avoid <a href=https://en.wikipedia.org/wiki/Data_redundancy>redundancies</a> when building batches from overlapped data points. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieved state of the art performance in both text and speech related tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1084.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1084.Supplementary_Material.pdf data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1084/>Global Autoregressive Models for Data-Efficient Sequence Learning</a></strong><br><a href=/people/t/tetiana-parshakova/>Tetiana Parshakova</a>
|
<a href=/people/j/jean-marc-andreoli/>Jean-Marc Andreoli</a>
|
<a href=/people/m/marc-dymetman/>Marc Dymetman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1084><div class="card-body p-3 small">Standard autoregressive seq2seq models are easily trained by max-likelihood, but tend to show poor results under small-data conditions. We introduce a class of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive component with a log-linear component, allowing the use of global a priori features to compensate for lack of data. We train these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in two steps. In the first step, we obtain an unnormalized GAM that maximizes the likelihood of the data, but is improper for fast inference or <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. In the second step, we use this GAM to train (by distillation) a second <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive model</a> that approximates the normalized distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive model</a> over the standard one.<i>a priori</i> features to compensate for lack of data. We train these models in two steps. In the first step, we obtain an <i>unnormalized</i> GAM that maximizes the likelihood of the data, but is improper for fast inference or evaluation. In the second step, we use this GAM to train (by distillation) a second autoregressive model that approximates the <i>normalized</i> distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second autoregressive model over the standard one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1085/>Learning Analogy-Preserving Sentence Embeddings for Answer Selection</a></strong><br><a href=/people/a/aissatou-diallo/>Aïssatou Diallo</a>
|
<a href=/people/m/markus-zopf/>Markus Zopf</a>
|
<a href=/people/j/johannes-furnkranz/>Johannes Fürnkranz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1085><div class="card-body p-3 small">Answer selection aims at identifying the correct answer for a given question from a set of potentially correct answers. Contrary to previous works, which typically focus on the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between a question and its answer, our hypothesis is that question-answer pairs are often in <a href=https://en.wikipedia.org/wiki/Analogy>analogical relation</a> to each other. Using <a href=https://en.wikipedia.org/wiki/Analogy>analogical inference</a> as our use case, we propose a framework and a neural network architecture for learning dedicated sentence embeddings that preserve analogical properties in the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>. We evaluate the proposed method on benchmark datasets for answer selection and demonstrate that our sentence embeddings indeed capture analogical properties better than conventional embeddings, and that analogy-based question answering outperforms a comparable similarity-based technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1086/>A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models</a></strong><br><a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1086><div class="card-body p-3 small">We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a>. The resultant <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can be seen as a combination of character-aware language model and simple word-level language model. Our <a href=https://en.wikipedia.org/wiki/Injection_(medicine)>injection method</a> can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these <a href=https://en.wikipedia.org/wiki/Injection_(medicine)>injection methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1087.Supplementary_Material.zip data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1087.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1087/>On Model Stability as a Function of Random Seed</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/r/rishabh-jain/>Rishabh Jain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1087><div class="card-body p-3 small">In this paper, we focus on quantifying <a href=https://en.wikipedia.org/wiki/Mathematical_model>model stability</a> as a function of random seed by investigating the effects of the induced randomness on <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance and the robustness of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in general. We specifically perform a controlled study on the effect of <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a> on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations. Our analysis suggests that <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a> can adversely affect the consistency of models resulting in <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual interpretations</a>. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, on average reducing the standard deviation of the model&#8217;s performance by 72 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1088/>Studying Generalisability across Abusive Language Detection Datasets</a></strong><br><a href=/people/s/steve-durairaj-swamy/>Steve Durairaj Swamy</a>
|
<a href=/people/a/anupam-jamatia/>Anupam Jamatia</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1088><div class="card-body p-3 small">Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1089/>Reduce & Attribute : Two-Step Authorship Attribution for Large-Scale Problems</a></strong><br><a href=/people/m/michael-tschuggnall/>Michael Tschuggnall</a>
|
<a href=/people/b/benjamin-murauer/>Benjamin Murauer</a>
|
<a href=/people/g/gunther-specht/>Günther Specht</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1089><div class="card-body p-3 small">Authorship attribution is an active research area which has been prevalent for many decades. Nevertheless, the majority of approaches consider problem sizes of a few candidate authors only, making them difficult to apply to recent scenarios incorporating thousands of authors emerging due to the manifold means to digitally share text. In this study, we focus on such large-scale problems and propose to effectively reduce the number of candidate authors before applying common attribution techniques. By utilizing document embeddings, we show on a novel, comprehensive dataset collection that the set of candidate authors can be reduced with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Moreover, we show that common authorship attribution methods substantially benefit from a preliminary reduction if thousands of authors are involved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1093.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1093.Supplementary_Material.zip data-toggle=tooltip data-placement=top title="Supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-1093/>A Personalized Sentiment Model with Textual and Contextual Information</a></strong><br><a href=/people/s/siwen-guo/>Siwen Guo</a>
|
<a href=/people/s/sviatlana-hohn/>Sviatlana Höhn</a>
|
<a href=/people/c/christoph-schommer/>Christoph Schommer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1093><div class="card-body p-3 small">In this paper, we look beyond the traditional population-level sentiment modeling and consider the individuality in a person&#8217;s expressions by discovering both textual and contextual information. In particular, we construct a hierarchical neural network that leverages valuable information from a person&#8217;s past expressions, and offer a better understanding of the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> from the expresser&#8217;s perspective. Additionally, we investigate how a person&#8217;s sentiment changes over time so that recent incidents or opinions may have more effect on the person&#8217;s current sentiment than the old ones. Psychological studies have also shown that individual variation exists in how easily people change their sentiments. In order to model such traits, we develop a modified <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> with Hawkes process applied on top of a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> for a user-specific design. Implemented with automatically labeled Twitter data, the proposed model has shown positive results employing different input formulations for representing the concerned information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1094/>Cluster-Gated Convolutional Neural Network for Short Text Classification</a></strong><br><a href=/people/h/haidong-zhang/>Haidong Zhang</a>
|
<a href=/people/w/wancheng-ni/>Wancheng Ni</a>
|
<a href=/people/m/meijing-zhao/>Meijing Zhao</a>
|
<a href=/people/z/ziqi-lin/>Ziqi Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1094><div class="card-body p-3 small">Text classification plays a crucial role for understanding natural language in a wide range of applications. Most existing <a href=https://en.wikipedia.org/wiki/Comparison_and_contrast_of_classification_schemes_in_linguistics_and_metadata>approaches</a> mainly focus on long text classification (e.g., <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, <a href=https://en.wikipedia.org/wiki/Document>documents</a>, paragraphs). However, they can not easily be applied to <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>short text</a> because of its sparsity and lack of context. In this paper, we propose a new model called cluster-gated convolutional neural network (CGCNN), which jointly explores word-level clustering and text classification in an end-to-end manner. Specifically, the proposed model firstly uses a bi-directional long short-term memory to learn word representations. Then, it leverages a soft clustering method to explore their semantic relation with the cluster centers, and takes <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> on text representations. It develops a cluster-dependent gated convolutional layer to further control the cluster-dependent feature flows. Experimental results on five commonly used datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-1096.Attachment.txt data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K19-1096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K19-1096/>Predicting the Role of Political Trolls in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/a/atanas-atanasov/>Atanas Atanasov</a>
|
<a href=/people/g/gianmarco-de-francisci-morales/>Gianmarco De Francisci Morales</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1096><div class="card-body p-3 small">We investigate the political roles of <a href=https://en.wikipedia.org/wiki/Internet_troll>Internet trolls</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> by using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> in a realistic setting. In particular, we show how to classify <a href=https://en.wikipedia.org/wiki/Internet_troll>trolls</a> according to their political role left, <a href=https://en.wikipedia.org/wiki/Web_feed>news feed</a>, right by using features extracted from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, i.e., <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, in two scenarios : (i) in a traditional supervised learning scenario, where labels for <a href=https://en.wikipedia.org/wiki/Internet_troll>trolls</a> are available, and (ii) in a distant supervision scenario, where labels for <a href=https://en.wikipedia.org/wiki/Internet_troll>trolls</a> are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, from which we extract several types of learned representations, i.e., <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, for the <a href=https://en.wikipedia.org/wiki/Internet_troll>trolls</a>. Experiments on the IRA Russian Troll dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1097/>Towards a Unified End-to-End Approach for Fully Unsupervised Cross-Lingual Sentiment Analysis</a></strong><br><a href=/people/y/yanlin-feng/>Yanlin Feng</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1097><div class="card-body p-3 small">Sentiment analysis in low-resource languages suffers from the lack of training data. Cross-lingual sentiment analysis (CLSA) aims to improve the performance on these <a href=https://en.wikipedia.org/wiki/Language>languages</a> by leveraging annotated data from other languages. Recent studies have shown that CLSA can be performed in a fully unsupervised manner, without exploiting either target language supervision or cross-lingual supervision. However, these methods rely heavily on unsupervised cross-lingual word embeddings (CLWE), which has been shown to have serious drawbacks on distant language pairs (e.g. English-Japanese). In this paper, we propose an end-to-end CLSA model by leveraging unlabeled data in multiple languages and multiple domains and eliminate the need for unsupervised CLWE. Our model applies to two CLSA settings : the traditional cross-lingual in-domain setting and the more challenging cross-lingual cross-domain setting. We empirically evaluate our approach on the multilingual multi-domain Amazon review dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baselines by a large margin despite its minimal resource requirement.</div></div></div><hr><div id=k19-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/K19-2/>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2000/>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</a></strong><br><a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-2001.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-2001/>MRP 2019 : Cross-Framework Meaning Representation Parsing<span class=acl-fixed-case>MRP</span> 2019: Cross-Framework Meaning Representation Parsing</a></strong><br><a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/jayeol-chun/>Jayeol Chun</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/z/zdenka-uresova/>Zdenka Uresova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2001><div class="card-body p-3 small">The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a> were represented in the training and evaluation data for the task, packaged in a uniform abstract graph representation and <a href=https://en.wikipedia.org/wiki/Serialization>serialization</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of additional training data, or involved one of the task co-organizers. All technical information regarding the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, including system submissions, official results, and links to supporting resources and software are available from the task web site at : http://mrp.nlpl.eu</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2003/>The ERG at MRP 2019 : Radically Compositional Semantic Dependencies<span class=acl-fixed-case>ERG</span> at <span class=acl-fixed-case>MRP</span> 2019: Radically Compositional Semantic Dependencies</a></strong><br><a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/d/dan-flickinger/>Dan Flickinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2003><div class="card-body p-3 small">The English Resource Grammar (ERG) is a broad-coverage computational grammar of English that outputs underspecified logical-form representations of meaning in a framework dubbed English Resource Semantics (ERS). Two of the target representations in the the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019) derive graph-based simplifications of ERS, viz. Elementary Dependency Structures (EDS) and DELPH-IN MRS Bi-Lexical Dependencies (DM). As a point of reference outside the official MRP competition, we parsed the evaluation strings using the ERG and converted the resulting meaning representations to EDS and DM. These graphs yield higher evaluation scores than the purely data-driven parsers in the actual shared task, suggesting that the general-purpose linguistic knowledge about <a href=https://en.wikipedia.org/wiki/English_grammar>English grammar</a> encoded in the ERG can add value when parsing into these meaning representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2004/>SJTU-NICT at MRP 2019 : Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NICT</span> at <span class=acl-fixed-case>MRP</span> 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2004><div class="card-body p-3 small">This paper describes our SJTU-NICT&#8217;s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). Our <a href=https://en.wikipedia.org/wiki/System>system</a> uses a graph-based approach to model a variety of semantic graph parsing tasks. Our main contributions in the submitted <a href=https://en.wikipedia.org/wiki/System>system</a> are summarized as follows : 1. Our model is fully end-to-end and is capable of being trained only on the given training set which does not rely on any other extra training source including the companion data provided by the organizer ; 2. We extend our graph pruning algorithm to a variety of semantic graphs, solving the problem of excessive semantic graph search space ; 3. We introduce <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for multiple objectives within the same <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. The evaluation results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved second place in the overall <a href=https://en.wikipedia.org/wiki/Grading_in_education>F_1 score</a> and achieved the best <a href=https://en.wikipedia.org/wiki/Grading_in_education>F_1 score</a> on the DM framework.<tex-math>F_1</tex-math> score and achieved the best <tex-math>F_1</tex-math> score on the DM framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-2010.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-2010/>CUHK at MRP 2019 : Transition-Based Parser with Cross-Framework Variable-Arity Resolve Action<span class=acl-fixed-case>CUHK</span> at <span class=acl-fixed-case>MRP</span> 2019: Transition-Based Parser with Cross-Framework Variable-Arity Resolve Action</a></strong><br><a href=/people/s/sunny-lai/>Sunny Lai</a>
|
<a href=/people/c/chun-hei-lo/>Chun Hei Lo</a>
|
<a href=/people/k/kwong-sak-leung/>Kwong Sak Leung</a>
|
<a href=/people/y/yee-leung/>Yee Leung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2010><div class="card-body p-3 small">This paper describes our system (RESOLVER) submitted to the CoNLL 2019 shared task on Cross-Framework Meaning Representation Parsing (MRP). Our system implements a transition-based parser with a <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>directed acyclic graph (DAG)</a> to tree preprocessor and a novel cross-framework variable-arity resolve action that generalizes over five different representations. Although we ranked low in the competition, we have shown the current limitations and potentials of including variable-arity action in MRP and concluded with directions for improvements in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2011/>Hitachi at MRP 2019 : Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing<span class=acl-fixed-case>MRP</span> 2019: Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing</a></strong><br><a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/k/kohsuke-yanai/>Kohsuke Yanai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2011><div class="card-body p-3 small">This paper describes the proposed system of the Hitachi team for the Cross-Framework Meaning Representation Parsing (MRP 2019) shared task. In this shared task, the participating systems were asked to predict <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a>, <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> and their attributes for five <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a>, each with different order of abstraction from input tokens. We proposed a unified encoder-to-biaffine network for all five frameworks, which effectively incorporates a shared encoder to extract rich input features, decoder networks to generate anchorless nodes in UCCA and AMR, and biaffine networks to predict edges. Our system was ranked fifth with the macro-averaged MRP F1 score of 0.7604, and outperformed the baseline unified transition-based MRP. Furthermore, post-evaluation experiments showed that we can boost the performance of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> by incorporating <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, whereas the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> could not. These imply efficacy of incorporating the biaffine network to the shared architecture for MRP and that learning heterogeneous meaning representations at once can boost the system performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K19-2015.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K19-2015/>FAL-Oslo at MRP 2019 : Garage Sale Semantic Parsing<span class=acl-fixed-case>ÚFAL</span>-<span class=acl-fixed-case>O</span>slo at <span class=acl-fixed-case>MRP</span> 2019: Garage Sale Semantic Parsing</a></strong><br><a href=/people/k/kira-droganova/>Kira Droganova</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/n/nikita-mediankin/>Nikita Mediankin</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2015><div class="card-body p-3 small">This paper describes the FALOslo system submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. The submission is based on several <a href=https://en.wikipedia.org/wiki/Parsing>third-party parsers</a>. Within the official shared task results, the submission ranked 11th out of 13 participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-2016/>Peking at MRP 2019 : Factorization- and Composition-Based Parsing for Elementary Dependency Structures<span class=acl-fixed-case>MRP</span> 2019: Factorization- and Composition-Based Parsing for Elementary Dependency Structures</a></strong><br><a href=/people/y/yufei-chen/>Yufei Chen</a>
|
<a href=/people/y/yajie-ye/>Yajie Ye</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-2016><div class="card-body p-3 small">We design, implement and evaluate two semantic parsers, which represent factorization- and composition-based approaches respectively, for Elementary Dependency Structures (EDS) at the CoNLL 2019 Shared Task on Cross-Framework Meaning Representation Parsing. The detailed evaluation of the two <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> gives us a new perception about parsing into linguistically enriched meaning representations : current neural EDS parsers are able to reach an accuracy at the inter-annotator agreement level in the same-epoch-and-domain setup.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>