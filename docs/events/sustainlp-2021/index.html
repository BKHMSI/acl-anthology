<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Simple and Efficient Natural Language Processing (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Simple and Efficient Natural Language Processing (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021sustainlp-1>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=2021sustainlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.sustainlp-1/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.0/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/a/ana-marasovic/>Ana MarasoviÄ‡</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.sustainlp-1.1.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.1/>Low Resource Quadratic Forms for Knowledge Graph Embeddings</a></strong><br><a href=/people/z/zachary-zhou/>Zachary Zhou</a>
|
<a href=/people/j/jeffery-kline/>Jeffery Kline</a>
|
<a href=/people/d/devin-conathan/>Devin Conathan</a>
|
<a href=/people/g/glenn-fung/>Glenn Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--1><div class="card-body p-3 small">We address the problem of link prediction between entities and relations of knowledge graphs. State of the art techniques that address this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a <a href=https://en.wikipedia.org/wiki/Quadratic_form>quadratic form</a> that is used as a <a href=https://en.wikipedia.org/wiki/Ranking_function>ranking function</a>. The structure of our <a href=https://en.wikipedia.org/wiki/Convex_optimization>convex program</a> is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model&#8217;s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.<i>link prediction</i> between entities and relations of knowledge graphs. State of the art techniques that address this problem, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a quadratic form that is used as a ranking function. The structure of our convex program is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model&#8217;s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.3/>Limitations of Knowledge Distillation for Zero-shot Transfer Learning</a></strong><br><a href=/people/s/saleh-soltan/>Saleh Soltan</a>
|
<a href=/people/h/haidar-khan/>Haidar Khan</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--3><div class="card-body p-3 small">Pretrained transformer-based encoders such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have been demonstrated to achieve state-of-the-art performance on numerous NLP tasks. Despite their success, BERT style encoders are large in size and have high latency during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> (especially on CPU machines) which make them unappealing for many <a href=https://en.wikipedia.org/wiki/Online_application>online applications</a>. Recently introduced compression and distillation methods have provided effective ways to alleviate this shortcoming. However, the focus of these works has been mainly on monolingual encoders. Motivated by recent successes in zero-shot cross-lingual transfer learning using multilingual pretrained encoders such as mBERT, we evaluate the effectiveness of Knowledge Distillation (KD) both during pretraining stage and during fine-tuning stage on multilingual BERT models. We demonstrate that in contradiction to the previous observation in the case of monolingual distillation, in multilingual settings, <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during pretraining is more effective than <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during fine-tuning for zero-shot transfer learning. Moreover, we observe that <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> during <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a> may hurt zero-shot cross-lingual performance. Finally, we demonstrate that distilling a larger model (BERT Large) results in the strongest distilled model that performs best both on the source language as well as target languages in zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sustainlp-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.7/>Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering</a></strong><br><a href=/people/g/georgios-sidiropoulos/>Georgios Sidiropoulos</a>
|
<a href=/people/n/nikos-voskarides/>Nikos Voskarides</a>
|
<a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--7><div class="card-body p-3 small">In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.8/>Learning to Rank in the Age of <a href=https://en.wikipedia.org/wiki/The_Muppets>Muppets</a> : EffectivenessEfficiency Tradeoffs in Multi-Stage Ranking</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/c/chengcheng-hu/>ChengCheng Hu</a>
|
<a href=/people/y/yuqi-liu/>Yuqi Liu</a>
|
<a href=/people/h/hui-fang/>Hui Fang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--8><div class="card-body p-3 small">It is well known that rerankers built on pretrained transformer models such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have dramatically improved retrieval effectiveness in many tasks. However, these gains have come at substantial costs in terms of <a href=https://en.wikipedia.org/wiki/Economic_efficiency>efficiency</a>, as noted by many researchers. In this work, we show that it is possible to retain the benefits of transformer-based rerankers in a multi-stage reranking pipeline by first using feature-based learning-to-rank techniques to reduce the number of candidate documents under consideration without adversely affecting their quality in terms of recall. Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a>, but with up to 18 increase in <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for traditional LTR techniques, and that we should not forget history.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.13/>Distiller : A Systematic Study of Model Distillation Methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/h/haoyu-he/>Haoyu He</a>
|
<a href=/people/x/xingjian-shi/>Xingjian Shi</a>
|
<a href=/people/j/jonas-mueller/>Jonas Mueller</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/g/george-karypis/>George Karypis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--13><div class="card-body p-3 small">Knowledge Distillation (KD) offers a natural way to reduce the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and memory / energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets / tasks, such as the data augmentation policy, the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component&#8217;s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MI-objective functions with better bias / variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following : 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.14/>Shrinking Bigfoot : Reducing wav2vec 2.0 footprint</a></strong><br><a href=/people/z/zilun-peng/>Zilun Peng</a>
|
<a href=/people/a/akshay-budhkar/>Akshay Budhkar</a>
|
<a href=/people/i/ilana-tuil/>Ilana Tuil</a>
|
<a href=/people/j/jason-levy/>Jason Levy</a>
|
<a href=/people/p/parinaz-sobhani/>Parinaz Sobhani</a>
|
<a href=/people/r/raphael-cohen/>Raphael Cohen</a>
|
<a href=/people/j/jumana-nassour/>Jumana Nassour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--14><div class="card-body p-3 small">Wav2vec 2.0 is a state-of-the-art <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition model</a> which maps speech audio waveforms into latent representations. The largest version of wav2vec 2.0 contains 317 million parameters. Hence, the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>inference latency</a> of wav2vec 2.0 will be a bottleneck in production, leading to high costs and a significant environmental footprint. To improve wav2vec&#8217;s applicability to a production setting, we explore multiple model compression methods borrowed from the domain of large language models. Using a teacher-student approach, we distilled the knowledge from the original wav2vec 2.0 model into a student model, which is 2 times faster, 4.8 times smaller than the original model. More importantly, the student model is 2 times more energy efficient than the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in terms of <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO2 emission</a>. This increase in performance is accomplished with only a 7 % degradation in word error rate (WER). Our quantized model is 3.6 times smaller than the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, with only a 0.1 % degradation in <a href=https://en.wikipedia.org/wiki/White_blood_cell>WER</a>. To the best of our knowledge, this is the first work that compresses wav2vec 2.0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sustainlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sustainlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sustainlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sustainlp-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sustainlp-1.17/>Unsupervised Contextualized Document Representation</a></strong><br><a href=/people/a/ankur-gupta/>Ankur Gupta</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sustainlp-1--17><div class="card-body p-3 small">Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>