<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Conference on Language Resources and Evaluation (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Conference on Language Resources and Evaluation (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020lrec-1>Proceedings of the 12th Language Resources and Evaluation Conference</a>
<span class="badge badge-info align-middle ml-1">269&nbsp;papers</span></li><li><a class=align-middle href=#2020aespen-1>Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020ai4hi-1>Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020bucc-1>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020calcs-1>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020cllrd-1>Proceedings of the LREC 2020 Workshop on "Citizen Linguistics in Language Resource Development"</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020clssts-1>Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020cmlc-1>Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020computerm-1>Proceedings of the 6th International Workshop on Computational Terminology</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020framenet-1>Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020gamnlp-1>Workshop on Games and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020globalex-1>Proceedings of the 2020 Globalex Workshop on Linked Lexicography</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020isa-1>16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020iwltp-1>Proceedings of the 1st International Workshop on Language Technology Platforms</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020ldl-1>Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020)</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020lincr-1>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020lr4sshoc-1>Proceedings of the Workshop about Language Resources for the SSH Cloud</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020lt4gov-1>Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov)</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020lt4hala-1>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020mmw-1>Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020)</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020multilingualbio-1>Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020onion-1>Proceedings of LREC2020 Workshop "People in language, vision and the mind" (ONION2020)</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020osact-1>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020parlaclarin-1>Proceedings of the Second ParlaCLARIN Workshop</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020rail-1>Proceedings of the first workshop on Resources for African Indigenous Languages</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020readi-1>Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020restup-1>Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020signlang-1>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#2020sltu-1>Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#2020stoc-1>Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020trac-1>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2020wac-1>Proceedings of the 12th Web as Corpus Workshop</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020wildre-1>Proceedings of the WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li></ul></div></div><div id=2020lrec-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lrec-1/>Proceedings of the 12th Language Resources and Evaluation Conference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.0/>Proceedings of the 12th Language Resources and Evaluation Conference</a></strong><br><a href=/people/n/nicoletta-calzolari/>Nicoletta Calzolari</a>
|
<a href=/people/f/frederic-bechet/>Frédéric Béchet</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/c/christopher-cieri/>Christopher Cieri</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/s/sara-goggi/>Sara Goggi</a>
|
<a href=/people/h/hitoshi-isahara/>Hitoshi Isahara</a>
|
<a href=/people/b/bente-maegaard/>Bente Maegaard</a>
|
<a href=/people/j/joseph-mariani/>Joseph Mariani</a>
|
<a href=/people/h/helene-mazo/>Hélène Mazo</a>
|
<a href=/people/a/asuncion-moreno/>Asuncion Moreno</a>
|
<a href=/people/j/jan-odijk/>Jan Odijk</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.1/>Neural Mention Detection</a></strong><br><a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--1><div class="card-body p-3 small">Mention detection is an important preprocessing step for annotation and interpretation in applications such as <a href=https://en.wikipedia.org/wiki/Near-infrared_spectroscopy>NER</a> and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, but few stand-alone neural models have been proposed able to handle the full range of mentions. In this work, we propose and compare three neural network-based approaches to mention detection. The first approach is based on the mention detection part of a state of the art coreference resolution system ; the second uses ELMO embeddings together with a bidirectional LSTM and a biaffine classifier ; the third approach uses the recently introduced BERT model. Our best model (using a biaffine classifier) achieves gains of up to 1.8 percentage points on <a href=https://en.wikipedia.org/wiki/Recall_(memory)>mention recall</a> when compared with a strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> in a HIGH RECALL coreference annotation setting. The same <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves improvements of up to 5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on the CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation setting. We then evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> by using mentions predicted by our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in start-of-the-art <a href=https://en.wikipedia.org/wiki/Coreference>coreference systems</a>. The enhanced <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved absolute improvements of up to 1.7 and 0.7 p.p. when compared with our strong baseline systems (pipeline system and end-to-end system) respectively. For nested NER, the evaluation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the GENIA corpora shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> matches or outperforms state-of-the-art models despite not being specifically designed for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.9/>Model-based Annotation of Coreference</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--9><div class="card-body p-3 small">Humans do not make inferences over texts, but over models of what texts are about. When annotators are asked to annotate coreferent spans of text, it is therefore a somewhat unnatural task. This paper presents an alternative in which we preprocess documents, linking entities to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, and turn the coreference annotation task in our case limited to <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> into an annotation task where annotators are asked to assign <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> to entities. Model-based annotation is shown to lead to faster <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and higher <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>, and we argue that it also opens up an alternative approach to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We present two new coreference benchmark datasets, for English Wikipedia and English teacher-student dialogues, and evaluate state-of-the-art coreference resolvers on them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.11/>Cross-lingual Zero Pronoun Resolution</a></strong><br><a href=/people/a/abdulrahman-aloraini/>Abdulrahman Aloraini</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--11><div class="card-body p-3 small">In languages like <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and many others, predicate arguments in certain syntactic positions are not realized instead of being realized as overt pronouns, and are thus called zero- or null-pronouns. Identifying and resolving such omitted arguments is crucial to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and other NLP tasks, but depends heavily on semantic coherence and lexical relationships. We propose a BERT-based cross-lingual model for zero pronoun resolution, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, ours is the first neural model of zero-pronoun resolution for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> ; and our model also outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. In the paper we also evaluate BERT feature extraction and fine-tune models on the task, and compare them with our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We also report on an investigation of BERT layers indicating which layer encodes the most suitable representation for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.14/>Affection Driven Neural Networks for Sentiment Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-wan/>Mingyu Wan</a>
|
<a href=/people/j/jinghang-gu/>Jinghang Gu</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--14><div class="card-body p-3 small">Deep neural network models have played a critical role in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with promising results in the recent decade. One of the essential challenges, however, is how external sentiment knowledge can be effectively utilized. In this work, we propose a novel affection-driven approach to incorporating affective knowledge into <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a>. The affective knowledge is obtained in the form of a lexicon under the Affect Control Theory (ACT), which is represented by vectors of three-dimensional attributes in Evaluation, Potency, and Activity (EPA). The EPA vectors are mapped to an affective influence value and then integrated into Long Short-term Memory (LSTM) models to highlight affective terms. Experimental results show a consistent improvement of our approach over conventional LSTM models by 1.0 % to 1.5 % in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on three large benchmark datasets. Evaluations across a variety of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> have also proven the effectiveness of leveraging affective terms for deep model enhancement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.19/>Linguistic, Kinematic and Gaze Information in Task Descriptions : The LKG-Corpus<span class=acl-fixed-case>LKG</span>-Corpus</a></strong><br><a href=/people/t/tim-reinboth/>Tim Reinboth</a>
|
<a href=/people/s/stephanie-gross/>Stephanie Gross</a>
|
<a href=/people/l/laura-bishop/>Laura Bishop</a>
|
<a href=/people/b/brigitte-krenn/>Brigitte Krenn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--19><div class="card-body p-3 small">Data from <a href=https://en.wikipedia.org/wiki/Neuroscience>neuroscience</a> and <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> suggest that sensorimotor cognition may be of central importance to <a href=https://en.wikipedia.org/wiki/Language>language</a>. Specifically, the linguistic structure of utterances referring to concrete actions may reflect the structure of the sensorimotor processing underlying the same action. To investigate this, we present the Linguistic, Kinematic and Gaze information in task descriptions Corpus (LKG-Corpus), comprising multimodal data on 13 humans, conducting take, put, and push actions, and describing these <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>actions</a> with 350 utterances. Recorded are audio, video, motion and eye-tracking data while participants perform an action and describe what they do. The dataset is annotated with orthographic transcriptions of utterances and information on : (a) gaze behaviours, (b) when a participant touched an object, (c) when an object was moved, (d) when a participant looked at the location s / he would next move the object to, (e) when the participant&#8217;s gaze was stable on an area. With the exception of the annotation of stable gaze, all <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> were performed manually. With the LKG-Corpus, we present a dataset that integrates linguistic, kinematic and gaze data with an explicit focus on relations between action and language. On this basis, we outline applications of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to both basic and applied research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.20/>The ACQDIV Corpus Database and Aggregation Pipeline<span class=acl-fixed-case>ACQDIV</span> Corpus Database and Aggregation Pipeline</a></strong><br><a href=/people/a/anna-jancso/>Anna Jancso</a>
|
<a href=/people/s/steven-moran/>Steven Moran</a>
|
<a href=/people/s/sabine-stoll/>Sabine Stoll</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--20><div class="card-body p-3 small">We present the ACQDIV corpus database and aggregation pipeline, a tool developed as part of the European Research Council (ERC) funded project ACQDIV, which aims to identify the universal cognitive processes that allow children to acquire any language. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus database</a> represents 15 corpora from 14 typologically maximally diverse languages. Here we give an overview of the project, <a href=https://en.wikipedia.org/wiki/Database>database</a>, and our extensible software package for adding more corpora to the current language sample. Lastly, we discuss how we use the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus database</a> to mine for universal patterns in child language acquisition corpora and we describe avenues for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.23/>Understanding the Dynamics of <a href=https://en.wikipedia.org/wiki/Second_language_writing>Second Language Writing</a> through Keystroke Logging and Complexity Contours</a></strong><br><a href=/people/e/elma-kerz/>Elma Kerz</a>
|
<a href=/people/f/fabio-pruneri/>Fabio Pruneri</a>
|
<a href=/people/d/daniel-wiechmann/>Daniel Wiechmann</a>
|
<a href=/people/y/yu-qiao/>Yu Qiao</a>
|
<a href=/people/m/marcus-strobel/>Marcus Ströbel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--23><div class="card-body p-3 small">The purpose of this paper is twofold : [ 1 ] to introduce, to our knowledge, the largest available resource of keystroke logging (KSL) data generated by Etherpad (https://etherpad.org/), an open-source, web-based collaborative real-time editor, that captures the dynamics of second language (L2) production and [ 2 ] to relate the behavioral data from KSL to indices of syntactic and lexical complexity of the texts produced obtained from a tool that implements a sliding window approach capturing the progression of complexity within a text. We present the procedures and <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> developed to analyze a sample of 14,913,009 keystrokes in 3,454 texts produced by 512 university students (upper-intermediate to advanced L2 learners of English) (95,354 sentences and 18,32,027 words) aiming to achieve a better alignment between keystroke-logging measures and underlying cognitive processes, on the one hand, and L2 writing performance measures, on the other hand. The resource introduced in this paper is a reflection of increasing recognition of the urgent need to obtain ecologically valid data that have the potential to transform our current understanding of mechanisms underlying the development of literacy (reading and writing) skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.28/>Developing a Corpus of Indirect Speech Act Schemas</a></strong><br><a href=/people/a/antonio-roque/>Antonio Roque</a>
|
<a href=/people/a/alexander-tsuetaki/>Alexander Tsuetaki</a>
|
<a href=/people/v/vasanth-sarathy/>Vasanth Sarathy</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--28><div class="card-body p-3 small">Resolving Indirect Speech Acts (ISAs), in which the intended meaning of an utterance is not identical to its literal meaning, is essential to enabling the participation of intelligent systems in peoples&#8217; everyday lives. Especially challenging are those cases in which the interpretation of such <a href=https://en.wikipedia.org/wiki/As_(Roman_coin)>ISAs</a> depends on context. To test a system&#8217;s ability to perform ISA resolution we need a corpus, but developing such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is difficult, especialy given the contex-dependent requirement. This paper addresses the difficult problems of constructing a corpus of ISAs, taking inspiration from relevant work in using corpora for reasoning tasks. We present a formal representation of ISA Schemas required for such testing, including a measure of the difficulty of a particular schema. We develop an approach to authoring these <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a> using <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus analysis</a> and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, to maximize realism and minimize the amount of expert authoring needed. Finally, we describe several characteristics of <a href=https://en.wikipedia.org/wiki/Data>collected data</a>, and potential future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.35/>MAGPIE : A Large Corpus of Potentially Idiomatic Expressions<span class=acl-fixed-case>MAGPIE</span>: A Large Corpus of Potentially Idiomatic Expressions</a></strong><br><a href=/people/h/hessel-haagsma/>Hessel Haagsma</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--35><div class="card-body p-3 small">Given the limited size of existing idiom corpora, we aim to enable progress in automatic idiom processing and <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a> by creating the largest-to-date corpus of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Using a fixed idiom list, automatic pre-extraction, and a strictly controlled crowdsourced annotation procedure, we show that it is feasible to build a high-quality corpus comprising more than 50 K instances, an order of a magnitude larger than previous resources. Crucial ingredients of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> were the selection of crowdworkers, clear and comprehensive instructions, and an interface that breaks down the task in small, manageable steps. Analysis of the resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> revealed strong effects of <a href=https://en.wikipedia.org/wiki/Genre>genre</a> on idiom distribution, providing new evidence for existing theories on what influences <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom usage</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> also contains rich metadata, and is made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.37/>Effort Estimation in Named Entity Tagging Tasks</a></strong><br><a href=/people/i/ines-gomes/>Inês Gomes</a>
|
<a href=/people/r/rui-correia/>Rui Correia</a>
|
<a href=/people/j/jorge-ribeiro/>Jorge Ribeiro</a>
|
<a href=/people/j/joao-freitas/>João Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--37><div class="card-body p-3 small">Named Entity Recognition (NER) is an essential component of many Natural Language Processing pipelines. However, building these language dependent models requires large amounts of annotated data. Crowdsourcing emerged as a scalable solution to collect and enrich data in a more time-efficient manner. To manage these annotations at scale, it is important to predict completion timelines and compute fair pricing for workers in advance. To achieve these goals, we need to know how much effort will be taken to complete each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we investigate which variables influence the time spent on a named entity annotation task by a human. Our results are two-fold : first, the understanding of the effort-impacting factors which we divided into <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> and input length ; and second, the performance of the prediction itself. On the latter, through model adaptation and <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>, we attained a Root Mean Squared Error (RMSE) of 25.68 words per minute with a Nearest Neighbors model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.40/>Constructing Multimodal Language Learner Texts Using LARA : Experiences with Nine Languages<span class=acl-fixed-case>LARA</span>: Experiences with Nine Languages</a></strong><br><a href=/people/e/elham-akhlaghi/>Elham Akhlaghi</a>
|
<a href=/people/b/branislav-bedi/>Branislav Bédi</a>
|
<a href=/people/f/fatih-bektas/>Fatih Bektaş</a>
|
<a href=/people/h/harald-berthelsen/>Harald Berthelsen</a>
|
<a href=/people/m/matthias-butterweck/>Matthias Butterweck</a>
|
<a href=/people/c/cathy-chua/>Cathy Chua</a>
|
<a href=/people/c/catia-cucchiarin/>Catia Cucchiarin</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a>
|
<a href=/people/j/johanna-gerlach/>Johanna Gerlach</a>
|
<a href=/people/h/hanieh-habibi/>Hanieh Habibi</a>
|
<a href=/people/n/neasa-ni-chiarain/>Neasa Ní Chiaráin</a>
|
<a href=/people/m/manny-rayner/>Manny Rayner</a>
|
<a href=/people/s/steinthor-steingrimsson/>Steinþór Steingrímsson</a>
|
<a href=/people/h/helmer-strik/>Helmer Strik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--40><div class="card-body p-3 small">LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a>. This involves semi-automatically tagging the text, adding other <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and recording audio. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> is suitable for creating texts in multiple languages via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing techniques</a> that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Persian_language>Farsi</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>, <a href=https://en.wikipedia.org/wiki/Irish_language>Irish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.41/>A Dataset for Investigating the Impact of <a href=https://en.wikipedia.org/wiki/Feedback>Feedback</a> on Student Revision Outcome</a></strong><br><a href=/people/i/ildiko-pilan/>Ildiko Pilan</a>
|
<a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/c/chak-yan-yeung/>Chak Yan Yeung</a>
|
<a href=/people/j/jonathan-j-webster/>Jonathan Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--41><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Annotation>annotation scheme</a> and a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of teacher feedback provided for texts written by non-native speakers of English. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>student-written sentences</a> in their original and revised versions with <a href=https://en.wikipedia.org/wiki/Teacher>teacher feedback</a> provided for the errors. Feedback appears both in the form of open-ended comments and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>error category tags</a>. We focus on a specific error type, namely linking adverbial (e.g. however, moreover) errors. The dataset has been annotated for two aspects : (i) revision outcome establishing whether the re-written student sentence was correct and (ii) directness, indicating whether teachers provided explicitly the correction in their feedback. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> allows for studies around the characteristics of teacher feedback and how these influence students&#8217; revision outcome. We describe the data preparation process and we present initial statistical investigations regarding the effect of different feedback characteristics on revision outcome. These show that open-ended comments and mitigating expressions appear in a higher proportion of successful revisions than unsuccessful ones, while directness and metalinguistic terms have no effect. Given that the use of this type of <a href=https://en.wikipedia.org/wiki/Data>data</a> is relatively unexplored in natural language processing (NLP) applications, we also report some observations and challenges when working with feedback data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.43/>Using Multilingual Resources to Evaluate CEFRLex for Learner Applications<span class=acl-fixed-case>CEFRL</span>ex for Learner Applications</a></strong><br><a href=/people/j/johannes-graen/>Johannes Graën</a>
|
<a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/g/gerold-schneider/>Gerold Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--43><div class="card-body p-3 small">The Common European Framework of Reference for Languages (CEFR) defines six levels of learner proficiency, and links them to particular communicative abilities. The CEFRLex project aims at compiling lexical resources that link single words and multi-word expressions to particular CEFR levels. The resources are thought to reflect second language learner needs as they are compiled from CEFR-graded textbooks and other learner-directed texts. In this work, we investigate the applicability of CEFRLex resources for building language learning applications. Our main concerns were that vocabulary in <a href=https://en.wikipedia.org/wiki/Language_pedagogy>language learning materials</a> might be sparse, i.e. that not all vocabulary items that belong to a particular level would also occur in materials for that level, and, on the other hand, that vocabulary items might be used on lower-level materials if required by the topic (e.g. with a simpler paraphrasing or translation). Our results indicate that the English CEFRLex resource is in accordance with external resources that we jointly employ as gold standard. Together with other values obtained from monolingual and parallel corpora, we can indicate which entries need to be adjusted to obtain values that are even more in line with this gold standard. We expect that this finding also holds for the other languages</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.48/>Toward a Paradigm Shift in Collection of Learner Corpora</a></strong><br><a href=/people/a/anisia-katinskaia/>Anisia Katinskaia</a>
|
<a href=/people/s/sardana-ivanova/>Sardana Ivanova</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--48><div class="card-body p-3 small">We present the first version of the longitudinal Revita Learner Corpus (ReLCo), for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. In contrast to traditional learner corpora, ReLCo is collected and annotated fully automatically, while students perform exercises using the Revita language-learning platform. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> currently contains 8 422 sentences exhibiting several types of errorsgrammatical, lexical, orthographic, etc.which were committed by learners during practice and were automatically annotated by Revita. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> provides valuable information about patterns of learner errors and can be used as a language resource for a number of research tasks, while its creation is much cheaper and faster than for traditional learner corpora. A crucial advantage of ReLCo that it grows continually while learners practice with Revita, which opens the possibility of creating an unlimited learner resource with <a href=https://en.wikipedia.org/wiki/Longitudinal_study>longitudinal data</a> collected over time. We make the pilot version of the Russian ReLCo publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.53/>MultiWOZ 2.1 : A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>WOZ</span> 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines</a></strong><br><a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/r/rahul-goel/>Rahul Goel</a>
|
<a href=/people/s/shachi-paul/>Shachi Paul</a>
|
<a href=/people/a/abhishek-sethi/>Abhishek Sethi</a>
|
<a href=/people/s/sanchit-agarwal/>Sanchit Agarwal</a>
|
<a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/a/adarsh-kumar/>Adarsh Kumar</a>
|
<a href=/people/a/anuj-goyal/>Anuj Goyal</a>
|
<a href=/people/p/peter-ku/>Peter Ku</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--53><div class="card-body p-3 small">MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there are substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with user dialogue acts. This leads to multiple co-existent versions of the same <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32 % of state annotations across 40 % of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.54/>A Comparison of Explicit and Implicit Proactive Dialogue Strategies for Conversational Recommendation</a></strong><br><a href=/people/m/matthias-kraus/>Matthias Kraus</a>
|
<a href=/people/f/fabian-fischbach/>Fabian Fischbach</a>
|
<a href=/people/p/pascal-jansen/>Pascal Jansen</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--54><div class="card-body p-3 small">Recommendation systems aim at facilitating <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> for users by taking into account their preferences. Based on previous user behaviour, such a <a href=https://en.wikipedia.org/wiki/System>system</a> suggests items or provides information that a user might like or find useful. Nonetheless, how to provide suggestions is still an open question. Depending on the way a recommendation is communicated influences the user&#8217;s perception of the system. This paper presents an empirical study on the effects of proactive dialogue strategies on <a href=https://en.wikipedia.org/wiki/Acceptance>user acceptance</a>. Therefore, an explicit <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> based on user preferences provided directly by the user, and an implicit proactive strategy, using autonomously gathered information, are compared. The results show that proactive dialogue systems significantly affect the perception of human-computer interaction. Although no significant differences are found between implicit and explicit strategies, <a href=https://en.wikipedia.org/wiki/Proactivity>proactivity</a> significantly influences the <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> compared to reactive system behaviour. The study contributes new insights to the human-agent interaction and the voice user interface design. Furthermore, we discover interesting tendencies that motivate futurework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.56/>Construction and Analysis of a Multimodal Chat-talk Corpus for Dialog Systems Considering Interpersonal Closeness</a></strong><br><a href=/people/y/yoshihiro-yamazaki/>Yoshihiro Yamazaki</a>
|
<a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/t/takashi-nose/>Takashi Nose</a>
|
<a href=/people/a/akinori-ito/>Akinori Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--56><div class="card-body p-3 small">There are high expectations for multimodal dialog systems that can make natural small talk with <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Gesture>gestures</a>, and gaze actions as next-generation dialog-based systems. Two important roles of the chat-talk system are keeping the user engaged and establishing rapport. Many studies have conducted user evaluations of such systems, some of which reported that considering the relationship with the user is an effective way to improve the subjective evaluation. To facilitate research of such dialog systems, we are currently constructing a large-scale multimodal dialog corpus focusing on the relationship between speakers. In this paper, we describe the data collection and annotation process, and analysis of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> collected in the early stage of the project. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 19,303 utterances (10 hours) from 19 pairs of participants. A dialog act tag is annotated to each utterance by two annotators. We compare the <a href=https://en.wikipedia.org/wiki/Frequency>frequency</a> and the <a href=https://en.wikipedia.org/wiki/Transition_probability>transition probability</a> of the tags between different closeness levels to help construct a dialog system for establishing a relationship with the user.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.58/>The JDDC Corpus : A Large-Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service<span class=acl-fixed-case>JDDC</span> Corpus: A Large-Scale Multi-Turn <span class=acl-fixed-case>C</span>hinese Dialogue Dataset for <span class=acl-fixed-case>E</span>-commerce Customer Service</a></strong><br><a href=/people/m/meng-chen/>Meng Chen</a>
|
<a href=/people/r/ruixue-liu/>Ruixue Liu</a>
|
<a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/s/shaozu-yuan/>Shaozu Yuan</a>
|
<a href=/people/j/jingyan-zhou/>Jingyan Zhou</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--58><div class="card-body p-3 small">Human conversations are complicated and building a human-like dialogue agent is an extremely challenging task. With the rapid development of deep learning techniques, data-driven models become more and more prevalent which need a huge amount of real conversation data. In this paper, we construct a large-scale real scenario Chinese E-commerce conversation corpus, JDDC, with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> reflects several characteristics of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-human conversations</a>, e.g., goal-driven, and long-term dependency among the context. It also covers various dialogue types including task-oriented, chitchat and <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a>. Extra intent information and three well-annotated challenge sets are also provided. Then, we evaluate several retrieval-based and generative models to provide basic benchmark performance on the JDDC corpus. And we hope JDDC can serve as an effective testbed and benefit the development of fundamental research in dialogue task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.59/>Cheese ! : a Corpus of Face-to-face French Interactions. A Case Study for Analyzing Smiling and Conversational Humor<span class=acl-fixed-case>F</span>rench Interactions. A Case Study for Analyzing Smiling and Conversational Humor</a></strong><br><a href=/people/b/beatrice-priego-valverde/>Béatrice Priego-Valverde</a>
|
<a href=/people/b/brigitte-bigi/>Brigitte Bigi</a>
|
<a href=/people/m/mary-amoyal/>Mary Amoyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--59><div class="card-body p-3 small">Cheese ! is a conversational corpus. It consists of 11 French face-to-face conversations lasting around 15 minutes each. Cheese ! is a duplication of an American corpus (ref) in order to conduct a cross-cultural comparison of participants&#8217; smiling behavior in humorous and non-humorous sequences in American English and French conversations. In this article, the methodology used to collect and enrich the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is presented : experimental protocol, technical choices, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a>, semi-automatic annotations, manual annotations of smiling and <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. An exploratory study investigating the links between <a href=https://en.wikipedia.org/wiki/Smile>smile</a> and <a href=https://en.wikipedia.org/wiki/Humour>humor</a> is then proposed. Based on the analysis of two interactions, two questions are asked : (1) Does <a href=https://en.wikipedia.org/wiki/Smile>smile</a> frame <a href=https://en.wikipedia.org/wiki/Humour>humor</a>? (2) Does <a href=https://en.wikipedia.org/wiki/Smile>smile</a> has an impact on its success or failure? If the experimental design of Cheese ! has been elaborated to study specifically smiles and <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in conversations, the high quality of the dataset obtained, and the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> used are also replicable and can be applied to analyze many other conversational activities and other multimodal modalities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.60/>The Margarita Dialogue Corpus : A Data Set for Time-Offset Interactions and Unstructured Dialogue Systems</a></strong><br><a href=/people/a/alberto-chierici/>Alberto Chierici</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/m/margarita-bicec/>Margarita Bicec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--60><div class="card-body p-3 small">Time-Offset Interaction Applications (TOIAs) are <a href=https://en.wikipedia.org/wiki/System>systems</a> that simulate <a href=https://en.wikipedia.org/wiki/Face-to-face_interaction>face-to-face conversations</a> between humans and <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>digital human avatars</a> recorded in the past. Developing a well-functioning TOIA involves several research areas : <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>, <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and dialogue systems. The first challenges are to define a sensible methodology for <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> and to create useful data sets for training the system to retrieve the best answer to a user&#8217;s question. In this paper, we present three main contributions : a methodology for creating the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> for a TOIA, a dialogue corpus, and baselines for single-turn answer retrieval. We develop the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> using a two-step strategy. First, we let the avatar maker list pairs by intuition, guessing what possible questions a user may ask to the avatar. Second, we record actual dialogues between random individuals and the avatar-maker. We make the Margarita Dialogue Corpus available to the research community. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> in text format, the video clips for each answer, and the annotated dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.61/>How Users React to Proactive Voice Assistant Behavior While Driving</a></strong><br><a href=/people/m/maria-schmidt/>Maria Schmidt</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/steffen-werner/>Steffen Werner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--61><div class="card-body p-3 small">Nowadays Personal Assistants (PAs) are available in multiple environments and become increasingly popular to use via <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>. Therefore, we aim to provide proactive PA suggestions to car drivers via <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. These suggestions should be neither obtrusive nor increase the drivers&#8217; cognitive load, while enhancing <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a>. To assess these factors, we conducted a <a href=https://en.wikipedia.org/wiki/Usability_study>usability study</a> in which 42 participants perceive proactive voice output in a Wizard-of-Oz study in a <a href=https://en.wikipedia.org/wiki/Driving_simulator>driving simulator</a>. Traffic density was varied during a <a href=https://en.wikipedia.org/wiki/Driving>highway drive</a> and it included six in-car-specific use cases. The latter were presented by a proactive voice assistant and in a non-proactive control condition. We assessed the users&#8217; subjective cognitive load and their satisfaction in different questionnaires during the interaction with both PA variants. Furthermore, we analyze the user reactions : both regarding their content and the elapsed response times to PA actions. The results show that proactive assistant behavior is rated similarly positive as non-proactive behavior. Furthermore, the participants agreed to 73.8 % of proactive suggestions. In line with previous research, driving-relevant use cases receive the best ratings, here we reach 82.5 % acceptance. Finally, the users reacted significantly faster to proactive PA actions, which we interpret as less <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> compared to non-proactive behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.62/>Emotional Speech Corpus for Persuasive Dialogue System</a></strong><br><a href=/people/s/sara-asai/>Sara Asai</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/seitaro-shinagawa/>Seitaro Shinagawa</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--62><div class="card-body p-3 small">Expressing emotion is known as an efficient way to persuade one&#8217;s dialogue partner to accept one&#8217;s claim or proposal. Emotional expression in speech can express the speaker&#8217;s emotion more directly than using only emotion expression in the text, which will lead to a more persuasive dialogue. In this paper, we built a speech dialogue corpus in a persuasive scenario that uses <a href=https://en.wikipedia.org/wiki/Emotion>emotional expressions</a> to build a persuasive dialogue system with <a href=https://en.wikipedia.org/wiki/Emotion>emotional expressions</a>. We extended an existing text dialogue corpus by adding variations of emotional responses to cover different combinations of broad dialogue context and a variety of emotional states by <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a>. Then, we recorded emotional speech consisting of of collected <a href=https://en.wikipedia.org/wiki/Emotion>emotional expressions</a> spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system&#8217;s emotion to users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.65/>Evaluation of Argument Search Approaches in the Context of Argumentative Dialogue Systems</a></strong><br><a href=/people/n/niklas-rach/>Niklas Rach</a>
|
<a href=/people/y/yuki-matsuda/>Yuki Matsuda</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/k/keiichi-yasumoto/>Keiichi Yasumoto</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--65><div class="card-body p-3 small">We present an approach to evaluate argument search techniques in view of their use in argumentative dialogue systems by assessing quality aspects of the retrieved arguments. To this end, we introduce a dialogue system that presents arguments by means of a virtual avatar and synthetic speech to users and allows them to rate the presented content in four different categories (Interesting, Convincing, Comprehensible, Relation). The approach is applied in a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> in order to compare two state of the art argument search engines to each other and with a system based on traditional <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a>. The results show a significant advantage of the two <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Moreover, the two <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> show significant advantages over each other in different categories, thereby reflecting strengths and weaknesses of the different underlying techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.68/>Estimating User Communication Styles for Spoken Dialogue Systems</a></strong><br><a href=/people/j/juliana-miehle/>Juliana Miehle</a>
|
<a href=/people/i/isabel-feustel/>Isabel Feustel</a>
|
<a href=/people/j/julia-hornauer/>Julia Hornauer</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--68><div class="card-body p-3 small">We present a neural network approach to estimate the communication style of spoken interaction, namely the stylistic variations elaborateness and directness, and investigate which type of input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to the <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> are necessary to achive good performance. First, we describe our annotated corpus of recordings in the health care domain and analyse the corpus statistics in terms of <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>agreement</a>, <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the ratings. We use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to estimate the <a href=https://en.wikipedia.org/wiki/Elaboration>elaborateness</a> and the directness of each utterance. We test different feature sets consisting of dialogue act features, grammatical features and linguistic features as input for our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and perform classification in two and three classes. Our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> use only <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> that can be automatically derived during an ongoing interaction in any spoken dialogue system without any prior annotation. Our results show that the elaborateness can be classified by only using the dialogue act and the amount of words contained in the corresponding utterance. The directness is a more difficult <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> and additional linguistic features in form of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results. Afterwards, we run a comparison with a <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine</a> and a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network classifier</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.70/>The AICO Multimodal Corpus Data Collection and Preliminary Analyses<span class=acl-fixed-case>AICO</span> Multimodal Corpus – Data Collection and Preliminary Analyses</a></strong><br><a href=/people/k/kristiina-jokinen/>Kristiina Jokinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--70><div class="card-body p-3 small">This paper describes data collection and the first explorative research on the AICO Multimodal Corpus. The corpus contains eye-gaze, <a href=https://en.wikipedia.org/wiki/Kinect>Kinect</a>, and video recordings of human-robot and human-human interactions, and was collected to study cooperation, engagement and attention of human participants in task-based as well as in chatty type interactive situations. In particular, the goal was to enable comparison between human-human and human-robot interactions, besides studying <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal behaviour</a> and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> in the different dialogue activities. The robot partner was a humanoid Nao robot, and it was expected that its agent-like behaviour would render humanrobot interactions similar to human-human interaction but also high-light important differences due to the robot&#8217;s limited conversational capabilities. The paper reports on the preliminary studies on the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, concerning the participants&#8217; eye-gaze and gesturing behaviours, which were chosen as objective measures to study differences in their multimodal behaviour patterns with a human and a robot partner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.75/>RDG-Map : A Multimodal Corpus of Pedagogical Human-Agent Spoken Interactions.<span class=acl-fixed-case>RDG</span>-Map: A Multimodal Corpus of Pedagogical Human-Agent Spoken Interactions.</a></strong><br><a href=/people/m/maike-paetzel/>Maike Paetzel</a>
|
<a href=/people/d/deepthi-karkada/>Deepthi Karkada</a>
|
<a href=/people/r/ramesh-manuvinakurike/>Ramesh Manuvinakurike</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--75><div class="card-body p-3 small">This paper presents a multimodal corpus of 209 spoken game dialogues between a human and a remote-controlled artificial agent. The interactions involve people collaborating with the agent to identify countries on the world map as quickly as possible, which allows studying rapid and spontaneous dialogue with complex anaphoras, disfluent utterances and incorrect descriptions. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of two parts : 8 hours of game interactions have been collected with a virtual unembodied agent online and 26.8 hours have been recorded with a physically embodied robot in a research lab. In addition to spoken audio recordings available for both parts, camera recordings and skeleton-, facial expression- and eye-gaze tracking data have been collected for the lab-based part of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. In this paper, we introduce the pedagogical reference resolution game (RDG-Map) and the characteristics of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> collected. We also present an annotation scheme we developed in order to study the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue strategies</a> utilized by the players. Based on a subset of 330 minutes of interactions annotated so far, we discuss initial insights into these strategies as well as the potential of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.77/>Alexa in the wild Collecting Unconstrained Conversations with a Modern Voice Assistant in a Public Environment<span class=acl-fixed-case>A</span>lexa in the wild” – Collecting Unconstrained Conversations with a Modern Voice Assistant in a Public Environment</a></strong><br><a href=/people/i/ingo-siegert/>Ingo Siegert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--77><div class="card-body p-3 small">Datasets featuring modern voice assistants such as <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, <a href=https://en.wikipedia.org/wiki/Siri>Siri</a>, <a href=https://en.wikipedia.org/wiki/Cortana>Cortana</a> and others allow an easy study of human-machine interactions. But <a href=https://en.wikipedia.org/wiki/Data_collection>data collections</a> offering an unconstrained, unscripted public interaction are quite rare. Many studies so far have focused on private usage, short pre-defined task or specific domains. This contribution presents a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> providing a large amount of unconstrained public interactions with a <a href=https://en.wikipedia.org/wiki/Speech_recognition>voice assistant</a>. Up to now around 40 hours of device directed utterances were collected during a science exhibition touring through Germany. The data recording was part of an exhibit that engages visitors to interact with a commercial voice assistant system (Amazon&#8217;s ALEXA), but did not restrict them to a specific topic. A specifically developed <a href=https://en.wikipedia.org/wiki/Quiz>quiz</a> was starting point of the conversation, as the voice assistant was presented to the visitors as a possible joker for the quiz. But the visitors were not forced to solve the quiz with the help of the voice assistant and thus many visitors had an open conversation. The provided dataset Voice Assistant Conversations in the wild (VACW) includes the transcripts of both visitors requests and Alexa answers, identified topics and sessions as well as acoustic characteristics automatically extractable from the visitors&#8217; audio files.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.80/>Dialogue Act Annotation in a Multimodal Corpus of First Encounter Dialogues</a></strong><br><a href=/people/c/costanza-navarretta/>Costanza Navarretta</a>
|
<a href=/people/p/patrizia-paggio/>Patrizia Paggio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--80><div class="card-body p-3 small">This paper deals with the annotation of dialogue acts in a multimodal corpus of first encounter dialogues, i.e. face-to- face dialogues in which two people who meet for the first time talk with no particular purpose other than just talking. More specifically, we describe the method used to annotate dialogue acts in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, including the evaluation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. Then, we present descriptive statistics of the annotation, particularly focusing on which <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue acts</a> often follow each other across speakers and which <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue acts</a> overlap with <a href=https://en.wikipedia.org/wiki/Gesture>gestural behaviour</a>. Finally, we discuss how <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> is expressed in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by means of <a href=https://en.wikipedia.org/wiki/Feedback>feedback dialogue acts</a> with or without co-occurring gestural behaviour, i.e. multimodal vs. unimodal feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.81/>A Conversation-Analytic Annotation of Turn-Taking Behavior in Japanese Multi-Party Conversation and its Preliminary Analysis<span class=acl-fixed-case>J</span>apanese Multi-Party Conversation and its Preliminary Analysis</a></strong><br><a href=/people/m/mika-enomoto/>Mika Enomoto</a>
|
<a href=/people/y/yasuharu-den/>Yasuharu Den</a>
|
<a href=/people/y/yuichi-ishimoto/>Yuichi Ishimoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--81><div class="card-body p-3 small">In this study, we propose a conversation-analytic annotation scheme for turn-taking behavior in multi-party conversations. The annotation scheme is motivated by a proposal of a proper <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of <a href=https://en.wikipedia.org/wiki/Turn-taking>turn-taking</a> incorporating various ideas developed in the literature of <a href=https://en.wikipedia.org/wiki/Conversation_analysis>conversation analysis</a>. Our <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> consists of two sets of tags : the beginning and the ending type of the utterance. Focusing on the ending-type tags, in some cases combined with the beginning-type tags, we emphasize the importance of the distinction among four selection types : i) selecting other participant as next speaker, ii) not selecting next speaker but followed by a switch of the speakership, iii) not selecting next speaker and followed by a continuation of the speakership, and iv)being inside a multi-unit turn. Based on the annotation of Japanese multi-party conversations, we analyze how syntactic and prosodic features of utterances vary across the four selection types. The results show that the above four-way distinction is essential to account for the distributions of the syntactic and prosodic features, suggesting the insufficiency of previous turn-taking models that do not consider the distinction between i) and ii) or between ii) or iii).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.86/>Dialogue-AMR : Abstract Meaning Representation for Dialogue<span class=acl-fixed-case>AMR</span>: <span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Dialogue</a></strong><br><a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/m/mitchell-abrams/>Mitchell Abrams</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/s/stephen-tratz/>Stephen Tratz</a>
|
<a href=/people/m/matthew-marge/>Matthew Marge</a>
|
<a href=/people/r/ron-artstein/>Ron Artstein</a>
|
<a href=/people/d/david-traum/>David Traum</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--86><div class="card-body p-3 small">This paper describes a schema that enriches Abstract Meaning Representation (AMR) in order to provide a semantic representation for facilitating Natural Language Understanding (NLU) in dialogue systems. AMR offers a valuable level of abstraction of the propositional content of an utterance ; however, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not capture the illocutionary force or speaker&#8217;s intended contribution in the broader dialogue context (e.g., make a request or ask a question), nor does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> capture <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> or aspect. We explore <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> in the domain of <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a>, where a conversational robot is engaged in search and navigation tasks with a human partner. To address the limitations of standard AMR, we develop an inventory of speech acts suitable for our domain, and present Dialogue-AMR, an enhanced AMR that represents not only the content of an utterance, but the illocutionary force behind it, as well as <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>. To showcase the coverage of the <a href=https://en.wikipedia.org/wiki/Database_schema>schema</a>, we use both manual and automatic methods to construct the DialAMR corpusa corpus of human-robot dialogue annotated with standard AMR and our enriched Dialogue-AMR schema. Our automated methods can be used to incorporate AMR into a larger NLU pipeline supporting <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot dialogue</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.87/>Relation between Degree of Empathy for <a href=https://en.wikipedia.org/wiki/Narrative>Narrative Speech</a> and Type of Responsive Utterance in Attentive Listening</a></strong><br><a href=/people/k/koichiro-ito/>Koichiro Ito</a>
|
<a href=/people/m/masaki-murata/>Masaki Murata</a>
|
<a href=/people/t/tomohiro-ohno/>Tomohiro Ohno</a>
|
<a href=/people/s/shigeki-matsubara/>Shigeki Matsubara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--87><div class="card-body p-3 small">Nowadays, <a href=https://en.wikipedia.org/wiki/Intelligent_agent>spoken dialogue agents</a> such as <a href=https://en.wikipedia.org/wiki/Speech_recognition>communication robots</a> and <a href=https://en.wikipedia.org/wiki/Smart_speaker>smart speakers</a> listen to narratives of humans. In order for such an <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agent</a> to be recognized as a listener of narratives and convey the attitude of attentive listening, it is necessary to generate responsive utterances. Moreover, responsive utterances can express <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> to narratives and showing an appropriate degree of <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> to narratives is significant for enhancing speaker&#8217;s motivation. The degree of <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> shown by responsive utterances is thought to depend on their type. However, the relation between responsive utterances and degrees of the <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> has not been explored yet. This paper describes the classification of responsive utterances based on the degree of empathy in order to explain that relation. In this research, responsive utterances are classified into five levels based on the effect of utterances and literature on attentive listening. Quantitative evaluations using 37,995 responsive utterances showed the appropriateness of the proposed <a href=https://en.wikipedia.org/wiki/Categorization>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.91/>Predicting Ratings of Real Dialogue Participants from Artificial Data and Ratings of Human Dialogue Observers</a></strong><br><a href=/people/k/kallirroi-georgila/>Kallirroi Georgila</a>
|
<a href=/people/c/carla-gordon/>Carla Gordon</a>
|
<a href=/people/v/volodymyr-yanov/>Volodymyr Yanov</a>
|
<a href=/people/d/david-traum/>David Traum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--91><div class="card-body p-3 small">We collected a corpus of dialogues in a Wizard of Oz (WOz) setting in the Internet of Things (IoT) domain. We asked users participating in these dialogues to rate the <a href=https://en.wikipedia.org/wiki/System>system</a> on a number of aspects, namely, <a href=https://en.wikipedia.org/wiki/Intelligence>intelligence</a>, naturalness, <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, friendliness, their enjoyment, overall quality, and whether they would recommend the system to others. Then we asked dialogue observers, i.e., Amazon Mechanical Turkers (MTurkers), to rate these dialogues on the same aspects. We also generated simulated dialogues between dialogue policies and simulated users and asked MTurkers to rate them again on the same aspects. Using <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression</a>, we developed dialogue evaluation functions based on features from the simulated dialogues and the MTurkers&#8217; ratings, the WOz dialogues and the MTurkers&#8217; ratings, and the WOz dialogues and the WOz participants&#8217; ratings. We applied all these dialogue evaluation functions to a held-out portion of our WOz dialogues, and we report results on the predictive power of these different types of dialogue evaluation functions. Our results suggest that for three conversational aspects (intelligence, naturalness, overall quality) just training evaluation functions on simulated data could be sufficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.92/>Which Model Should We Use for a Real-World Conversational Dialogue System? a Cross-Language Relevance Model or a <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Net</a>?</a></strong><br><a href=/people/s/seyed-hossein-alavi/>Seyed Hossein Alavi</a>
|
<a href=/people/a/anton-leuski/>Anton Leuski</a>
|
<a href=/people/d/david-traum/>David Traum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--92><div class="card-body p-3 small">We compare two models for corpus-based selection of dialogue responses : one based on cross-language relevance with a cross-language LSTM model. Each <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is tested on multiple corpora, collected from two different types of dialogue source material. Results show that while the LSTM model performs adequately on a very large corpus (millions of utterances), its performance is dominated by the cross-language relevance model for a more moderate-sized corpus (ten thousands of utterances).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.94/>AMUSED : A Multi-Stream Vector Representation Method for Use in Natural Dialogue<span class=acl-fixed-case>AMUSED</span>: A Multi-Stream Vector Representation Method for Use in Natural Dialogue</a></strong><br><a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/r/rishabh-joshi/>Rishabh Joshi</a>
|
<a href=/people/j/jaspreet-singh/>Jaspreet Singh</a>
|
<a href=/people/p/promod-yenigalla/>Promod Yenigalla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--94><div class="card-body p-3 small">The problem of building a coherent and non-monotonous conversational agent with proper <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture that learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the neighborhood of the entities from a Knowledge Base (KB). We benchmark these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> on the next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.95/>An Annotation Approach for Social and Referential Gaze in Dialogue</a></strong><br><a href=/people/v/vidya-somashekarappa/>Vidya Somashekarappa</a>
|
<a href=/people/c/christine-howes/>Christine Howes</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--95><div class="card-body p-3 small">This paper introduces an approach for annotating <a href=https://en.wikipedia.org/wiki/Eye_gaze>eye gaze</a> considering both its social and the referential functions in multi-modal human-human dialogue. Detecting and interpreting the temporal patterns of gaze behavior cues is natural for humans and also mostly an unconscious process. However, these <a href=https://en.wikipedia.org/wiki/Sensory_cue>cues</a> are difficult for conversational agents such as <a href=https://en.wikipedia.org/wiki/Robot>robots</a> or <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatars</a> to process or generate. The key factor is to recognize these variants and carry out a successful conversation, as misinterpretation can lead to total failure of the given interaction. This paper introduces an annotation scheme for <a href=https://en.wikipedia.org/wiki/Eye_contact>eye-gaze</a> in human-human dyadic interactions that is intended to facilitate the learning of eye-gaze patterns in multi-modal natural dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.99/>The Royal Society Corpus 6.0 : Providing 300 + Years of Scientific Writing for <a href=https://en.wikipedia.org/wiki/Humanistic_studies>Humanistic Study</a></a></strong><br><a href=/people/s/stefan-fischer/>Stefan Fischer</a>
|
<a href=/people/j/jorg-knappen/>Jörg Knappen</a>
|
<a href=/people/k/katrin-menzel/>Katrin Menzel</a>
|
<a href=/people/e/elke-teich/>Elke Teich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--99><div class="card-body p-3 small">We present a new, extended version of the Royal Society Corpus (RSC), a diachronic corpus of scientific English now covering 300 + years of scientific writing (16651996). The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 47 837 texts, primarily scientific articles, and is based on publications of the Royal Society of London, mainly its Philosophical Transactions and Proceedings. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has been built on the basis of the FAIR principles and is freely available under a Creative Commons license, excluding copy-righted parts. We provide information on how the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can be found, the file formats available for download as well as accessibility via a web-based corpus query platform. We show a number of analytic tools that we have implemented for better usability and provide an example of use of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a> as well as examples of subsequent, external uses of earlier releases. We place the RSC against the background of existing English diachronic / scientific corpora, elaborating on its value for linguistic and humanistic study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.104/>RiQuA : A Corpus of Rich Quotation Annotation for English Literary Text<span class=acl-fixed-case>R</span>i<span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>A</span>: A Corpus of Rich Quotation Annotation for <span class=acl-fixed-case>E</span>nglish Literary Text</a></strong><br><a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--104><div class="card-body p-3 small">We introduce RiQuA (RIch QUotation Annotations), a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that provides <a href=https://en.wikipedia.org/wiki/Quotation>quotations</a>, including their interpersonal structure (speakers and addressees) for English literary text. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 11 works of 19th-century literature that were manually doubly annotated for direct and indirect quotations. For each <a href=https://en.wikipedia.org/wiki/Quotation>quotation</a>, its span, speaker, addressee, and cue are identified (if present). This provides a rich view of dialogue structures not available from other available <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>. We detail the process of creating this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, discuss the annotation guidelines, and analyze the resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> in terms of inter-annotator agreement and its properties. RiQuA, along with its annotations guidelines and associated scripts, are publicly available for use, modification, and experimentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.106/>The BDCames Collection of Portuguese Literary Documents : a Research Resource for Digital Humanities and Language Technology<span class=acl-fixed-case>BDC</span>amões Collection of <span class=acl-fixed-case>P</span>ortuguese Literary Documents: a Research Resource for Digital Humanities and Language Technology</a></strong><br><a href=/people/s/sara-grilo/>Sara Grilo</a>
|
<a href=/people/m/marcia-bolrinha/>Márcia Bolrinha</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/r/rui-vaz/>Rui Vaz</a>
|
<a href=/people/a/antonio-branco/>António Branco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--106><div class="card-body p-3 small">This paper presents the BDCames Collection of <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese Literary Documents</a>, a new corpus of literary texts written in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> that in its inaugural version includes close to 4 million words from over 200 complete documents from 83 authors in 14 genres, covering a time span from the 16th to the 21st century, and adhering to different orthographic conventions. Many of the texts in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> have also been automatically parsed with state-of-the-art language processing tools, forming the BDCames Treebank subcorpus. This set of characteristics makes of BDCames an invaluable resource for research in <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a> (e.g. authorship detection, <a href=https://en.wikipedia.org/wiki/Genre>genre classification</a>, etc.) and in <a href=https://en.wikipedia.org/wiki/Linguistics>language science</a> and <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a> (e.g. comparative literature, <a href=https://en.wikipedia.org/wiki/Historical_linguistics>diachronic linguistics</a>, etc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.109/>NLP Scholar : A Dataset for Examining the State of NLP Research<span class=acl-fixed-case>NLP</span> Scholar: A Dataset for Examining the State of <span class=acl-fixed-case>NLP</span> Research</a></strong><br><a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--109><div class="card-body p-3 small">Google Scholar is the largest <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engine</a> for <a href=https://en.wikipedia.org/wiki/Academic_literature>academic literature</a> that also provides access to <a href=https://en.wikipedia.org/wiki/Metadata>rich metadata</a> associated with the papers. The ACL Anthology (AA) is the largest repository of articles on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. We extracted information from AA for about 44 thousand NLP papers and identified authors who published at least three papers there. We then extracted citation information from <a href=https://en.wikipedia.org/wiki/Google_Scholar>Google Scholar</a> for all their papers (not just their AA papers). This resulted in a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 1.1 million papers and associated <a href=https://en.wikipedia.org/wiki/Google_Scholar>Google Scholar information</a>. We aligned the information in the AA and Google Scholar datasets to create the NLP Scholar Dataset a single unified source of information (from both AA and Google Scholar) for tens of thousands of NLP papers. It can be used to identify broad trends in productivity, focus, and impact of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. We present here initial work on analyzing the volume of research in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> over the years and identifying the most cited papers in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We also list a number of additional potential applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.110/>The DReaM Corpus : A Multilingual Annotated Corpus of Grammars for the World’s Languages<span class=acl-fixed-case>DR</span>ea<span class=acl-fixed-case>M</span> Corpus: A Multilingual Annotated Corpus of Grammars for the World’s Languages</a></strong><br><a href=/people/s/shafqat-mumtaz-virk/>Shafqat Mumtaz Virk</a>
|
<a href=/people/h/harald-hammarstrom/>Harald Hammarström</a>
|
<a href=/people/m/markus-forsberg/>Markus Forsberg</a>
|
<a href=/people/s/soren-wichmann/>Søren Wichmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--110><div class="card-body p-3 small">There exist as many as 7000 natural languages in the world, and a huge number of documents describing those <a href=https://en.wikipedia.org/wiki/Language>languages</a> have been produced over the years. Most of those documents are in paper format. Any attempts to use modern computational techniques and tools to process those documents will require them to be digitized first. In this paper, we report a multilingual digitized version of thousands of such documents searchable through some well-established corpus infrastructures. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is annotated with various meta, word, and text level attributes to make searching and analysis easier and more useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.118/>Identification of Indigenous Knowledge Concepts through <a href=https://en.wikipedia.org/wiki/Semantic_network>Semantic Networks</a>, Spelling Tools and Word Embeddings</a></strong><br><a href=/people/r/renato-rocha-souza/>Renato Rocha Souza</a>
|
<a href=/people/a/amelie-dorn/>Amelie Dorn</a>
|
<a href=/people/b/barbara-piringer/>Barbara Piringer</a>
|
<a href=/people/e/eveline-wandl-vogt/>Eveline Wandl-Vogt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--118><div class="card-body p-3 small">In order to access indigenous, regional knowledge contained in <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>language corpora</a>, semantic tools and <a href=https://en.wikipedia.org/wiki/Network_theory>network methods</a> are most typically employed. In this paper we present an approach for the identification of dialectal variations of words, or words that do not pertain to High German, on the example of non-standard language legacy collection questionnaires of the Bavarian Dialects in Austria (DB). Based on selected <a href=https://en.wikipedia.org/wiki/Culture>cultural categories</a> relevant to the wider project context, common words from each of these <a href=https://en.wikipedia.org/wiki/Culture>cultural categories</a> and their lemmas using GermaLemma were identified. Through word embedding models the semantic vicinity of each word was explored, followed by the use of German Wordnet (Germanet) and the Hunspell tool. Whilst none of these tools have a comprehensive coverage of standard German words, they serve as an indication of dialects in specific semantic hierarchies. Methods and tools applied in this study may serve as an example for other similar projects dealing with non-standard or endangered language collections, aiming to access, analyze and ultimately preserve native regional language heritage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.120/>An Annotated Corpus of Adjective-Adverb Interfaces in Romance Languages<span class=acl-fixed-case>R</span>omance Languages</a></strong><br><a href=/people/k/katharina-gerhalter/>Katharina Gerhalter</a>
|
<a href=/people/g/gerlinde-schneider/>Gerlinde Schneider</a>
|
<a href=/people/c/christopher-pollin/>Christopher Pollin</a>
|
<a href=/people/m/martin-hummel/>Martin Hummel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--120><div class="card-body p-3 small">The final outcome of the project Open Access Database : Adjective-Adverb Interfaces in <a href=https://en.wikipedia.org/wiki/Romance_languages>Romance</a> is an annotated and lemmatised corpus of various linguistic phenomena related to Romance adjectives with adverbial functions. The data is published under open-access and aims to serve linguistic research based on transparent and accessible corpus-based data. The annotation model was developed to offer a cross-linguistic categorization model for the heterogeneous word-class adverb, based on its diverse forms, functions and meanings. The project focuses on the interoperability and accessibility of data, with particular respect to reusability in the sense of the FAIR Data Principles. Topics presented by this paper include data compilation and creation, annotation in XML / TEI, data preservation and publication process by means of the GAMS repository and accessibility via a search interface. These aspects are tied together by <a href=https://en.wikipedia.org/wiki/Semantic_technology>semantic technologies</a>, using an ontology-based approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.128/>Chinese Discourse Parsing : Model and Evaluation<span class=acl-fixed-case>C</span>hinese Discourse Parsing: Model and Evaluation</a></strong><br><a href=/people/l/lin-chuan-an/>Lin Chuan-An</a>
|
<a href=/people/s/shyh-shiun-hung/>Shyh-Shiun Hung</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--128><div class="card-body p-3 small">Chinese discourse parsing, which aims to identify the hierarchical relationships of Chinese elementary discourse units, has not yet a consistent evaluation metric. Although <a href=https://en.wikipedia.org/wiki/Parseval>Parseval</a> is commonly used, variations of evaluation differ from three aspects : micro vs. macro F1 scores, binary vs. multiway ground truth, and left-heavy vs. right-heavy binarization. In this paper, we first propose a neural network model that unifies a pre-trained transformer and CKY-like algorithm, and then compare it with the previous models with different evaluation scenarios. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous <a href=https://en.wikipedia.org/wiki/System>systems</a>. We conclude that (1) the pre-trained context embedding provides effective solutions to deal with implicit semantics in Chinese texts, and (2) using multiway ground truth is helpful since different binarization approaches lead to significant differences in performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.133/>The Potsdam Commentary Corpus 2.2 : Extending Annotations for Shallow Discourse Parsing<span class=acl-fixed-case>P</span>otsdam Commentary Corpus 2.2: Extending Annotations for Shallow Discourse Parsing</a></strong><br><a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--133><div class="card-body p-3 small">We present the Potsdam Commentary Corpus 2.2, a German corpus of news editorials annotated on several different levels. New in the 2.2 version of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> are two additional annotation layers for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence relations</a> following the Penn Discourse TreeBank framework. Specifically, we add relation senses to an already existing layer of discourse connectives and their arguments, and we introduce a new layer with additional coherence relation types, resulting in a German corpus that mirrors the PDTB. The aim of this is to increase usability of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for the task of <a href=https://en.wikipedia.org/wiki/Shallow_parsing>shallow discourse parsing</a>. In this paper, we provide inter-annotator agreement figures for the new <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and compare corpus statistics based on the new <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to the equivalent statistics extracted from the PDTB.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.138/>DiMLex-Bangla : A Lexicon of Bangla Discourse Connectives<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>ML</span>ex-<span class=acl-fixed-case>B</span>angla: A Lexicon of <span class=acl-fixed-case>B</span>angla Discourse Connectives</a></strong><br><a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/s/soumya-sankar-ghosh/>Soumya Sankar Ghosh</a>
|
<a href=/people/l/lahari-chatterjee/>Lahari Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--138><div class="card-body p-3 small">We present DiMLex-Bangla, a newly developed lexicon of discourse connectives in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla</a>. The <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>, upon completion of its first version, contains 123 Bangla connective entries, which are primarily compiled from the linguistic literature and translation of English discourse connectives. The lexicon compilation is later augmented by adding more connectives from a currently developed <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, called the Bangla RST Discourse Treebank (Das and Stede, 2018). DiMLex-Bangla provides information on syntactic categories of Bangla connectives, their discourse semantics and non-connective uses (if any). It uses the format of the German connective lexicon DiMLex (Stede and Umbach, 1998), which provides a cross-linguistically applicable XML schema. The resource is the first of its kind in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla</a>, and is freely available for use in studies on discourse structure and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.146/>What Speakers really Mean when they Ask Questions : Classification of Intentions with a Supervised Approach</a></strong><br><a href=/people/a/angele-barbedette/>Angèle Barbedette</a>
|
<a href=/people/i/iris-eshkol/>Iris Eshkol-Taravella</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--146><div class="card-body p-3 small">This paper focuses on the automatic detection of hidden intentions of speakers in questions asked during meals. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is composed of a set of transcripts of spontaneous oral conversations from ESLO&#8217;s corpora. We suggest a typology of these intentions based on our research work and the exploration and annotation of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, in which we define two explicit categories (request for agreement and request for information) and three implicit categories (opinion, will and doubt). We implement a supervised automatic classification model based on annotated data and selected <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> and we evaluate its results and performances. We finally try to interpret these results by looking more deeply and specifically into the predictions of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> and the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> it used. There are many motivations for this work which are part of ongoing challenges such as opinion analysis, irony detection or the development of conversational agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.147/>Modeling Dialogue in Conversational Cognitive Health Screening Interviews</a></strong><br><a href=/people/s/shahla-farzana/>Shahla Farzana</a>
|
<a href=/people/m/mina-valizadeh/>Mina Valizadeh</a>
|
<a href=/people/n/natalie-parde/>Natalie Parde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--147><div class="card-body p-3 small">Automating straightforward clinical tasks can reduce workload for healthcare professionals, increase accessibility for geographically-isolated patients, and alleviate some of the economic burdens associated with healthcare. A variety of preliminary <a href=https://en.wikipedia.org/wiki/Screening_(medicine)>screening procedures</a> are potentially suitable for automation, and one such domain that has remained underexplored to date is that of structured clinical interviews. A task-specific dialogue agent is needed to automate the collection of conversational speech for further (either manual or automated) analysis, and to build such an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>, a <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a> must be trained to respond to patient utterances in a manner similar to a human interviewer. To facilitate the development of such an agent, we propose an annotation schema for assigning dialogue act labels to utterances in patient-interviewer conversations collected as part of a clinically-validated cognitive health screening task. We build a labeled corpus using the <a href=https://en.wikipedia.org/wiki/Database_schema>schema</a>, and show that it is characterized by high inter-annotator agreement. We establish a benchmark dialogue act classification model for the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, thereby providing a proof of concept for the proposed annotation schema. The resulting dialogue act corpus is the first such <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> specifically designed to facilitate automated cognitive health screening, and lays the groundwork for future exploration in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.148/>Stigma Annotation Scheme and Stigmatized Language Detection in Health-Care Discussions on Social Media</a></strong><br><a href=/people/n/nadiya-straton/>Nadiya Straton</a>
|
<a href=/people/h/hyeju-jang/>Hyeju Jang</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--148><div class="card-body p-3 small">Much research has been done within the <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> on the interpretation and influence of <a href=https://en.wikipedia.org/wiki/Social_stigma>stigma</a> on human behaviour and health, which result in out-of-group exclusion, distancing, cognitive separation, status loss, <a href=https://en.wikipedia.org/wiki/Discrimination>discrimination</a>, in-group pressure, and often lead to disengagement, non-adherence to treatment plan, and prescriptions by the doctor. However, little work has been conducted on computational identification of stigma in general and in social media discourse in particular. In this paper, we develop the annotation scheme and improve the annotation process for stigma identification, which can be applied to other health-care domains. The data from pro-vaccination and anti-vaccination discussion groups are annotated by trained annotators who have professional background in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a> and health-care studies, therefore the group can be considered experts on the subject in comparison to non-expert crowd. Amazon MTurk annotators is another group of annotator with no knowledge on their education background, they are initially treated as non-expert crowd on the subject matter of stigma. We analyze the annotations with visualisation techniques, features from LIWC (Linguistic Inquiry and Word Count) list and make prediction based on bi-grams with traditional and deep learning models. Data augmentation method and application of CNN show high performance accuracy in comparison to other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Success of the rigorous annotation process on identifying stigma is reconfirmed by achieving high prediction rate with <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.159/>Discovering Biased News Articles Leveraging Multiple Human Annotations</a></strong><br><a href=/people/k/konstantina-lazaridou/>Konstantina Lazaridou</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a>
|
<a href=/people/m/maria-mestre/>Maria Mestre</a>
|
<a href=/people/f/felix-naumann/>Felix Naumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--159><div class="card-body p-3 small">Unbiased and fair reporting is an integral part of <a href=https://en.wikipedia.org/wiki/Ethical_journalism>ethical journalism</a>. Yet, <a href=https://en.wikipedia.org/wiki/Propaganda>political propaganda</a> and one-sided views can be found in the news and can cause distrust in media. Both accidental and deliberate political bias affect the readers and shape their views. We contribute to a trustworthy media ecosystem by automatically identifying politically biased news articles. We introduce novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> annotated by two <a href=https://en.wikipedia.org/wiki/Community>communities</a>, i.e., domain experts and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd workers</a>, and we also consider automatic article labels inferred by the newspapers&#8217; ideologies. Our goal is to compare <a href=https://en.wikipedia.org/wiki/Expert>domain experts</a> to <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd workers</a> and also to prove that <a href=https://en.wikipedia.org/wiki/Media_bias>media bias</a> can be detected automatically. We classify <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> with a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and we also improve our performance in a self-supervised manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.160/>Corpora and Baselines for Humour Recognition in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a><span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a>
|
<a href=/people/a/andre-clemencio/>André Clemêncio</a>
|
<a href=/people/a/ana-alves/>Ana Alves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--160><div class="card-body p-3 small">Having in mind the lack of work on the automatic recognition of verbal humour in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, a topic connected with fluency in a <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, we describe the creation of three corpora, covering two styles of <a href=https://en.wikipedia.org/wiki/Humour>humour</a> and four sources of non-humorous text, that may be used for related studies. We then report on some experiments where the created <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> were used for training and testing <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> that exploit content and linguistic features for humour recognition. The obtained results helped us taking some conclusions about this challenge and may be seen as baselines for those willing to tackle it in the future, using the same <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.163/>Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Networks</a> with Intra- and Inter-Sentence Context to Classify Suicidal Behaviour</a></strong><br><a href=/people/x/xingyi-song/>Xingyi Song</a>
|
<a href=/people/j/johnny-downs/>Johnny Downs</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a>
|
<a href=/people/r/rachel-holden/>Rachel Holden</a>
|
<a href=/people/m/maxim-kikoler/>Maxim Kikoler</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/r/rina-dutta/>Rina Dutta</a>
|
<a href=/people/a/angus-roberts/>Angus Roberts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--163><div class="card-body p-3 small">Identifying statements related to suicidal behaviour in psychiatric electronic health records (EHRs) is an important step when modeling that <a href=https://en.wikipedia.org/wiki/Behavior>behaviour</a>, and when assessing suicide risk. We apply a deep neural network based classification model with a lightweight context encoder, to classify sentence level suicidal behaviour in <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a>. We show that incorporating information from sentences to left and right of the target sentence significantly improves classification accuracy. Our approach achieved the best performance when classifying <a href=https://en.wikipedia.org/wiki/Suicide>suicidal behaviour</a> in Autism Spectrum Disorder patient records. The results could have implications for suicidality research and <a href=https://en.wikipedia.org/wiki/Clinical_surveillance>clinical surveillance</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.165/>Habibi-a multi Dialect multi National Arabic Song Lyrics Corpus<span class=acl-fixed-case>A</span>rabic Song Lyrics Corpus</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--165><div class="card-body p-3 small">This paper introduces Habibi the first Arabic Song Lyrics corpus. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises more than 30,000 <a href=https://en.wikipedia.org/wiki/Arabic_music>Arabic song lyrics</a> in 6 <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> for singers from 18 different Arabic countries. The <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a> are segmented into more than 500,000 sentences (song verses) with more than 3.5 million words. I provide the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> in both comma separated value (csv) and annotated plain text (txt) file formats. In addition, I converted the csv version into JavaScript Object Notation (json) and eXtensible Markup Language (xml) file formats. To experiment with the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> I run extensive binary and multi-class experiments for dialect and country-of-origin identification. The identification tasks include the use of several classical machine learning and deep learning models utilising different <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. For the binary dialect identification task the best performing <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> achieved a testing accuracy of 93 %. This was achieved using a word-based Convolutional Neural Network (CNN) utilising a Continuous Bag of Words (CBOW) word embeddings model. The results overall show all classical and deep learning models to outperform our baseline, which demonstrates the suitability of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for both dialect and country-of-origin identification tasks. I am making the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and the trained CBOW word embeddings freely available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.167/>Email Classification Incorporating <a href=https://en.wikipedia.org/wiki/Social_network>Social Networks</a> and Thread Structure</a></strong><br><a href=/people/s/sakhar-alkhereyf/>Sakhar Alkhereyf</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--167><div class="card-body p-3 small">Existing methods for different document classification tasks in the context of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> typically only capture the semantics of texts, while ignoring the users who exchange the text and the network they form. However, some work has shown that incorporating the social network information in addition to information from language is effective for various NLP applications including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, inferring user attributes, and predicting inter-personal relations. In this paper, we present an empirical study of email classification into Business and Personal categories. We represent the <a href=https://en.wikipedia.org/wiki/Email>email communication</a> using various <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structures</a>. As <a href=https://en.wikipedia.org/wiki/Software_feature>features</a>, we use both the textual information from the email content and <a href=https://en.wikipedia.org/wiki/Social_network>social network information</a> from the <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>communication graphs</a>. We also model the <a href=https://en.wikipedia.org/wiki/Thread_(computing)>thread structure</a> for <a href=https://en.wikipedia.org/wiki/Email>emails</a>. We focus on detecting personal emails, and we evaluate our methods on two corpora, only one of which we train on. The experimental results reveal that incorporating <a href=https://en.wikipedia.org/wiki/Social_network>social network information</a> improves over the performance of an approach based on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual information</a> only. The results also show that considering the thread structure of emails improves the performance further. Furthermore, our approach improves over a state-of-the-art baseline which uses node embeddings based on both lexical and social network information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.169/>Alector : A Parallel Corpus of Simplified French Texts with Alignments of Misreadings by Poor and Dyslexic Readers<span class=acl-fixed-case>A</span>lector: A Parallel Corpus of Simplified <span class=acl-fixed-case>F</span>rench Texts with Alignments of Misreadings by Poor and Dyslexic Readers</a></strong><br><a href=/people/n/nuria-gala/>Núria Gala</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/l/ludivine-javourey-drevet/>Ludivine Javourey-Drevet</a>
|
<a href=/people/t/thomas-francois/>Thomas François</a>
|
<a href=/people/j/johannes-c-ziegler/>Johannes C. Ziegler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--169><div class="card-body p-3 small">In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> addressed to researchers, teachers, and speech therapists interested in text simplification as a means of alleviating difficulties in children learning to read. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is composed of excerpts drawn from 79 authentic literary (tales, stories) and scientific (documentary) texts commonly used in <a href=https://en.wikipedia.org/wiki/List_of_schools_in_France>French schools</a> for children aged between 7 to 9 years old. The excerpts were manually simplified at the lexical, morpho-syntactic, and discourse levels in order to propose a parallel corpus for reading tests and for the development of automatic text simplification tools. A sample of 21 poor-reading and dyslexic children with an average reading delay of 2.5 years read a portion of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. The transcripts of readings errors were integrated into the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with the goal of identifying lexical difficulty in the target population. By means of <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>statistical testing</a>, we provide evidence that the manual simplifications significantly reduced reading errors, highlighting that the words targeted for simplification were not only well-chosen but also substituted with substantially easier alternatives. The entire corpus is available for consultation through a web interface and available on demand for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.172/>An Evaluation of Progressive Neural Networksfor <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> in Natural Language Processing</a></strong><br><a href=/people/a/abdul-moeed/>Abdul Moeed</a>
|
<a href=/people/g/gerhard-hagerer/>Gerhard Hagerer</a>
|
<a href=/people/s/sumit-dugar/>Sumit Dugar</a>
|
<a href=/people/s/sarthak-gupta/>Sarthak Gupta</a>
|
<a href=/people/m/mainak-ghosh/>Mainak Ghosh</a>
|
<a href=/people/h/hannah-danner/>Hannah Danner</a>
|
<a href=/people/o/oliver-mitevski/>Oliver Mitevski</a>
|
<a href=/people/a/andreas-nawroth/>Andreas Nawroth</a>
|
<a href=/people/g/georg-groh/>Georg Groh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--172><div class="card-body p-3 small">A major challenge in modern <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> is the utilization of previous knowledge for new tasks in an effective manner, otherwise known as <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Fine-tuning, the most widely used method for achieving this, suffers from catastrophic forgetting. The problem is often exacerbated in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. In this work, we assess progressive neural networks (PNNs) as an alternative to <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. The <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is based on common NLP tasks such as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> and <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. By gauging PNNs across a range of <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>, datasets, and tasks, we observe improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> throughout all experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.178/>DecOp : A Multilingual and Multi-domain Corpus For Detecting Deception In Typed Text<span class=acl-fixed-case>D</span>ec<span class=acl-fixed-case>O</span>p: A Multilingual and Multi-domain Corpus For Detecting Deception In Typed Text</a></strong><br><a href=/people/p/pasquale-capuozzo/>Pasquale Capuozzo</a>
|
<a href=/people/i/ivano-lauriola/>Ivano Lauriola</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/f/fabio-aiolli/>Fabio Aiolli</a>
|
<a href=/people/g/giuseppe-sartori/>Giuseppe Sartori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--178><div class="card-body p-3 small">In recent years, the increasing interest in the development of automatic approaches for unmasking deception in online sources led to promising results. Nonetheless, among the others, two major issues remain still unsolved : the stability of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> performances across different domains and languages. Tackling these issues is challenging since labelled corpora involving multiple domains and compiled in more than one language are few in the scientific literature. For filling this gap, in this paper we introduce DecOp (Deceptive Opinions), a new language resource developed for automatic deception detection in cross-domain and cross-language scenarios. DecOp is composed of 5000 examples of both truthful and deceitful first-person opinions balanced both across five different domains and two languages and, to the best of our knowledge, is the largest corpus allowing cross-domain and cross-language comparisons in deceit detection tasks. In this paper, we describe the collection procedure of the DecOp corpus and his main characteristics. Moreover, the human performance on the DecOp test-set and preliminary experiments by means of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> based on Transformer architecture are shown.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.181/>VICTOR : a Dataset for Brazilian Legal Documents Classification<span class=acl-fixed-case>VICTOR</span>: a Dataset for <span class=acl-fixed-case>B</span>razilian Legal Documents Classification</a></strong><br><a href=/people/p/pedro-henrique-luz-de-araujo/>Pedro Henrique Luz de Araujo</a>
|
<a href=/people/t/teofilo-emidio-de-campos/>Teófilo Emídio de Campos</a>
|
<a href=/people/f/fabricio-ataides-braz/>Fabricio Ataides Braz</a>
|
<a href=/people/n/nilton-correia-da-silva/>Nilton Correia da Silva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--181><div class="card-body p-3 small">This paper describes VICTOR, a novel dataset built from Brazil&#8217;s Supreme Court digitalized legal documents, composed of more than 45 thousand appeals, which includes roughly 692 thousand documentsabout 4.6 million pages. The dataset contains labeled text data and supports two types of tasks : document type classification ; and theme assignment, a multilabel problem. We present baseline results using <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words models</a>, <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and <a href=https://en.wikipedia.org/wiki/Boosting_(machine_learning)>boosting algorithms</a>. We also experiment using linear-chain Conditional Random Fields to leverage the sequential nature of the lawsuits, which we find to lead to improvements on document type classification. Finally we compare a theme classification approach where we use <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to filter out the less informative document pages to the default one where we use all pages. Contrary to the Court experts&#8217; expectations, we find that using all available data is the better method. We make the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> available in three versions of different sizes and contents to encourage explorations of better <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.191/>Abusive language in Spanish children and young teenager’s conversations : <a href=https://en.wikipedia.org/wiki/Data_preparation>data preparation</a> and short text classification with contextual word embeddings<span class=acl-fixed-case>S</span>panish children and young teenager’s conversations: data preparation and short text classification with contextual word embeddings</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/e/esther-gonzalez/>Esther González</a>
|
<a href=/people/a/asuncion-moreno/>Asuncion Moreno</a>
|
<a href=/people/e/eudald-cumalat/>Eudald Cumalat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--191><div class="card-body p-3 small">Abusive texts are reaching the interests of the scientific and social community. How to automatically detect them is onequestion that is gaining interest in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing community</a>. The main contribution of this paper is toevaluate the quality of the recently developed Spanish Database for cyberbullying prevention for the purpose of trainingclassifiers on detecting abusive short texts. We compare classical machine learning techniques to the use of a more ad-vanced model : the contextual word embeddings in the particular case of classification of abusive short-texts for the <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanishlanguage</a>. As contextual word embeddings, we use Bidirectional Encoder Representation from Transformers (BERT), pro-posed at the end of 2018. We show that BERT mostly outperforms classical techniques. Far beyond the experimentalimpact of our research, this project aims at planting the seeds for an innovative technological tool with a high potentialsocial impact and aiming at being part of the initiatives in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> for social good.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.195/>SOLO : A Corpus of Tweets for Examining the State of Being Alone<span class=acl-fixed-case>SOLO</span>: A Corpus of Tweets for Examining the State of Being Alone</a></strong><br><a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a>
|
<a href=/people/w/will-hipson/>Will Hipson</a>
|
<a href=/people/r/robert-coplan/>Robert Coplan</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--195><div class="card-body p-3 small">The state of being alone can have a substantial impact on our lives, though experiences with time alone diverge significantly among individuals. Psychologists distinguish between the concept of <a href=https://en.wikipedia.org/wiki/Solitude>solitude</a>, a positive state of voluntary aloneness, and the concept of <a href=https://en.wikipedia.org/wiki/Loneliness>loneliness</a>, a negative state of dissatisfaction with the quality of one&#8217;s social interactions. Here, for the first time, we conduct a large-scale computational analysis to explore how the <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms</a> associated with the state of being alone are used in online language. We present SOLO (State of Being Alone), a corpus of over 4 million tweets collected with query terms solitude, <a href=https://en.wikipedia.org/wiki/Loneliness>lonely</a>, and <a href=https://en.wikipedia.org/wiki/Loneliness>loneliness</a>. We use SOLO to analyze the language and emotions associated with the state of being alone. We show that the term <a href=https://en.wikipedia.org/wiki/Solitude>solitude</a> tends to co-occur with more <a href=https://en.wikipedia.org/wiki/Dominance_(linguistics)>positive, high-dominance words</a> (e.g., enjoy, bliss) while the terms <a href=https://en.wikipedia.org/wiki/Loneliness>lonely</a> and <a href=https://en.wikipedia.org/wiki/Loneliness>loneliness</a> frequently co-occur with <a href=https://en.wikipedia.org/wiki/Dominance_(linguistics)>negative, low-dominance words</a> (e.g., scared, depressed), which confirms the conceptual distinctions made in psychology. We also show that women are more likely to report on negative feelings of being lonely as compared to men, and there are more teenagers among the tweeters that use the word lonely than among the tweeters that use the word solitude.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.199/>Korean-Specific Emotion Annotation Procedure Using N-Gram-Based Distant Supervision and Korean-Specific-Feature-Based Distant Supervision<span class=acl-fixed-case>K</span>orean-Specific Emotion Annotation Procedure Using <span class=acl-fixed-case>N</span>-Gram-Based Distant Supervision and <span class=acl-fixed-case>K</span>orean-Specific-Feature-Based Distant Supervision</a></strong><br><a href=/people/y/young-jun-lee/>Young-Jun Lee</a>
|
<a href=/people/c/chae-gyun-lim/>Chae-Gyun Lim</a>
|
<a href=/people/h/ho-jin-choi/>Ho-Jin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--199><div class="card-body p-3 small">Detecting emotions from texts is considerably important in an NLP task, but it has the limitation of the scarcity of manually labeled data. To overcome this limitation, many researchers have annotated unlabeled data with certain frequently used annotation procedures. However, most of these studies are focused mainly on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and do not consider the characteristics of the <a href=https://en.wikipedia.org/wiki/Korean_language>Korean language</a>. In this paper, we present a Korean-specific annotation procedure, which consists of two parts, namely n-gram-based distant supervision and Korean-specific-feature-based distant supervision. We leverage the distant supervision with the n-gram and Korean emotion lexicons. Then, we consider the Korean-specific emotion features. Through experiments, we showed the effectiveness of our <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> by comparing with the KTEA dataset. Additionally, we constructed a large-scale emotion-labeled dataset, Korean Movie Review Emotion (KMRE) Dataset, using our procedure. In order to construct our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we used a large-scale sentiment movie review corpus as the unlabeled dataset. Moreover, we used a Korean emotion lexicon provided by KTEA. We also performed an <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification task</a> and a human evaluation on the KMRE dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.204/>An Emotional Mess ! Deciding on a Framework for Building a Dutch Emotion-Annotated Corpus<span class=acl-fixed-case>D</span>utch Emotion-Annotated Corpus</a></strong><br><a href=/people/l/luna-de-bruyne/>Luna De Bruyne</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--204><div class="card-body p-3 small">Seeing the myriad of existing emotion models, with the categorical versus dimensional opposition the most important dividing line, building an emotion-annotated corpus requires some well thought-out strategies concerning framework choice. In our work on automatic emotion detection in Dutch texts, we investigate this problem by means of two case studies. We find that the labels <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Love>love</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, sadness and <a href=https://en.wikipedia.org/wiki/Fear>fear</a> are well-suited to annotate texts coming from various domains and topics, but that the connotation of the labels strongly depends on the origin of the texts. Moreover, it seems that information is lost when an <a href=https://en.wikipedia.org/wiki/Emotion>emotional state</a> is forcedly classified in a limited set of categories, indicating that a bi-representational format is desirable when creating an emotion corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.208/>Identification of Primary and Collateral Tracks in Stuttered Speech</a></strong><br><a href=/people/r/rachid-riad/>Rachid Riad</a>
|
<a href=/people/a/anne-catherine-bachoud-levi/>Anne-Catherine Bachoud-Lévi</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--208><div class="card-body p-3 small">Disfluent speech has been previously addressed from two main perspectives : the clinical perspective focusing on <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnostic</a>, and the Natural Language Processing (NLP) perspective aiming at modeling these events and detect them for downstream tasks. In addition, previous works often used different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> depending on whether the input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are text or speech, making it difficult to compare the different contributions. Here, we introduce a new evaluation framework for disfluency detection inspired by the clinical and NLP perspective together with the theory of performance from (Clark, 1996) which distinguishes between primary and collateral tracks. We introduce a novel forced-aligned disfluency dataset from a corpus of semi-directed interviews, and present baseline results directly comparing the performance of text-based features (word and span information) and speech-based (acoustic-prosodic information). Finally, we introduce new audio features inspired by the word-based span features. We show experimentally that using these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> outperformed the baselines for speech-based predictions on the present <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.211/>HardEval : Focusing on Challenging Tokens to Assess Robustness of NER<span class=acl-fixed-case>H</span>ard<span class=acl-fixed-case>E</span>val: Focusing on Challenging Tokens to Assess Robustness of <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--211><div class="card-body p-3 small">To assess the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of NER systems, we propose an evaluation method that focuses on subsets of tokens that represent specific sources of errors : unknown words and label shift or <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. These <a href=https://en.wikipedia.org/wiki/Subset>subsets</a> provide a system-agnostic basis for evaluating specific sources of NER errors and assessing room for improvement in terms of robustness. We analyze these subsets of challenging tokens in two widely-used NER benchmarks, then exploit them to evaluate NER systems in both in-domain and out-of-domain settings. Results show that these challenging tokens explain the majority of errors made by modern NER systems, although they represent only a small fraction of test tokens. They also indicate that label shift is harder to deal with than unknown words, and that there is much more room for improvement than the standard NER evaluation procedure would suggest. We hope this work will encourage NLP researchers to adopt rigorous and meaningful evaluation methods, and will help them develop more robust models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.217/>Linguistic Appropriateness and Pedagogic Usefulness of Reading Comprehension Questions</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/i/itziar-aldabe/>Itziar Aldabe</a>
|
<a href=/people/m/marie-bexte/>Marie Bexte</a>
|
<a href=/people/o/oier-lopez-de-lacalle/>Oier Lopez de Lacalle</a>
|
<a href=/people/m/montse-maritxalar/>Montse Maritxalar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--217><div class="card-body p-3 small">Automatic generation of reading comprehension questions is a topic receiving growing interest in the NLP community, but there is currently no consensus on evaluation metrics and many approaches focus on linguistic quality only while ignoring the pedagogic value and appropriateness of questions. This paper overcomes such weaknesses by a new evaluation scheme where questions from the questionnaire are structured in a hierarchical way to avoid confronting human annotators with evaluation measures that do not make sense for a certain question. We show through an annotation study that our scheme can be applied, but that expert annotators with some level of expertise are needed. We also created and evaluated two new evaluation data sets from the biology domain for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, composed of questions written by people with an educational background, which will be publicly released. Results show that manually generated questions are in general both of higher linguistic as well as pedagogic quality and that among the human generated questions, teacher-generated ones tend to be most useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.228/>An In-Depth Comparison of 14 Spelling Correction Tools on a Common Benchmark</a></strong><br><a href=/people/m/markus-nather/>Markus Näther</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--228><div class="card-body p-3 small">Determining and correcting spelling and grammar errors in text is an important but surprisingly difficult task. There are several reasons why this remains challenging. Errors may consist of simple <a href=https://en.wikipedia.org/wiki/Typographical_error>typing errors</a> like deleted, substituted, or wrongly inserted letters, but may also consist of word confusions where a word was replaced by another one. In addition, words may be erroneously split into two parts or get concatenated. Some words can contain <a href=https://en.wikipedia.org/wiki/Hyphen>hyphens</a>, because they were split at the end of a line or are <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound words</a> with a mandatory <a href=https://en.wikipedia.org/wiki/Hyphen>hyphen</a>. In this paper, we provide an extensive evaluation of 14 spelling correction tools on a common <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a>. In particular, the evaluation provides a detailed comparison with respect to 12 <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error categories</a>. The <a href=https://en.wikipedia.org/wiki/Benchmark_(surveying)>benchmark</a> consists of sentences from the <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a>, which were distorted using a realistic <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error model</a>. Measuring the quality of an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> with respect to these error categories requires an alignment of the original text, the distorted text and the corrected text provided by the tool. We make our benchmark generation and evaluation tools publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.232/>Stress Test Evaluation of Transformer-based Models in Natural Language Understanding Tasks</a></strong><br><a href=/people/c/carlos-aspillaga/>Carlos Aspillaga</a>
|
<a href=/people/a/andres-carvallo/>Andrés Carvallo</a>
|
<a href=/people/v/vladimir-araujo/>Vladimir Araujo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--232><div class="card-body p-3 small">There has been significant progress in recent years in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have shown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand their actual behavior under different scenarios, showing that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are taking advantage of clues or failures of datasets and that slight perturbations on the input data can severely reduce their performance. In contrast, recent <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have not been systematically tested with adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three Transformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NLI) and Question Answering (QA) tasks to know if they are more robust or if they have the same flaws as their predecessors. As a result, our experiments reveal that RoBERTa, XLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks. Nevertheless, they are still very fragile and demonstrate various unexpected behaviors, thus revealing that there is still room for future improvement in this field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.233/>Brand-Product Relation Extraction Using Heterogeneous Vector Space Representations</a></strong><br><a href=/people/a/arkadiusz-janz/>Arkadiusz Janz</a>
|
<a href=/people/l/lukasz-kopocinski/>Łukasz Kopociński</a>
|
<a href=/people/m/maciej-piasecki/>Maciej Piasecki</a>
|
<a href=/people/a/agnieszka-pluwak/>Agnieszka Pluwak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--233><div class="card-body p-3 small">Relation Extraction is a fundamental NLP task. In this paper we investigate the impact of underlying text representation on the performance of neural classification models in the task of Brand-Product relation extraction. We also present the methodology of preparing annotated textual corpora for this task and we provide valuable insight into the properties of Brand-Product relations existing in <a href=https://en.wikipedia.org/wiki/Text_corpus>textual corpora</a>. The <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is approached from a practical angle of applications <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> in facilitating commercial Internet monitoring.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.236" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.236/>TableBank : Table Benchmark for Image-based Table Detection and Recognition<span class=acl-fixed-case>T</span>able<span class=acl-fixed-case>B</span>ank: Table Benchmark for Image-based Table Detection and Recognition</a></strong><br><a href=/people/m/minghao-li/>Minghao Li</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--236><div class="card-body p-3 small">We present TableBank, a new image-based table detection and recognition dataset built with novel weak supervision from Word and Latex documents on the internet. Existing research for image-based table detection and recognition usually fine-tunes pre-trained models on out-of-domain data with a few thousand human-labeled examples, which is difficult to generalize on real-world applications. With TableBank that contains 417 K high quality labeled tables, we build several strong baselines using state-of-the-art models with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. We make TableBank publicly available and hope it will empower more deep learning approaches in the table detection and recognition task. The dataset and models can be downloaded from https://github.com/doc-analysis/TableBank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.237/>WIKIR : A Python Toolkit for Building a Large-scale Wikipedia-based English Information Retrieval Dataset<span class=acl-fixed-case>WIKIR</span>: A Python Toolkit for Building a Large-scale <span class=acl-fixed-case>W</span>ikipedia-based <span class=acl-fixed-case>E</span>nglish Information Retrieval Dataset</a></strong><br><a href=/people/j/jibril-frej/>Jibril Frej</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/j/jean-pierre-chevallet/>Jean-Pierre Chevallet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--237><div class="card-body p-3 small">Over the past years, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> allowed for new state-of-the-art results in ad-hoc information retrieval. However such <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> usually require large amounts of annotated data to be effective. Since most standard ad-hoc information retrieval datasets publicly available for academic research (e.g. Robust04, ClueWeb09) have at most 250 annotated queries, the recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> perform poorly on these datasets. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> (e.g. DUET, Conv-KNRM) are trained and evaluated on data collected from commercial search engines not publicly available for academic research which is a problem for reproducibility and the advancement of research. In this paper, we propose WIKIR : an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source toolkit</a> to automatically build large-scale English information retrieval datasets based on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. WIKIR is publicly available on GitHub. We also provide wikIR59k : a large-scale publicly available dataset that contains 59,252 queries and 2,617,003 (query, relevant documents) pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.242/>HBCP Corpus : A New Resource for the Analysis of Behavioural Change Intervention Reports<span class=acl-fixed-case>HBCP</span> Corpus: A New Resource for the Analysis of Behavioural Change Intervention Reports</a></strong><br><a href=/people/f/francesca-bonin/>Francesca Bonin</a>
|
<a href=/people/m/martin-gleize/>Martin Gleize</a>
|
<a href=/people/a/ailbhe-finnerty/>Ailbhe Finnerty</a>
|
<a href=/people/c/candice-moore/>Candice Moore</a>
|
<a href=/people/c/charles-jochim/>Charles Jochim</a>
|
<a href=/people/e/emma-norris/>Emma Norris</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/a/alison-j-wright/>Alison J. Wright</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/e/emily-hayes/>Emily Hayes</a>
|
<a href=/people/s/silje-zink/>Silje Zink</a>
|
<a href=/people/a/alessandra-pascale/>Alessandra Pascale</a>
|
<a href=/people/p/pol-mac-aonghusa/>Pol Mac Aonghusa</a>
|
<a href=/people/s/susan-michie/>Susan Michie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--242><div class="card-body p-3 small">Due to the fast pace at which research reports in <a href=https://en.wikipedia.org/wiki/Behavior_change_(public_health)>behaviour change</a> are published, researchers, consultants and policymakers would benefit from more automatic ways to process these reports. Automatic extraction of the reports&#8217; intervention content, population, settings and their results etc. are essential in synthesising and summarising the literature. However, to the best of our knowledge, no unique resource exists at the moment to facilitate this synthesis. In this paper, we describe the construction of a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of published behaviour change intervention evaluation reports</a> aimed at <a href=https://en.wikipedia.org/wiki/Smoking_cessation>smoking cessation</a>. We also describe and release the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of 57 entities, that can be used as an off-the-shelf data resource for tasks such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity recognition</a>, etc. Both the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and the annotation dataset are being made available to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--248 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.248/>Do not let the history haunt you : Mitigating Compounding Errors in Conversational Question Answering</a></strong><br><a href=/people/a/angrosh-mandya/>Angrosh Mandya</a>
|
<a href=/people/j/james-o-neill/>James O’ Neill</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/f/frans-coenen/>Frans Coenen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--248><div class="card-body p-3 small">The Conversational Question Answering (CoQA) task involves answering a sequence of inter-related conversational questions about a contextual paragraph. Although existing approaches employ human-written ground-truth answers for answering conversational questions at test time, in a realistic scenario, the CoQA model will not have any access to ground-truth answers for the previous questions, compelling the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to rely upon its own previously predicted answers for answering the subsequent questions. In this paper, we find that compounding errors occur when using previously predicted answers at test time, significantly lowering the performance of CoQA systems. To solve this problem, we propose a <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategy</a> that dynamically selects between target answers and model predictions during training, thereby closely simulating the situation at test time. Further, we analyse the severity of this phenomena as a function of the question type, conversation length and domain type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.249/>CLEEK : A Chinese Long-text Corpus for Entity Linking<span class=acl-fixed-case>CLEEK</span>: A <span class=acl-fixed-case>C</span>hinese Long-text Corpus for Entity Linking</a></strong><br><a href=/people/w/weixin-zeng/>Weixin Zeng</a>
|
<a href=/people/x/xiang-zhao/>Xiang Zhao</a>
|
<a href=/people/j/jiuyang-tang/>Jiuyang Tang</a>
|
<a href=/people/z/zhen-tan/>Zhen Tan</a>
|
<a href=/people/x/xuqian-huang/>Xuqian Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--249><div class="card-body p-3 small">Entity linking, as one of the fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, is crucial to knowledge fusion, knowledge base construction and update. Nevertheless, in contrast to the research on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> for <a href=https://en.wikipedia.org/wiki/English_language>English text</a>, which undergoes continuous development, the Chinese counterpart is still in its infancy. One prominent issue lies in publicly available annotated datasets and <a href=https://en.wikipedia.org/wiki/Benchmarking>evaluation benchmarks</a>, which are lacking and deficient. In specific, existing Chinese corpora for <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> were mainly constructed from noisy short texts, such as <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a> and news headings, where long texts were largely overlooked, which yet constitute a wider spectrum of real-life scenarios. To address the issue, in this work, we build CLEEK, a Chinese corpus of multi-domain long text for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, in order to encourage advancement of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> in languages besides <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of 100 documents from diverse domains, and is publicly accessible. Moreover, we devise a <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> to evaluate the difficulty of documents with respect to <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a>, which is then used to characterize the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Additionally, the results of two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and seven state-of-the-art solutions on CLEEK are reported and compared. The empirical results validate the usefulness of CLEEK and the effectiveness of proposed difficulty measure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.258/>Event Extraction from Unstructured Amharic Text<span class=acl-fixed-case>A</span>mharic Text</a></strong><br><a href=/people/e/ephrem-tadesse/>Ephrem Tadesse</a>
|
<a href=/people/r/rosa-tsegaye/>Rosa Tsegaye</a>
|
<a href=/people/k/kuulaa-qaqqabaa/>Kuulaa Qaqqabaa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--258><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> is one of the types that extract the specific knowledge of certain incidents from texts. Event extraction has been done on different languages text but not on one of the <a href=https://en.wikipedia.org/wiki/Semitic_languages>Semitic language</a>, <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a>. In this study, we present a <a href=https://en.wikipedia.org/wiki/System>system</a> that extracts an <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event</a> from unstructured Amharic text. The <a href=https://en.wikipedia.org/wiki/System>system</a> has designed by the integration of <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> and rule-based approaches. We call this <a href=https://en.wikipedia.org/wiki/System>system</a> a hybrid system. The system uses the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> to detect events from the text and the handcrafted and the rule-based rules to extract the event from the text. For the <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a>, we have been using <a href=https://en.wikipedia.org/wiki/Event_(computing)>event arguments</a>. Event arguments identify event triggering words or phrases that clearly express the occurrence of the event. The event argument attributes can be verbs, nouns, sometimes adjectives (such as rg / wedding) and time as well. The <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> has compared with the standalone rule-based method that is well known for <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a>. The study has shown that the <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> has outperformed the standalone rule-based method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.261/>Towards Entity Spaces</a></strong><br><a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/p/paul-groth/>Paul Groth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--261><div class="card-body p-3 small">Entities are a central element of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and are important input to many knowledge-centric tasks including <a href=https://en.wikipedia.org/wiki/Text_mining>text analysis</a>. For example, they allow us to find documents relevant to a specific entity irrespective of the underlying <a href=https://en.wikipedia.org/wiki/Syntax>syntactic expression</a> within a document. However, the entities that are commonly represented in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are often a simplification of what is truly being referred to in text. For example, in a knowledge base, we may have an entity for <a href=https://en.wikipedia.org/wiki/Germany>Germany</a> as a country but not for the more fuzzy concept of Germany that covers notions of German Population, German Drivers, and the German Government. Inspired by recent advances in contextual word embeddings, we introduce the concept of entity spaces-specific representations of a set of associated entities with near-identity. Thus, these <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity spaces</a> provide a handle to an amorphous grouping of entities. We developed a proof-of-concept for <a href=https://en.wikipedia.org/wiki/English_language>English</a> showing how, through the introduction of entity spaces in the form of disambiguation pages, the recall of entity linking can be improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.264/>Populating Legal Ontologies using Semantic Role Labeling</a></strong><br><a href=/people/l/llio-humphreys/>Llio Humphreys</a>
|
<a href=/people/g/guido-boella/>Guido Boella</a>
|
<a href=/people/l/luigi-di-caro/>Luigi Di Caro</a>
|
<a href=/people/l/livio-robaldo/>Livio Robaldo</a>
|
<a href=/people/l/leon-van-der-torre/>Leon van der Torre</a>
|
<a href=/people/s/sepideh-ghanavati/>Sepideh Ghanavati</a>
|
<a href=/people/r/robert-muthuri/>Robert Muthuri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--264><div class="card-body p-3 small">This paper is concerned with the goal of maintaining legal information and compliance systems : the &#8216;resource consumption bottleneck&#8217; of creating <a href=https://en.wikipedia.org/wiki/Semantic_technology>semantic technologies</a> manually. The use of automated information extraction techniques could significantly reduce this bottleneck. The research question of this paper is : How to address the resource bottleneck problem of creating specialist knowledge management systems? In particular, how to semi-automate the extraction of norms and their elements to populate legal ontologies? This paper shows that the acquisition paradox can be addressed by combining state-of-the-art general-purpose NLP modules with pre- and post-processing using rules based on <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. It describes a Semantic Role Labeling based information extraction system to extract <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> from <a href=https://en.wikipedia.org/wiki/Legislation>legislation</a> and represent them as structured norms in legal ontologies. The output is intended to help make laws more accessible, understandable, and searchable in legal document management systems such as Eunomos (Boella et al., 2016).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.270/>Domain Adapted Distant Supervision for Pedagogically Motivated Relation Extraction</a></strong><br><a href=/people/o/oscar-sainz/>Oscar Sainz</a>
|
<a href=/people/o/oier-lopez-de-lacalle/>Oier Lopez de Lacalle</a>
|
<a href=/people/i/itziar-aldabe/>Itziar Aldabe</a>
|
<a href=/people/m/montse-maritxalar/>Montse Maritxalar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--270><div class="card-body p-3 small">In this paper we present a relation extraction system that given a text extracts pedagogically motivated relation types, as a previous step to obtaining a semantic representation of the text which will make possible to automatically generate questions for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. The system maps pedagogically motivated relations with relations from <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> and deploys Distant Supervision for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. We run a study on a subset of those relationships in order to analyse the viability of our approach. For that, we build a domain-specific relation extraction system and explore two relation extraction models : a state-of-the-art model based on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and a discrete feature based machine learning model. Experiments show that the neural model obtains better results in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> and we yield promising results on the subset of relations suitable for pedagogical purposes. We thus consider that distant supervision for relation extraction is a valid <a href=https://en.wikipedia.org/wiki/Computational_model>approach</a> in our target domain, i.e. biology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--275 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.275/>NLP Analytics in Finance with DoRe : A French 250 M Tokens Corpus of Corporate Annual Reports<span class=acl-fixed-case>NLP</span> Analytics in Finance with <span class=acl-fixed-case>D</span>o<span class=acl-fixed-case>R</span>e: A <span class=acl-fixed-case>F</span>rench 250<span class=acl-fixed-case>M</span> Tokens Corpus of Corporate Annual Reports</a></strong><br><a href=/people/c/corentin-masson/>Corentin Masson</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--275><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Neural_computing>neural computing</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for <a href=https://en.wikipedia.org/wiki/Semantic_processing>semantic processing</a> open many new applications areas which had been left unaddressed so far because of inadequate language understanding capacity. But this new kind of approaches rely even more on training data to be operational. Corpora for financial applications exists, but most of them concern <a href=https://en.wikipedia.org/wiki/Stock_market_prediction>stock market prediction</a> and are in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. To address this need for the French language and regulation oriented applications which require a deeper understanding of the text content, we hereby present DoRe, a French and dialectal French Corpus for NLP analytics in Finance, Regulation and Investment. This corpus is composed of : (a) 1769 Annual Reports from 336 companies among the most capitalized companies in : France (Euronext Paris) & Belgium (Euronext Brussels), covering a time frame from 2009 to 2019, and (b) related MetaData containing information for each company about its <a href=https://en.wikipedia.org/wiki/International_Securities_Identification_Number>ISIN code</a>, capitalization and sector. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is designed to be as modular as possible in order to allow for maximum reuse in different tasks pertaining to <a href=https://en.wikipedia.org/wiki/Economics>Economics</a>, Finance and Regulation. After presenting existing resources, we relate the construction of the DoRe corpus and the rationale behind our choices, concluding on the spectrum of possible uses of this new <a href=https://en.wikipedia.org/wiki/Resource>resource</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.285/>Evaluation Dataset and Methodology for Extracting Application-Specific Taxonomies from the Wikipedia Knowledge Graph<span class=acl-fixed-case>W</span>ikipedia Knowledge Graph</a></strong><br><a href=/people/g/georgeta-bordea/>Georgeta Bordea</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/f/fleur-mougin/>Fleur Mougin</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a>
|
<a href=/people/g/gayo-diallo/>Gayo Diallo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--285><div class="card-body p-3 small">In this work, we address the task of extracting application-specific taxonomies from the category hierarchy of Wikipedia. Previous work on pruning the Wikipedia knowledge graph relied on silver standard taxonomies which can only be automatically extracted for a small subset of domains rooted in relatively focused nodes, placed at an intermediate level in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. In this work, we propose an iterative methodology to extract an application-specific gold standard dataset from a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and an evaluation framework to comparatively assess the quality of noisy automatically extracted taxonomies. We employ an existing state of the art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> in an iterative manner and we propose several sampling strategies to reduce the amount of manual work needed for evaluation. A first gold standard dataset is released to the research community for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> along with a companion evaluation framework. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> addresses a real-world application from the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>, namely the extraction of food-drug and herb-drug interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--286 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.286/>Subjective Evaluation of Comprehensibility in Movie Interactions</a></strong><br><a href=/people/e/estelle-randria/>Estelle Randria</a>
|
<a href=/people/l/lionel-fontan/>Lionel Fontan</a>
|
<a href=/people/m/maxime-le-coz/>Maxime Le Coz</a>
|
<a href=/people/i/isabelle-ferrane/>Isabelle Ferrané</a>
|
<a href=/people/j/julien-pinquier/>Julien Pinquier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--286><div class="card-body p-3 small">Various research works have dealt with the comprehensibility of textual, audio, or audiovisual documents, and showed that factors related to <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> (e.g. linguistic complexity), <a href=https://en.wikipedia.org/wiki/Phone_(phonetics)>sound</a> (e.g. speech intelligibility), image (e.g. presence of visual context), or even to cognition and emotion can play a major role in the ability of humans to understand the semantic and pragmatic contents of a given document. However, to date, no reference human data is available that could help investigating the role of the linguistic and extralinguistic information present at these different levels (i.e., linguistic, audio / phonetic, and visual) in multimodal documents (e.g., movies). The present work aimed at building a corpus of human annotations that would help to study further how much and in which way the human perception of comprehensibility (i.e., of the difficulty of comprehension, referred in this paper as overall difficulty) of audiovisual documents is affected (1) by lexical complexity, grammatical complexity, and <a href=https://en.wikipedia.org/wiki/Intelligibility_(communication)>speech intelligibility</a>, and (2) by the modality / ies (text, audio, video) available to the human recipient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.290/>Prtlet : A Hungarian Corpus of Propaganda Texts from the Hungarian Socialist Era<span class=acl-fixed-case>P</span>ártélet: A <span class=acl-fixed-case>H</span>ungarian Corpus of Propaganda Texts from the <span class=acl-fixed-case>H</span>ungarian Socialist Era</a></strong><br><a href=/people/z/zoltan-kmetty/>Zoltán Kmetty</a>
|
<a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/o/orsolya-ring/>Orsolya Ring</a>
|
<a href=/people/b/balazs-nagy/>Balázs Nagy</a>
|
<a href=/people/m/martina-katalin-szabo/>Martina Katalin Szabó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--290><div class="card-body p-3 small">In this paper, we present Prtlet, a digitized Hungarian corpus of Communist propaganda texts. Prtlet was the official journal of the governing party during the Hungarian socialism from 1956 to 1989, hence it represents the direct political agitation and propaganda of the dictatorial system in question. The paper has a dual purpose : first, to present a general review of the corpus compilation process and the basic statistical data of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and second, to demonstrate through two case studies what the dataset can be used for. We show that our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> provides a unique opportunity for conducting research on Hungarian propaganda discourse, as well as analyzing changes of this discourse over a 35-year period of time with computer-assisted methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.294/>Processing <a href=https://en.wikipedia.org/wiki/Languages_of_South_Asia>South Asian Languages</a> Written in the <a href=https://en.wikipedia.org/wiki/Latin_script>Latin Script</a> : the Dakshina Dataset<span class=acl-fixed-case>S</span>outh <span class=acl-fixed-case>A</span>sian Languages Written in the <span class=acl-fixed-case>L</span>atin Script: the <span class=acl-fixed-case>D</span>akshina Dataset</a></strong><br><a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/c/cibu-johny/>Cibu Johny</a>
|
<a href=/people/i/isin-demirsahin/>Isin Demirsahin</a>
|
<a href=/people/k/keith-hall/>Keith Hall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--294><div class="card-body p-3 small">This paper describes the Dakshina dataset, a new resource consisting of text in both the Latin and native scripts for 12 <a href=https://en.wikipedia.org/wiki/Languages_of_South_Asia>South Asian languages</a>. The dataset includes, for each language : 1) native script Wikipedia text ; 2) a romanization lexicon ; and 3) full sentence parallel data in both a native script of the language and the basic Latin alphabet. We document the methods used for preparation and selection of the Wikipedia text in each language ; collection of attested romanizations for sampled lexicons ; and manual romanization of held-out sentences from the native script collections. We additionally provide baseline results on several tasks made possible by the dataset, including <a href=https://en.wikipedia.org/wiki/Transliteration>single word transliteration</a>, <a href=https://en.wikipedia.org/wiki/Transliteration>full sentence transliteration</a>, and language modeling of <a href=https://en.wikipedia.org/wiki/Script_language>native script</a> and <a href=https://en.wikipedia.org/wiki/Romanization>romanized text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.301/>Adaptation of Deep Bidirectional Transformers for Afrikaans Language<span class=acl-fixed-case>A</span>frikaans Language</a></strong><br><a href=/people/s/sello-ralethe/>Sello Ralethe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--301><div class="card-body p-3 small">The recent success of pretrained language models in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> has sparked interest in training such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for languages other than English. Currently, training of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can either be monolingual or multilingual based. In the case of multilingual models, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are trained on concatenated data of multiple languages. We introduce AfriBERT, a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> for the <a href=https://en.wikipedia.org/wiki/Afrikaans>Afrikaans language</a> based on Bidirectional Encoder Representation from Transformers (BERT). We compare the performance of AfriBERT against multilingual BERT in multiple downstream tasks, namely <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>, and dependency parsing. Our results show that AfriBERT improves the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in most of the tasks we considered, and that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from multilingual to monolingual model can have a significant performance improvement on downstream tasks. We release the pretrained model for AfriBERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.302/>FlauBERT : Unsupervised Language Model Pre-training for <a href=https://en.wikipedia.org/wiki/French_language>French</a><span class=acl-fixed-case>F</span>lau<span class=acl-fixed-case>BERT</span>: Unsupervised Language Model Pre-training for <span class=acl-fixed-case>F</span>rench</a></strong><br><a href=/people/h/hang-le/>Hang Le</a>
|
<a href=/people/l/loic-vial/>Loïc Vial</a>
|
<a href=/people/j/jibril-frej/>Jibril Frej</a>
|
<a href=/people/v/vincent-segonne/>Vincent Segonne</a>
|
<a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benjamin-lecouteux/>Benjamin Lecouteux</a>
|
<a href=/people/a/alexandre-allauzen/>Alexandre Allauzen</a>
|
<a href=/people/b/benoit-crabbe/>Benoit Crabbé</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--302><div class="card-body p-3 small">Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextualization</a> at the sentence level. This has been widely demonstrated for <a href=https://en.wikipedia.org/wiki/English_language>English</a> using contextualized representations (Dai and Le, 2015 ; Peters et al., 2018 ; Howard and Ruder, 2018 ; Radford et al., 2018 ; Devlin et al., 2019 ; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--306 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.306/>Modeling Factual Claims with Semantic Frames</a></strong><br><a href=/people/f/fatma-arslan/>Fatma Arslan</a>
|
<a href=/people/j/josue-caraballo/>Josue Caraballo</a>
|
<a href=/people/d/damian-jimenez/>Damian Jimenez</a>
|
<a href=/people/c/chengkai-li/>Chengkai Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--306><div class="card-body p-3 small">In this paper, we introduce an extension of the Berkeley FrameNet for the structured and semantic modeling of factual claims. Modeling is a robust tool that can be leveraged in many different tasks such as matching claims to existing <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checks</a> and translating claims to structured queries. Our work introduces 11 new manually crafted frames along with 9 existing FrameNet frames, all of which have been selected with <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> in mind. Along with these <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a>, we are also providing 2,540 fully annotated sentences, which can be used to understand how these <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> are intended to work and to train <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. Finally, we are also releasing our annotation tool to facilitate other researchers to make their own local extensions to <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.308/>Geographically-Balanced Gigaword Corpora for 50 Language Varieties<span class=acl-fixed-case>G</span>igaword Corpora for 50 Language Varieties</a></strong><br><a href=/people/j/jonathan-dunn/>Jonathan Dunn</a>
|
<a href=/people/b/ben-adams/>Ben Adams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--308><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> have been steadily increasing in overall size, even very large corpora are not designed to represent global population demographics. For example, recent work has shown that existing English gigaword corpora over-represent inner-circle varieties from the US and the UK. To correct implicit geographic and demographic biases, this paper uses country-level population demographics to guide the construction of gigaword web corpora. The resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> explicitly match the ground-truth geographic distribution of each language, thus equally representing language users from around the world. This is important because it ensures that speakers of <a href=https://en.wikipedia.org/wiki/Language_planning>under-resourced language varieties</a> (i.e., <a href=https://en.wikipedia.org/wiki/Indian_English>Indian English</a> or Algerian French) are represented, both in the corpora themselves but also in derivative resources like <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.309/>Data Augmentation using <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Fake News Detection in the Urdu Language<span class=acl-fixed-case>U</span>rdu Language</a></strong><br><a href=/people/m/maaz-amjad/>Maaz Amjad</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a>
|
<a href=/people/a/alisa-zhila/>Alisa Zhila</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--309><div class="card-body p-3 small">The task of fake news detection is to distinguish legitimate news articles that describe real facts from those which convey deceiving and fictitious information. As the <a href=https://en.wikipedia.org/wiki/Fake_news>fake news phenomenon</a> is omnipresent across all languages, it is crucial to be able to efficiently solve this problem for <a href=https://en.wikipedia.org/wiki/Language>languages</a> other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. A common approach to this task is <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classification</a> using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of various <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. Yet <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> requires substantial amount of annotated data. For <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a small number of other languages, annotated data availability is much higher, whereas for the vast majority of languages, it is almost scarce. We investigate whether <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> at its present state could be successfully used as an automated technique for annotated corpora creation and augmentation for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a> focusing on the English-Urdu language pair. We train a fake news classifier for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a> on (1) the manually annotated dataset originally in <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a> and (2) the machine-translated version of an existing annotated fake news dataset originally in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We show that at the present state of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation quality</a> for the <a href=https://en.wikipedia.org/wiki/Urdu>English-Urdu language pair</a>, the fully automated data augmentation through <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> did not provide improvement for fake news detection in <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.310/>Evaluation of Greek Word Embeddings<span class=acl-fixed-case>G</span>reek Word Embeddings</a></strong><br><a href=/people/s/stamatis-outsios/>Stamatis Outsios</a>
|
<a href=/people/c/christos-karatsalos/>Christos Karatsalos</a>
|
<a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--310><div class="card-body p-3 small">Since <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> have been the most popular input for many NLP tasks, evaluating their quality is critical. Most research efforts are focusing on English word embeddings. This paper addresses the problem of training and evaluating such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a>. We present a new word analogy test set considering the original English Word2vec analogy test set and some specific linguistic aspects of the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a> as well. Moreover, we create a Greek version of WordSim353 test collection for a basic evaluation of word similarities. Produced resources are available for download. We test seven word vector models and our evaluation shows that we are able to create meaningful <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a>. Last, we discover that the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> of the <a href=https://en.wikipedia.org/wiki/Greek_language>Greek language</a> and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> can influence the quality of the resulting <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.311/>A Dataset of Mycenaean Linear B Sequences<span class=acl-fixed-case>M</span>ycenaean <span class=acl-fixed-case>L</span>inear <span class=acl-fixed-case>B</span> Sequences</a></strong><br><a href=/people/k/katerina-papavassiliou/>Katerina Papavassiliou</a>
|
<a href=/people/g/gareth-owens/>Gareth Owens</a>
|
<a href=/people/d/dimitrios-kosmopoulos/>Dimitrios Kosmopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--311><div class="card-body p-3 small">We present our work towards a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of Mycenaean Linear B sequences gathered from the <a href=https://en.wikipedia.org/wiki/Mycenaean_Greek>Mycenaean inscriptions</a> written in the 13th and 14th century B.C. (c. 1400-1200 B.C.). The dataset contains sequences of <a href=https://en.wikipedia.org/wiki/Mycenaean_Greek>Mycenaean words</a> and <a href=https://en.wikipedia.org/wiki/Ideogram>ideograms</a> according to the rules of the <a href=https://en.wikipedia.org/wiki/Mycenaean_Greek>Mycenaean Greek language</a> in the Late Bronze Age. Our ultimate goal is to contribute to the study, reading and understanding of ancient scripts and languages. Focusing on sequences, we seek to exploit the structure of the entire language, not just the <a href=https://en.wikipedia.org/wiki/Mycenaean_Greek>Mycenaean vocabulary</a>, to analyse sequential patterns. We use the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to experiment on estimating the missing symbols in damaged inscriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.313/>Exploring Bilingual Word Embeddings for <a href=https://en.wikipedia.org/wiki/Hiligaynon_language>Hiligaynon</a>, a Low-Resource Language<span class=acl-fixed-case>H</span>iligaynon, a Low-Resource Language</a></strong><br><a href=/people/l/leah-michel/>Leah Michel</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--313><div class="card-body p-3 small">This paper investigates the use of bilingual word embeddings for mining Hiligaynon translations of English words. There is very little research on <a href=https://en.wikipedia.org/wiki/Hiligaynon_language>Hiligaynon</a>, an extremely low-resource language of Malayo-Polynesian origin with over 9 million speakers in the Philippines (we found just one paper). We use a publicly available Hiligaynon corpus with only 300 K words, and match it with a comparable corpus in English. As there are no bilingual resources available, we manually develop a English-Hiligaynon lexicon and use this to train bilingual word embeddings. But we fail to mine accurate translations due to the small amount of data. To find out if the same holds true for a related language pair, we simulate the same low-resource setup on <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> and arrive at similar results. We then vary the size of the comparable English and German corpora to determine the minimum corpus size necessary to achieve competitive results. Further, we investigate the role of the seed lexicon. We show that with the same corpus size but with a smaller seed lexicon, performance can surpass results of previous studies. We release the lexicon of 1,200 English-Hiligaynon word pairs we created to encourage further investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.317/>Evaluating Sentence Segmentation in Different Datasets of <a href=https://en.wikipedia.org/wiki/Neuropsychological_test>Neuropsychological Language Tests</a> in <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a><span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/e/edresson-casanova/>Edresson Casanova</a>
|
<a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/l/lilian-hubner/>Lilian Hübner</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--317><div class="card-body p-3 small">Automatic analysis of connected speech by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing techniques</a> is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain : the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> used in <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives visual and oral via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture : (i) the inclusion of a Linear Chain CRF ; (ii) the inclusion of a self-attention mechanism ; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--320 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.320/>Development of a Guarani-Spanish Parallel Corpus<span class=acl-fixed-case>G</span>uarani - <span class=acl-fixed-case>S</span>panish Parallel Corpus</a></strong><br><a href=/people/l/luis-chiruzzo/>Luis Chiruzzo</a>
|
<a href=/people/p/pedro-amarilla/>Pedro Amarilla</a>
|
<a href=/people/a/adolfo-rios/>Adolfo Ríos</a>
|
<a href=/people/g/gustavo-gimenez-lugo/>Gustavo Giménez Lugo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--320><div class="card-body p-3 small">This paper presents the development of a Guarani-Spanish parallel corpus with sentence-level alignment. The Guarani sentences of the corpus use the Jopara Guarani dialect, the dialect of Guarani spoken in Paraguay, which is based on Guarani grammar and may include several Spanish loanwords or neologisms. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has around 14,500 sentence pairs aligned using a semi-automatic process, containing 228,000 Guarani tokens and 336,000 Spanish tokens extracted from <a href=https://en.wikipedia.org/wiki/Web_archiving>web sources</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.322/>Processing Language Resources of Under-Resourced and Endangered Languages for the Generation of Augmentative Alternative Communication Boards</a></strong><br><a href=/people/a/anne-ferger/>Anne Ferger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--322><div class="card-body p-3 small">Under-resourced and endangered or small languages yield problems for automatic processing and exploiting because of the small amount of available data. This paper shows an approach using different annotations of enriched linguistic research data to create communication boards commonly used in Alternative Augmentative Communication (AAC). Using manually created lexical analysis and rich annotation (instead of high data quantity) allows for an automated creation of AAC communication boards. The example presented in this paper uses data of the indigenous language Dolgan (an endangered Turkic language of Northern Siberia) created in the project INEL(Arkhipov and Dbritz, 2018) to generate a basic communication board with audio snippets to be used in e.g. hospital communication or for multilingual settings. The created <a href=https://en.wikipedia.org/wiki/Printed_circuit_board>boards</a> can be importet into various <a href=https://en.wikipedia.org/wiki/Advanced_Audio_Coding>AAC software</a>. In addition, the usage of standard formats makes this <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> applicable to various different use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--327 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.327/>Towards a Spell Checker for Zamboanga Chavacano Orthography<span class=acl-fixed-case>Z</span>amboanga <span class=acl-fixed-case>C</span>havacano Orthography</a></strong><br><a href=/people/m/marcelo-yuji-himoro/>Marcelo Yuji Himoro</a>
|
<a href=/people/a/antonio-pareja-lora/>Antonio Pareja-Lora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--327><div class="card-body p-3 small">Zamboanga Chabacano (ZC) is the most vibrant variety of <a href=https://en.wikipedia.org/wiki/Philippine_Creole_Spanish>Philippine Creole Spanish</a>, with over 400,000 native speakers in the Philippines (as of 2010). Following its introduction as a subject and a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 in 2012, an official orthography for this variety-the so-called Zamboanga Chavacano Orthography-has been approved in 2014. Its complexity, however, is a barrier to most speakers, since it does not necessarily reflect the particular phonetic evolution in ZC, but favours <a href=https://en.wikipedia.org/wiki/Etymology>etymology</a> instead. The distance between the correct spelling and the different spelling variations is often so great that delivering acceptable performance with the current de facto <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checking technologies</a> may be challenging. The goals of this research have been to propose i) a spelling error taxonomy for ZC, formalised as an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. Our results show that this approach is suitable for the goals mentioned and that it could be combined with other current <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checking technologies</a> to achieve even higher performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.329/>Automatic Creation of Text Corpora for Low-Resource Languages from the Internet : The Case of <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss German</a><span class=acl-fixed-case>I</span>nternet: The Case of <span class=acl-fixed-case>S</span>wiss <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/l/lucy-linder/>Lucy Linder</a>
|
<a href=/people/m/michael-jungo/>Michael Jungo</a>
|
<a href=/people/j/jean-hennebert/>Jean Hennebert</a>
|
<a href=/people/c/claudiu-cristian-musat/>Claudiu Cristian Musat</a>
|
<a href=/people/a/andreas-fischer/>Andreas Fischer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--329><div class="card-body p-3 small">This paper presents SwissCrawl, the largest Swiss German text corpus to date. Composed of more than half a million sentences, it was generated using a customized web scraping tool that could be applied to other low-resource languages as well. The approach demonstrates how freely available <a href=https://en.wikipedia.org/wiki/Web_page>web pages</a> can be used to construct comprehensive text corpora, which are of fundamental importance for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In an experimental evaluation, we show that using the new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> leads to significant improvements for the task of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.333/>Evaluating the Impact of Sub-word Information and Cross-lingual Word Embeddings on Mi’kmaq Language Modelling</a></strong><br><a href=/people/j/jeremie-boudreau/>Jeremie Boudreau</a>
|
<a href=/people/a/akankshya-patra/>Akankshya Patra</a>
|
<a href=/people/a/ashima-suvarna/>Ashima Suvarna</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--333><div class="card-body p-3 small">Mi&#8217;kmaq is an <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>Indigenous language</a> spoken primarily in Eastern Canada. It is polysynthetic and low-resource. In this paper we consider a range of n-gram and RNN language models for <a href=https://en.wikipedia.org/wiki/Mi&#42892;kmaq_language>Mi&#8217;kmaq</a>. We find that an RNN language model, initialized with pre-trained fastText embeddings, performs best, highlighting the importance of sub-word information for Mi&#8217;kmaq language modelling. We further consider approaches to <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> that incorporate cross-lingual word embeddings, but do not see improvements with these models. Finally we consider language models that operate over segmentations produced by SentencePiece which include sub-word units as tokens as opposed to word-level models. We see improvements for this approach over word-level language models, again indicating that sub-word modelling is important for Mi&#8217;kmaq language modelling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.335/>Massive vs. Curated Embeddings for Low-Resourced Languages : the Case of Yorb and Twi<span class=acl-fixed-case>Y</span>orùbá and <span class=acl-fixed-case>T</span>wi</a></strong><br><a href=/people/j/jesujoba-alabi/>Jesujoba Alabi</a>
|
<a href=/people/k/kwabena-amponsah-kaakyire/>Kwabena Amponsah-Kaakyire</a>
|
<a href=/people/d/david-adelani/>David Adelani</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--335><div class="card-body p-3 small">The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such <a href=https://en.wikipedia.org/wiki/System_resource>resources</a> is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yorb and <a href=https://en.wikipedia.org/wiki/Twi>Twi</a>, and compare the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> obtained in this way, with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into Yorb and <a href=https://en.wikipedia.org/wiki/Twi>Twi</a>. We extend the analysis to contextual word embeddings and evaluate multilingual BERT on a named entity recognition task. For this, we annotate with named entities the Global Voices corpus for Yorb. As output of the work, we provide corpora, embeddings and the test suits for both languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--336 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.336/>TRopBank : Turkish PropBank V2.0<span class=acl-fixed-case>TR</span>op<span class=acl-fixed-case>B</span>ank: <span class=acl-fixed-case>T</span>urkish <span class=acl-fixed-case>P</span>rop<span class=acl-fixed-case>B</span>ank V2.0</a></strong><br><a href=/people/n/neslihan-kara/>Neslihan Kara</a>
|
<a href=/people/d/deniz-baran-aslan/>Deniz Baran Aslan</a>
|
<a href=/people/b/busra-marsan/>Büşra Marşan</a>
|
<a href=/people/o/ozge-bakay/>Özge Bakay</a>
|
<a href=/people/k/koray-ak/>Koray Ak</a>
|
<a href=/people/o/olcay-taner-yildiz/>Olcay Taner Yıldız</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--336><div class="card-body p-3 small">In this paper, we present and explain TRopBank Turkish PropBank v2.0. PropBank is a hand-annotated corpus of propositions which is used to obtain the predicate-argument information of a language. Predicate-argument information of a language can help understand semantic roles of arguments. Turkish PropBank v2.0, unlike PropBank v1.0, has a much more extensive list of Turkish verbs, with 17.673 verbs in total.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--338 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.338/>An Empirical Evaluation of Annotation Practices in Corpora from Language Documentation</a></strong><br><a href=/people/k/kilu-von-prince/>Kilu von Prince</a>
|
<a href=/people/s/sebastian-nordhoff/>Sebastian Nordhoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--338><div class="card-body p-3 small">For most of the world&#8217;s languages, no primary data are available, even as many languages are disappearing. Throughout the last two decades, however, language documentation projects have produced substantial amounts of primary data from a wide variety of endangered languages. These <a href=https://en.wikipedia.org/wiki/Natural_resource>resources</a> are still in the early days of their exploration. One of the factors that makes them hard to use is a relative lack of standardized annotation conventions. In this paper, we will describe common practices in existing <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> in order to facilitate their future processing. After a brief introduction of the main formats used for annotation files, we will focus on commonly used tiers in the widespread ELAN and Toolbox formats. Minimally, <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> from <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> contain a transcription tier and an aligned translation tier, which means they constitute parallel corpora. Additional common annotations include named references, morpheme separation, morpheme-by-morpheme glosses, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and notes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.340/>Building a Task-oriented Dialog System for Languages with no Training Data : the Case for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a><span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/m/maddalen-lopez-de-lacalle/>Maddalen López de Lacalle</a>
|
<a href=/people/x/xabier-saralegi/>Xabier Saralegi</a>
|
<a href=/people/i/inaki-san-vicente/>Iñaki San Vicente</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--340><div class="card-body p-3 small">This paper presents an approach for developing a task-oriented dialog system for less-resourced languages in scenarios where training data is not available. Both <a href=https://en.wikipedia.org/wiki/Intention_(criminal_law)>intent classification</a> and slot filling are tackled. We project the existing annotations in rich-resource languages by means of Neural Machine Translation (NMT) and posterior word alignments. We then compare training on the projected monolingual data with direct model transfer alternatives. Intent Classifiers and slot filling sequence taggers are implemented using a BiLSTM architecture or by fine-tuning BERT transformer models. Models learnt exclusively from Basque projected data provide better accuracies for slot filling. Combining Basque projected train data with rich-resource languages data outperforms consistently models trained solely on projected data for intent classification. At any rate, we achieve competitive performance in both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, with accuracies of 81 % for intent classification and 77 % for slot filling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.342/>A Major Wordnet for a Minority Language : Scottish Gaelic<span class=acl-fixed-case>W</span>ordnet for a Minority Language: <span class=acl-fixed-case>S</span>cottish <span class=acl-fixed-case>G</span>aelic</a></strong><br><a href=/people/g/gabor-bella/>Gábor Bella</a>
|
<a href=/people/f/fiona-mcneill/>Fiona McNeill</a>
|
<a href=/people/r/rody-gorman/>Rody Gorman</a>
|
<a href=/people/c/caoimhin-o-donnaile/>Caoimhin O Donnaile</a>
|
<a href=/people/k/kirsty-macdonald/>Kirsty MacDonald</a>
|
<a href=/people/y/yamini-chandrashekar/>Yamini Chandrashekar</a>
|
<a href=/people/a/abed-alhakim-freihat/>Abed Alhakim Freihat</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--342><div class="card-body p-3 small">We present a new wordnet resource for <a href=https://en.wikipedia.org/wiki/Scottish_Gaelic>Scottish Gaelic</a>, a Celtic minority language spoken by about 60,000 speakers, most of whom live in Northwestern Scotland. The <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> contains over 15 thousand word senses and was constructed by merging ten thousand new, high-quality translations, provided and validated by language experts, with an existing <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> derived from <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. This new, considerably extended wordnetcurrently among the 30 largest in the worldtargets multiple communities : language speakers and learners ; linguists ; computer scientists solving problems related to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. By publishing it as a freely downloadable resource, we hope to contribute to the long-term preservation of <a href=https://en.wikipedia.org/wiki/Scottish_Gaelic>Scottish Gaelic</a> as a living language, both offline and on the Web.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--345 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.345/>Learnings from Technological Interventions in a Low Resource Language : A Case-Study on Gondi<span class=acl-fixed-case>G</span>ondi</a></strong><br><a href=/people/d/devansh-mehta/>Devansh Mehta</a>
|
<a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/r/ramaravind-kommiya-mothilal/>Ramaravind Kommiya Mothilal</a>
|
<a href=/people/b/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a>
|
<a href=/people/a/alok-sharma/>Alok Sharma</a>
|
<a href=/people/a/anurag-shukla/>Anurag Shukla</a>
|
<a href=/people/v/vishnu-prasad/>Vishnu Prasad</a>
|
<a href=/people/v/venkanna-u/>Venkanna U</a>
|
<a href=/people/a/amit-sharma/>Amit Sharma</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--345><div class="card-body p-3 small">The primary obstacle to developing <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> for low-resource languages is the lack of usable data. In this paper, we report the adaption and deployment of 4 technology-driven methods of data collection for <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi</a>, a low-resource vulnerable language spoken by around 2.3 million tribal people in south and central India. In the process of data collection, we also help in its revival by expanding access to information in <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi</a> through the creation of linguistic resources that can be used by the community, such as a dictionary, children&#8217;s stories, an app with <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi content</a> from multiple sources and an Interactive Voice Response (IVR) based mass awareness platform. At the end of these interventions, we collected a little less than 12,000 translated words and/or sentences and identified more than 650 community members whose help can be solicited for future translation efforts. The larger goal of the project is collecting enough data in <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi</a> to build and deploy viable language technologies like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and speech to text systems that can help take the language onto the internet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.348/>Optimizing Annotation Effort Using Active Learning Strategies : A Sentiment Analysis Case Study in Persian<span class=acl-fixed-case>P</span>ersian</a></strong><br><a href=/people/s/seyed-arad-ashrafi-asli/>Seyed Arad Ashrafi Asli</a>
|
<a href=/people/b/behnam-sabeti/>Behnam Sabeti</a>
|
<a href=/people/z/zahra-majdabadi/>Zahra Majdabadi</a>
|
<a href=/people/p/preni-golazizian/>Preni Golazizian</a>
|
<a href=/people/r/reza-fahmi/>Reza Fahmi</a>
|
<a href=/people/o/omid-momenzadeh/>Omid Momenzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--348><div class="card-body p-3 small">Deep learning models are the current State-of-the-art methodologies towards many real-world problems. However, they need a substantial amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> to be trained appropriately. Acquiring labeled data can be challenging in some particular domains or less-resourced languages. There are some practical solutions regarding these issues, such as <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a> and <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a>. Active learning&#8217;s idea is simple : let the model choose the samples for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> instead of labeling the whole dataset. This <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> leads to a more efficient annotation process. Active Learning models can achieve the baseline performance (the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on the whole dataset), with a considerably lower amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. Several active learning approaches are tested in this work, and their compatibility with <a href=https://en.wikipedia.org/wiki/Persian_language>Persian</a> is examined using a brand-new sentiment analysis dataset that is also introduced in this work. MirasOpinion, which to our knowledge is the largest Persian sentiment analysis dataset, is crawled from a Persian e-commerce website and annotated using a crowd-sourcing policy. LDA sampling, which is an efficient Active Learning strategy using Topic Modeling, is proposed in this research. Active Learning Strategies have shown promising results in the <a href=https://en.wikipedia.org/wiki/Persian_language>Persian language</a>, and LDA sampling showed a competitive performance compared to other approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.365/>CA-EHN : Commonsense Analogy from E-HowNet<span class=acl-fixed-case>CA</span>-<span class=acl-fixed-case>EHN</span>: Commonsense Analogy from <span class=acl-fixed-case>E</span>-<span class=acl-fixed-case>H</span>ow<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/p/peng-hsuan-li/>Peng-Hsuan Li</a>
|
<a href=/people/t/tsan-yu-yang/>Tsan-Yu Yang</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--365><div class="card-body p-3 small">Embedding commonsense knowledge is crucial for end-to-end models to generalize <a href=https://en.wikipedia.org/wiki/Inference>inference</a> beyond training corpora. However, existing word analogy datasets have tended to be handcrafted, involving permutations of hundreds of words with only dozens of pre-defined relations, mostly <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological relations</a> and <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. In this work, we model commonsense knowledge down to word-level analogical reasoning by leveraging E-HowNet, an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> that annotates 88 K Chinese words with their structured sense definitions and English translations. We present CA-EHN, the first commonsense word analogy dataset containing 90,505 <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> covering 5,656 words and 763 relations. Experiments show that CA-EHN stands out as a great indicator of how well word representations embed <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. The dataset is publicly available at.<url>https://github.com/ckiplab/CA-EHN</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.366/>Building Semantic Grams of Human Knowledge</a></strong><br><a href=/people/v/valentina-leone/>Valentina Leone</a>
|
<a href=/people/g/giovanni-siragusa/>Giovanni Siragusa</a>
|
<a href=/people/l/luigi-di-caro/>Luigi Di Caro</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--366><div class="card-body p-3 small">Word senses are typically defined with textual definitions for human consumption and, in computational lexicons, put in context via lexical-semantic relations such as <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>, <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a>, <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hypernymy</a>, etc. In this paper we embrace a radically different <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> that provides a slot-filler structure, called <a href=https://en.wikipedia.org/wiki/Semagram>semagram</a>, to define the meaning of words in terms of their prototypical semantic information. We propose a semagram-based knowledge model composed of 26 semantic relationships which integrates features from a range of different sources, such as computational lexicons and property norms. We describe an annotation exercise regarding 50 concepts over 10 different categories and put forward different automated approaches for extending the semagram base to thousands of concepts. We finally evaluated the impact of the proposed resource on a semantic similarity task, showing significant improvements over state-of-the-art word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.372/>Introducing Lexical Masks : a New Representation of Lexical Entries for Better Evaluation and Exchange of Lexicons</a></strong><br><a href=/people/b/bruno-cartoni/>Bruno Cartoni</a>
|
<a href=/people/d/daniel-calvelo-aros/>Daniel Calvelo Aros</a>
|
<a href=/people/d/denny-vrandecic/>Denny Vrandecic</a>
|
<a href=/people/s/saran-lertpradit/>Saran Lertpradit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--372><div class="card-body p-3 small">The evaluation and exchange of large lexicon databases remains a challenge in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Despite the existence of commonly accepted standards for the format and the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> used in a lexicon, there is still a lack of precise and interoperable specification requirements about how lexical entries of a particular language should look like, both in terms of the numbers of forms and in terms of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> associated with these forms. This paper presents the notion of lexical masks, a powerful tool used to evaluate and exchange lexicon databases in many languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.376/>Odi et Amo. Creating, Evaluating and Extending Sentiment Lexicons for <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>.<span class=acl-fixed-case>A</span>mo. Creating, Evaluating and Extending Sentiment Lexicons for <span class=acl-fixed-case>L</span>atin.</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/m/marco-passarotti/>Marco Passarotti</a>
|
<a href=/people/d/daniela-corbetta/>Daniela Corbetta</a>
|
<a href=/people/a/andrea-peverelli/>Andrea Peverelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--376><div class="card-body p-3 small">Sentiment lexicons are essential for developing automatic sentiment analysis systems, but the resources currently available mostly cover modern languages. Lexicons for <a href=https://en.wikipedia.org/wiki/Ancient_language>ancient languages</a> are few and not evaluated with high-quality gold standards. However, the study of attitudes and emotions in <a href=https://en.wikipedia.org/wiki/Ancient_literature>ancient texts</a> is a growing field of research which poses specific issues (e.g., lack of <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a>, limited amount of data, unusual textual genres for the sentiment analysis task, such as philosophical or documentary texts) and can have an impact on the work of scholars coming from several disciplines besides computational linguistics, e.g. historians and <a href=https://en.wikipedia.org/wiki/Philology>philologists</a>. The work presented in this paper aims at providing the research community with a set of sentiment lexicons built by taking advantage of manually-curated resources belonging to the long tradition of Latin corpora and lexicons creation. Our interdisciplinary approach led us to release : i) two automatically generated sentiment lexicons ; ii) a <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a> developed by two Latin language and culture experts ; iii) a <a href=https://en.wikipedia.org/wiki/Silver_standard>silver standard</a> in which semantic and derivational relations are exploited so to extend the list of lexical items of the gold standard. In addition, the evaluation procedure is described together with a first application of the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to a Latin tragedy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.377.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--377 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.377 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.377/>WordWars : A Dataset to Examine the Natural Selection of Words<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>W</span>ars: A Dataset to Examine the Natural Selection of Words</a></strong><br><a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--377><div class="card-body p-3 small">There is a growing body of work on how <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meaning</a> changes over time : <a href=https://en.wikipedia.org/wiki/Mutation>mutation</a>. In contrast, there is very little work on how different words compete to represent the same meaning, and how the degree of success of words in that competition changes over time : <a href=https://en.wikipedia.org/wiki/Natural_selection>natural selection</a>. We present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, <a href=https://en.wikipedia.org/wiki/WordWars>WordWars</a>, with historical frequency data from the early 1800s to the early 2000s for monosemous English words in over 5000 synsets. We explore three broad questions with the dataset : (1) what is the degree to which predominant words in these synsets have changed, (2) how do prominent word features such as <a href=https://en.wikipedia.org/wiki/Frequency>frequency</a>, <a href=https://en.wikipedia.org/wiki/Length>length</a>, and <a href=https://en.wikipedia.org/wiki/Concreteness>concreteness</a> impact <a href=https://en.wikipedia.org/wiki/Natural_selection>natural selection</a>, and (3) what are the differences between the predominant words of the 2000s and the predominant words of early 1800s. We show that close to one third of the synsets undergo a change in the predominant word in this time period. Manual annotation of these pairs shows that about 15 % of these are orthographic variations, 25 % involve affix changes, and 60 % have completely different roots. We find that <a href=https://en.wikipedia.org/wiki/Frequency>frequency</a>, <a href=https://en.wikipedia.org/wiki/Length>length</a>, and <a href=https://en.wikipedia.org/wiki/Concreteness>concreteness</a> all impact <a href=https://en.wikipedia.org/wiki/Natural_selection>natural selection</a>, albeit in different ways.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.378/>Challenge Dataset of Cognates and False Friend Pairs from <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a><span class=acl-fixed-case>I</span>ndian Languages</a></strong><br><a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/m/malhar-kulkarni/>Malhar Kulkarni</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--378><div class="card-body p-3 small">Cognates are present in multiple variants of the same text across different languages (e.g., hund in <a href=https://en.wikipedia.org/wiki/German_language>German</a> and hound in the English language mean dog). They pose a challenge to various Natural Language Processing (NLP) applications such as <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, Cross-lingual Sense Disambiguation, <a href=https://en.wikipedia.org/wiki/Computational_phylogenetics>Computational Phylogenetics</a>, and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a>. A possible solution to address this challenge is to identify <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian languages namely <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Assamese_language>Assamese</a>, <a href=https://en.wikipedia.org/wiki/Odia_language>Oriya</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>. We digitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate sets. Additionally, we use the Wordnet data to create a False Friends&#8217; dataset for eleven language pairs. We also evaluate the efficacy of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.379/>Development of a Japanese Personality Dictionary based on Psychological Methods<span class=acl-fixed-case>J</span>apanese Personality Dictionary based on Psychological Methods</a></strong><br><a href=/people/r/ritsuko-iwai/>Ritsuko Iwai</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/t/takatsune-kumada/>Takatsune Kumada</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--379><div class="card-body p-3 small">We propose a new approach to constructing a personality dictionary with psychological evidence. In this study, we collect personality words, using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and construct a personality dictionary with weights for <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five traits</a>. The weights are calculated based on the responses of the large sample (N=1,938, female = 1,004, M=49.8years old:20-78, SD=16.3). All the respondents answered a 20-item personality questionnaire and 537 personality items derived from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We present the <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> to examine the qualities of responses with <a href=https://en.wikipedia.org/wiki/Psychology>psychological methods</a> and to calculate the weights. These result in a personality dictionary with two sub-dictionaries. We also discuss an application of the acquired resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--380 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.380/>A Lexicon-Based Approach for Detecting Hedges in Informal Text</a></strong><br><a href=/people/j/jumayel-islam/>Jumayel Islam</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a>
|
<a href=/people/r/robert-e-mercer/>Robert E. Mercer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--380><div class="card-body p-3 small">Hedging is a commonly used strategy in conversational management to show the speaker&#8217;s lack of commitment to what they communicate, which may signal problems between the speakers. Our project is interested in examining the presence of hedging words and phrases in identifying the tension between an interviewer and interviewee during a survivor interview. While there have been studies on hedging detection in the natural language processing literature, all existing work has focused on <a href=https://en.wikipedia.org/wiki/Structured_text>structured texts</a> and formal communications. Our project thus investigated a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of eight <a href=https://en.wikipedia.org/wiki/Unstructured_interview>unstructured conversational interviews</a> about the <a href=https://en.wikipedia.org/wiki/Rwandan_genocide>Rwanda Genocide</a> and identified <a href=https://en.wikipedia.org/wiki/Hedge>hedging patterns</a> in the interviewees&#8217; responses. Our work produced three manually constructed lists of hedge words, booster words, and hedging phrases. Leveraging these <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, we developed a rule-based algorithm that detects sentence-level hedges in informal conversations such as survivor interviews. Our work also produced a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 3000 sentences having the categories Hedge and Non-hedge annotated by three researchers. With experiments on this annotated dataset, we verify the efficacy of our proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Our work contributes to the further development of tools that identify <a href=https://en.wikipedia.org/wiki/Hedge_(finance)>hedges</a> from informal conversations and discussions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--382 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.382/>Inducing Universal Semantic Tag Vectors</a></strong><br><a href=/people/d/da-huo/>Da Huo</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--382><div class="card-body p-3 small">Given the well-established usefulness of part-of-speech tag annotations in many syntactically oriented downstream NLP tasks, the recently proposed notion of semantic tagging (Bjerva et al. 2016) aims at tagging words with tags informed by semantic distinctions, which are likely to be useful across a range of semantic tasks. To this end, their annotation scheme distinguishes, for instance, privative attributes from subsective ones. While annotated corpora exist, their size is limited and thus many words are out-of-vocabulary words. In this paper, we study to what extent we can automatically predict the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> associated with unseen words. We draw on large-scale word representation data to derive a large new Semantic Tag lexicon. Our experiments show that we can infer semantic tags for words with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> both monolingually and cross-lingually.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.384/>Towards a Semi-Automatic Detection of Reflexive and Reciprocal Constructions and Their Representation in a Valency Lexicon</a></strong><br><a href=/people/v/vaclava-kettnerova/>Václava Kettnerová</a>
|
<a href=/people/m/marketa-lopatkova/>Marketa Lopatkova</a>
|
<a href=/people/a/anna-vernerova/>Anna Vernerová</a>
|
<a href=/people/p/petra-barancikova/>Petra Barancikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--384><div class="card-body p-3 small">Valency lexicons usually describe <a href=https://en.wikipedia.org/wiki/Valency_(linguistics)>valency behavior</a> of verbs in non-reflexive and non-reciprocal constructions. However, reflexive and reciprocal constructions are common morphosyntactic forms of verbs. Both of these constructions are characterized by regular changes in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic properties</a> of verbs, thus they can be described by <a href=https://en.wikipedia.org/wiki/Grammar>grammatical rules</a>. On the other hand, the possibility to create reflexive and/or reciprocal constructions can not be trivially derived from the morphosyntactic structure of verbs as it is conditioned by their semantic properties as well. A large-coverage valency lexicon allowing for rule based generation of all well formed verb constructions should thus integrate the information on <a href=https://en.wikipedia.org/wiki/Reflexive_verb>reflexivity</a> and reciprocity. In this paper, we propose a semi-automatic procedure, based on grammatical constraints on <a href=https://en.wikipedia.org/wiki/Reflexivity_(grammar)>reflexivity</a> and reciprocity, detecting those verbs that form reflexive and reciprocal constructions in <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus data</a>. However, exploitation of <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus data</a> for this purpose is complicated due to the diverse functions of reflexive markers crossing the domain of <a href=https://en.wikipedia.org/wiki/Reflex>reflexivity</a> and reciprocity. The list of verbs identified by the previous procedure is thus further used in an automatic experiment, applying <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for detecting semantically similar verbs. These candidate verbs have been manually verified and annotation of their reflexive and reciprocal constructions has been integrated into the valency lexicon of Czech verbs VALLEX.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.385/>Languages Resources for Poorly Endowed Languages : The Case Study of Classical Armenian<span class=acl-fixed-case>C</span>lassical <span class=acl-fixed-case>A</span>rmenian</a></strong><br><a href=/people/c/chahan-vidal-gorene/>Chahan Vidal-Gorène</a>
|
<a href=/people/a/alienor-decours-perez/>Aliénor Decours-Perez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--385><div class="card-body p-3 small">Classical Armenian is a poorly endowed language, that despite a great tradition of <a href=https://en.wikipedia.org/wiki/Lexicography>lexicographical erudition</a> is coping with a lack of resources. Although numerous initiatives exist to preserve the <a href=https://en.wikipedia.org/wiki/Classical_Armenian>Classical Armenian language</a>, the lack of precise and complete grammatical and lexicographical resources remains. This article offers a situation analysis of the existing resources for <a href=https://en.wikipedia.org/wiki/Classical_Armenian>Classical Armenian</a> and presents the new digital resources provided on the Calfa platform. The Calfa project gathers existing resources and updates, enriches and enhances their content to offer the richest database for <a href=https://en.wikipedia.org/wiki/Classical_Armenian>Classical Armenian</a> today. Faced with the challenges specific to a poorly endowed language, the Calfa project is also developing new technologies and solutions to enable <a href=https://en.wikipedia.org/wiki/Language_preservation>preservation</a>, advanced research, and larger systems and developments for the <a href=https://en.wikipedia.org/wiki/Armenian_language>Armenian language</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.389/>Linking the TUFS Basic Vocabulary to the Open Multilingual Wordnet<span class=acl-fixed-case>TUFS</span> Basic Vocabulary to the Open Multilingual <span class=acl-fixed-case>W</span>ordnet</a></strong><br><a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/h/hiroki-nomoto/>Hiroki Nomoto</a>
|
<a href=/people/l/luis-morgado-da-costa/>Luís Morgado da Costa</a>
|
<a href=/people/a/arthur-bond/>Arthur Bond</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--389><div class="card-body p-3 small">We describe the linking of the TUFS Basic Vocabulary Modules, created for <a href=https://en.wikipedia.org/wiki/Educational_technology>online language learning</a>, with the Open Multilingual Wordnet. The TUFS modules have roughly 500 lexical entries in 30 languages, each with the lemma, a link across the languages, an example sentence, usage notes and sound files. The Open Multilingual Wordnet has 34 languages (11 shared with TUFS) organized into synsets linked by semantic relations, with examples and definitions for some languages. The links can be used to (i) evaluate existing wordnets, (ii) add data to these wordnets and (iii) create new open wordnets for <a href=https://en.wikipedia.org/wiki/Khmer_language>Khmer</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, <a href=https://en.wikipedia.org/wiki/Lao_language>Lao</a>, <a href=https://en.wikipedia.org/wiki/Mongolian_language>Mongolian</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog</a>, <a href=https://en.wikipedia.org/wiki/Urdu>Urdua</a> nd Vietnamese</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.390/>Some Issues with Building a Multilingual Wordnet<span class=acl-fixed-case>W</span>ordnet</a></strong><br><a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/l/luis-morgado-da-costa/>Luis Morgado da Costa</a>
|
<a href=/people/m/michael-wayne-goodman/>Michael Wayne Goodman</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a>
|
<a href=/people/a/ahti-lohk/>Ahti Lohk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--390><div class="card-body p-3 small">In this paper we discuss the experience of bringing together over 40 different <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a>. We introduce some extensions to the GWA wordnet LMF format proposed in Vossen et al. (2016) and look at how this new information can be displayed. Notable extensions include : <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a>, corpus frequency, orthographic variants, lexicalized and non-lexicalized synsets and lemmas, new <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts of speech</a>, and more. Many of these <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> already exist in multiple <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a> the challenge was to find a compatible <a href=https://en.wikipedia.org/wiki/Group_representation>representation</a>. To this end, we introduce a new version of the Open Multilingual Wordnet (Bond and Foster, 2013), that integrates a new set of tools that tests the extensions introduced by this new format, while also ensuring the integrity of the Collaborative Interlingual Index (CILI : Bond et al., 2016), avoiding the same new concept to be introduced through multiple projects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.394/>Automatic Reconstruction of Missing Romanian Cognates and Unattested Latin Words<span class=acl-fixed-case>R</span>omanian Cognates and Unattested <span class=acl-fixed-case>L</span>atin Words</a></strong><br><a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a>
|
<a href=/people/l/laurentiu-zoicas/>Laurentiu Zoicas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--394><div class="card-body p-3 small">Producing related words is a key concern in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. Given an input word, the task is to automatically produce either its proto-word, a <a href=https://en.wikipedia.org/wiki/Cognate>cognate pair</a> or a <a href=https://en.wikipedia.org/wiki/Modern_language>modern word</a> derived from it. In this paper, we apply a method for producing related words based on <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, aiming to fill in the gaps in incomplete cognate sets in <a href=https://en.wikipedia.org/wiki/Romance_languages>Romance languages</a> with Latin etymology (producing Romanian cognates that are missing) and to reconstruct uncertified Latin words. We further investigate an ensemble-based aggregation for combining and re-ranking the word productions of multiple languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.395/>A Multilingual Evaluation Dataset for Monolingual Word Sense Alignment</a></strong><br><a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a>
|
<a href=/people/s/sanni-nimb/>Sanni Nimb</a>
|
<a href=/people/f/fahad-khan/>Fahad Khan</a>
|
<a href=/people/m/monica-monachini/>Monica Monachini</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/t/tanja-wissik/>Tanja Wissik</a>
|
<a href=/people/a/andrea-bellandi/>Andrea Bellandi</a>
|
<a href=/people/i/irene-pisani/>Irene Pisani</a>
|
<a href=/people/t/thomas-troelsgard/>Thomas Troelsgård</a>
|
<a href=/people/s/sussi-olsen/>Sussi Olsen</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/v/veronika-lipp/>Veronika Lipp</a>
|
<a href=/people/t/tamas-varadi/>Tamás Váradi</a>
|
<a href=/people/l/laszlo-simon/>László Simon</a>
|
<a href=/people/a/andras-gyorffy/>András Gyorffy</a>
|
<a href=/people/c/carole-tiberius/>Carole Tiberius</a>
|
<a href=/people/t/tanneke-schoonheim/>Tanneke Schoonheim</a>
|
<a href=/people/y/yifat-ben-moshe/>Yifat Ben Moshe</a>
|
<a href=/people/m/maya-rudich/>Maya Rudich</a>
|
<a href=/people/r/raya-abu-ahmad/>Raya Abu Ahmad</a>
|
<a href=/people/d/dorielle-lonke/>Dorielle Lonke</a>
|
<a href=/people/k/kira-kovalenko/>Kira Kovalenko</a>
|
<a href=/people/m/margit-langemets/>Margit Langemets</a>
|
<a href=/people/j/jelena-kallas/>Jelena Kallas</a>
|
<a href=/people/o/oksana-dereza/>Oksana Dereza</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/d/david-cillessen/>David Cillessen</a>
|
<a href=/people/d/david-lindemann/>David Lindemann</a>
|
<a href=/people/m/mikel-alonso/>Mikel Alonso</a>
|
<a href=/people/a/ana-salgado/>Ana Salgado</a>
|
<a href=/people/j/jose-luis-sancho/>José Luis Sancho</a>
|
<a href=/people/r/rafael-j-urena-ruiz/>Rafael-J. Ureña-Ruiz</a>
|
<a href=/people/j/jordi-porta-zamorano/>Jordi Porta Zamorano</a>
|
<a href=/people/k/kiril-simov/>Kiril Simov</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/z/zara-kancheva/>Zara Kancheva</a>
|
<a href=/people/i/ivaylo-radev/>Ivaylo Radev</a>
|
<a href=/people/r/ranka-stankovic/>Ranka Stanković</a>
|
<a href=/people/a/andrej-perdih/>Andrej Perdih</a>
|
<a href=/people/d/dejan-gabrovsek/>Dejan Gabrovsek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--395><div class="card-body p-3 small">Aligning senses across resources and languages is a challenging task with beneficial applications in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and electronic lexicography. In this paper, we describe our efforts in manually aligning <a href=https://en.wikipedia.org/wiki/Monolingual_dictionary>monolingual dictionaries</a>. The alignment is carried out at sense-level for various resources in 15 languages. Moreover, senses are annotated with possible semantic relationships such as broadness, narrowness, <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a>, and <a href=https://en.wikipedia.org/wiki/Equivalence_relation>equivalence</a>. In comparison to previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, this dataset covers a wide range of languages and resources and focuses on the more challenging task of linking general-purpose language. We believe that our data will pave the way for further advances in alignment and evaluation of word senses by creating new solutions, particularly those notoriously requiring data such as <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Our resources are publicly available at https://github.com/elexis-eu/MWSA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--401 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.401/>The ACoLi Dictionary Graph<span class=acl-fixed-case>AC</span>o<span class=acl-fixed-case>L</span>i Dictionary Graph</a></strong><br><a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/c/christian-fath/>Christian Fäth</a>
|
<a href=/people/m/maxim-ionov/>Maxim Ionov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--401><div class="card-body p-3 small">In this paper, we report the release of the ACoLi Dictionary Graph, a large-scale collection of multilingual open source dictionaries available in two machine-readable formats, a graph representation in RDF, using the OntoLex-Lemon vocabulary, and a simple tabular data format to facilitate their use in NLP tasks, such as translation inference across dictionaries. We describe the mapping and harmonization of the underlying data structures into a unified representation, its serialization in <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a> and TSV, and the release of a massive and coherent amount of lexical data under open licenses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.402/>Resources in Underrepresented Languages : Building a Representative Romanian Corpus<span class=acl-fixed-case>R</span>omanian Corpus</a></strong><br><a href=/people/l/ludmila-midrigan-ciochina/>Ludmila Midrigan - Ciochina</a>
|
<a href=/people/v/victoria-boyd/>Victoria Boyd</a>
|
<a href=/people/l/lucila-sanchez-ortega/>Lucila Sanchez-Ortega</a>
|
<a href=/people/d/diana-malancea-malac/>Diana Malancea_Malac</a>
|
<a href=/people/d/doina-midrigan/>Doina Midrigan</a>
|
<a href=/people/d/david-p-corina/>David P. Corina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--402><div class="card-body p-3 small">The effort in the field of <a href=https://en.wikipedia.org/wiki/Linguistics>Linguistics</a> to develop theories that aim to explain language-dependent effects on <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> is greatly facilitated by the availability of reliable resources representing different languages. This project presents a detailed description of the process of creating a large and representative corpus in Romanian a relatively under-resourced language with unique structural and typological characteristics, that can be used as a reliable language resource for linguistic studies. The decisions that have guided the construction of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, including the type of <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, its size and component resource files are discussed. Issues related to <a href=https://en.wikipedia.org/wiki/Data>data collection</a>, data organization and storage, as well as characteristics of the <a href=https://en.wikipedia.org/wiki/Data>data</a> included in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> are described. Currently, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has approximately 5,500,000 tokens originating from <a href=https://en.wikipedia.org/wiki/Written_language>written text</a> and 100,000 tokens of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>. it includes language samples that represent a wide variety of registers (i.e. written language-16 registers and 5 registers of spoken language), as well as different authors and speakers</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.405/>The CLARIN Knowledge Centre for Atypical Communication Expertise<span class=acl-fixed-case>CLARIN</span> Knowledge Centre for Atypical Communication Expertise</a></strong><br><a href=/people/h/henk-van-den-heuvel/>Henk van den Heuvel</a>
|
<a href=/people/n/nelleke-oostdijk/>Nelleke Oostdijk</a>
|
<a href=/people/c/caroline-rowland/>Caroline Rowland</a>
|
<a href=/people/p/paul-trilsbeek/>Paul Trilsbeek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--405><div class="card-body p-3 small">This paper introduces a new CLARIN Knowledge Center which is the K-Centre for Atypical Communication Expertise (ACE for short) which has been established at the Centre for Language and Speech Technology (CLST) at Radboud University. Atypical communication is an umbrella term used here to denote language use by <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learners</a>, people with <a href=https://en.wikipedia.org/wiki/Language_disorder>language disorders</a> or those suffering from language disabilities, but also more broadly by <a href=https://en.wikipedia.org/wiki/Multilingualism>bilinguals</a> and users of <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>. It involves multiple modalities (text, <a href=https://en.wikipedia.org/wiki/Speech>speech</a>, <a href=https://en.wikipedia.org/wiki/Sign_language>sign</a>, gesture) and encompasses different developmental stages. ACE closely collaborates with The Language Archive (TLA) at the Max Planck Institute for Psycholinguistics in order to safeguard GDPR-compliant data storage and access. We explain the mission of <a href=https://en.wikipedia.org/wiki/ACE_(software)>ACE</a> and show its potential on a number of showcases and a use case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.407/>The European Language Technology Landscape in 2020 : Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe<span class=acl-fixed-case>E</span>uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric <span class=acl-fixed-case>AI</span> for Cross-Cultural Communication in Multilingual <span class=acl-fixed-case>E</span>urope</a></strong><br><a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/s/stefanie-hegele/>Stefanie Hegele</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/a/andrejs-vasiljevs/>Andrejs Vasiļjevs</a>
|
<a href=/people/g/gerhard-backfried/>Gerhard Backfried</a>
|
<a href=/people/c/christoph-prinz/>Christoph Prinz</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a>
|
<a href=/people/l/luc-meertens/>Luc Meertens</a>
|
<a href=/people/p/paul-lukowicz/>Paul Lukowicz</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/a/andrea-losch/>Andrea Lösch</a>
|
<a href=/people/p/philipp-slusallek/>Philipp Slusallek</a>
|
<a href=/people/m/morten-irgens/>Morten Irgens</a>
|
<a href=/people/p/patrick-gatellier/>Patrick Gatellier</a>
|
<a href=/people/j/joachim-kohler/>Joachim Köhler</a>
|
<a href=/people/l/laure-le-bars/>Laure Le Bars</a>
|
<a href=/people/d/dimitra-anastasiou/>Dimitra Anastasiou</a>
|
<a href=/people/a/albina-auksoriute/>Albina Auksoriūtė</a>
|
<a href=/people/n/nuria-bel/>Núria Bel</a>
|
<a href=/people/a/antonio-branco/>António Branco</a>
|
<a href=/people/g/gerhard-budin/>Gerhard Budin</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/k/koenraad-de-smedt/>Koenraad De Smedt</a>
|
<a href=/people/r/radovan-garabik/>Radovan Garabík</a>
|
<a href=/people/m/maria-gavriilidou/>Maria Gavriilidou</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/s/svetla-koeva/>Svetla Koeva</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/c/cvetana-krstev/>Cvetana Krstev</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a>
|
<a href=/people/j/jan-odijk/>Jan Odijk</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/e/eirikur-rognvaldsson/>Eiríkur Rögnvaldsson</a>
|
<a href=/people/m/michael-rosner/>Mike Rosner</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/i/inguna-skadina/>Inguna Skadiņa</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a>
|
<a href=/people/d/dan-tufis/>Dan Tufiș</a>
|
<a href=/people/t/tamas-varadi/>Tamás Váradi</a>
|
<a href=/people/k/kadri-vider/>Kadri Vider</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--407><div class="card-body p-3 small">Multilingualism is a cultural cornerstone of Europe and firmly anchored in the <a href=https://en.wikipedia.org/wiki/Treaties_of_the_European_Union>European treaties</a> including <a href=https://en.wikipedia.org/wiki/Linguistic_rights>full language equality</a>. However, <a href=https://en.wikipedia.org/wiki/Language_barrier>language barriers</a> impacting business, <a href=https://en.wikipedia.org/wiki/Cross-cultural_communication>cross-lingual and cross-cultural communication</a> are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> and technologies tailored to Europe&#8217;s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> including many opportunities, synergies but also misconceptions has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.408/>A Framework for Shared Agreement of Language Tags beyond ISO 639<span class=acl-fixed-case>ISO</span> 639</a></strong><br><a href=/people/f/frances-gillis-webber/>Frances Gillis-Webber</a>
|
<a href=/people/s/sabine-tittel/>Sabine Tittel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--408><div class="card-body p-3 small">The identification and annotation of languages in an unambiguous and standardized way is essential for the description of linguistic data. It is the prerequisite for machine-based interpretation, <a href=https://en.wikipedia.org/wiki/Data_aggregation>aggregation</a>, and re-use of the <a href=https://en.wikipedia.org/wiki/Data>data</a> with respect to different languages. This makes it a key aspect especially for <a href=https://en.wikipedia.org/wiki/Linked_data>Linked Data</a> and the multilingual Semantic Web. The standard for language tags is defined by IETF&#8217;s BCP 47 and ISO 639 provides the language codes that are the tags&#8217; main constituents. However, for the identification of lesser-known languages, <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered languages</a>, <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>regional varieties</a> or historical stages of a language, the ISO 639 codes are insufficient. Also, the optional language sub-tags compliant with <a href=https://en.wikipedia.org/wiki/BCP_47>BCP 47</a> do not offer a possibility fine-grained enough to represent <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a>. We propose a versatile pattern that extends the BCP 47 sub-tag &#8216;privateuse&#8217; and is, thus, able to overcome the limits of <a href=https://en.wikipedia.org/wiki/BCP_47>BCP 47</a> and ISO 639. Sufficient coverage of the <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> is demonstrated with the use case of linguistic Linked Data of the endangered Gascon language. We show how to use a <a href=https://en.wikipedia.org/wiki/Uniform_Resource_Identifier>URI shortcode</a> for the extended <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>sub-tag</a>, making the length compliant with <a href=https://en.wikipedia.org/wiki/BCP_47>BCP 47</a>. We achieve this with a <a href=https://en.wikipedia.org/wiki/Web_application>web application</a> and API developed to encode and decode the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>language tag</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--411 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.411/>A CLARIN Transcription Portal for Interview Data<span class=acl-fixed-case>CLARIN</span> Transcription Portal for Interview Data</a></strong><br><a href=/people/c/christoph-draxler/>Christoph Draxler</a>
|
<a href=/people/h/henk-van-den-heuvel/>Henk van den Heuvel</a>
|
<a href=/people/a/arjan-van-hessen/>Arjan van Hessen</a>
|
<a href=/people/s/silvia-calamai/>Silvia Calamai</a>
|
<a href=/people/l/louise-corti/>Louise Corti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--411><div class="card-body p-3 small">In this paper we present a first version of a transcription portal for audio files based on automatic speech recognition (ASR) in various languages. The <a href=https://en.wikipedia.org/wiki/Web_portal>portal</a> is implemented in the CLARIN resources research network and intended for use by non-technical scholars. We explain the background and interdisciplinary nature of interview data, the perks and quirks of using ASR for transcribing the audio in a research context, the dos and don&#8217;ts for optimal use of the portal, and future developments foreseen. The portal is promoted in a range of workshops, but there are a number of challenges that have to be met. These challenges concern <a href=https://en.wikipedia.org/wiki/Privacy>privacy issues</a>, ASR quality, and <a href=https://en.wikipedia.org/wiki/Cost>cost</a>, amongst others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.415/>Constructing a Bilingual Hadith Corpus Using a Segmentation Tool</a></strong><br><a href=/people/s/shatha-altammami/>Shatha Altammami</a>
|
<a href=/people/e/eric-atwell/>Eric Atwell</a>
|
<a href=/people/a/ammar-alsalka/>Ammar Alsalka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--415><div class="card-body p-3 small">This article describes the process of gathering and constructing a bilingual parallel corpus of Islamic Hadith, which is the set of narratives reporting different aspects of the prophet Muhammad&#8217;s life. The corpus data is gathered from the six canonical <a href=https://en.wikipedia.org/wiki/Hadith_studies>Hadith collections</a> using a custom segmentation tool that automatically segments and annotates the two <a href=https://en.wikipedia.org/wiki/Hadith_terminology>Hadith components</a> with 92 % accuracy. This Hadith segmenter minimises the costs of language resource creation and produces consistent results independently from previous knowledge and experiences that usually influence human annotators. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> includes more than 10 M tokens and will be freely available via the LREC repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.416/>Facilitating Corpus Usage : Making Icelandic Corpora More Accessible for Researchers and Language Users<span class=acl-fixed-case>I</span>celandic Corpora More Accessible for Researchers and Language Users</a></strong><br><a href=/people/s/steinthor-steingrimsson/>Steinþór Steingrímsson</a>
|
<a href=/people/s/starkadur-barkarson/>Starkaður Barkarson</a>
|
<a href=/people/g/gunnar-thor-ornolfsson/>Gunnar Thor Örnólfsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--416><div class="card-body p-3 small">We introduce an array of open and accessible tools to facilitate the use of the Icelandic Gigaword Corpus, in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> as well as for students, linguists, sociologists and others benefitting from using large corpora. A KWIC engine, powered by the Swedish Korp tool is adapted to the specifics of the corpus. An n-gram viewer, highly customizable to suit different needs, allows users to study <a href=https://en.wikipedia.org/wiki/Word_usage>word usage</a> throughout the period of our <a href=https://en.wikipedia.org/wiki/Text_corpus>text collection</a>. A frequency dictionary provides much sought after information about word frequency statistics, computed for each subcorpus as well as aggregate, disambiguating homographs based on their respective lemmas and morphosyntactic tags. Furthermore, we provide n-grams based on the corpus, and a variety of pre-trained word embeddings models, based on <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> and ELMo. For three of the model types, multiple word embedding models are available trained with different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> and using either lemmatised or unlemmatised texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--419 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.419/>Privacy by Design and Language Resources</a></strong><br><a href=/people/p/pawel-kamocki/>Pawel Kamocki</a>
|
<a href=/people/a/andreas-witt/>Andreas Witt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--419><div class="card-body p-3 small">Privacy by Design (also referred to as Data Protection by Design) is an approach in which solutions and mechanisms addressing <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> and data protection are embedded through the entire project lifecycle, from the early design stage, rather than just added as an additional lawyer to the final product. Formulated in the 1990 by the Privacy Commissionner of Ontario, the principle of Privacy by Design has been discussed by institutions and policymakers on both sides of the Atlantic, and mentioned already in the 1995 EU Data Protection Directive (95/46 / EC). More recently, <a href=https://en.wikipedia.org/wiki/Privacy_by_Design>Privacy by Design</a> was introduced as one of the requirements of the <a href=https://en.wikipedia.org/wiki/General_Data_Protection_Regulation>General Data Protection Regulation (GDPR)</a>, obliging data controllers to define and adopt, already at the conception phase, appropriate measures and safeguards to implement data protection principles and protect the rights of the data subject. Failing to meet this obligation may result in a hefty fine, as it was the case in the Uniontrad decision by the French Data Protection Authority (CNIL). The ambition of the proposed paper is to analyse the practical meaning of Privacy by Design in the context of Language Resources, and propose measures and safeguards that can be implemented by the community to ensure respect of this principle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.420/>Making Metadata Fit for Next Generation Language Technology Platforms : The Metadata Schema of the European Language Grid<span class=acl-fixed-case>E</span>uropean Language Grid</a></strong><br><a href=/people/p/penny-labropoulou/>Penny Labropoulou</a>
|
<a href=/people/k/katerina-gkirtzou/>Katerina Gkirtzou</a>
|
<a href=/people/m/maria-gavriilidou/>Maria Gavriilidou</a>
|
<a href=/people/m/miltos-deligiannis/>Miltos Deligiannis</a>
|
<a href=/people/d/dimitrios-galanis/>Dimitris Galanis</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/m/maria-berger/>Maria Berger</a>
|
<a href=/people/v/valerie-mapelli/>Valérie Mapelli</a>
|
<a href=/people/m/michael-rigault/>Michael Rigault</a>
|
<a href=/people/v/victoria-arranz/>Victoria Arranz</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/g/gerhard-backfried/>Gerhard Backfried</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a>
|
<a href=/people/a/andres-garcia-silva/>Andres Garcia-Silva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--420><div class="card-body p-3 small">The current scientific and technological landscape is characterised by the increasing availability of <a href=https://en.wikipedia.org/wiki/Data_(computing)>data resources</a> and processing tools and services. In this setting, <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> have emerged as a key factor facilitating management, sharing and usage of such <a href=https://en.wikipedia.org/wiki/Digital_asset>digital assets</a>. In this paper we present ELG-SHARE, a rich metadata schema catering for the description of Language Resources and Technologies (processing and generation services and tools, models, corpora, term lists, etc.), as well as related entities (e.g., organizations, projects, supporting documents, etc.). The <a href=https://en.wikipedia.org/wiki/Database_schema>schema</a> powers the European Language Grid platform that aims to be the primary hub and marketplace for industry-relevant Language Technology in Europe. ELG-SHARE has been based on various metadata schemas, vocabularies, and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a>, as well as related recommendations and guidelines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--426 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.426/>Semi-supervised Development of ASR Systems for Multilingual Code-switched Speech in Under-resourced Languages<span class=acl-fixed-case>ASR</span> Systems for Multilingual Code-switched Speech in Under-resourced Languages</a></strong><br><a href=/people/a/astik-biswas/>Astik Biswas</a>
|
<a href=/people/e/emre-yilmaz/>Emre Yilmaz</a>
|
<a href=/people/f/febe-de-wet/>Febe De Wet</a>
|
<a href=/people/e/ewald-van-der-westhuizen/>Ewald Van der westhuizen</a>
|
<a href=/people/t/thomas-niesler/>Thomas Niesler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--426><div class="card-body p-3 small">This paper reports on the semi-supervised development of acoustic and language models for under-resourced, code-switched speech in five <a href=https://en.wikipedia.org/wiki/Languages_of_South_Africa>South African languages</a>. Two approaches are considered. The first constructs four separate bilingual automatic speech recognisers (ASRs) corresponding to four different language pairs between which speakers switch frequently. The second uses a single, unified, five-lingual ASR system that represents all the languages (English, <a href=https://en.wikipedia.org/wiki/Zulu_language>isiZulu</a>, <a href=https://en.wikipedia.org/wiki/Xhosa_language>isiXhosa</a>, <a href=https://en.wikipedia.org/wiki/Tswana_language>Setswana</a> and Sesotho). We evaluate the effectiveness of these two approaches when used to add additional data to our extremely sparse training sets. Results indicate that batch-wise semi-supervised training yields better results than a non-batch-wise approach. Furthermore, while the separate <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual systems</a> achieved better recognition performance than the unified system, they benefited more from pseudolabels generated by the five-lingual system than from those generated by the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.427/>CLFD : A Novel Vectorization Technique and Its Application in Fake News Detection<span class=acl-fixed-case>CLFD</span>: A Novel Vectorization Technique and Its Application in Fake News Detection</a></strong><br><a href=/people/m/michail-mersinias/>Michail Mersinias</a>
|
<a href=/people/s/stergos-afantenos/>Stergos Afantenos</a>
|
<a href=/people/g/georgios-chalkiadakis/>Georgios Chalkiadakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--427><div class="card-body p-3 small">In recent years, fake news detection has been an emerging research area. In this paper, we put forward a novel statistical approach for the generation of feature vectors to describe a document. Our so-called class label frequency distance (clfd), is shown experimentally to provide an effective way for boosting the performance of machine learning methods. Specifically, our experiments, carried out in the fake news detection domain, verify that efficient traditional machine learning methods that use our vectorization approach, consistently outperform deep learning methods that use word embeddings for small and medium sized datasets, while the results are comparable for large datasets. In addition, we demonstrate that a novel hybrid method that utilizes both a clfd-boosted logistic regression classifier and a deep learning one, clearly outperforms deep learning methods even in large datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--429 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.429/>Jamo Pair Encoding : Subcharacter Representation-based Extreme Korean Vocabulary Compression for Efficient Subword Tokenization<span class=acl-fixed-case>K</span>orean Vocabulary Compression for Efficient Subword Tokenization</a></strong><br><a href=/people/s/sangwhan-moon/>Sangwhan Moon</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--429><div class="card-body p-3 small">In the context of multilingual language model pre-training, <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> for languages with a broad set of potential characters is an unsolved problem. We propose two algorithms applicable in any unsupervised multilingual pre-training task, increasing the elasticity of budget required for building the vocabulary in Byte-Pair Encoding inspired tokenizers, significantly reducing the cost of supporting <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> in a multilingual model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.431/>Semi-supervised Deep Embedded Clustering with <a href=https://en.wikipedia.org/wiki/Anomaly_detection>Anomaly Detection</a> for Semantic Frame Induction</a></strong><br><a href=/people/z/zheng-xin-yong/>Zheng Xin Yong</a>
|
<a href=/people/t/tiago-timponi-torrent/>Tiago Timponi Torrent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--431><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> is recognized as one of the most fine-grained lexical databases, its coverage of <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical units</a> is still limited. To tackle this issue, we propose a two-step frame induction process : for a set of <a href=https://en.wikipedia.org/wiki/Lexical_unit>lexical units</a> not yet present in Berkeley FrameNet data release 1.7, first remove those that can not fit into any existing <a href=https://en.wikipedia.org/wiki/Semantic_frame>semantic frame</a> in <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> ; then, assign the remaining <a href=https://en.wikipedia.org/wiki/Lexical_unit>lexical units</a> to their correct frames. We also present the Semi-supervised Deep Embedded Clustering with Anomaly Detection (SDEC-AD) modelan algorithm that maps high-dimensional contextualized vector representations of lexical units to a low-dimensional latent space for better frame prediction and uses reconstruction error to identify lexical units that can not evoke frames in <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>. SDEC-AD outperforms the state-of-the-art methods in both steps of the frame induction process. Empirical results also show that definitions provide contextual information for representing and characterizing the frame membership of lexical units.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.433/>Automated Phonological Transcription of Akkadian Cuneiform Text<span class=acl-fixed-case>A</span>kkadian Cuneiform Text</a></strong><br><a href=/people/a/aleksi-sahala/>Aleksi Sahala</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/a/antti-arppe/>Antti Arppe</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--433><div class="card-body p-3 small">Akkadian was an <a href=https://en.wikipedia.org/wiki/East_Semitic_languages>East-Semitic language</a> spoken in ancient Mesopotamia. The <a href=https://en.wikipedia.org/wiki/Language>language</a> is attested on hundreds of thousands of cuneiform clay tablets. Several Akkadian text corpora contain only the <a href=https://en.wikipedia.org/wiki/Transliteration>transliterated text</a>. In this paper, we investigate <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>automated phonological transcription</a> of the transliterated corpora. The phonological transcription provides a linguistically appealing form to represent <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian</a>, because the <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a> is normalized according to the grammatical description of a given dialect and explicitly shows the <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian renderings</a> for Sumerian logograms. Because <a href=https://en.wikipedia.org/wiki/Cuneiform>cuneiform text</a> does not mark the <a href=https://en.wikipedia.org/wiki/Inflection>inflection</a> for <a href=https://en.wikipedia.org/wiki/Logogram>logograms</a>, the inflected form needs to be inferred from the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>sentence context</a>. To the best of our knowledge, this is the first documented attempt to automatically transcribe <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian</a>. Using a context-aware neural network model, we are able to automatically transcribe syllabic tokens at near human performance with 96 % recall @ 3, while the logogram transcription remains more challenging at 82 % recall @ 3.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--435 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.435/>Automatic In-the-wild Dataset Annotation with Deep Generalized Multiple Instance Learning</a></strong><br><a href=/people/j/joana-correia/>Joana Correia</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/b/bhiksha-raj/>Bhiksha Raj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--435><div class="card-body p-3 small">The automation of the diagnosis and monitoring of speech affecting diseases in real life situations, such as <a href=https://en.wikipedia.org/wiki/Depression_(mood)>Depression</a> or <a href=https://en.wikipedia.org/wiki/Parkinson&#8217;s_disease>Parkinson&#8217;s disease</a>, depends on the existence of rich and large datasets that resemble real life conditions, such as those collected from in-the-wild multimedia repositories like <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>. However, the cost of manually labeling these <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a> can be prohibitive. In this work, we propose to overcome this problem by automating the annotation process, without any requirements for human intervention. We formulate the annotation problem as a Multiple Instance Learning (MIL) problem, and propose a novel solution that is based on end-to-end differentiable neural networks. Our solution has the additional advantage of generalizing the MIL framework to more scenarios where the data is stil organized in bags but does not meet the MIL bag label conditions. We demonstrate the performance of the proposed method in labeling the in-the-Wild Speech Medical (WSM) Corpus, using simple textual cues extracted from videos and their metadata. Furthermore we show what is the contribution of each type of textual cues for the final model performance, as well as study the influence of the size of the bags of instances in determining the difficulty of the learning problem</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.436/>How Much Data Do You Need? About the Creation of a Ground Truth for Black Letter and the Effectiveness of Neural OCR<span class=acl-fixed-case>OCR</span></a></strong><br><a href=/people/p/phillip-benjamin-strobel/>Phillip Benjamin Ströbel</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/m/martin-volk/>Martin Volk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--436><div class="card-body p-3 small">Recent advances in Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) have led to more accurate textrecognition of historical documents. The <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a> heavily profit from these developments, but they still struggle whenchoosing from the plethora of OCR systems available on the one hand and when defining workflows for their projects on the other hand. In this work, we present our approach to build a ground truth for a historical German-language newspaper published in black letter. Wealso report how we used <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to systematically evaluate the performance of different <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR engines</a>. Additionally, we used this ground truthto make an informed estimate as to how much data is necessary to achieve high-quality <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> results. The outcomes of our experimentsshow that HTR architectures can successfully recognise black letter text and that a ground truth size of 50 newspaper pages suffices toachieve good OCR accuracy. Moreover, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform equally well on data they have not seen during training, which means thatadditional manual correction for diverging data is superfluous.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.438/>On The Performance of Time-Pooling Strategies for End-to-End Spoken Language Identification</a></strong><br><a href=/people/j/joao-monteiro/>Joao Monteiro</a>
|
<a href=/people/m/md-jahangir-alam/>Md Jahangir Alam</a>
|
<a href=/people/t/tiago-falk/>Tiago Falk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--438><div class="card-body p-3 small">Automatic speech processing applications often have to deal with the problem of aggregating local descriptors (i.e., representations of input speech data corresponding to specific portions across the time dimension) and turning them into a single fixed-dimension representation, known as global descriptor, on top of which downstream classification tasks can be performed. In this paper, we provide an empirical assessment of different time pooling strategies when used with state-of-the-art <a href=https://en.wikipedia.org/wiki/Representation_theory>representation learning models</a>. In particular, insights are provided as to when it is suitable to use simple statistics of local descriptors or when more sophisticated approaches are needed. Here, <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> is used as a case study and a <a href=https://en.wikipedia.org/wiki/Database>database</a> containing ten <a href=https://en.wikipedia.org/wiki/Languages_of_Asia>oriental languages</a> under varying test conditions (short-duration test recordings, confusing languages, unseen languages) is used. Experiments are performed with <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on top of global descriptors to provide insights on open-set evaluation performance and show that appropriate selection of such pooling strategies yield embeddings able to outperform well-known benchmark systems as well as previously results based on attention only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.442/>SEDAR : a Large Scale French-English Financial Domain Parallel Corpus<span class=acl-fixed-case>SEDAR</span>: a Large Scale <span class=acl-fixed-case>F</span>rench-<span class=acl-fixed-case>E</span>nglish Financial Domain Parallel Corpus</a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--442><div class="card-body p-3 small">This paper describes the acquisition, preprocessing and characteristics of <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a>, a large scale English-French parallel corpus for the financial domain. Our extensive experiments on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> show that <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> is essential to obtain good performance on <a href=https://en.wikipedia.org/wiki/Finance>finance</a>. We observe a large gain in the performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> trained on <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> when tested on finance, which makes <a href=https://en.wikipedia.org/wiki/SEDAR>SEDAR</a> suitable to study domain adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The first release of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 8.6 million high quality sentence pairs that are publicly available for research at https://github.com/autorite/sedar-bitext.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--443 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.443/>JParaCrawl : A Large Scale Web-Based English-Japanese Parallel Corpus<span class=acl-fixed-case>JP</span>ara<span class=acl-fixed-case>C</span>rawl: A Large Scale Web-Based <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>J</span>apanese Parallel Corpus</a></strong><br><a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--443><div class="card-body p-3 small">Recent machine translation algorithms mainly rely on <a href=https://en.wikipedia.org/wiki/Parallel_corpora>parallel corpora</a>. However, since the availability of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> remains limited, only some resource-rich language pairs can benefit from them. We constructed a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> for <a href=https://en.wikipedia.org/wiki/Standard_Japanese>English-Japanese</a>, for which the amount of publicly available parallel corpora is still limited. We constructed the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> by broadly crawling the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and automatically aligning parallel sentences. Our collected <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, called JParaCrawl, amassed over 8.7 million sentence pairs. We show how it includes a broader range of domains and how a neural machine translation model trained with it works as a good pre-trained model for fine-tuning specific domains. The pre-training and fine-tuning approaches achieved or surpassed performance comparable to model training from the initial state and reduced the training time. Additionally, we trained the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--446 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.446/>NMT and PBSMT Error Analyses in English to Brazilian Portuguese Automatic Translations<span class=acl-fixed-case>NMT</span> and <span class=acl-fixed-case>PBSMT</span> Error Analyses in <span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Automatic Translations</a></strong><br><a href=/people/h/helena-caseli/>Helena Caseli</a>
|
<a href=/people/m/marcio-inacio/>Marcio Inácio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--446><div class="card-body p-3 small">Machine Translation (MT) is one of the most important natural language processing applications. Independently of the applied MT approach, a MT system automatically generates an equivalent version (in some target language) of an input sentence (in some source language). Recently, a new MT approach has been proposed : neural machine translation (NMT). NMT systems have already outperformed traditional phrase-based statistical machine translation (PBSMT) systems for some pairs of languages. However, any MT approach outputs errors. In this work we present a comparative study of MT errors generated by a NMT system and a PBSMT system trained on the same English Brazilian Portuguese parallel corpus. This is the first study of this kind involving <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> for <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>. Furthermore, the analyses and conclusions presented here point out the specific problems of NMT outputs in relation to PBSMT ones and also give lots of insights into how to implement automatic post-editing for a NMT system. Finally, the corpora annotated with MT errors generated by both PBSMT and NMT systems are also available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--448 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.448/>Better Together : Modern Methods Plus Traditional Thinking in NP Alignment<span class=acl-fixed-case>NP</span> Alignment</a></strong><br><a href=/people/a/adam-kovacs/>Ádám Kovács</a>
|
<a href=/people/j/judit-acs/>Judit Ács</a>
|
<a href=/people/a/andras-kornai/>Andras Kornai</a>
|
<a href=/people/g/gabor-recski/>Gábor Recski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--448><div class="card-body p-3 small">We study a typical intermediary task to <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, the alignment of NPs in the <a href=https://en.wikipedia.org/wiki/Bitext>bitext</a>. After arguing that the task remains relevant even in an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end paradigm</a>, we present simple, dictionary- and word vector-based baselines and a BERT-based system. Our results make clear that even state of the art systems relying on the best end-to-end methods can be improved by bringing in old-fashioned methods such as <a href=https://en.wikipedia.org/wiki/Stopword>stopword removal</a>, <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>, and <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--449 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.449" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.449/>Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation<span class=acl-fixed-case>C</span>oursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation</a></strong><br><a href=/people/h/haiyue-song/>Haiyue Song</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--449><div class="card-body p-3 small">Lectures translation is a case of <a href=https://en.wikipedia.org/wiki/Translation>spoken language translation</a> and there is a lack of publicly available <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> for this purpose. To address this, we examine a framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> over continuous-space sentence representations. We also show how to use the resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For JapaneseEnglish lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we have released our code for parallel data creation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--450 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.450/>Being Generous with Sub-Words towards Small NMT Children<span class=acl-fixed-case>NMT</span> Children</a></strong><br><a href=/people/a/arne-defauw/>Arne Defauw</a>
|
<a href=/people/t/tom-vanallemeersch/>Tom Vanallemeersch</a>
|
<a href=/people/k/koen-van-winckel/>Koen Van Winckel</a>
|
<a href=/people/s/sara-szoc/>Sara Szoc</a>
|
<a href=/people/j/joachim-van-den-bogaert/>Joachim Van den Bogaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--450><div class="card-body p-3 small">In the context of under-resourced neural machine translation (NMT), <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from an NMT model trained on a high resource language pair, or from a multilingual NMT (M-NMT) model, has been shown to boost performance to a large extent. In this paper, we focus on so-called cold start transfer learning from an M-NMT model, which means that the parent model is not trained on any of the child data. Such a set-up enables quick adaptation of M-NMT models to new languages. We investigate the effectiveness of cold start transfer learning from a many-to-many M-NMT model to an under-resourced child. We show that sufficiently large sub-word vocabularies should be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to be effective in such a scenario. When adopting relatively large sub-word vocabularies we observe increases in performance thanks to <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a parent M-NMT model, both when translating to and from the under-resourced language. Our proposed approach involving dynamic vocabularies is both practical and effective. We report results on two under-resourced language pairs, i.e. Icelandic-English and Irish-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.457.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--457 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.457 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.457/>A Test Set for Discourse Translation from <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a><span class=acl-fixed-case>J</span>apanese to <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--457><div class="card-body p-3 small">We made a test set for Japanese-to-English discourse translation to evaluate the power of context-aware machine translation. For each discourse phenomenon, we systematically collected examples where the translation of the second sentence depends on the first sentence. Compared with a previous study on test sets for English-to-French discourse translation (CITATION), we needed different approaches to make the data because <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> has zero pronouns and represents different senses in different characters. We improved the <a href=https://en.wikipedia.org/wiki/Translation>translation accuracy</a> using context-aware neural machine translation, and the improvement mainly reflects the betterment of the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of zero pronouns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.463/>To Case or not to case : Evaluating Casing Methods for Neural Machine Translation</a></strong><br><a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/h/harritxu-gete/>Harritxu Gete</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--463><div class="card-body p-3 small">We present a comparative evaluation of casing methods for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>, to help establish an optimal pre- and post-processing methodology. We trained and compared system variants on data prepared with the main casing methods available, namely translation of raw data without case normalisation, lowercasing with recasing, truecasing, case factors and inline casing. Machine translation models were prepared on WMT 2017 English-German and English-Turkish datasets, for all translation directions, and the evaluation includes reference metric results as well as a targeted analysis of case preservation accuracy. Inline casing, where case information is marked along lowercased words in the training data, proved to be the optimal approach overall in these experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.464/>The MARCELL Legislative Corpus<span class=acl-fixed-case>MARCELL</span> Legislative Corpus</a></strong><br><a href=/people/t/tamas-varadi/>Tamás Váradi</a>
|
<a href=/people/s/svetla-koeva/>Svetla Koeva</a>
|
<a href=/people/m/martin-yamalov/>Martin Yamalov</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a>
|
<a href=/people/b/balint-sass/>Bálint Sass</a>
|
<a href=/people/b/bartlomiej-niton/>Bartłomiej Nitoń</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/p/piotr-pezik/>Piotr Pęzik</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/r/radu-ion/>Radu Ion</a>
|
<a href=/people/e/elena-irimia/>Elena Irimia</a>
|
<a href=/people/m/maria-mitrofan/>Maria Mitrofan</a>
|
<a href=/people/v/vasile-pais/>Vasile Păiș</a>
|
<a href=/people/d/dan-tufis/>Dan Tufiș</a>
|
<a href=/people/r/radovan-garabik/>Radovan Garabík</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/a/andraz-repar/>Andraz Repar</a>
|
<a href=/people/m/matjaz-rihtar/>Matjaž Rihtar</a>
|
<a href=/people/j/janez-brank/>Janez Brank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--464><div class="card-body p-3 small">This article presents the current outcomes of the MARCELL CEF Telecom project aiming to collect and deeply annotate a large comparable corpus of legal documents. The MARCELL corpus includes 7 monolingual sub-corpora (Bulgarian, Croatian, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, Slovak and Slovenian) containing the total body of respective national legislative documents. These sub-corpora are automatically sentence split, tokenized, lemmatized and morphologically and syntactically annotated. The monolingual sub-corpora are complemented by a thematically related parallel corpus (Croatian-English). The metadata and the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are uniformly provided for each language specific sub-corpus. Besides the standard <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic analysis</a> plus named entity and dependency annotation, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is enriched with the IATE and EUROVOC labels. The file format is CoNLL-U Plus Format, containing the ten columns specific to the CoNLL-U format and four extra columns specific to our corpora. The MARCELL corpora represents a rich and valuable source for further studies and developments in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, cross-lingual terminological data extraction and classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.469/>Handle with Care : A Case Study in Comparable Corpora Exploitation for Neural Machine Translation</a></strong><br><a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/h/harritxu-gete/>Harritxu Gete</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--469><div class="card-body p-3 small">We present the results of a case study in the exploitation of comparable corpora for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>. A large comparable corpus for Basque-Spanish was prepared, on the basis of independently-produced news by the Basque public broadcaster EiTB, and we discuss the impact of various techniques to exploit the original data in order to determine optimal variants of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. In particular, we show that filtering in terms of alignment thresholds and <a href=https://en.wikipedia.org/wiki/Outlier>length-difference outliers</a> has a significant impact on translation quality. The impact of tags identifying comparable data in the training datasets is also evaluated, with results indicating that this technique might be useful to help the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> discriminate noisy information, in the form of informational imbalance between aligned sentences. The final corpus was prepared according to the experimental results and is made available to the scientific community for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--470 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.470/>The FISKM Project : Resources and Tools for Finnish-Swedish Machine Translation and Cross-Linguistic Research<span class=acl-fixed-case>FISKMÖ</span> Project: Resources and Tools for <span class=acl-fixed-case>F</span>innish-<span class=acl-fixed-case>S</span>wedish Machine Translation and Cross-Linguistic Research</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/t/tommi-nieminen/>Tommi Nieminen</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/a/akseli-leino/>Akseli Leino</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/n/niko-papula/>Niko Papula</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--470><div class="card-body p-3 small">This paper presents FISKM</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.471.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--471 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.471 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.471/>Multiword Expression aware Neural Machine Translation</a></strong><br><a href=/people/a/andrea-zaninello/>Andrea Zaninello</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--471><div class="card-body p-3 small">Multiword Expressions (MWEs) are a frequently occurring phenomenon found in all <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> that is of great importance to <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>linguistic theory</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. Neural Machine Translation (NMT) architectures do not handle these expressions well and previous studies have rarely addressed MWEs in this framework. In this work, we show that <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and data augmentation, using external linguistic resources, can improve both translation of MWEs that occur in the source, and the generation of MWEs on the target, and increase performance by up to 5.09 BLEU points on MWE test sets. We also devise a MWE score to specifically assess the quality of MWE translation which agrees with human evaluation. We make available the MWE score implementation along with MWE-annotated training sets and corpus-based lists of MWEs for reproduction and extension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--473 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.473/>Finite State Machine Pattern-Root Arabic Morphological Generator, Analyzer and Diacritizer<span class=acl-fixed-case>A</span>rabic Morphological Generator, Analyzer and Diacritizer</a></strong><br><a href=/people/m/maha-alkhairy/>Maha Alkhairy</a>
|
<a href=/people/a/afshan-jafri/>Afshan Jafri</a>
|
<a href=/people/d/david-a-smith/>David Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--473><div class="card-body p-3 small">We describe and evaluate the Finite-State Arabic Morphologizer (FSAM) a concatenative (prefix-stem-suffix) and templatic (root- pattern) morphologizer that generates and analyzes undiacritized Modern Standard Arabic (MSA) words, and diacritizes them. Our bidirectional unified-architecture finite state machine (FSM) is based on morphotactic MSA grammatical rules. The FSM models the root-pattern structure related to <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, making it readily scalable unlike stem-tabulations in prevailing systems. We evaluate the <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of our model, with <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a> being percentage of words in Tashkeela (a large corpus) that can be analyzed. Accuracy is computed against a <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a>, comprising words and properties, created from the intersection of UD PADT treebank and Tashkeela. Coverage of analysis (extraction of root and properties from word) is 82 %. Accuracy results are : <a href=https://en.wikipedia.org/wiki/Zero_of_a_function>root</a> computed from a word (92 %), word generation from a <a href=https://en.wikipedia.org/wiki/Zero_of_a_function>root</a> (100 %), non-root properties of a word (97 %), and <a href=https://en.wikipedia.org/wiki/Diacritic>diacritization</a> (84 %). FSAM&#8217;s non-root results match or surpass MADAMIRA&#8217;s, and root result comparisons are not made because of the concatenative nature of publicly available morphologizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.474.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--474 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.474 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.474" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.474/>An <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>Unsupervised Method</a> for Weighting Finite-state Morphological Analyzers</a></strong><br><a href=/people/a/amr-keleg/>Amr Keleg</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/n/nick-howell/>Nick Howell</a>
|
<a href=/people/t/tommi-a-pirinen/>Tommi Pirinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--474><div class="card-body p-3 small">Morphological analysis is one of the tasks that have been studied for years. Different techniques have been used to develop <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for performing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>. Models based on finite state transducers have proved to be more suitable for languages with low available resources. In this paper, we have developed a method for weighting a <a href=https://en.wikipedia.org/wiki/Morphological_analysis>morphological analyzer</a> built using finite state transducers in order to disambiguate its results. The method is based on a word2vec model that is trained in a completely unsupervised way using raw untagged corpora and is able to capture the semantic meaning of the words. Most of the methods used for disambiguating the results of a <a href=https://en.wikipedia.org/wiki/Morphological_analysis>morphological analyzer</a> relied on having tagged corpora that need to manually built. Additionally, the method developed uses information about the token irrespective of its context unlike most of the other techniques that heavily rely on the word&#8217;s context to disambiguate its set of candidate analyses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--478 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.478/>Glawinette : a Linguistically Motivated Derivational Description of French Acquired from GLAWI<span class=acl-fixed-case>G</span>lawinette: a Linguistically Motivated Derivational Description of <span class=acl-fixed-case>F</span>rench Acquired from <span class=acl-fixed-case>GLAWI</span></a></strong><br><a href=/people/n/nabil-hathout/>Nabil Hathout</a>
|
<a href=/people/f/franck-sajous/>Franck Sajous</a>
|
<a href=/people/b/basilio-calderone/>Basilio Calderone</a>
|
<a href=/people/f/fiammetta-namer/>Fiammetta Namer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--478><div class="card-body p-3 small">Glawinette is a derivational lexicon of French that will be used to feed the Dmonette database. It has been created from the GLAWI machine readable dictionary. We collected couples of words from the definitions and the morphological sections of the dictionary and then selected the ones that form regular formal analogies and that instantiate frequent enough formal patterns. The <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> of the morphological families has then been used to identify for each couple of lexemes derivational patterns that are close to the intuition of the morphologists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.479/>BabyFST-Towards a Finite-State Based Computational Model of Ancient Babylonian<span class=acl-fixed-case>B</span>aby<span class=acl-fixed-case>FST</span> - Towards a Finite-State Based Computational Model of Ancient Babylonian</a></strong><br><a href=/people/a/aleksi-sahala/>Aleksi Sahala</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/a/antti-arppe/>Antti Arppe</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--479><div class="card-body p-3 small">Akkadian is a fairly well resourced extinct language that does not yet have a comprehensive <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzer</a> available. In this paper we describe a general finite-state based morphological model for <a href=https://en.wikipedia.org/wiki/Akkadian_language>Babylonian</a>, a southern dialect of the <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian language</a>, that can achieve a coverage up to 97.3 % and recall up to 93.7 % on <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and POS-tagging task on token level from a transcribed input. Since <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian word forms</a> exhibit a high degree of morphological ambiguity, in that only 20.1 % of running word tokens receive a single unambiguous analysis, we attempt a first pass at weighting our <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>finite-state transducer</a>, using existing extensive Akkadian corpora which have been partially validated for their lemmas and parts-of-speech but not the entire morphological analyses. The resultant <a href=https://en.wikipedia.org/wiki/Weighted_finite-state_transducer>weighted finite-state transducer</a> yields a moderate improvement so that for 57.4 % of the word tokens the highest ranked analysis is the correct one. We conclude with a short discussion on how morphological ambiguity in the analysis of <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian</a> could be further reduced with improvements in the training data used in weighting the <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>finite-state transducer</a> as well as through other, context-based techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--480 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.480/>Morphological Analysis and Disambiguation for <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a> : The Interplay between Resources and Methods<span class=acl-fixed-case>G</span>ulf <span class=acl-fixed-case>A</span>rabic: The Interplay between Resources and Methods</a></strong><br><a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--480><div class="card-body p-3 small">In this paper we present the first full morphological analysis and disambiguation system for <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a>. We use an existing state-of-the-art morphological disambiguation system to investigate the effects of different data sizes and different combinations of morphological analyzers for <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic</a>, <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf Arabic</a>. We find that in very low settings, morphological analyzers help boost the performance of the full morphological disambiguation task. However, as the size of resources increase, the value of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzers</a> decreases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--484 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.484/>Building the Spanish-Croatian Parallel Corpus<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>C</span>roatian Parallel Corpus</a></strong><br><a href=/people/b/bojana-mikelenic/>Bojana Mikelenić</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--484><div class="card-body p-3 small">This paper describes the building of the first Spanish-Croatian unidirectional parallel corpus, which has been constructed at the Faculty of Humanities and Social Sciences of the University of Zagreb. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is comprised of eleven <a href=https://en.wikipedia.org/wiki/Spanish_literature>Spanish novels</a> and their translations to Croatian done by six different professional translators. All the texts were published between 1999 and 2012. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has more than 2 Mw, with approximately 1 Mw for each language. It was automatically sentence segmented and aligned, as well as manually post-corrected, and contains 71,778 translation units. In order to protect the copyright and to make the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> available under permissive CC-BY licence, the aligned translation units are shuffled. This limits the usability of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for research of language units at sentence and lower language levels only. There are two versions of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> in TMX format that will be available for download through META-SHARE and CLARIN ERIC infrastructure. The former contains plain TMX, while the latter is lemmatised and POS-tagged and stored in the aTMX format.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.486.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--486 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.486 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.486" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.486/>Morfessor EM+Prune : Improved Subword Segmentation with Expectation Maximization and Pruning<span class=acl-fixed-case>M</span>orfessor <span class=acl-fixed-case>EM</span>+<span class=acl-fixed-case>P</span>rune: Improved Subword Segmentation with Expectation Maximization and Pruning</a></strong><br><a href=/people/s/stig-arne-gronroos/>Stig-Arne Grönroos</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/m/mikko-kurimo/>Mikko Kurimo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--486><div class="card-body p-3 small">Data-driven segmentation of words into subword units has been used in various natural language processing applications such as <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> for almost 20 years. Recently it has became more widely adopted, as models based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> often benefit from subword units even for morphologically simpler languages. In this paper, we discuss and compare <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithms</a> for a unigram subword model, based on the Expectation Maximization algorithm and lexicon pruning. Using English, Finnish, North Sami, and Turkish data sets, we show that this approach is able to find better solutions to the optimization problem defined by the Morfessor Baseline model than its original recursive training algorithm. The improved <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> also leads to higher morphological segmentation accuracy when compared to a linguistic gold standard. We publish implementations of the new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in the widely-used Morfessor software package.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--487 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.487/>Machine Learning and Deep Neural Network-Based Lemmatization and Morphosyntactic Tagging for <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a><span class=acl-fixed-case>S</span>erbian</a></strong><br><a href=/people/r/ranka-stankovic/>Ranka Stankovic</a>
|
<a href=/people/b/branislava-sandrih/>Branislava Šandrih</a>
|
<a href=/people/c/cvetana-krstev/>Cvetana Krstev</a>
|
<a href=/people/m/milos-utvic/>Miloš Utvić</a>
|
<a href=/people/m/mihailo-skoric/>Mihailo Skoric</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--487><div class="card-body p-3 small">The training of new tagger models for <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a> is primarily motivated by the enhancement of the existing tagset with the grammatical category of a gender. The harmonization of resources that were manually annotated within different projects over a long period of time was an important task, enabled by the development of tools that support partial automation. The supporting tools take into account different <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>taggers</a> and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagsets</a>. This paper focuses on TreeTagger and spaCy taggers, and the annotation schema alignment between Serbian morphological dictionaries, MULTEXT-East and Universal Part-of-Speech tagset. The trained models will be used to publish the new version of the Corpus of Contemporary Serbian as well as the <a href=https://en.wikipedia.org/wiki/Serbian_literature>Serbian literary corpus</a>. The performance of developed taggers were compared and the impact of training set size was investigated, which resulted in around 98 % PoS-tagging precision per token for both new models. The sr_basic annotated dataset will also be published.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--488 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.488/>Fine-grained Morphosyntactic Analysis and Generation Tools for More Than One Thousand Languages</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/d/dylan-lewis/>Dylan Lewis</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/a/aaron-mueller/>Aaron Mueller</a>
|
<a href=/people/w/winston-wu/>Winston Wu</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--488><div class="card-body p-3 small">Exploiting the broad translation of the Bible into the world&#8217;s languages, we train and distribute morphosyntactic tools for approximately one thousand languages, vastly outstripping previous distributions of tools devoted to the processing of inflectional morphology. Evaluation of the tools on a subset of available inflectional dictionaries demonstrates strong initial models, supplemented and improved through ensembling and dictionary-based reranking. Likewise, a novel type-to-token based evaluation metric allows us to confirm that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generalize well across rare and common forms alike</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.494.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--494 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.494 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.494" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.494/>CCNet : Extracting High Quality Monolingual Datasets from Web Crawl Data<span class=acl-fixed-case>CCN</span>et: Extracting High Quality Monolingual Datasets from Web Crawl Data</a></strong><br><a href=/people/g/guillaume-wenzek/>Guillaume Wenzek</a>
|
<a href=/people/m/marie-anne-lachaux/>Marie-Anne Lachaux</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--494><div class="card-body p-3 small">Pre-training text representations have led to significant improvements in many areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The quality of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from <a href=https://en.wikipedia.org/wiki/Common_Crawl>Common Crawl</a> for a variety of languages. Our <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> follows the <a href=https://en.wikipedia.org/wiki/Data_processing>data processing</a> introduced in <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Mikolov et al., 2017 ; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> with a filtering step to select documents that are close to high quality corpora like <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--501 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.501/>Multilingual Culture-Independent Word Analogy Datasets</a></strong><br><a href=/people/m/matej-ulcar/>Matej Ulčar</a>
|
<a href=/people/k/kristiina-vaik/>Kristiina Vaik</a>
|
<a href=/people/j/jessica-lindstrom/>Jessica Lindström</a>
|
<a href=/people/m/milda-dailidenaite/>Milda Dailidėnaitė</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--501><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> mostly use word embeddings as an input. Embeddings have to ensure that relations between words are reflected through distances in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>high-dimensional numeric space</a>. To compare the quality of different text embeddings, typically, we use <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. We present a collection of such datasets for the word analogy task in nine languages : <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/Latvian_language>Latvian</a>, Lithuanian, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We designed the monolingual analogy task to be much more culturally independent and also constructed cross-lingual analogy datasets for the involved languages. We present basic statistics of the created <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and their initial evaluation using fastText embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--506 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.506/>UniSent : Universal Adaptable Sentiment Lexica for 1000 + Languages<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>S</span>ent: Universal Adaptable Sentiment Lexica for 1000+ Languages</a></strong><br><a href=/people/e/ehsaneddin-asgari/>Ehsaneddin Asgari</a>
|
<a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a>
|
<a href=/people/c/christoph-ringlstetter/>Christoph Ringlstetter</a>
|
<a href=/people/m/mohammad-mofrad/>Mohammad Mofrad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--506><div class="card-body p-3 small">In this paper, we introduce UniSent universal sentiment lexica for <a href=https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers>1000 + languages</a>. Sentiment lexica are vital for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in absence of document-level annotations, a very common scenario for low-resource languages. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of the number of covered languages, including many low resource ones. In this work, we use a massively parallel Bible corpus to project <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to other languages for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter data</a>. We introduce a method called DomDrift to mitigate the huge domain mismatch between <a href=https://en.wikipedia.org/wiki/Bible>Bible</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> by a confidence weighting scheme that uses domain-specific embeddings to compare the <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbors</a> for a candidate sentiment word in the source (Bible) and target (Twitter) domain. We evaluate the quality of UniSent in a subset of languages for which manually created ground truth was available, <a href=https://en.wikipedia.org/wiki/Macedonian_language>Macedonian</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We show that the quality of UniSent is comparable to manually created sentiment resources when it is used as the sentiment seed for the task of word sentiment prediction on top of embedding representations. In addition, we show that emoticon sentiments could be reliably predicted in the Twitter domain using only UniSent and monolingual embeddings in <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. With the publication of this paper, we release the UniSent sentiment lexica at http://language-lab.info/unisent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--509 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.509/>A Dataset for Multi-lingual Epidemiological Event Extraction</a></strong><br><a href=/people/s/stephen-mutuvi/>Stephen Mutuvi</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a>
|
<a href=/people/g/gael-lejeune/>Gaël Lejeune</a>
|
<a href=/people/m/moses-odeo/>Moses Odeo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--509><div class="card-body p-3 small">This paper proposes a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for the development and evaluation of tools and techniques for identifying emerging infectious disease threats in online news text. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can not only be used for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, but also for other <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tasks</a> such as <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We make use of articles published on the Program for Monitoring Emerging Diseases (ProMED) platform, which provides current information about outbreaks of infectious diseases globally. Among the key pieces of information present in the articles is the uniform resource locator (URL) to the online news sources where the outbreaks were originally reported. We detail the procedure followed to build the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which includes leveraging the source URLs to retrieve the news reports and subsequently pre-processing the retrieved documents. We also report on experimental results of <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using the Data Analysis for Information Extraction in any Language(DAnIEL) system. DAnIEL is a multilingual news surveillance system that leverages unique attributes associated with news reporting to extract events : <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> and <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency</a>. The system has wide geographical and language coverage, including low-resource languages. In addition, we compare different classification approaches in terms of their ability to differentiate between epidemic-related and unrelated news articles that constitute the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--511 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.511/>Analysis of GlobalPhone and Ethiopian Languages Speech Corpora for Multilingual ASR<span class=acl-fixed-case>G</span>lobal<span class=acl-fixed-case>P</span>hone and <span class=acl-fixed-case>E</span>thiopian Languages Speech Corpora for Multilingual <span class=acl-fixed-case>ASR</span></a></strong><br><a href=/people/m/martha-yifiru-tachbelie/>Martha Yifiru Tachbelie</a>
|
<a href=/people/s/solomon-teferra-abate/>Solomon Teferra Abate</a>
|
<a href=/people/t/tanja-schultz/>Tanja Schultz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--511><div class="card-body p-3 small">In this paper, we present the analysis of GlobalPhone (GP) and speech corpora of Ethiopian languages (Amharic, <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna</a>, <a href=https://en.wikipedia.org/wiki/Oromo_language>Oromo</a> and Wolaytta). The aim of the analysis is to select speech data from GP for the development of multilingual Automatic Speech Recognition (ASR) system for the <a href=https://en.wikipedia.org/wiki/Ethiopian_Semitic_languages>Ethiopian languages</a>. To this end, phonetic overlaps among GP and <a href=https://en.wikipedia.org/wiki/Ethiopian_Semitic_languages>Ethiopian languages</a> have been analyzed. The result of our analysis shows that there is much phonetic overlap among <a href=https://en.wikipedia.org/wiki/Ethiopian_languages>Ethiopian languages</a> although they are from three different language families. From GP, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a> and Croatian are found to have much overlap with the <a href=https://en.wikipedia.org/wiki/Ethiopian_languages>Ethiopian languages</a>. On the other hand, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> has less <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic overlap</a> with the rest of the languages. Moreover, morphological complexity of the GP and Ethiopian languages, reflected by type to token ration (TTR) and out of vocabulary (OOV) rate, has been analyzed. Both <a href=https://en.wikipedia.org/wiki/Grammatical_number>metrics</a> indicated the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> of the <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Korean and <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> have been identified as extremely morphologically complex compared to the other languages. Tigrigna, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, etc. are also among the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically complex languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--512 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.512/>Multilingualization of <a href=https://en.wikipedia.org/wiki/Medical_terminology>Medical Terminology</a> : Semantic and Structural Embedding Approaches</a></strong><br><a href=/people/l/long-huei-chen/>Long-Huei Chen</a>
|
<a href=/people/k/kyo-kageura/>Kyo Kageura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--512><div class="card-body p-3 small">The multilingualization of terminology is an essential step in the translation pipeline, to ensure the correct transfer of domain-specific concepts. Many institutions and language service providers construct and maintain multilingual terminologies, which constitute important assets. However, the curation of such multilingual resources requires significant human effort ; though automatic multilingual term extraction methods have been proposed so far, they are of limited success as term translation can not be satisfied by simply conveying meaning, but requires the terminologists and domain experts&#8217; knowledge to fit the term within the existing terminology. Here we propose a method to encode the structural property of a term by aligning their embeddings using graph convolutional networks trained from separate languages. We observe that the structural information can augment the semantic methods also explored in this work, and recognize the unique nature of terminologies allows our method to fully take advantage and produce superior results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--517 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.517/>CoVoST : A Diverse Multilingual Speech-To-Text Translation Corpus<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>V</span>o<span class=acl-fixed-case>ST</span>: A Diverse Multilingual Speech-To-Text Translation Corpus</a></strong><br><a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/a/anne-wu/>Anne Wu</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--517><div class="card-body p-3 small">Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> involve language pairs with English as a source language, involve very specific domains or are low resource. We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the <a href=https://en.wikipedia.org/wiki/Data>data</a>. We also provide initial <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free to use. We also provide additional evaluation data derived from <a href=https://en.wikipedia.org/wiki/Tatoeba>Tatoeba</a> under <a href=https://en.wikipedia.org/wiki/Creative_Commons_license>CC licenses</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--521 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.521/>Massively Multilingual Pronunciation Modeling with WikiPron<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>P</span>ron</a></strong><br><a href=/people/j/jackson-l-lee/>Jackson L. Lee</a>
|
<a href=/people/l/lucas-f-e-ashby/>Lucas F.E. Ashby</a>
|
<a href=/people/m/m-elizabeth-garza/>M. Elizabeth Garza</a>
|
<a href=/people/y/yeonju-lee-sikka/>Yeonju Lee-Sikka</a>
|
<a href=/people/s/sean-miller/>Sean Miller</a>
|
<a href=/people/a/alan-wong/>Alan Wong</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--521><div class="card-body p-3 small">We introduce WikiPron, an open-source command-line tool for extracting pronunciation data from <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>, a collaborative multilingual online dictionary. We first describe the design and use of WikiPron. We then discuss the challenges faced scaling this <a href=https://en.wikipedia.org/wiki/Tool>tool</a> to create an automatically-generated database of 1.7 million <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciations</a> from 165 languages. Finally, we validate the pronunciation database by using it to train and evaluating a collection of generic grapheme-to-phoneme models. The <a href=https://en.wikipedia.org/wiki/Software>software</a>, <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciation data</a>, and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are all made available under permissive open-source licenses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--522 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.522/>HELFI : a Hebrew-Greek-Finnish Parallel Bible Corpus with Cross-Lingual Morpheme Alignment<span class=acl-fixed-case>HELFI</span>: a <span class=acl-fixed-case>H</span>ebrew-<span class=acl-fixed-case>G</span>reek-<span class=acl-fixed-case>F</span>innish Parallel <span class=acl-fixed-case>B</span>ible Corpus with Cross-Lingual Morpheme Alignment</a></strong><br><a href=/people/a/anssi-yli-jyra/>Anssi Yli-Jyrä</a>
|
<a href=/people/j/josi-purhonen/>Josi Purhonen</a>
|
<a href=/people/m/matti-liljeqvist/>Matti Liljeqvist</a>
|
<a href=/people/a/arto-antturi/>Arto Antturi</a>
|
<a href=/people/p/pekka-nieminen/>Pekka Nieminen</a>
|
<a href=/people/k/kari-m-rantila/>Kari M. Räntilä</a>
|
<a href=/people/v/valtter-luoto/>Valtter Luoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--522><div class="card-body p-3 small">Twenty-five years ago, morphologically aligned Hebrew-Finnish and Greek-Finnish bitexts (texts accompanied by a translation) were constructed manually in order to create an analytical concordance (Luoto et al., eds. 1997) for a <a href=https://en.wikipedia.org/wiki/Bible_translations_into_Finnish>Finnish Bible translation</a>. The creators of the bitexts recently secured the publisher&#8217;s permission to release its fine-grained alignment, but the alignment was still dependent on proprietary, third-party resources such as a copyrighted text edition and proprietary <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyses</a> of the source texts. In this paper, we describe a nontrivial editorial process starting from the creation of the original one-purpose database and ending with its reconstruction using only freely available text editions and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. This process produced an openly available dataset that contains (i) the source texts and their translations, (ii) the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyses</a>, (iii) the cross-lingual morpheme alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.527/>Visual Grounding Annotation of Recipe Flow Graph</a></strong><br><a href=/people/t/taichi-nishimura/>Taichi Nishimura</a>
|
<a href=/people/s/suzushi-tomori/>Suzushi Tomori</a>
|
<a href=/people/h/hayato-hashimoto/>Hayato Hashimoto</a>
|
<a href=/people/a/atsushi-hashimoto/>Atsushi Hashimoto</a>
|
<a href=/people/y/yoko-yamakata/>Yoko Yamakata</a>
|
<a href=/people/j/jun-harashima/>Jun Harashima</a>
|
<a href=/people/y/yoshitaka-ushiku/>Yoshitaka Ushiku</a>
|
<a href=/people/s/shinsuke-mori/>Shinsuke Mori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--527><div class="card-body p-3 small">In this paper, we provide a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that gives visual grounding annotations to recipe flow graphs. A recipe flow graph is a representation of the cooking workflow, which is designed with the aim of understanding the <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Such a <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> will increase its value when grounded to real-world activities, and visual grounding is a way to do so. Visual grounding is provided as bounding boxes to image sequences of recipes, and each bounding box is linked to an element of the <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a>. Because the <a href=https://en.wikipedia.org/wiki/Workflow>workflows</a> are also linked to the text, this <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> gives visual grounding with workflow&#8217;s contextual information between procedural text and visual observation in an indirect manner. We subsidiarily annotated two types of event attributes with each bounding box : doing-the-action, or done-the-action. As a result of the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, we got 2,300 <a href=https://en.wikipedia.org/wiki/Bounding_box>bounding boxes</a> in 272 flow graph recipes. Various experiments showed that the proposed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> enables us to estimate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> described in recipe flow graphs from an <a href=https://en.wikipedia.org/wiki/Image>image sequence</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.531/>Offensive Video Detection : Dataset and Baseline Results</a></strong><br><a href=/people/c/cleber-alcantara/>Cleber Alcântara</a>
|
<a href=/people/v/viviane-moreira/>Viviane Moreira</a>
|
<a href=/people/d/diego-feijo/>Diego Feijo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--531><div class="card-body p-3 small">Web-users produce and publish high volumes of data of various types, such as <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Image>images</a>, and <a href=https://en.wikipedia.org/wiki/Video>videos</a>. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> try to restrain their users from publishing offensive content to keep a friendly and respectful environment and rely on moderators to filter the posts. However, this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is insufficient due to the high volume of publications. The identification of offensive material can be performed automatically using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, which needs <a href=https://en.wikipedia.org/wiki/Annotation>annotated datasets</a>. Among the published datasets in this matter, the <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese language</a> is underrepresented, and <a href=https://en.wikipedia.org/wiki/Video>videos</a> are little explored. We investigated the problem of offensive video detection by assembling and publishing a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Video>videos</a> in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> containing mostly textual features. We ran experiments using popular <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> used in this domain and reported our findings, alongside multiple evaluation metrics. We found that using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> with Deep Learning classifiers achieved the best results on average. CNN architectures, <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a>, and <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forest</a> ranked top among different experiments. Transfer Learning models outperformed Classic <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> when processing video transcriptions, but scored lower using other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sets</a>. These findings can be used as a baseline for future works on this subject.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--537 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.537/>A Domain-Specific Dataset of Difficulty Ratings for German Noun Compounds in the Domains DIY, Cooking and Automotive<span class=acl-fixed-case>G</span>erman Noun Compounds in the Domains <span class=acl-fixed-case>DIY</span>, Cooking and Automotive</a></strong><br><a href=/people/j/julia-bettinger/>Julia Bettinger</a>
|
<a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/m/michael-dorna/>Michael Dorna</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--537><div class="card-body p-3 small">We present a dataset with difficulty ratings for 1,030 German closed noun compounds extracted from domain-specific texts for do-it-ourself (DIY), cooking and automotive. The dataset includes two-part compounds for cooking and DIY, and two- to four-part compounds for automotive. The <a href=https://en.wikipedia.org/wiki/Chemical_compound>compounds</a> were identified in text using the Simple Compound Splitter (Weller-Di Marco, 2017) ; a subset was filtered and balanced for frequency and productivity criteria as basis for manual annotation and fine-grained interpretation. This study presents the creation, the final <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with ratings from 20 annotators and statistics over the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, to provide insight into the perception of domain-specific term difficulty. It is particularly striking that annotators agree on a coarse, binary distinction between easy vs. difficult domain-specific compounds but that a more fine grained distinction of difficulty is not meaningful. We finally discuss the challenges of an <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for difficulty, which includes both the task description as well as the selection of the data basis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--538 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.538/>All That Glitters is Not Gold : A Gold Standard of Adjective-Noun Collocations for <a href=https://en.wikipedia.org/wiki/German_language>German</a><span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/y/yana-strakatova/>Yana Strakatova</a>
|
<a href=/people/n/neele-falk/>Neele Falk</a>
|
<a href=/people/i/isabel-fuhrmann/>Isabel Fuhrmann</a>
|
<a href=/people/e/erhard-hinrichs/>Erhard Hinrichs</a>
|
<a href=/people/d/daniela-rossmann/>Daniela Rossmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--538><div class="card-body p-3 small">In this paper we present the GerCo dataset of adjective-noun collocations for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, such as alter Freund &#8216;old friend&#8217; and tiefe Liebe &#8216;deep love&#8217;. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> has been performed by experts based on the <a href=https://en.wikipedia.org/wiki/Annotation>annotation scheme</a> introduced in this paper. The resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains 4,732 positive and negative instances of collocations and covers all the 16 semantic classes of adjectives as defined in the German wordnet GermaNet. The dataset can serve as a reliable empirical basis for comparing different theoretical frameworks concerned with <a href=https://en.wikipedia.org/wiki/Collocation>collocations</a> or as material for data-driven approaches to the studies of <a href=https://en.wikipedia.org/wiki/Collocation>collocations</a> including different machine learning experiments. This paper addresses the latter issue by using the GerCo dataset for evaluating different models on the task of automatic collocation identification. We compare lexical association measures with static and contextualized word embeddings. The experiments show that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> outperform methods based on statistical association measures by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.543/>A Joint Approach to Compound Splitting and Idiomatic Compound Detection</a></strong><br><a href=/people/i/irina-krotova/>Irina Krotova</a>
|
<a href=/people/s/sergey-aksenov/>Sergey Aksenov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--543><div class="card-body p-3 small">Applications such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> require efficient handling of noun compounds as they are one of the possible sources for out of vocabulary words. In-depth processing of noun compounds requires not only splitting them into smaller components (or even roots) but also the identification of instances that should remain unsplitted as they are of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomatic nature</a>. We develop a two-fold deep learning-based approach of noun compound splitting and idiomatic compound detection for the <a href=https://en.wikipedia.org/wiki/German_language>German language</a> that we train using a newly collected corpus of annotated German compounds. Our neural noun compound splitter operates on a sub-word level and outperforms the current state of the art by about 5 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.551" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.551/>A Dataset of German Legal Documents for Named Entity Recognition<span class=acl-fixed-case>G</span>erman Legal Documents for Named Entity Recognition</a></strong><br><a href=/people/e/elena-leitner/>Elena Leitner</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/j/julian-moreno-schneider/>Julian Moreno-Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--551><div class="card-body p-3 small">We describe a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> developed for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> in German federal court decisions. It consists of approx. 67,000 sentences with over 2 million tokens. The resource contains 54,000 manually annotated entities, mapped to 19 fine-grained semantic classes : person, judge, lawyer, country, city, street, landscape, organization, company, institution, court, brand, law, ordinance, European legal norm, <a href=https://en.wikipedia.org/wiki/Regulation>regulation</a>, <a href=https://en.wikipedia.org/wiki/Contract>contract</a>, court decision, and <a href=https://en.wikipedia.org/wiki/Legal_literature>legal literature</a>. The <a href=https://en.wikipedia.org/wiki/Legal_instrument>legal documents</a> were, furthermore, automatically annotated with more than 35,000 TimeML-based time expressions. The dataset, which is available under a CC-BY 4.0 license in the CoNNL-2002 format, was developed for training an NER service for German legal documents in the EU project Lynx.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.553/>Named Entities in Medical Case Reports : Corpus and Experiments</a></strong><br><a href=/people/s/sarah-schulz/>Sarah Schulz</a>
|
<a href=/people/j/jurica-seva/>Jurica Ševa</a>
|
<a href=/people/s/samuel-rodriguez/>Samuel Rodriguez</a>
|
<a href=/people/m/malte-ostendorff/>Malte Ostendorff</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--553><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprising annotations of medical entities in <a href=https://en.wikipedia.org/wiki/Case_report>case reports</a>, originating from <a href=https://en.wikipedia.org/wiki/PubMed_Central>PubMed Central&#8217;s open access library</a>. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a>. As such, <a href=https://en.wikipedia.org/wiki/This_(journal)>this</a> is the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of this kind made available to the scientific community in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence / paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--570 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.570/>Building OCR / NER Test Collections<span class=acl-fixed-case>OCR</span>/<span class=acl-fixed-case>NER</span> Test Collections</a></strong><br><a href=/people/d/dawn-lawrie/>Dawn Lawrie</a>
|
<a href=/people/j/james-mayfield/>James Mayfield</a>
|
<a href=/people/d/david-etter/>David Etter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--570><div class="card-body p-3 small">Named entity recognition (NER) identifies spans of text that contain <a href=https://en.wikipedia.org/wiki/Name>names</a>. Many researchers have reported the results of <a href=https://en.wikipedia.org/wiki/Near-infrared_spectroscopy>NER</a> on text created through optical character recognition (OCR) over the past two decades. Unfortunately, the test collections that support this research are annotated with <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> after <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition (OCR)</a> has been run. This means that the <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> must be re-annotated if the <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR output</a> changes. Instead by tying <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to character locations on the page, a <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> can be built that supports <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR and NER research</a> without requiring re-annotation when either improves. This means that <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> are annotated on the <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed text</a>. The transcribed text is all that is needed to evaluate the performance of <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a>. For NER evaluation, the tagged OCR output is aligned to the transcriptions the aligned files, creating modified files of each, which are scored. This paper presents a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for building such a test collection and releases a collection of Chinese OCR-NER data constructed using the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>. The paper provides performance baselines for current OCR and NER systems applied to this new <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--574 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.574/>Video Caption Dataset for Describing Human Actions in Japanese<span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/y/yutaro-shigeto/>Yutaro Shigeto</a>
|
<a href=/people/y/yuya-yoshikawa/>Yuya Yoshikawa</a>
|
<a href=/people/j/jiaqing-lin/>Jiaqing Lin</a>
|
<a href=/people/a/akikazu-takeuchi/>Akikazu Takeuchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--574><div class="card-body p-3 small">In recent years, automatic video caption generation has attracted considerable attention. This paper focuses on the generation of Japanese captions for describing <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>human actions</a>. While most currently available video caption datasets have been constructed for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, there is no equivalent Japanese dataset. To address this, we constructed a large-scale Japanese video caption dataset consisting of 79,822 videos and 399,233 captions. Each caption in our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> describes a video in the form of who does what and where. To describe <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>human actions</a>, it is important to identify the details of a person, place, and action. Indeed, when we describe human actions, we usually mention the <a href=https://en.wikipedia.org/wiki/Scene_(drama)>scene</a>, person, and action. In our experiments, we evaluated two caption generation methods to obtain benchmark results. Further, we investigated whether those generation methods could specify who does what and where.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.576/>Best Student Forcing : A Simple Training Mechanism in Adversarial Language Generation</a></strong><br><a href=/people/j/jonathan-sauder/>Jonathan Sauder</a>
|
<a href=/people/t/ting-hu/>Ting Hu</a>
|
<a href=/people/x/xiaoyin-che/>Xiaoyin Che</a>
|
<a href=/people/g/goncalo-mordido/>Goncalo Mordido</a>
|
<a href=/people/h/haojin-yang/>Haojin Yang</a>
|
<a href=/people/c/christoph-meinel/>Christoph Meinel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--576><div class="card-body p-3 small">Language models trained with Maximum Likelihood Estimation (MLE) have been considered as a mainstream solution in Natural Language Generation (NLG) for years. Recently, various approaches with Generative Adversarial Nets (GANs) have also been proposed. While offering exciting new prospects, GANs in NLG by far are nevertheless reportedly suffering from training instability and mode collapse, and therefore outperformed by conventional MLE models. In this work, we propose techniques for improving GANs in NLG, namely Best Student Forcing (BSF), a novel yet simple adversarial training mechanism in which generated sequences of high quality are selected as temporary ground-truth to further train the generator. We also use an ensemble of discriminators to increase training stability and sample diversity. Evaluation shows that the combination of BSF and multiple discriminators consistently performs better than previous GAN approaches over various metrics, and outperforms a baseline MLE in terms of Fr ech et Distance, a recently proposed metric capturing both sample quality and diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.578.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--578 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.578 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.578/>Exploring Transformer Text Generation for Medical Dataset Augmentation</a></strong><br><a href=/people/a/ali-amin-nejad/>Ali Amin-Nejad</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--578><div class="card-body p-3 small">Natural Language Processing (NLP) can help unlock the vast troves of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a> in clinical text and thus improve <a href=https://en.wikipedia.org/wiki/Medical_research>healthcare research</a>. However, a big barrier to developments in this field is data access due to patient confidentiality which prohibits the sharing of this data, resulting in small, fragmented and sequestered openly available datasets. Since NLP model development requires large quantities of data, we aim to help side-step this roadblock by exploring the usage of Natural Language Generation in augmenting datasets such that they can be used for NLP model development on downstream clinically relevant tasks. We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> guiding the generation with structured patient information in a sequence-to-sequence manner. We experiment with state-of-the-art Transformer models and demonstrate that our augmented dataset is capable of beating our baselines on a downstream classification task. Finally, we also create a <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> and release the scripts to train generation models to stimulate further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--585 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.585/>Towards a Gold Standard for Evaluating Danish Word Embeddings<span class=acl-fixed-case>D</span>anish Word Embeddings</a></strong><br><a href=/people/n/nina-schneidermann/>Nina Schneidermann</a>
|
<a href=/people/r/rasmus-hvingelby/>Rasmus Hvingelby</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--585><div class="card-body p-3 small">This paper presents the process of compiling a model-agnostic similarity goal standard for evaluating <a href=https://en.wikipedia.org/wiki/Danish_language>Danish word embeddings</a> based on human judgments made by 42 native speakers of <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. Word embeddings resemble semantic similarity solely by distribution (meaning that word vectors do not reflect relatedness as differing from similarity), and we argue that this generalization poses a problem in most intrinsic evaluation scenarios. In order to be able to evaluate on both dimensions, our human-generated dataset is therefore designed to reflect the distinction between <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a> and <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a>. The goal standard is applied for evaluating the goodness of six existing word embedding models for <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, and it is discussed how a relatively low correlation can be explained by the fact that <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> is substantially more challenging to model than relatedness, and that there seems to be a need for future human judgments to measure similarity in full context and along more than a single spectrum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.586.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--586 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.586 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.586/>Urban Dictionary Embeddings for Slang NLP Applications<span class=acl-fixed-case>NLP</span> Applications</a></strong><br><a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/b/barbara-mcgillivray/>Barbara McGillivray</a>
|
<a href=/people/k/kiran-garimella/>Kiran Garimella</a>
|
<a href=/people/g/gareth-tyson/>Gareth Tyson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--586><div class="card-body p-3 small">The choice of the corpus on which <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are trained can have a sizable effect on the learned <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>, the types of analyses that can be performed with them, and their utility as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. To contribute to the existing sets of pre-trained word embeddings, we introduce and release the first set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained on the content of <a href=https://en.wikipedia.org/wiki/Urban_Dictionary>Urban Dictionary</a>, a crowd-sourced dictionary for slang words and phrases. We show that although these embeddings are trained on fewer total tokens (by at least an order of magnitude compared to most popular pre-trained embeddings), they have high performance across a range of common word embedding evaluations, ranging from <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> to word clustering tasks. Further, for some extrinsic tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and sarcasm detection where we expect to require some knowledge of colloquial language on social media data, initializing classifiers with the Urban Dictionary Embeddings resulted in improved performance compared to initializing with a range of other well-known, pre-trained embeddings that are order of magnitude larger in size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.597.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--597 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.597 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.597/>Aligning <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> with <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> : a Review and Evaluation of Different Techniques<span class=acl-fixed-case>W</span>ikipedia with <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et:a Review and Evaluation of Different Techniques</a></strong><br><a href=/people/a/antoni-oliver/>Antoni Oliver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--597><div class="card-body p-3 small">In this paper we explore techniques for aligning Wikipedia articles with WordNet synsets, their successful alignment being our main goal. We evaluate techniques that use the definitions and sense relations in <a href=https://en.wikipedia.org/wiki/Wordnet>Wordnet</a> and the text and categories in Wikipedia articles. The results we present are based on two evaluation strategies : one uses a new gold and silver standard (for which the creation process is explained) ; the other creates <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a> in other languages and then compares them with existing <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a> for those languages found in the Open Multilingual Wordnet project. A reliable alignment between <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> is a very valuable resource for the creation of new <a href=https://en.wikipedia.org/wiki/WordNet>wordnets</a> in other languages and for the development of existing <a href=https://en.wikipedia.org/wiki/WordNet>wordnets</a>. The evaluation of alignments between <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and lexical resources is a difficult and time-consuming task, but the evaluation strategy using the Open Multilingual Wordnet can be used as an automated evaluation measure to assess the quality of alignments between these two resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.598/>The MWN.PT WordNet for <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> : Projection, Validation, Cross-lingual Alignment and Distribution<span class=acl-fixed-case>MWN</span>.<span class=acl-fixed-case>PT</span> <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et for <span class=acl-fixed-case>P</span>ortuguese: Projection, Validation, Cross-lingual Alignment and Distribution</a></strong><br><a href=/people/a/antonio-branco/>António Branco</a>
|
<a href=/people/s/sara-grilo/>Sara Grilo</a>
|
<a href=/people/m/marcia-bolrinha/>Márcia Bolrinha</a>
|
<a href=/people/c/chakaveh-saedi/>Chakaveh Saedi</a>
|
<a href=/people/r/ruben-branco/>Ruben Branco</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/a/andreia-querido/>Andreia Querido</a>
|
<a href=/people/r/rita-de-carvalho/>Rita de Carvalho</a>
|
<a href=/people/r/rosa-del-gaudio/>Rosa Gaudio</a>
|
<a href=/people/m/mariana-avelas/>Mariana Avelãs</a>
|
<a href=/people/c/clara-pinto/>Clara Pinto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--598><div class="card-body p-3 small">The objective of the present paper is twofold, to present the MWN.PT WordNet and to report on its construction and on the lessons learned with it. The MWN.PT WordNet for <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> includes 41,000 concepts, expressed by 38,000 <a href=https://en.wikipedia.org/wiki/Lexical_unit>lexical units</a>. Its <a href=https://en.wikipedia.org/wiki/Synonym>synsets</a> were manually validated and are linked to semantically equivalent synsets of the Princeton WordNet of English, and thus transitively to the many wordnets for other languages that are also linked to this English wordnet. To the best of our knowledge, it is the largest high quality, manually validated and cross-lingually integrated, wordnet of Portuguese distributed for reuse. Its construction was initiated more than one decade ago and its description is published for the first time in the present paper. It follows a three step projection, validation with alignment, completion methodology consisting on the manual validation and expansion of the outcome of an automatic projection procedure of synsets and their hypernym relations, followed by another automatic procedure that transferred the relations of remaining semantic types across wordnets of different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--600 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.600/>The Ontology of Bulgarian Dialects Architecture and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a><span class=acl-fixed-case>B</span>ulgarian Dialects – Architecture and Information Retrieval</a></strong><br><a href=/people/r/rositsa-dekova/>Rositsa Dekova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--600><div class="card-body p-3 small">Following a concise description of the structure, the paper focuses on the potential of the Ontology of the Bulgarian Dialects, which demonstrates a novel usage of the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological modelling</a> for the purposes of dialect digital archiving and <a href=https://en.wikipedia.org/wiki/Information_processing>information processing</a>. The <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> incorporates information on the dialects of the Bulgarian language and includes data from 84 dialects, spoken not only on the territory of the Republic of Bulgaria, but also abroad. It encodes both their geographical distribution and some of their main diagnostic features, such as the different mutations (also referred to as reflexes) of some of the Old Bulgarian vowels. The mutations modelled so far in the ontology include the reflex of the back nasal vowel // under stress, the reflex of the back er vowel // under stress, and the reflex of the yat vowel // under stress when it precedes a syllable with a back vowel. Besides the opportunity for formal structuring of the considerable amount of data gathered through the years by dialectologists, the <a href=https://en.wikipedia.org/wiki/Ontology>ontology</a> also provides numerous possibilities for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> searches by dialect, country, dialect region, city or village, various combinations of diagnostic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--604 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.604/>Metaphorical Expressions in Automatic Arabic Sentiment Analysis<span class=acl-fixed-case>A</span>rabic Sentiment Analysis</a></strong><br><a href=/people/i/israa-alsiyat/>Israa Alsiyat</a>
|
<a href=/people/s/scott-s-l-piao/>Scott Piao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--604><div class="card-body p-3 small">Over the recent years, Arabic language resources and NLP tools have been under rapid development. One of the important tasks for Arabic natural language processing is the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. While a significant improvement has been achieved in this research area, the existing computational models and tools still suffer from the lack of capability of dealing with Arabic metaphorical expressions. Metaphor has an important role in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> due to its unique history and culture. Metaphors provide a linguistic mechanism for expressing ideas and notions that can be different from their surface form. Therefore, in order to efficiently identify true <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> of Arabic language data, a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> needs to be able to read between lines. In this paper, we examine the issue of <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> in automatic Arabic sentiment analysis by carrying out an experiment, in which we observe the performance of a state-of-art Arabic sentiment tool on <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> and analyse the result to gain a deeper insight into the issue. Our experiment evidently shows that <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> have a significant impact on the performance of current Arabic sentiment tools, and it is an important task to develop Arabic language resources and <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> for Arabic metaphors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--606 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.606/>Doctor Who? Framing Through Names and Titles in German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/e/esther-van-den-berg/>Esther van den Berg</a>
|
<a href=/people/k/katharina-korfhage/>Katharina Korfhage</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--606><div class="card-body p-3 small">Entity framing is the selection of aspects of an entity to promote a particular viewpoint towards that entity. We investigate entity framing of political figures through the use of names and titles in German online discourse, enhancing current research in entity framing through <a href=https://en.wikipedia.org/wiki/Titling>titling</a> and <a href=https://en.wikipedia.org/wiki/Naming>naming</a> that concentrates on English only. We collect tweets that mention prominent <a href=https://en.wikipedia.org/wiki/Politics_of_Germany>German politicians</a> and annotate them for stance. We find that the formality of naming in these <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> correlates positively with their <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>. This confirms sociolinguistic observations that naming and titling can have a status-indicating function and suggests that this function is dominant in German tweets mentioning political figures. We also find that this status-indicating function is much weaker in tweets from users that are politically left-leaning than in tweets by right-leaning users. This is in line with observations from <a href=https://en.wikipedia.org/wiki/Moral_psychology>moral psychology</a> that left-leaning and right-leaning users assign different importance to maintaining <a href=https://en.wikipedia.org/wiki/Social_stratification>social hierarchies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--608 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.608/>An Empirical Examination of Online Restaurant Reviews</a></strong><br><a href=/people/h/hyun-jung-kang/>Hyun Jung Kang</a>
|
<a href=/people/i/iris-eshkol/>Iris Eshkol-Taravella</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--608><div class="card-body p-3 small">In the wake of (Pang et al., 2002 ; Turney, 2002 ; Liu, 2012) inter alia, <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> have focused on extracting either positive or negative opinions from texts and determining the targets of these <a href=https://en.wikipedia.org/wiki/Opinion>opinions</a>. In this study, we go beyond the coarse-grained positive vs. negative opposition and propose a corpus-based scheme that detects evaluative language at a finer-grained level. We classify each sentence into one of four evaluation types based on the proposed scheme : (1) the reviewer&#8217;s opinion on the restaurant (positive, negative, or mixed) ; (2) the reviewer&#8217;s input / feedback to potential customers and restaurant owners (suggestion, advice, or warning) (3) whether the reviewer wants to return to the restaurant (intention) ; (4) the factual statement about the experience (description). We apply classical machine learning and deep learning methods to show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. We also interpret the performances that we obtained for each category by taking into account the specificities of the corpus treated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.611/>Annotating Perspectives on Vaccination</a></strong><br><a href=/people/r/roser-morante/>Roser Morante</a>
|
<a href=/people/c/chantal-van-son/>Chantal van Son</a>
|
<a href=/people/i/isa-maks/>Isa Maks</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--611><div class="card-body p-3 small">In this paper we present the Vaccination Corpus, a corpus of texts related to the online vaccination debate that has been annotated with three layers of information about perspectives : attribution, claims and opinions. Additionally, events related to the vaccination debate are also annotated. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 294 documents from the Internet which reflect different views on vaccinations. It has been compiled to study the language of online debates, with the final goal of experimenting with methodologies to extract and contrast perspectives in the framework of the vaccination debate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--617 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.617/>Dataset Creation and Evaluation of Aspect Based Sentiment Analysis in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, a Low Resource Language<span class=acl-fixed-case>T</span>elugu, a Low Resource Language</a></strong><br><a href=/people/y/yashwanth-reddy-regatte/>Yashwanth Reddy Regatte</a>
|
<a href=/people/r/rama-rohit-reddy-gangula/>Rama Rohit Reddy Gangula</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--617><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> has gained popularity as it is essential to moderate and analyse the information across the internet. It has various applications like <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>, <a href=https://en.wikipedia.org/wiki/Social_media_monitoring>social media monitoring</a>, and <a href=https://en.wikipedia.org/wiki/Market_research>market research</a>. Aspect Based Sentiment Analysis (ABSA) is an area of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> which deals with sentiment at a finer level. ABSA classifies sentiment with respect to each aspect to gain greater insights into the sentiment expressed. Significant contributions have been made in ABSA, but this progress is limited only to a few languages with adequate resources. Telugu lags behind in this area of research despite being one of the most spoken languages in India and an enormous amount of data being created each day. In this paper, we create a reliable <a href=https://en.wikipedia.org/wiki/Resource>resource</a> for aspect based sentiment analysis in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>. The data is annotated for three tasks namely Aspect Term Extraction, Aspect Polarity Classification and Aspect Categorisation. Further, we develop <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> demonstrating the reliability and usefulness of the <a href=https://en.wikipedia.org/wiki/Resource>resource</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--618 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.618" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.618/>A Fine-grained Sentiment Dataset for Norwegian<span class=acl-fixed-case>N</span>orwegian</a></strong><br><a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/p/petter-maehlum/>Petter Mæhlum</a>
|
<a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--618><div class="card-body p-3 small">We here introduce NoReC_fine, a dataset for fine-grained sentiment analysis in <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a>, annotated with respect to polar expressions, targets and holders of opinion. The underlying texts are taken from a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including <a href=https://en.wikipedia.org/wiki/Literature>literature</a>, <a href=https://en.wikipedia.org/wiki/Video_game>games</a>, <a href=https://en.wikipedia.org/wiki/Music>music</a>, <a href=https://en.wikipedia.org/wiki/Product_(business)>products</a>, <a href=https://en.wikipedia.org/wiki/Film>movies</a> and more. We here present a detailed description of this <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> effort. We provide an overview of the developed annotation guidelines, illustrated with examples and present an analysis of <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>. We also report the first experimental results on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, intended as a preliminary benchmark for further experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--619 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.619/>The Design and Construction of a Chinese Sarcasm Dataset<span class=acl-fixed-case>C</span>hinese Sarcasm Dataset</a></strong><br><a href=/people/x/xiaochang-gong/>Xiaochang Gong</a>
|
<a href=/people/q/qin-zhao/>Qin Zhao</a>
|
<a href=/people/j/jun-zhang/>Jun Zhang</a>
|
<a href=/people/r/ruibin-mao/>Ruibin Mao</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--619><div class="card-body p-3 small">As a typical multi-layered semi-conscious language phenomenon, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> is widely existed in <a href=https://en.wikipedia.org/wiki/Social_media>social media text</a> for enhancing the <a href=https://en.wikipedia.org/wiki/Emotional_expression>emotion expression</a>. Thus, the detection and processing of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> is important to social media analysis. However, most existing <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm dataset</a> are in English and there is still a lack of authoritative Chinese sarcasm dataset. In this paper, we presents the design and construction of a largest high-quality Chinese sarcasm dataset, which contains 2,486 manual annotated sarcastic texts and 89,296 non-sarcastic texts. Furthermore, a balanced dataset through elaborately sampling the same amount non-sarcastic texts for training sarcasm classifier. Using the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as the benchmark, some sarcasm classification methods are evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--620 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.620/>Target-based Sentiment Annotation in Chinese Financial News<span class=acl-fixed-case>C</span>hinese Financial News</a></strong><br><a href=/people/c/chaofa-yuan/>Chaofa Yuan</a>
|
<a href=/people/y/yuhan-liu/>Yuhan Liu</a>
|
<a href=/people/r/rongdi-yin/>Rongdi Yin</a>
|
<a href=/people/j/jun-zhang/>Jun Zhang</a>
|
<a href=/people/q/qinling-zhu/>Qinling Zhu</a>
|
<a href=/people/r/ruibin-mao/>Ruibin Mao</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--620><div class="card-body p-3 small">This paper presents the design and construction of a large-scale target-based sentiment annotation corpus on Chinese financial news text. Different from the most existing paragraph / document-based annotation corpus, in this study, target-based fine-grained sentiment annotation is performed. The companies, brands and other financial entities are regarded as the targets. The clause reflecting the <a href=https://en.wikipedia.org/wiki/Profit_(accounting)>profitability</a>, loss or other business status of financial entities is regarded as the sentiment expression for determining the polarity. Based on high quality annotation guideline and effective quality control strategy, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with 8,314 target-level sentiment annotation is constructed on 6,336 paragraphs from Chinese financial news text. Based on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, several state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis models</a> are evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.622/>Reproduction and Revival of the Argument Reasoning Comprehension Task</a></strong><br><a href=/people/j/joao-rodrigues/>João António Rodrigues</a>
|
<a href=/people/r/ruben-branco/>Ruben Branco</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/a/antonio-branco/>António Branco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--622><div class="card-body p-3 small">Reproduction of scientific findings is essential for scientific development across all scientific disciplines and reproducing results of previous works is a basic requirement for validating the hypothesis and conclusions put forward by them. This paper reports on the scientific reproduction of several <a href=https://en.wikipedia.org/wiki/System>systems</a> addressing the Argument Reasoning Comprehension Task of SemEval2018. Given a recent publication that pointed out spurious statistical cues in the data set used in the shared task, and that produced a revised version of it, we also evaluated the reproduced systems with this new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. The exercise reported here shows that, in general, the reproduction of these <a href=https://en.wikipedia.org/wiki/System>systems</a> is successful with scores in line with those reported in SemEval2018. However, the performance scores are worst than those, and even below the random baseline, when the reproduced systems are run over the revised data set expunged from data artifacts. This demonstrates that this task is actually a much harder challenge than what could have been perceived from the inflated, close to human-level performance scores obtained with the data set used in SemEval2018. This calls for a revival of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as there is much room for improvement until systems may come close to the upper bound provided by <a href=https://en.wikipedia.org/wiki/Human_performance>human performance</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--624 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.624/>ParlVote : A Corpus for Sentiment Analysis of Political Debates<span class=acl-fixed-case>P</span>arl<span class=acl-fixed-case>V</span>ote: A Corpus for Sentiment Analysis of Political Debates</a></strong><br><a href=/people/g/gavin-abercrombie/>Gavin Abercrombie</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--624><div class="card-body p-3 small">Debate transcripts from the UK Parliament contain information about the positions taken by politicians towards important topics, but are difficult for people to process manually. While sentiment analysis of debate speeches could facilitate understanding of the speakers&#8217; stated opinions, datasets currently available for this task are small when compared to the benchmark corpora in other domains. We present ParlVote, a new, larger corpus of parliamentary debate speeches for use in the evaluation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis systems</a> for the political domain. We also perform a number of initial experiments on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, testing a variety of approaches to the classification of sentiment polarity in <a href=https://en.wikipedia.org/wiki/Public_speaking>debate speeches</a>. These include a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> as well as a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> trained using a transformer word embedding model (BERT), and fine-tuned on the parliamentary speeches. We find that in many scenarios, a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> trained on a bag-of-words text representation achieves the best results. However, with the largest <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, the transformer-based model combined with a neural classifier provides the best performance. We suggest that further experimentation with classification models and observations of the debate content and structure are required, and that there remains much room for improvement in parliamentary sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--629 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.629" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.629/>Offensive Language Identification in Greek<span class=acl-fixed-case>G</span>reek</a></strong><br><a href=/people/z/zesis-pitenis/>Zesis Pitenis</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--629><div class="card-body p-3 small">As offensive language has become a rising issue for online communities and social media platforms, researchers have been investigating ways of coping with abusive content and developing systems to detect its different types : <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a>, etc. With a few notable exceptions, most research on this topic so far has dealt with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This is mostly due to the availability of language resources for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. To address this shortcoming, this paper presents the first Greek annotated dataset for offensive language identification : the Offensive Greek Tweet Dataset (OGTD). OGTD is a manually annotated dataset containing 4,779 posts from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> annotated as offensive and not offensive. Along with a detailed description of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we evaluate several <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> trained and tested on this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--636 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.636/>GRAIN-S : Manually Annotated Syntax for German Interviews<span class=acl-fixed-case>GRAIN</span>-<span class=acl-fixed-case>S</span>: Manually Annotated Syntax for <span class=acl-fixed-case>G</span>erman Interviews</a></strong><br><a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a>
|
<a href=/people/z/zoltan-czesznak/>Zoltán Czesznak</a>
|
<a href=/people/k/kerstin-jung/>Kerstin Jung</a>
|
<a href=/people/m/moritz-volkel/>Moritz Völkel</a>
|
<a href=/people/w/wolfgang-seeker/>Wolfgang Seeker</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--636><div class="card-body p-3 small">We present GRAIN-S, a set of manually created syntactic annotations for radio interviews in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> extends an existing corpus GRAIN and comes with constituency and dependency trees for six interviews. The rare combination of gold- and silver-standard annotation layers coming from <a href=https://en.wikipedia.org/wiki/GRAIN>GRAIN</a> with high-quality syntax trees can serve as a useful resource for speech- and text-based research. Moreover, since interviews can be put between carefully prepared speech and spontaneous conversational speech, they cover phenomena not seen in traditional newspaper-based treebanks. Therefore, GRAIN-S can contribute to research into techniques for model adaptation and for building more corpus-independent tools. GRAIN-S follows TIGER, one of the established syntactic treebanks of <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We describe the annotation process and discuss decisions necessary to adapt the original TIGER guidelines to the interviews domain. Next, we give details on the conversion from TIGER-style trees to dependency trees. We provide data statistics and demonstrate differences between the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and existing out-of-domain test sets annotated with TIGER syntactic structures. Finally, we provide baseline parsing results for further comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--644 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.644/>The EDGeS Diachronic Bible Corpus<span class=acl-fixed-case>EDG</span>e<span class=acl-fixed-case>S</span> Diachronic <span class=acl-fixed-case>B</span>ible Corpus</a></strong><br><a href=/people/g/gerlof-bouma/>Gerlof Bouma</a>
|
<a href=/people/e/evie-cousse/>Evie Coussé</a>
|
<a href=/people/t/trude-dijkstra/>Trude Dijkstra</a>
|
<a href=/people/n/nicoline-van-der-sijs/>Nicoline van der Sijs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--644><div class="card-body p-3 small">We present the EDGeS Diachronic Bible Corpus : a diachronically and synchronically parallel corpus of Bible translations in <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, with texts from the 14th century until today. It is compiled in the context of an intended longitudinal and contrastive study of complex verb constructions in <a href=https://en.wikipedia.org/wiki/Germanic_languages>Germanic</a>. The paper discusses the corpus design principles, its selection of 36 Bibles, and the information and metadata encoded for the corpus texts. The EDGeS corpus will be available in two forms : the whole corpus will be accessible for researchers behind a login in the well-known OPUS search infrastructure, and the open subpart of the corpus will be available for download.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--647 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.647/>THEL : Automatically Extracted Typelogical Derivations for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a><span class=acl-fixed-case>ÆTHEL</span>: Automatically Extracted Typelogical Derivations for <span class=acl-fixed-case>D</span>utch</a></strong><br><a href=/people/k/konstantinos-kogkalidis/>Konstantinos Kogkalidis</a>
|
<a href=/people/m/michael-moortgat/>Michael Moortgat</a>
|
<a href=/people/r/richard-moot/>Richard Moot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--647><div class="card-body p-3 small">We present THEL, a semantic compositionality dataset for <a href=https://en.wikipedia.org/wiki/Dutch_language>written Dutch</a>. THEL consists of two parts. First, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, THEL further provides 72 192 validated derivations, presented in four formats : natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. THEL&#8217;s types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how &#8216;virtual elements&#8217; in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with THEL : the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--656 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.656/>AlloVera : A Multilingual Allophone Database<span class=acl-fixed-case>A</span>llo<span class=acl-fixed-case>V</span>era: A Multilingual Allophone Database</a></strong><br><a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/x/xinjian-li/>Xinjian Li</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/a/alexis-michaud/>Alexis Michaud</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--656><div class="card-body p-3 small">We introduce a new resource, AlloVera, which provides mappings from 218 <a href=https://en.wikipedia.org/wiki/Allophone>allophones</a> to <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> for 14 languages. Phonemes are contrastive phonological units, and allophones are their various concrete realizations, which are predictable from phonological context. While <a href=https://en.wikipedia.org/wiki/Phoneme>phonemic representations</a> are language specific, <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic representations</a> (stated in terms of (allo)phones) are much closer to a universal (language-independent) transcription. AlloVera allows the training of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition models</a> that output <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>phonetic transcriptions</a> in the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>International Phonetic Alphabet (IPA)</a>, regardless of the input language. We show that a universal allophone model, <a href=https://en.wikipedia.org/wiki/Allosaurus>Allosaurus</a>, built with AlloVera, outperforms universal phonemic models and language-specific models on a speech-transcription task. We explore the implications of this <a href=https://en.wikipedia.org/wiki/Technology>technology</a> (and related technologies) for the documentation of endangered and minority languages. We further explore other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> for which AlloVera will be suitable as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> grows, including phonological typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.659.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--659 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.659 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.659/>AccentDB : A Database of Non-Native English Accents to Assist Neural Speech Recognition<span class=acl-fixed-case>A</span>ccent<span class=acl-fixed-case>DB</span>: A Database of Non-Native <span class=acl-fixed-case>E</span>nglish Accents to Assist Neural Speech Recognition</a></strong><br><a href=/people/a/afroz-ahamad/>Afroz Ahamad</a>
|
<a href=/people/a/ankit-anand/>Ankit Anand</a>
|
<a href=/people/p/pranesh-bhargava/>Pranesh Bhargava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--659><div class="card-body p-3 small">Modern Automatic Speech Recognition (ASR) technology has evolved to identify the speech spoken by native speakers of a language very well. However, identification of the speech spoken by non-native speakers continues to be a major challenge for <a href=https://en.wikipedia.org/wiki/It_(novel)>it</a>. In this work, we first spell out the key requirements for creating a well-curated database of speech samples in <a href=https://en.wikipedia.org/wiki/Accent_(sociolinguistics)>non-native accents</a> for training and testing robust ASR systems. We then introduce AccentDB, one such database that contains samples of 4 Indian-English accents collected by us, and a compilation of samples from 4 native-English, and a metropolitan Indian-English accent. We also present an analysis on separability of the collected accent data. Further, we present several accent classification models and evaluate them thoroughly against human-labelled accent classes. We test the generalization of our classifier models in a variety of setups of seen and unseen data. Finally, we introduce accent neutralization of non-native accents to native accents using autoencoder models with task-specific architectures. Thus, our work aims to aid ASR systems at every stage of development with a <a href=https://en.wikipedia.org/wiki/Database>database</a> for training, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a> for feature augmentation, and neutralization systems for acoustic transformations of non-native accents of English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.664.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--664 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.664 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.664" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.664/>Propagate-Selector : Detecting Supporting Sentences for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> via Graph Neural Networks</a></strong><br><a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--664><div class="card-body p-3 small">In this study, we propose a novel graph neural network called propagate-selector (PS), which propagates information over sentences to understand information that can not be inferred when considering sentences in isolation. First, we design a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> in which each node represents an individual sentence, and some pairs of <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> are selectively connected based on the text structure. Then, we develop an iterative attentive aggregation and a skip-combine method in which a <a href=https://en.wikipedia.org/wiki/Node_(computer_science)>node</a> interacts with its neighborhood nodes to accumulate the necessary information. To evaluate the performance of the proposed approaches, we conduct experiments with the standard HotpotQA dataset. The empirical results demonstrate the superiority of our proposed approach, which obtains the best performances, compared to the widely used answer-selection models that do not consider the intersentential relationship.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.666.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--666 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.666 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.666/>Cross-sentence Pre-trained Model for Interactive QA matching<span class=acl-fixed-case>QA</span> matching</a></strong><br><a href=/people/j/jinmeng-wu/>Jinmeng Wu</a>
|
<a href=/people/y/yanbin-hao/>Yanbin Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--666><div class="card-body p-3 small">Semantic matching measures the dependencies between query and answer representations, it is an important criterion for evaluating whether the matching is successful. In fact, such matching does not examine each sentence individually, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> outside a sentence should be considered equally important to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>syntactic context</a> inside a sentence. We proposed a new QA matching model, built upon a cross-sentence context-aware architecture. An interactive attention mechanism with a pre-trained language model is proposed to automatically select salient positional answer representations that contribute more significantly to the answer relevance of a given question. In addition to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> captured at each word position, we incorporate a new quantity of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information jump</a> to facilitate the attention weight formulation. This reflects the amount of new information brought by the next word and is computed by modeling the <a href=https://en.wikipedia.org/wiki/Joint_probability>joint probability</a> between two adjacent word states. The proposed method is compared to multiple state-of-the-art ones evaluated using the TREC library, WikiQA, and the Yahoo ! community question datasets. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms satisfactorily the competing ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--667 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.667/>SQuAD2-CR : Semi-supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0<span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span>2-<span class=acl-fixed-case>CR</span>: Semi-supervised Annotation for Cause and Rationales for Unanswerability in <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span> 2.0</a></strong><br><a href=/people/g/gyeongbok-lee/>Gyeongbok Lee</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/h/hyunsouk-cho/>Hyunsouk Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--667><div class="card-body p-3 small">Existing machine reading comprehension models are reported to be brittle for adversarially perturbed questions when optimizing only for accuracy, which led to the creation of new reading comprehension benchmarks, such as SQuAD 2.0 which contains such type of questions. However, despite the super-human accuracy of existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, it is still unclear how the model predicts the answerability of the question, potentially due to the absence of a shared annotation for the explanation. To address such absence, we release SQuAD2-CR dataset, which contains annotations on unanswerable questions from the SQuAD 2.0 dataset, to enable an explanatory analysis of the model prediction. Specifically, we annotate (1) explanation on why the most plausible answer span can not be the answer and (2) which part of the question causes unanswerability. We share intuitions and experimental results that how this dataset can be used to analyze and improve the interpretability of existing reading comprehension model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.668.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--668 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.668 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.668/>Generating Responses that Reflect Meta Information in User-Generated Question Answer Pairs</a></strong><br><a href=/people/t/takashi-kodama/>Takashi Kodama</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/k/koh-mitsuda/>Koh Mitsuda</a>
|
<a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/y/yushi-aono/>Yushi Aono</a>
|
<a href=/people/r/ryuta-nakamura/>Ryuta Nakamura</a>
|
<a href=/people/n/noritake-adachi/>Noritake Adachi</a>
|
<a href=/people/h/hidetoshi-kawabata/>Hidetoshi Kawabata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--668><div class="card-body p-3 small">This paper concerns the problem of realizing consistent personalities in neural conversational modeling by using user generated question-answer pairs as training data. Using the framework of role play-based question answering, we collected single-turn question-answer pairs for particular characters from online users. Meta information was also collected such as <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> and <a href=https://en.wikipedia.org/wiki/Intimate_relationship>intimacy</a> related to question-answer pairs. We verified the quality of the collected data and, by subjective evaluation, we also verified their usefulness in training neural conversational models for generating utterances reflecting the meta information, especially <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.692.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--692 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.692 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.692/>KGvec2go Knowledge Graph Embeddings as a Service<span class=acl-fixed-case>KG</span>vec2go – Knowledge Graph Embeddings as a Service</a></strong><br><a href=/people/j/jan-portisch/>Jan Portisch</a>
|
<a href=/people/m/michael-hladik/>Michael Hladik</a>
|
<a href=/people/h/heiko-paulheim/>Heiko Paulheim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--692><div class="card-body p-3 small">In this paper, we present KGvec2go, a <a href=https://en.wikipedia.org/wiki/Application_programming_interface>Web API</a> for accessing and consuming <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> in a light-weight fashion in downstream applications. Currently, we serve pre-trained <a href=https://en.wikipedia.org/wiki/Graph_embedding>embeddings</a> for four <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. We introduce the service and its usage, and we show further that the trained models have semantic value by evaluating them on multiple semantic benchmarks. The evaluation also reveals that the combination of multiple models can lead to a better outcome than the best individual model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.694.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--694 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.694 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.694" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.694/>Defying <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> : Validation of Terminological Relations in the Web of Data<span class=acl-fixed-case>W</span>ikidata: Validation of Terminological Relations in the Web of Data</a></strong><br><a href=/people/p/patricia-martin-chozas/>Patricia Martín-Chozas</a>
|
<a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/e/elena-montiel-ponsoda/>Elena Montiel-Ponsoda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--694><div class="card-body p-3 small">In this paper we present an approach to validate terminological data retrieved from open encyclopaedic knowledge bases. This need arises from the enrichment of automatically extracted terms with information from existing resources in theLinguistic Linked Open Data cloud. Specifically, the resource employed for this enrichment is <a href=https://en.wikipedia.org/wiki/Wikidata>WIKIDATA</a>, since it is one of the biggest <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> freely available within the <a href=https://en.wikipedia.org/wiki/Semantic_Web>Semantic Web</a>. During the experiment, we noticed that certain <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF properties</a> in the Knowledge Base did not contain the data they are intended to represent, but a different type of information. In this paper we propose an approach to validate the retrieved data based on four axioms that rely on two linguistic theories : the <a href=https://en.wikipedia.org/wiki/X-bar_theory>x-bar theory</a> and the multidimensional theory of terminology. The validation process is supported by a second <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> specialised in linguistic data ; in this case, CONCEPTNET. In our experiment, we validate terms from the legal domain in four languages : <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The final aim is to generate a set of sound and reliable terminological resources in <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a> to contribute to the population of the Linguistic Linked Open Data cloud.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.699.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--699 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.699 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.699/>The Universal Decompositional Semantics Dataset and Decomp Toolkit</a></strong><br><a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/e/elias-stengel-eskin/>Elias Stengel-Eskin</a>
|
<a href=/people/s/siddharth-vashishtha/>Siddharth Vashishtha</a>
|
<a href=/people/v/venkata-subrahmanyan-govindarajan/>Venkata Subrahmanyan Govindarajan</a>
|
<a href=/people/d/dee-ann-reisinger/>Dee Ann Reisinger</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/k/kyle-rawlins/>Kyle Rawlins</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--699><div class="card-body p-3 small">We present the Universal Decompositional Semantics (UDS) dataset (v1.0), which is bundled with the Decomp toolkit (v0.1). UDS1.0 unifies five high-quality, decompositional semantics-aligned annotation sets within a single semantic graph specificationwith graph structures defined by the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using <a href=https://en.wikipedia.org/wiki/SPARQL>SPARQL</a>. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--700 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.700 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.700/>Are Word Embeddings Really a Bad Fit for the Estimation of Thematic Fit?</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/l/ludovica-pannitto/>Ludovica Pannitto</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--700><div class="card-body p-3 small">While neural embeddings represent a popular choice for word representation in a wide variety of NLP tasks, their usage for thematic fit modeling has been limited, as they have been reported to lag behind syntax-based count models. In this paper, we propose a complete evaluation of count models and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on thematic fit estimation, by taking into account a larger number of parameters and verb roles and introducing also dependency-based embeddings in the comparison. Our results show a complex scenario, where a determinant factor for the performance seems to be the availability to the model of reliable syntactic information for building the distributional representations of the roles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.701/>Ciron : a New Benchmark Dataset for Chinese Irony Detection<span class=acl-fixed-case>C</span>iron: a New Benchmark Dataset for <span class=acl-fixed-case>C</span>hinese Irony Detection</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/x/xuefeng-gao/>Xuefeng Gao</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/a/anran-li/>Anran Li</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--701><div class="card-body p-3 small">Automatic Chinese irony detection is a challenging task, and it has a strong impact on <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic research</a>. However, Chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Iron>Ciron</a>, the first Chinese benchmark dataset available for irony detection for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. Ciron includes more than 8.7 K posts, collected from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Weibo</a>, a <a href=https://en.wikipedia.org/wiki/Microblogging_in_China>micro blogging platform</a>. Most importantly, <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> proves the usefulness of <a href=https://en.wikipedia.org/wiki/Ciron>Ciron</a> as an important resource for Chinese irony detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--704 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.704" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.704/>NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution<span class=acl-fixed-case>N</span>eg<span class=acl-fixed-case>BERT</span>: A Transfer Learning Approach for Negation Detection and Scope Resolution</a></strong><br><a href=/people/a/aditya-khandelwal/>Aditya Khandelwal</a>
|
<a href=/people/s/suraj-sawant/>Suraj Sawant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--704><div class="card-body p-3 small">Negation is an important characteristic of <a href=https://en.wikipedia.org/wiki/Language>language</a>, and a major component of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> from text. This subtask is of considerable importance to the biomedical domain. Over the years, multiple approaches have been explored to address this problem : <a href=https://en.wikipedia.org/wiki/Rule-based_system>Rule-based systems</a>, Machine Learning classifiers, Conditional Random Field models, CNNs and more recently BiLSTMs. In this paper, we look at applying <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. First, we extensively review previous literature addressing Negation Detection and Scope Resolution across the 3 datasets that have gained popularity over the years : the BioScope Corpus, the Sherlock dataset, and the SFU Review Corpus. We then explore the decision choices involved with using BERT, a popular transfer learning model, for this task, and report state-of-the-art results for scope resolution across all 3 datasets. Our model, referred to as NegBERT, achieves a token level F1 score on <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>scope resolution</a> of 92.36 on the Sherlock dataset, 95.68 on the BioScope Abstracts subcorpus, 91.24 on the BioScope Full Papers subcorpus, 90.95 on the SFU Review Corpus, outperforming the previous state-of-the-art systems by a significant margin. We also analyze the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s generalizability to datasets on which <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not trained.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--705 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.705/>Spatial Multi-Arrangement for <a href=https://en.wikipedia.org/wiki/Cluster_analysis>Clustering</a> and Multi-way Similarity Dataset Construction</a></strong><br><a href=/people/o/olga-majewska/>Olga Majewska</a>
|
<a href=/people/d/diana-mccarthy/>Diana McCarthy</a>
|
<a href=/people/j/jasper-van-den-bosch/>Jasper van den Bosch</a>
|
<a href=/people/n/nikolaus-kriegeskorte/>Nikolaus Kriegeskorte</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--705><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for fast bottom-up creation of large-scale semantic similarity resources to support development and evaluation of NLP systems. Our work targets verb similarity, but the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is equally applicable to other parts of speech. Our approach circumvents the bottleneck of slow and expensive manual development of lexical resources by leveraging semantic intuitions of native speakers and adapting a spatial multi-arrangement approach from <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a>, used before only with visual stimuli, to lexical stimuli. Our approach critically obtains judgments of word similarity in the context of a set of related words, rather than of word pairs in isolation. We also handle lexical ambiguity as a natural consequence of a two-phase process where verbs are placed in broad semantic classes prior to the fine-grained spatial similarity judgments. Our proposed design produces a large-scale verb resource comprising 17 relatedness-based classes and a verb similarity dataset containing similarity scores for 29,721 unique verb pairs and 825 target verbs, which we release with this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.706/>A Short Survey on Sense-Annotated Corpora</a></strong><br><a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--706><div class="card-body p-3 small">Large sense-annotated datasets are increasingly necessary for training deep supervised systems in <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. However, gathering high-quality sense-annotated data for as many instances as possible is a laborious and expensive task. This has led to the proliferation of automatic and semi-automatic methods for overcoming the so-called knowledge-acquisition bottleneck. In this short survey we present an overview of sense-annotated corpora, annotated either manually- or (semi)automatically, that are currently available for different languages and featuring distinct <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a> as inventory of senses, i.e. WordNet, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>. Furthermore, we provide the reader with general statistics of each dataset and an analysis of their specific features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--708 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.708/>NUBes : A Corpus of Negation and Uncertainty in Spanish Clinical Texts<span class=acl-fixed-case>NUB</span>es: A Corpus of Negation and Uncertainty in <span class=acl-fixed-case>S</span>panish Clinical Texts</a></strong><br><a href=/people/s/salvador-lima-lopez/>Salvador Lima Lopez</a>
|
<a href=/people/n/naiara-perez/>Naiara Perez</a>
|
<a href=/people/m/montse-cuadros/>Montse Cuadros</a>
|
<a href=/people/g/german-rigau/>German Rigau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--708><div class="card-body p-3 small">This paper introduces the first version of the NUBes corpus (Negation and Uncertainty annotations in Biomedical texts in Spanish). The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is part of an on-going research and currently consists of 29,682 sentences obtained from anonymised health records annotated with negation and uncertainty. The article includes an exhaustive comparison with similar <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> in Spanish, and presents the main annotation and design decisions. Additionally, we perform preliminary experiments using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithms</a> to validate the annotated dataset. As far as we know, NUBes is the largest available corpora for negation in Spanish and the first that also incorporates the annotation of speculation cues, scopes, and <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--709 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.709/>Decomposing and Comparing Meaning Relations : <a href=https://en.wikipedia.org/wiki/Paraphrase>Paraphrasing</a>, Textual Entailment, <a href=https://en.wikipedia.org/wiki/Contradiction>Contradiction</a>, and Specificity</a></strong><br><a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/d/darina-gold/>Darina Gold</a>
|
<a href=/people/m/m-antonia-marti/>M. Antonia Marti</a>
|
<a href=/people/m/maria-salamo/>Maria Salamo</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--709><div class="card-body p-3 small">In this paper, we present a methodology for decomposing and comparing multiple meaning relations (paraphrasing, textual entailment, <a href=https://en.wikipedia.org/wiki/Contradiction>contradiction</a>, and specificity). The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> includes SHARel-a new <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typology</a> that consists of 26 linguistic and 8 reason-based categories. We use the <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> to annotate a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 520 sentence pairs in English and we demonstrate that unlike previous typologies, SHARel can be applied to all relations of interest with a high inter-annotator agreement. We analyze and compare the frequency and distribution of the linguistic and reason-based phenomena involved in <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>, <a href=https://en.wikipedia.org/wiki/Logical_consequence>textual entailment</a>, <a href=https://en.wikipedia.org/wiki/Contradiction>contradiction</a>, and <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>. This comparison allows for a much more in-depth analysis of the workings of the individual relations and the way they interact and compare with each other. We release all resources (typology, annotation guidelines, and annotated corpus) to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.712/>Figure Me Out : A Gold Standard Dataset for Metaphor Interpretation</a></strong><br><a href=/people/o/omnia-zayed/>Omnia Zayed</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--712><div class="card-body p-3 small">Metaphor comprehension and understanding is a complex cognitive task that requires interpreting metaphors by grasping the interaction between the meaning of their target and source concepts. This is very challenging for humans, let alone computers. Thus, automatic metaphor interpretation is understudied in part due to the lack of publicly available datasets. The creation and manual annotation of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> is a demanding task which requires huge cognitive effort and time. Moreover, there will always be a question of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and consistency of the annotated data due to the subjective nature of the problem. This work addresses these issues by presenting an annotation scheme to interpret verb-noun metaphoric expressions in text. The proposed <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is designed with the goal of reducing the workload on annotators and maintain <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a>. Our methodology employs an automatic retrieval approach which utilises external lexical resources, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> to generate possible interpretations of identified metaphors in order to enable quick and accurate <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. We validate our proposed approach by annotating around 1,500 metaphors in tweets which were annotated by six native English speakers. As a result of this work, we publish as <a href=https://en.wikipedia.org/wiki/Linked_data>linked data</a> the first gold standard dataset for metaphor interpretation which will facilitate research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--714 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.714/>Dataset and Enhanced Model for Eligibility Criteria-to-SQL Semantic Parsing<span class=acl-fixed-case>SQL</span> Semantic Parsing</a></strong><br><a href=/people/x/xiaojing-yu/>Xiaojing Yu</a>
|
<a href=/people/t/tianlong-chen/>Tianlong Chen</a>
|
<a href=/people/z/zhengjie-yu/>Zhengjie Yu</a>
|
<a href=/people/h/huiyu-li/>Huiyu Li</a>
|
<a href=/people/y/yang-yang/>Yang Yang</a>
|
<a href=/people/x/xiaoqian-jiang/>Xiaoqian Jiang</a>
|
<a href=/people/a/anxiao-jiang/>Anxiao Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--714><div class="card-body p-3 small">Clinical trials often require that patients meet eligibility criteria (e.g., have specific conditions) to ensure the safety and the effectiveness of studies. However, retrieving eligible patients for a trial from the electronic health record (EHR) database remains a challenging task for clinicians since it requires not only medical knowledge about eligibility criteria, but also an adequate understanding of structured query language (SQL). In this paper, we introduce a new dataset that includes the first-of-its-kind eligibility-criteria corpus and the corresponding <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a> for criteria-to-sql (Criteria2SQL), a task translating the eligibility criteria to executable SQL queries. Compared to existing datasets, the queries in the dataset here are derived from the eligibility criteria of clinical trials and include Order-sensitive, Counting-based, and Boolean-type cases which are not seen before. In addition to the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we propose a novel neural semantic parser as a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Extensive experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> outperforms existing state-of-the-art general-purpose text-to-sql models while highlighting the challenges presented by the new dataset. The uniqueness and the diversity of the dataset leave a lot of research opportunities for future improvement.<i>Order-sensitive, Counting-based, and Boolean-type</i> cases which are not seen before. In addition to the dataset, we propose a novel neural semantic parser as a strong baseline model. Extensive experiments show that the proposed parser outperforms existing state-of-the-art general-purpose text-to-sql models while highlighting the challenges presented by the new dataset. The uniqueness and the diversity of the dataset leave a lot of research opportunities for future improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--716 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.716/>Word Attribute Prediction Enhanced by Lexical Entailment Tasks</a></strong><br><a href=/people/m/mika-hasegawa/>Mika Hasegawa</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--716><div class="card-body p-3 small">Human semantic knowledge about concepts acquired through perceptual inputs and daily experiences can be expressed as a bundle of attributes. Unlike the conventional distributed word representations that are purely induced from a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>, a semantic attribute is associated with a designated dimension in attribute-based vector representations. Thus, semantic attribute vectors can effectively capture the commonalities and differences among concepts. However, as semantic attributes have been generally created by psychological experimental settings involving human annotators, an automatic method to create or extend such resources is highly demanded in terms of language resource development and maintenance. This study proposes a two-stage neural network architecture, Word2Attr, in which initially acquired attribute representations are then fine-tuned by employing supervised lexical entailment tasks. The quantitative empirical results demonstrated that the <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> was indeed effective in improving the performances of semantic / visual similarity / relatedness evaluation tasks. Although the qualitative analysis confirmed that the proposed method could often discover valid but not-yet human-annotated attributes, they also exposed future issues to be worked : we should refine the inventory of semantic attributes that currently relies on an existing dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--718 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.718/>Representing Verbs with Visual Argument Vectors</a></strong><br><a href=/people/i/irene-sucameli/>Irene Sucameli</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--718><div class="card-body p-3 small">Is it possible to use <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> to model verb semantic similarities? Starting from this core question, we developed two textual distributional semantic models and a visual one. We found particularly interesting and challenging to investigate this Part of Speech since verbs are not often analysed in researches focused on multimodal distributional semantics. After the creation of the visual and textual distributional space, the three models were evaluated in relation to SimLex-999, a gold standard resource. Through this evaluation, we demonstrate that, using visual distributional models, it is possible to extract meaningful information and to effectively capture the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between verbs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--721 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.721/>A French Version of the FraCaS Test Suite<span class=acl-fixed-case>F</span>rench Version of the <span class=acl-fixed-case>F</span>ra<span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>S</span> Test Suite</a></strong><br><a href=/people/m/maxime-amblard/>Maxime Amblard</a>
|
<a href=/people/c/clement-beysson/>Clément Beysson</a>
|
<a href=/people/p/philippe-de-groote/>Philippe de Groote</a>
|
<a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/s/sylvain-pogodalla/>Sylvain Pogodalla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--721><div class="card-body p-3 small">This paper presents a French version of the FraCaS test suite. This <a href=https://en.wikipedia.org/wiki/Test_suite>test suite</a>, originally written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, contains problems illustrating semantic inference in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We describe linguistic choices we had to make when translating the FraCaS test suite in <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and discuss some of the issues that were raised by the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We also report an experiment we ran in order to test both the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and the <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>logical semantics</a> underlying the problems of the <a href=https://en.wikipedia.org/wiki/Test_suite>test suite</a>. This provides a way of checking formal semanticists&#8217; hypotheses against actual semantic capacity of speakers (in the present case, French speakers), and allow us to compare the results we obtained with the ones of similar experiments that have been conducted for other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.728/>Word Sense Disambiguation for 158 Languages using Word Embeddings Only</a></strong><br><a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/d/denis-teslenko/>Denis Teslenko</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--728><div class="card-body p-3 small">Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, (i) the inherent <a href=https://en.wikipedia.org/wiki/Zipfian_distribution>Zipfian distribution</a> of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. Models and system are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.732.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--732 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.732 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.732/>One Classifier for All Ambiguous Words : Overcoming Data Sparsity by Utilizing Sense Correlations Across Words</a></strong><br><a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--732><div class="card-body p-3 small">Most supervised word sense disambiguation (WSD) systems build word-specific classifiers by leveraging labeled data. However, when using word-specific classifiers, the sparseness of annotations leads to inferior <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>sense disambiguation</a> performance on less frequently seen words. To combat data sparsity, we propose to learn a single model that derives sense representations and meanwhile enforces congruence between a word instance and its right sense by using both sense-annotated data and lexical resources. The model is shared across words that allows utilizing sense correlations across words, and therefore helps to transfer common disambiguation rules from annotation-rich words to annotation-lean words. Empirical evaluation on benchmark datasets shows that the proposed shared model outperforms the equivalent classifier-based models by 1.7 %, 2.5 % and 3.8 % in F1-score when using GloVe, ELMo and BERT word embeddings respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.735.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--735 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.735 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.735/>What Comes First : Combining Motion Capture and Eye Tracking Data to Study the Order of Articulators in Constructed Action in Sign Language Narratives</a></strong><br><a href=/people/t/tommi-jantunen/>Tommi Jantunen</a>
|
<a href=/people/a/anna-puupponen/>Anna Puupponen</a>
|
<a href=/people/b/birgitta-burger/>Birgitta Burger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--735><div class="card-body p-3 small">We use synchronized 120 fps motion capture and 50 fps eye tracking data from two native signers to investigate the temporal order in which the dominant hand, the head, the chest and the eyes start producing overt constructed action from regular narration in seven short Finnish Sign Language stories. From the material, we derive a sample of ten instances of regular narration to overt constructed action transfers in <a href=https://en.wikipedia.org/wiki/ELAN_(programming_language)>ELAN</a> which we then further process and analyze in Matlab. The results indicate that the temporal order of articulators shows both contextual and individual variation but that there are also repeated patterns which are similar across all the analyzed sequences and signers. Most notably, when the discourse strategy changes from regular narration to overt constructed action, the head and the eyes tend to take the leading role, and the chest and the dominant hand tend to start acting last. Consequences of the findings are discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.736.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--736 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.736 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.736/>LSF-ANIMAL : A Motion Capture Corpus in <a href=https://en.wikipedia.org/wiki/French_Sign_Language>French Sign Language</a> Designed for the Animation of Signing Avatars<span class=acl-fixed-case>LSF</span>-<span class=acl-fixed-case>ANIMAL</span>: A Motion Capture Corpus in <span class=acl-fixed-case>F</span>rench <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Designed for the Animation of Signing Avatars</a></strong><br><a href=/people/l/lucie-naert/>Lucie Naert</a>
|
<a href=/people/c/caroline-larboulette/>Caroline Larboulette</a>
|
<a href=/people/s/sylvie-gibet/>Sylvie Gibet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--736><div class="card-body p-3 small">Signing avatars allow deaf people to access information in their preferred language using an interactive visualization of the sign language spatio-temporal content. However, <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatars</a> are often procedurally animated, resulting in robotic and unnatural movements, which are therefore rejected by the community for which they are intended. To overcome this lack of authenticity, solutions in which the <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatar</a> is animated from <a href=https://en.wikipedia.org/wiki/Motion_capture>motion capture data</a> are promising. Yet, the initial data set drastically limits the range of signs that the avatar can produce. Therefore, it can be interesting to enrich the initial <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with new content by editing the captured motions. For this purpose, we collected the LSF-ANIMAL corpus, a French Sign Language (LSF) corpus composed of captured isolated signs and full sentences that can be used both to study LSF features and to generate new signs and utterances. This paper presents the precise definition and content of this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, technical considerations relative to the motion capture process (including the marker set definition), the post-processing steps required to obtain data in a standard motion format and the annotation scheme used to label the data. The quality of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with respect to <a href=https://en.wikipedia.org/wiki/Intelligibility_(communication)>intelligibility</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Realism_(arts)>realism</a> is perceptually evaluated by 41 participants including native LSF signers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.737.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--737 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.737 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.737/>Sign Language Recognition with Transformer Networks</a></strong><br><a href=/people/m/mathieu-de-coster/>Mathieu De Coster</a>
|
<a href=/people/m/mieke-van-herreweghe/>Mieke Van Herreweghe</a>
|
<a href=/people/j/joni-dambre/>Joni Dambre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--737><div class="card-body p-3 small">Sign languages are <a href=https://en.wikipedia.org/wiki/Complex_language>complex languages</a>. Research into them is ongoing, supported by large video corpora of which only small parts are annotated. Sign language recognition can be used to speed up the annotation process of these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, in order to aid research into <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> and sign language recognition. Previous research has approached sign language recognition in various ways, using feature extraction techniques or end-to-end deep learning. In this work, we apply a combination of <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> using OpenPose for human keypoint estimation and end-to-end feature learning with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a>. The proven multi-head attention mechanism used in <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> is applied to recognize isolated signs in the Flemish Sign Language corpus. Our proposed method significantly outperforms the previous state of the art of sign language recognition on the Flemish Sign Language corpus : we obtain an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.7 % on a vocabulary of 100 classes. Our results will be implemented as a <a href=https://en.wikipedia.org/wiki/Suggestion_system>suggestion system</a> for sign language corpus annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.740.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--740 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.740 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.740/>Dicta-Sign-LSF-v2 : Remake of a Continuous French Sign Language Dialogue Corpus and a First Baseline for Automatic Sign Language Processing<span class=acl-fixed-case>D</span>icta-<span class=acl-fixed-case>S</span>ign-<span class=acl-fixed-case>LSF</span>-v2: Remake of a Continuous <span class=acl-fixed-case>F</span>rench <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Dialogue Corpus and a First Baseline for Automatic Sign Language Processing</a></strong><br><a href=/people/v/valentin-belissen/>Valentin Belissen</a>
|
<a href=/people/a/annelies-braffort/>Annelies Braffort</a>
|
<a href=/people/m/michele-gouiffes/>Michèle Gouiffès</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--740><div class="card-body p-3 small">While the research in automatic Sign Language Processing (SLP) is growing, it has been almost exclusively focused on recognizing lexical signs, whether isolated or within continuous SL production. However, Sign Languages include many other gestural units like iconic structures, which need to be recognized in order to go towards a true SL understanding. In this paper, we propose a newer version of the publicly available SL corpus Dicta-Sign, limited to its French Sign Language part. Involving 16 different signers, this dialogue corpus was produced with very few constraints on the style and content. It includes lexical and non-lexical annotations over 11 hours of <a href=https://en.wikipedia.org/wiki/Video>video recording</a>, with 35000 manual units. With the aim of stimulating research in SL understanding, we also provide a baseline for the recognition of lexical signs and non-lexical structures on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. A very compact modeling of a signer is built and a Convolutional-Recurrent Neural Network is trained and tested on Dicta-Sign-LSF-v2, with state-of-the-art results, including the ability to detect <a href=https://en.wikipedia.org/wiki/Iconicity>iconicity</a> in SL production.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.745.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--745 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.745 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.745/>Evaluation of Manual and Non-manual Components for Sign Language Recognition</a></strong><br><a href=/people/m/medet-mukushev/>Medet Mukushev</a>
|
<a href=/people/a/arman-sabyrov/>Arman Sabyrov</a>
|
<a href=/people/a/alfarabi-imashev/>Alfarabi Imashev</a>
|
<a href=/people/k/kenessary-koishybay/>Kenessary Koishybay</a>
|
<a href=/people/v/vadim-kimmelman/>Vadim Kimmelman</a>
|
<a href=/people/a/anara-sandygulova/>Anara Sandygulova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--745><div class="card-body p-3 small">The motivation behind this work lies in the need to differentiate between similar signs that differ in non-manual components present in any sign. To this end, we recorded full sentences signed by five native signers and extracted 5200 isolated sign samples of twenty frequently used signs in Kazakh-Russian Sign Language (K-RSL), which have similar manual components but differ in <a href=https://en.wikipedia.org/wiki/Sign_language>non-manual components</a> (i.e. facial expressions, eyebrow height, <a href=https://en.wikipedia.org/wiki/Human_mouth>mouth</a>, and head orientation). We conducted a series of evaluations in order to investigate whether non-manual components would improve sign&#8217;s recognition accuracy. Among standard machine learning approaches, <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a> produced the best results, 78.2 % of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for dataset with 20 signs and 77.9 % of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 2 classes (statement vs question). Dataset can be downloaded from the following website : https://krslproject.github.io/krsl20/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.747.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--747 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.747 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.747/>A Survey on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> for Fake News Detection</a></strong><br><a href=/people/r/ray-oshikawa/>Ray Oshikawa</a>
|
<a href=/people/j/jing-qian/>Jing Qian</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--747><div class="card-body p-3 small">Fake news detection is a critical yet challenging problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. The rapid rise of <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking platforms</a> has not only yielded a vast increase in <a href=https://en.wikipedia.org/wiki/Accessibility>information accessibility</a> but has also accelerated the spread of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Thus, the effect of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of <a href=https://en.wikipedia.org/wiki/Web_content>Web content</a>, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. In this paper, we describe the challenges involved in <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a> and also describe related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We systematically review and compare the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task formulations</a>, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and NLP solutions that have been developed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a> and other related tasks, and the importance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP solutions</a> for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.748.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--748 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.748 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.748" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.748/>RP-DNN : A Tweet Level Propagation Context Based Deep Neural Networks for Early Rumor Detection in Social Media<span class=acl-fixed-case>RP</span>-<span class=acl-fixed-case>DNN</span>: A Tweet Level Propagation Context Based Deep Neural Networks for Early Rumor Detection in Social Media</a></strong><br><a href=/people/j/jie-gao/>Jie Gao</a>
|
<a href=/people/s/sooji-han/>Sooji Han</a>
|
<a href=/people/x/xingyi-song/>Xingyi Song</a>
|
<a href=/people/f/fabio-ciravegna/>Fabio Ciravegna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--748><div class="card-body p-3 small">Early rumor detection (ERD) on <a href=https://en.wikipedia.org/wiki/Social_media>social media platform</a> is very challenging when limited, incomplete and noisy information is available. Most of the existing methods have largely worked on event-level detection that requires the collection of posts relevant to a specific event and relied only on <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a>. They are not appropriate to detect rumor sources in the very early stages, before an event unfolds and becomes widespread. In this paper, we address the task of <a href=https://en.wikipedia.org/wiki/Enterprise_resource_planning>ERD</a> at the <a href=https://en.wikipedia.org/wiki/Message>message level</a>. We present a novel hybrid neural network architecture, which combines a task-specific character-based bidirectional language model and stacked Long Short-Term Memory (LSTM) networks to represent textual contents and social-temporal contexts of input source tweets, for modelling propagation patterns of rumors in the early stages of their development. We apply multi-layered attention models to jointly learn attentive context embeddings over multiple context inputs. Our experiments employ a stringent leave-one-out cross-validation (LOO-CV) evaluation setup on seven publicly available real-life rumor event data sets. Our models achieve state-of-the-art(SoA) performance for detecting unseen rumors on large augmented data which covers more than 12 events and 2,967 <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a>. An ablation study is conducted to understand the relative contribution of each component of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.755.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--755 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.755 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.755/>Fakeddit : A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection<span class=acl-fixed-case>F</span>akeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection</a></strong><br><a href=/people/k/kai-nakamura/>Kai Nakamura</a>
|
<a href=/people/s/sharon-levy/>Sharon Levy</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--755><div class="card-body p-3 small">Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> research and detection model development. Prior fake news datasets do not provide multimodal text and image data, <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of <a href=https://en.wikipedia.org/wiki/Multimodality>multimodality</a> and fine-grained classification unique to Fakeddit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.760.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--760 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.760 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.760/>I Feel Offended, Do n’t Be Abusive ! Implicit / Explicit Messages in Offensive and Abusive Language<span class=acl-fixed-case>I</span> Feel Offended, Don’t Be Abusive! Implicit/Explicit Messages in Offensive and Abusive Language</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/i/inga-kartoziya/>Inga Kartoziya</a>
|
<a href=/people/m/michael-granitzer/>Michael Granitzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--760><div class="card-body p-3 small">Abusive language detection is an unsolved and challenging problem for the NLP community. Recent literature suggests various approaches to distinguish between different language phenomena (e.g., <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> vs. <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> vs. offensive language) and factors (degree of explicitness and target) that may help to classify different abusive language phenomena. There are data sets that annotate the target of abusive messages (i.e. OLID / OffensEval (Zampieri et al., 2019a)). However, there is a lack of data sets that take into account the degree of explicitness. In this paper, we propose annotation guidelines to distinguish between explicit and implicit abuse in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and apply them to OLID / OffensEval. The outcome is a newly created <a href=https://en.wikipedia.org/wiki/Web_resource>resource</a>, AbuseEval v1.0, which aims to address some of the existing issues in the annotation of offensive and abusive language (e.g., explicitness of the message, presence of a target, need of context, and interaction across different phenomena).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.766.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--766 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.766 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.766/>Detecting Troll Tweets in a Bilingual Corpus</a></strong><br><a href=/people/l/lin-miao/>Lin Miao</a>
|
<a href=/people/m/mark-last/>Mark Last</a>
|
<a href=/people/m/marina-litvak/>Marina Litvak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--766><div class="card-body p-3 small">During the past several years, a large amount of troll accounts has emerged with efforts to manipulate <a href=https://en.wikipedia.org/wiki/Public_opinion>public opinion</a> on <a href=https://en.wikipedia.org/wiki/Social_networking_service>social network sites</a>. They are often involved in <a href=https://en.wikipedia.org/wiki/Misinformation>spreading misinformation</a>, fake news, and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> with the intent of distracting and sowing discord. This paper aims to detect troll tweets in both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> assuming that the tweets are generated by some troll farm. We reduce this task to the authorship verification problem of determining whether a single tweet is authored by a troll farm account or not. We evaluate a supervised classification approach with monolingual, cross-lingual, and bilingual training scenarios, using several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>, including <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The best results are attained by the bilingual learning, showing the area under the ROC curve (AUC) of 0.875 and 0.828, for tweet classification in English and Russian test sets, respectively. It is noteworthy that these results are obtained using only raw text features, which do not require manual feature engineering efforts. In this paper, we introduce a resource of <a href=https://en.wikipedia.org/wiki/English_language>English and Russian troll tweets</a> containing original tweets and translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. It is available for academic purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.769.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--769 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.769 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.769/>Norm It ! Lexical Normalization for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and Its Downstream Effects for Dependency Parsing<span class=acl-fixed-case>I</span>talian and Its Downstream Effects for Dependency Parsing</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/m/michele-cafagna/>Michele Cafagna</a>
|
<a href=/people/l/lorenzo-de-mattei/>Lorenzo De Mattei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--769><div class="card-body p-3 small">Lexical normalization is the task of translating non-standard social media data to a standard form. Previous work has shown that this is beneficial for many downstream tasks in multiple languages. However, for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, there is no benchmark available for lexical normalization, despite the presence of many benchmarks for other tasks involving social media data. In this paper, we discuss the creation of a lexical normalization dataset for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. After two rounds of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, a <a href=https://en.wikipedia.org/wiki/Cohen&#8217;s_kappa>Cohen&#8217;s kappa score</a> of 78.64 is obtained. During this process, we also analyze the inter-annotator agreement for this task, which is only rarely done on datasets for lexical normalization, and when it is reported, the analysis usually remains shallow. Furthermore, we utilize this dataset to train a lexical normalization model and show that it can be used to improve dependency parsing of social media data. All annotated data and the code to reproduce the results are available at : http://bitbucket.org/robvanderg/normit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.781.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--781 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.781 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.781/>Large Corpus of Czech Parliament Plenary Hearings<span class=acl-fixed-case>C</span>zech Parliament Plenary Hearings</a></strong><br><a href=/people/j/jonas-kratochvil/>Jonas Kratochvil</a>
|
<a href=/people/p/peter-polak/>Peter Polak</a>
|
<a href=/people/o/ondrej-bojar/>Ondrej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--781><div class="card-body p-3 small">We present a large corpus of <a href=https://en.wikipedia.org/wiki/Parliament_of_the_Czech_Republic>Czech parliament plenary sessions</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of approximately 1200 hours of <a href=https://en.wikipedia.org/wiki/Speech_corpus>speech data</a> and corresponding <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>text transcriptions</a>. The whole <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has been segmented to short audio segments making <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> suitable for both training and evaluation of automatic speech recognition (ASR) systems. The source language of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, which makes it a valuable resource for future research as only a few public datasets are available in the <a href=https://en.wikipedia.org/wiki/Czech_language>Czech language</a>. We complement the data release with experiments of two baseline ASR systems trained on the presented data : the more traditional approach implemented in the Kaldi ASRtoolkit which combines hidden Markov models and deep neural networks (NN) and a modern ASR architecture implemented in Jaspertoolkit which uses deep NNs in an end-to-end fashion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.785.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--785 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.785 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.785/>Automatic Period Segmentation of Oral French<span class=acl-fixed-case>F</span>rench</a></strong><br><a href=/people/n/natalia-kalashnikova/>Natalia Kalashnikova</a>
|
<a href=/people/l/loic-grobol/>Loïc Grobol</a>
|
<a href=/people/i/iris-eshkol/>Iris Eshkol-Taravella</a>
|
<a href=/people/f/francois-delafontaine/>François Delafontaine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--785><div class="card-body p-3 small">Natural Language Processing in oral speech segmentation is still looking for a minimal unit to analyze. In this work, we present a comparison of two automatic segmentation methods of macro-syntactic periods which allows to take into account syntactic and prosodic components of speech. We compare the performances of an existing tool Analor (Avanzi, Lacheret-Dujour, Victorri, 2008) developed for automatic segmentation of prosodic periods and of CRF models relying on syntactic and / or prosodic features. We find that Analor tends to divide speech into smaller segments and that CRF models detect larger segments rather than macro-syntactic periods. However, in general CRF models perform better results than Analor in terms of <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.792.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--792 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.792 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.792/>DNN-based Speech Synthesis Using Abundant Tags of Spontaneous Speech Corpus<span class=acl-fixed-case>DNN</span>-based Speech Synthesis Using Abundant Tags of Spontaneous Speech Corpus</a></strong><br><a href=/people/y/yuki-yamashita/>Yuki Yamashita</a>
|
<a href=/people/t/tomoki-koriyama/>Tomoki Koriyama</a>
|
<a href=/people/y/yuki-saito/>Yuki Saito</a>
|
<a href=/people/s/shinnosuke-takamichi/>Shinnosuke Takamichi</a>
|
<a href=/people/y/yusuke-ijima/>Yusuke Ijima</a>
|
<a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/h/hiroshi-saruwatari/>Hiroshi Saruwatari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--792><div class="card-body p-3 small">In this paper, we investigate the effectiveness of using rich annotations in deep neural network (DNN)-based statistical speech synthesis. DNN-based frameworks typically use <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> as input features called context instead of directly using text. In such frameworks, we can synthesize not only reading-style speech but also <a href=https://en.wikipedia.org/wiki/Speech>speech</a> with paralinguistic and nonlinguistic features by adding such information to the context. However, it is not clear what kind of information is crucial for reproducing paralinguistic and nonlinguistic features. Therefore, we investigate the effectiveness of rich tags in DNN-based speech synthesis according to the Corpus of Spontaneous Japanese (CSJ), which has a large amount of annotations on paralinguistic features such as <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a>, disfluency, and morphological features. Experimental evaluation results shows that the reproducibility of paralinguistic features of synthetic speech was enhanced by adding such information as context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.795.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--795 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.795 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.795/>Lexical Tone Recognition in Mizo using Acoustic-Prosodic Features</a></strong><br><a href=/people/p/parismita-gogoi/>Parismita Gogoi</a>
|
<a href=/people/a/abhishek-dey/>Abhishek Dey</a>
|
<a href=/people/w/wendy-lalhminghlui/>Wendy Lalhminghlui</a>
|
<a href=/people/p/priyankoo-sarmah/>Priyankoo Sarmah</a>
|
<a href=/people/s/s-r-mahadeva-prasanna/>S R Mahadeva Prasanna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--795><div class="card-body p-3 small">Mizo is an under-studied <a href=https://en.wikipedia.org/wiki/Tibeto-Burman_languages>Tibeto-Burman tonal language</a> of the North-East India. Preliminary research findings have confirmed that four distinct tones of <a href=https://en.wikipedia.org/wiki/Mizo_language>Mizo</a> (High, Low, Rising and Falling) appear in the <a href=https://en.wikipedia.org/wiki/Language>language</a>. In this work, an attempt is made to automatically recognize four <a href=https://en.wikipedia.org/wiki/Tone_(linguistics)>phonological tones</a> in <a href=https://en.wikipedia.org/wiki/Mizo_language>Mizo</a> distinctively using acoustic-prosodic parameters as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>. Six features computed from Fundamental Frequency (F0) contours are considered and two classifier models based on Support Vector Machine (SVM) & Deep Neural Network (DNN) are implemented for automatic tonerecognition task respectively. The Mizo database consists of 31950 iterations of the four Mizo tones, collected from 19 speakers using trisyllabic phrases. A four-way classification of tones is attempted with a balanced (equal number of iterations per tone category) dataset for each tone of <a href=https://en.wikipedia.org/wiki/Mizo_language>Mizo</a>. it is observed that the DNN based classifier shows comparable performance in correctly recognizing four phonological Mizo tones as of the SVM based classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--800 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.800 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.800/>Open-source Multi-speaker Speech Corpora for Building <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, Tamil and Telugu Speech Synthesis Systems<span class=acl-fixed-case>G</span>ujarati, <span class=acl-fixed-case>K</span>annada, <span class=acl-fixed-case>M</span>alayalam, <span class=acl-fixed-case>M</span>arathi, <span class=acl-fixed-case>T</span>amil and <span class=acl-fixed-case>T</span>elugu Speech Synthesis Systems</a></strong><br><a href=/people/f/fei-he/>Fei He</a>
|
<a href=/people/s/shan-hui-cathy-chu/>Shan-Hui Cathy Chu</a>
|
<a href=/people/o/oddur-kjartansson/>Oddur Kjartansson</a>
|
<a href=/people/c/clara-rivera/>Clara Rivera</a>
|
<a href=/people/a/anna-katanova/>Anna Katanova</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a>
|
<a href=/people/i/isin-demirsahin/>Isin Demirsahin</a>
|
<a href=/people/c/cibu-johny/>Cibu Johny</a>
|
<a href=/people/m/martin-jansche/>Martin Jansche</a>
|
<a href=/people/s/supheakmungkol-sarin/>Supheakmungkol Sarin</a>
|
<a href=/people/k/knot-pipatsrisawat/>Knot Pipatsrisawat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--800><div class="card-body p-3 small">We present free high quality multi-speaker speech corpora for <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, which are six of the twenty two official languages of India spoken by 374 million native speakers. The datasets are primarily intended for use in text-to-speech (TTS) applications, such as constructing multilingual voices or being used for speaker or language adaptation. Most of the corpora (apart from <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, which is a female-only database) consist of at least 2,000 recorded lines from female and male native speakers of the language. We present the methodological details behind corpora acquisition, which can be scaled to acquiring data for other languages of interest. We describe the experiments in building a multilingual text-to-speech model that is constructed by combining our corpora. Our results indicate that using these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> results in good quality voices, with Mean Opinion Scores (MOS) 3.6, for all the languages tested. We believe that these resources, released with an open-source license, and the described methodology will help in the progress of speech applications for the languages described and aid corpora development for other, smaller, languages of India and beyond.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--801 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.801/>Crowdsourcing Latin American Spanish for Low-Resource Text-to-Speech<span class=acl-fixed-case>L</span>atin <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>S</span>panish for Low-Resource Text-to-Speech</a></strong><br><a href=/people/a/adriana-guevara-rukoz/>Adriana Guevara-Rukoz</a>
|
<a href=/people/i/isin-demirsahin/>Isin Demirsahin</a>
|
<a href=/people/f/fei-he/>Fei He</a>
|
<a href=/people/s/shan-hui-cathy-chu/>Shan-Hui Cathy Chu</a>
|
<a href=/people/s/supheakmungkol-sarin/>Supheakmungkol Sarin</a>
|
<a href=/people/k/knot-pipatsrisawat/>Knot Pipatsrisawat</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a>
|
<a href=/people/a/alena-butryna/>Alena Butryna</a>
|
<a href=/people/o/oddur-kjartansson/>Oddur Kjartansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--801><div class="card-body p-3 small">In this paper we present a multidialectal corpus approach for building a text-to-speech voice for a new dialect in a language with existing resources, focusing on various South American dialects of Spanish. We first present public speech datasets for Argentinian, Chilean, Colombian, Peruvian, Puerto Rican and Venezuelan Spanish specifically constructed with text-to-speech applications in mind using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a>. We then compare the monodialectal voices built with minimal data to a multidialectal model built by pooling all the resources from all dialects. Our results show that the multidialectal model outperforms the monodialectal baseline models. We also experiment with a zero-resource dialect scenario where we build a multidialectal voice for a dialect while holding out target dialect recordings from the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--806 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.806/>A Large Scale Speech Sentiment Corpus</a></strong><br><a href=/people/e/eric-chen/>Eric Chen</a>
|
<a href=/people/z/zhiyun-lu/>Zhiyun Lu</a>
|
<a href=/people/h/hao-xu/>Hao Xu</a>
|
<a href=/people/l/liangliang-cao/>Liangliang Cao</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/j/james-fan/>James Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--806><div class="card-body p-3 small">We present a multimodal corpus for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> based on the existing Switchboard-1 Telephone Speech Corpus released by the <a href=https://en.wikipedia.org/wiki/Linguistic_Data_Consortium>Linguistic Data Consortium</a>. This <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> extends the Switchboard-1 Telephone Speech Corpus by adding sentiment labels from 3 different human annotators for every transcript segment. Each sentiment label can be one of three options : positive, negative, and neutral. Annotators are recruited using Google Cloud&#8217;s data labeling service and the labeling task was conducted over the internet. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains a total of 49500 labeled speech segments covering 140 hours of audio. To the best of our knowledge, this is the largest multimodal Corpus for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> that includes both speech and text features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--807 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.807/>SibLing Corpus of Russian Dialogue Speech Designed for Research on Speech Entrainment<span class=acl-fixed-case>S</span>ib<span class=acl-fixed-case>L</span>ing Corpus of <span class=acl-fixed-case>R</span>ussian Dialogue Speech Designed for Research on Speech Entrainment</a></strong><br><a href=/people/t/tatiana-kachkovskaia/>Tatiana Kachkovskaia</a>
|
<a href=/people/t/tatiana-chukaeva/>Tatiana Chukaeva</a>
|
<a href=/people/v/vera-evdokimova/>Vera Evdokimova</a>
|
<a href=/people/p/pavel-kholiavin/>Pavel Kholiavin</a>
|
<a href=/people/n/natalia-kriakina/>Natalia Kriakina</a>
|
<a href=/people/d/daniil-kocharov/>Daniil Kocharov</a>
|
<a href=/people/a/anna-mamushina/>Anna Mamushina</a>
|
<a href=/people/a/alla-menshikova/>Alla Menshikova</a>
|
<a href=/people/s/svetlana-zimina/>Svetlana Zimina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--807><div class="card-body p-3 small">The paper presents a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of dialogue speech</a> designed specifically for research in the field of speech entrainment. Given that the degree of accommodation may depend on a number of social factors, the corpus is designed to encompass 5 types of relations between the interlocutors : those between siblings, close friends, strangers of the same gender, strangers of the other gender, strangers of which one has a higher job position and greater age. Another critical decision taken in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is that in all these social settings one speaker is kept the same. This allows us to trace the changes in his / her speech depending on the interlocutor. The basic set of speakers consists of 10 pairs of same-gender siblings (including 4 pairs of identical twins) aged 23-40, and each of them was recorded in the 5 settings mentioned above. In total we obtained 90 dialogues of 25-60 minutes each. The speakers played a <a href=https://en.wikipedia.org/wiki/Card_game>card game</a> and a map game ; they were recorded in a soundproof studio without being able to see each other due to a non-transparent screen between them. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains orthographic, phonetic and prosodic annotation and is segmented into turns and inter-pausal units.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--808 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.808/>PhonBank and Data Sharing : Recent Developments in <a href=https://en.wikipedia.org/wiki/European_Portuguese>European Portuguese</a><span class=acl-fixed-case>P</span>hon<span class=acl-fixed-case>B</span>ank and Data Sharing: Recent Developments in <span class=acl-fixed-case>E</span>uropean <span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/a/ana-margarida-ramalho/>Ana Margarida Ramalho</a>
|
<a href=/people/m/maria-joao-freitas/>Maria João Freitas</a>
|
<a href=/people/y/yvan-rose/>Yvan Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--808><div class="card-body p-3 small">84 104 105 115 32 112 97 112 101 114 32 112 114 101 115 101 110 116 115 32 116 104 101 32 114 101 99 101 110 116 108 121 32 112 117 98 108 105 115 104 101 100 32 82 65 77 65 76 72 79 45 69 80 32 97 110 100 32 80 72 79 78 79 68 73 83 32 99 111 114 112 111 114 97 46 32 66 111 116 104 32 105 110 99 108 117 100 101 32 69 117 114 111 112 101 97 110 32 80 111 114 116 117 103 117 101 115 101 32 112 114 111 100 117 99 116 105 111 110 32 100 97 116 97 32 102 114 111 109 32 80 111 114 116 117 103 117 101 115 101 32 99 104 105 108 100 114 101 110 32 119 105 116 104 32 116 121 112 105 99 97 108 32 40 82 65 77 65 76 72 79 45 69 80 41 32 97 110 100 32 112 114 111 116 114 97 99 116 101 100 32 40 80 72 79 78 79 68 73 83 41 32 112 104 111 110 111 108 111 103 105 99 97 108 32 100 101 118 101 108 111 112 109 101 110 116 46 32 84 104 101 32 100 97 116 97 32 105 110 32 116 104 101 32 116 119 111 32 99 111 114 112 111 114 97 32 119 101 114 101 32 99 111 108 108 101 99 116 101 100 32 117 115 105 110 103 32 116 104 101 32 112 104 111 110 111 108 111 103 105 99 97 108 32 97 115 115 101 115 115 109 101 110 116 32 116 111 111 108 32 67 76 67 80 45 69 80 44 32 100 101 118 101 108 111 112 101 100 32 105 110 32 116 104 101 32 99 111 110 116 101 120 116 32 111 102 32 116 104 101 32 67 114 111 115 115 108 105 110 103 117 105 115 116 105 99 32 67 104 105 108 100 32 80 104 111 110 111 108 111 103 121 32 80 114 111 106 101 99 116 44 32 99 111 111 114 100 105 110 97 116 101 100 32 98 121 32 66 97 114 98 97 114 97 32 66 101 114 110 104 97 114 100 116 32 97 110 100 32 74 111 101 32 83 116 101 109 98 101 114 103 101 114 32 40 85 110 105 118 101 114 115 105 116 121 32 111 102 32 66 114 105 116 105 115 104 32 67 111 108 117 109 98 105 97 32 40 85 66 67 41 44 32 67 97 110 97 100 97 41 46 32 32 66 111 116 104 32 99 111 114 112 111 114 97 32 97 114 101 32 112 97 114 116 32 111 102 32 116 104 101 32 80 104 111 110 66 97 110 107 32 80 114 111 106 101 99 116 32 40 66 114 105 97 110 32 77 97 99 87 104 105 110 110 101 121 32 40 67 97 114 110 101 103 105 101 32 77 101 108 108 111 110 44 32 85 83 65 41 32 97 110 100 32 89 118 97 110 32 82 111 115 101 32 40 77 101 109 111 114 105 97 108 32 85 110 105 118 101 114 115 105 116 121 32 111 102 32 78 101 119 102 111 117 110 100 108 97 110 100 44 32 67 97 110 97 100 97 41 44 32 119 104 105 99 104 32 105 115 32 116 104 101 32 99 104 105 108 100 32 112 104 111 110 111 108 111 103 121 32 99 111 109 112 111 110 101 110 116 32 111 102 32 84 97 108 107 66 97 110 107 44 32 99 111 111 114 100 105 110 97 116 101 100 32 98 121 32 66 114 105 97 110 32 77 97 99 87 104 105 110 110 101 121 46 32 84 104 101 32 100 97 116 97 32 97 116 32 80 104 111 110 66 97 110 107 32 105 115 32 101 100 105 116 101 100 32 105 110 32 80 104 111 110 32 102 111 114 109 97 116 44 32 97 32 108 97 110 103 117 97 103 101 32 116 111 111 108 32 100 101 115 105 103 110 101 100 32 97 110 100 32 98 117 105 108 116 32 98 121 32 89 118 97 110 32 82 111 115 101 32 97 110 100 32 71 114 101 103 32 72 101 100 108 117 110 100 32 40 77 101 109 111 114 105 97 108 32 85 110 105 118 101 114 115 105 116 121 32 111 102 32 78 101 119 102 111 117 110 100 108 97 110 100 41 32 97 110 100 32 119 105 100 101 108 121 32 117 115 101 100 32 98 121 32 114 101 115 101 97 114 99 104 101 114 115 32 119 111 114 107 105 110 103 32 105 110 32 116 104 101 32 102 105 101 108 100 32 111 102 32 112 104 111 110 111 108 111 103 105 99 97 108 32 97 99 113 117 105 115 105 116 105 111 110 46 32 82 65 77 65 76 72 79 45 69 80 32 99 111 110 116 97 105 110 115 32 112 114 111 100 117 99 116 105 111 110 32 100 97 116 97 32 102 114 111 109 32 56 55 32 116 121 112 105 99 97 108 108 121 32 100 101 118 101 108 111 112 105 110 103 32 99 104 105 108 100 114 101 110 44 32 97 103 101 100 32 50 59 49 49 32 116 111 32 54 59 48 52 44 32 97 108 108 32 109 111 110 111 108 105 110 103 117 97 108 115 46 32 80 72 79 78 79 68 73 83 32 105 110 99 108 117 100 101 115 32 112 114 111 100 117 99 116 105 111 110 32 100 97 116 97 32 102 114 111 109 32 50 50 32 99 104 105 108 100 114 101 110 32 100 105 97 103 110 111 115 101 100 32 119 105 116 104 32 100 105 102 102 101 114 101 110 116 32 116 121 112 101 115 32 111 102 32 115 112 101 101 99 104 32 97 110 100 32 108 97 110 103 117 97 103 101 32 100 105 115 111 114 100 101 114 115 44 32 97 108 108 32 69 80 32 109 111 110 111 108 105 110 103 117 97 108 115 44 32 97 103 101 100 32 51 59 50 32 116 111 32 49 49 44 48 53 46 32 66 111 116 104 32 99 111 114 112 111 114 97 32 97 114 101 32 111 112 101 110 32 97 99 99 101 115 115 32 108 97 110 103 117 97 103 101 32 114 101 115 111 117 114 99 101 115 32 97 110 100 32 99 111 110 116 114 105 98 117 116 101 32 116 111 32 101 110 108 97 114 103 101 32 116 104 101 32 97 109 111 117 110 116 32 111 102 32 112 114 111 100 117 99 116 105 111 110 32 100 97 116 97 32 111 110 32 116 104 101 32 97 99 113 117 105 115 105 116 105 111 110 32 111 102 32 69 117 114 111 112 101 97 110 32 80 111 114 116 117 103 117 101 115 101 32 97 118 97 105 108 97 98 108 101 32 105 110 32 80 104 111 110 66 97 110 107 46</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--809 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.809/>SMASH Corpus : A Spontaneous Speech Corpus Recording Third-person Audio Commentaries on Gameplay<span class=acl-fixed-case>SMASH</span> Corpus: A Spontaneous Speech Corpus Recording Third-person Audio Commentaries on Gameplay</a></strong><br><a href=/people/y/yuki-saito/>Yuki Saito</a>
|
<a href=/people/s/shinnosuke-takamichi/>Shinnosuke Takamichi</a>
|
<a href=/people/h/hiroshi-saruwatari/>Hiroshi Saruwatari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--809><div class="card-body p-3 small">Developing a spontaneous speech corpus would be beneficial for spoken language processing and understanding. We present a <a href=https://en.wikipedia.org/wiki/Speech_corpus>speech corpus</a> named the SMASH corpus, which includes spontaneous speech of two Japanese male commentators that made third-person audio commentaries during the gameplay of a fighting game. Each commentator ad-libbed while watching the gameplay with various topics covering not only explanations of each moment to convey the information on the fight but also comments to entertain listeners. We made <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>topic tags</a> as annotations on the recorded commentaries with our two-step method. We first made automatic and manual transcriptions of the commentaries and then manually annotated the topic tags. This paper describes how we constructed the SMASH corpus and reports some results of the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--813 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.813/>Gender Representation in Open Source Speech Resources</a></strong><br><a href=/people/m/mahault-garnerin/>Mahault Garnerin</a>
|
<a href=/people/s/solange-rossato/>Solange Rossato</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--813><div class="card-body p-3 small">With the rise of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence (AI)</a> and the growing use of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning architectures</a>, the question of ethics, <a href=https://en.wikipedia.org/wiki/Transparency_(behavior)>transparency</a> and fairness of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a> has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited / non elicited speech, low / high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the <a href=https://en.wikipedia.org/wiki/Speech_processing>speech systems</a> built using such <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--816 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.816/>Call My Net 2 : A New Resource for Speaker Recognition</a></strong><br><a href=/people/k/karen-sparck-jones/>Karen Jones</a>
|
<a href=/people/s/stephanie-strassel/>Stephanie Strassel</a>
|
<a href=/people/k/kevin-walker/>Kevin Walker</a>
|
<a href=/people/j/jonathan-wright/>Jonathan Wright</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--816><div class="card-body p-3 small">We introduce the Call My Net 2 (CMN2) Corpus, a new resource for speaker recognition featuring Tunisian Arabic conversations between friends and family, incorporating both traditional telephony and VoIP data. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains data from over 400 <a href=https://en.wikipedia.org/wiki/Tunisian_Arabic>Tunisian Arabic speakers</a> collected via a custom-built platform deployed in Tunis, with each speaker making 10 or more calls each lasting up to 10 minutes. Calls include speech in various realistic and natural acoustic settings, both noisy and non-noisy. Speakers used a variety of handsets, including landline and mobile devices, and made <a href=https://en.wikipedia.org/wiki/Voice_over_IP>VoIP calls</a> from tablets or computers. All calls were subject to a series of manual and automatic quality checks, including speech duration, <a href=https://en.wikipedia.org/wiki/Sound_quality>audio quality</a>, language identity and speaker identity. The CMN2 corpus has been used in two NIST Speaker Recognition Evaluations (SRE18 and SRE19), and the SRE test sets as well as the full CMN2 corpus will be published in the Linguistic Data Consortium Catalog. We describe CMN2 corpus requirements, the telephone collection platform, and procedures for call collection. We review properties of the CMN2 dataset and discuss features of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that distinguish it from prior SRE collection efforts, including some of the technical challenges encountered with collecting VoIP data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--818 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.818/>Development and Evaluation of Speech Synthesis Corpora for Latvian<span class=acl-fixed-case>L</span>atvian</a></strong><br><a href=/people/r/roberts-dargis/>Roberts Darģis</a>
|
<a href=/people/p/peteris-paikens/>Peteris Paikens</a>
|
<a href=/people/n/normunds-gruzitis/>Normunds Gruzitis</a>
|
<a href=/people/i/ilze-auzina/>Ilze Auzina</a>
|
<a href=/people/a/agate-akmane/>Agate Akmane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--818><div class="card-body p-3 small">Text to speech (TTS) systems are necessary for all languages to ensure accessibility and availability of digital language services. Recent advances in neural speech synthesis have eText to speech (TTS) systems are necessary for any language to ensure accessibility and availability of digital language services. Recent advances in neural speech synthesis have enabled the development of such systems with a data-driven approach that does not require significant development of language-specific tools. However, smaller languages often lack <a href=https://en.wikipedia.org/wiki/Speech_corpus>speech corpora</a> that would be sufficient for training current neural TTS models, which require at least 30 hours of good quality audio recordings from a single speaker in a noiseless environment with matching transcriptions. Making such a corpus manually can be cost prohibitive. This paper presents an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to obtain a suitable <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> from unannotated recordings using <a href=https://en.wikipedia.org/wiki/Speech_recognition>automated speech recognition</a> for <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a>, as well as automated speaker segmentation and identification. The proposed method and software tools are applied and evaluated on a case study for developing a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> suitable for Latvian speech synthesis based on Latvian public radio archive data.nabled the development of such systems with a data-driven approach that does not require much language-specific tool development. However, smaller languages often lack <a href=https://en.wikipedia.org/wiki/Speech_corpus>speech corpora</a> that would be sufficient for training current neural TTS models, which require approximately 30 hours of good quality audio recordings from a single speaker in a noiseless environment with matching transcriptions. Making such a corpus manually can be cost prohibitive. This paper presents an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to obtain a suitable <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> from unannotated recordings using <a href=https://en.wikipedia.org/wiki/Speech_recognition>automated speech recognition</a> for <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a>, as well as automated speaker segmentation and identification. The proposed methods and software tools are applied and evaluated on a case study for developing a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> suitable for Latvian speech synthesis based on Latvian public radio archive data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.820.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--820 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.820 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.820" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.820/>GameWikiSum : a Novel Large Multi-Document Summarization Dataset<span class=acl-fixed-case>G</span>ame<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>S</span>um: a Novel Large Multi-Document Summarization Dataset</a></strong><br><a href=/people/d/diego-antognini/>Diego Antognini</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--820><div class="card-body p-3 small">Today&#8217;s research progress in the field of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> is obstructed by the small number of available datasets. Since the acquisition of reference summaries is costly, existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> contain only hundreds of samples at most, resulting in heavy reliance on hand-crafted features or necessitating additional, manually annotated data. The lack of large corpora therefore hinders the development of sophisticated <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Additionally, most publicly available multi-document summarization corpora are in the <a href=https://en.wikipedia.org/wiki/News_media>news domain</a>, and no analogous <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> exists in the video game domain. In this paper, we propose GameWikiSum, a new domain-specific dataset for <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>, which is one hundred times larger than commonly used datasets, and in another domain than <a href=https://en.wikipedia.org/wiki/News>news</a>. Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages. We analyze the proposed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and show that both abstractive and extractive models can be trained on it. We release GameWikiSum for further research : https://github.com/Diego999/GameWikiSum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--827 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.827/>Summarization Beyond News : The Automatically Acquired Fandom Corpora</a></strong><br><a href=/people/b/benjamin-hattasch/>Benjamin Hättasch</a>
|
<a href=/people/n/nadja-geisler/>Nadja Geisler</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/c/carsten-binnig/>Carsten Binnig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--827><div class="card-body p-3 small">Large state-of-the-art corpora for training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to create abstractive summaries are mostly limited to the news genre, as it is expensive to acquire human-written summaries for other types of text at a large scale. In this paper, we present a novel automatic corpus construction approach to tackle this issue as well as three new large open-licensed summarization corpora based on our approach that can be used for training abstractive summarization models. Our constructed corpora contain fictional narratives, descriptive texts, and summaries about <a href=https://en.wikipedia.org/wiki/Film>movies</a>, <a href=https://en.wikipedia.org/wiki/Television>television</a>, and <a href=https://en.wikipedia.org/wiki/Book_series>book series</a> from different domains. All sources use a creative commons (CC) license, hence we can provide the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> for download. In addition, we also provide a ready-to-use framework that implements our automatic construction approach to create custom corpora with desired parameters like the length of the target summary and the number of source documents from which to create the summary. The main idea behind our automatic construction approach is to use existing <a href=https://en.wikipedia.org/wiki/Text_corpus>large text collections</a> (e.g., thematic wikis) and automatically classify whether the texts can be used as (query-focused) multi-document summaries and align them with potential source texts. As a final contribution, we show the usefulness of our automatic construction approach by running state-of-the-art summarizers on the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> and through a manual evaluation with human annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--828 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.828/>Invisible to People but not to Machines : Evaluation of Style-aware HeadlineGeneration in Absence of Reliable Human Judgment<span class=acl-fixed-case>H</span>eadline<span class=acl-fixed-case>G</span>eneration in Absence of Reliable Human Judgment</a></strong><br><a href=/people/l/lorenzo-de-mattei/>Lorenzo De Mattei</a>
|
<a href=/people/m/michele-cafagna/>Michele Cafagna</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--828><div class="card-body p-3 small">We automatically generate <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> that are expected to comply with the specific styles of two different <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Italy>Italian newspapers</a>. Through a data alignment strategy and different training / testing settings, we aim at decoupling content from style and preserve the latter in generation. In order to evaluate the generated headlines&#8217; quality in terms of their specific newspaper-compliance, we devise a fine-grained evaluation strategy based on automatic classification. We observe that our <a href=https://en.wikipedia.org/wiki/Model_organism>models</a> do indeed learn newspaper-specific style. Importantly, we also observe that humans are n&#8217;t reliable judges for this task, since although familiar with the <a href=https://en.wikipedia.org/wiki/Newspaper>newspapers</a>, they are not able to discern their specific styles even in the original human-written headlines. The utility of automatic evaluation goes therefore beyond saving the costs and hurdles of manual annotation, and deserves particular care in its design.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.829.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--829 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.829 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.829" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.829/>Align then Summarize : Automatic Alignment Methods for Summarization Corpus Creation</a></strong><br><a href=/people/p/paul-tardy/>Paul Tardy</a>
|
<a href=/people/d/david-janiszek/>David Janiszek</a>
|
<a href=/people/y/yannick-esteve/>Yannick Estève</a>
|
<a href=/people/v/vincent-nguyen/>Vincent Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--829><div class="card-body p-3 small">Summarizing texts is not a straightforward task. Before even considering text summarization, one should determine what kind of summary is expected. How much should the information be compressed? Is it relevant to reformulate or should the summary stick to the original phrasing? State-of-the-art on <a href=https://en.wikipedia.org/wiki/Automatic_text_summarization>automatic text summarization</a> mostly revolves around <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We suggest that considering a wider variety of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> would lead to an improvement in the field, in terms of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>. We explore meeting summarization : generating reports from automatic transcriptions. Our work consists in segmenting and aligning <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> with respect to reports, to get a suitable <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for neural summarization. Using a bootstrapping approach, we provide pre-alignments that are corrected by human annotators, making a validation set against which we evaluate automatic models. This consistently reduces annotators&#8217; efforts by providing iteratively better pre-alignment and maximizes the corpus size by using annotations from our automatic alignment models. Evaluation is conducted on publicmeetings, a novel corpus of aligned public meetings. We report automatic alignment and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> performances on this corpus and show that automatic alignment is relevant for data annotation since it leads to large improvement of almost +4 on all ROUGE scores on the <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.832.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--832 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.832 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.832/>Diverging Divergences : Examining Variants of <a href=https://en.wikipedia.org/wiki/Jensen_Shannon_divergence>Jensen Shannon Divergence</a> for Corpus Comparison Tasks<span class=acl-fixed-case>J</span>ensen <span class=acl-fixed-case>S</span>hannon Divergence for Corpus Comparison Tasks</a></strong><br><a href=/people/j/jinghui-lu/>Jinghui Lu</a>
|
<a href=/people/m/maeve-henchion/>Maeve Henchion</a>
|
<a href=/people/b/brian-mac-namee/>Brian Mac Namee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--832><div class="card-body p-3 small">Jensen-Shannon divergence (JSD) is a distribution similarity measurement widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In corpus comparison tasks, where keywords are extracted to reveal the divergence between different corpora (for example, social media posts from proponents of different views on a political issue), two variants of JSD have emerged in the literature. One of these uses a <a href=https://en.wikipedia.org/wiki/Weighting>weighting</a> based on the relative sizes of the corpora being compared. In this paper we argue that this <a href=https://en.wikipedia.org/wiki/Weighting>weighting</a> is unnecessary and, in fact, can lead to misleading results. We recommend that this <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted version</a> is not used. We base this recommendation on an analysis of the JSD variants and experiments showing how they impact corpus comparison results as the relative sizes of the corpora being compared change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.834.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--834 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.834 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.834/>SC-CoMIcs : A Superconductivity Corpus for Materials Informatics<span class=acl-fixed-case>SC</span>-<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>MI</span>cs: A Superconductivity Corpus for Materials Informatics</a></strong><br><a href=/people/k/kyosuke-yamaguchi/>Kyosuke Yamaguchi</a>
|
<a href=/people/r/ryoji-asahi/>Ryoji Asahi</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--834><div class="card-body p-3 small">This paper describes a novel corpus tailored for the text mining of superconducting materials in Materials Informatics (MI), named SuperConductivety Corpus for Materials Informatics (SC-CoMIcs). Different from <a href=https://en.wikipedia.org/wiki/Health_informatics>biomedical informatics</a>, there exist very few <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> targeting Materials Science and Engineering (MSE). Especially, there is no sizable corpus which can be used to assist the search of superconducting materials. A team of materials scientists and natural language processing experts jointly designed the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and constructed a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of manually-annotated 1,000 MSE abstracts related to <a href=https://en.wikipedia.org/wiki/Superconductivity>superconductivity</a>. We conducted experiments on the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with a neural Named Entity Recognition (NER) tool. The experimental results show that NER performance over the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is around 77 % in terms of micro-F1, which is comparable to human annotator agreement rates. Using the trained NER model, we automatically annotated 9,000 abstracts and created a term retrieval tool based on the term similarity. This tool can find superconductivity terms relevant to a query term within a specified Named Entity category, which demonstrates the power of our SC-CoMIcs, efficiently providing knowledge for Materials Informatics applications from rapidly expanding publications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.843.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--843 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.843 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.843/>Japanese Realistic Textual Entailment Corpus<span class=acl-fixed-case>J</span>apanese Realistic Textual Entailment Corpus</a></strong><br><a href=/people/y/yuta-hayashibe/>Yuta Hayashibe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--843><div class="card-body p-3 small">We perform the textual entailment (TE) corpus construction for the <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese Language</a> with the following three characteristics : First, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of realistic sentences ; that is, all sentences are spontaneous or almost equivalent. It does not need <a href=https://en.wikipedia.org/wiki/Writing_system>manual writing</a> which causes hidden biases. Second, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains adversarial examples. We collect challenging examples that can not be solved by a recent pre-trained language model. Third, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains explanations for a part of non-entailment labels. We perform the reasoning annotation where annotators are asked to check which tokens in hypotheses are the reason why the relations are labeled. It makes easy to validate the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and analyze system errors. The resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of 48,000 realistic Japanese examples. It is the largest among publicly available Japanese TE corpora. Additionally, it is the first Japanese TE corpus that includes reasons for the annotation as we know. We are planning to distribute this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to the NLP community at the time of publication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.846.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--846 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.846 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.846/>HypoNLI : Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference<span class=acl-fixed-case>H</span>ypo<span class=acl-fixed-case>NLI</span>: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference</a></strong><br><a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/z/zheng-xin/>Zheng Xin</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--846><div class="card-body p-3 small">Many recent studies have shown that for models trained on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out &#8216;hard&#8217; and &#8216;easy&#8217; instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (BERT, RoBerta, XLNet) and competitive non-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias : down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in NLI debiasing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.848.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--848 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.848 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.848/>TaPaCo : A Corpus of Sentential Paraphrases for 73 Languages<span class=acl-fixed-case>T</span>a<span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>C</span>o: A Corpus of Sentential Paraphrases for 73 Languages</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--848><div class="card-body p-3 small">This paper presents TaPaCo, a freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. Tatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences and translations for particular <a href=https://en.wikipedia.org/wiki/Construct_(philosophy)>linguistic constructions</a> and words. The paraphrase corpus is created by populating a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with Tatoeba sentences and equivalence links between sentences meaning the same thing. This <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is then traversed to extract sets of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Several language-independent filters and pruning steps are applied to remove uninteresting sentences. A manual evaluation performed on three languages shows that between half and three quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains a total of 1.9 million sentences, with 200-250 000 sentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available at https://doi.org/10.5281/zenodo.3707949.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.850.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--850 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.850 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.850/>Towards the Necessity for Debiasing Natural Language Inference Datasets</a></strong><br><a href=/people/m/mithun-paul-panenghat/>Mithun Paul Panenghat</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/f/faiz-rafique/>Faiz Rafique</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--850><div class="card-body p-3 small">Modeling natural language inference is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. With large annotated data sets available it has now become feasible to train complex neural network based inference methods which achieve state of the art performance. However, it has been shown that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> also learn from the subtle biases inherent in these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (CITATION). In this work we explore two techniques for delexicalization that modify the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in such a way that we can control the importance that neural-network based methods place on <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical entities</a>. We demonstrate that the proposed methods not only maintain the performance in-domain but also improve performance in some out-of-domain settings. For example, when using the delexicalized version of the FEVER dataset, the in-domain performance of a state of the art <a href=https://en.wikipedia.org/wiki/Neural_network>neural network method</a> dropped only by 1.12 % while its out-of-domain performance on the FNC dataset improved by 4.63 %. We release the delexicalized versions of three common datasets used in natural language inference. These datasets are delexicalized using two methods : one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.851.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--851 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.851 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.851/>A French Corpus for Semantic Similarity<span class=acl-fixed-case>F</span>rench Corpus for Semantic Similarity</a></strong><br><a href=/people/r/remi-cardon/>Rémi Cardon</a>
|
<a href=/people/n/natalia-grabar/>Natalia Grabar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--851><div class="card-body p-3 small">Semantic similarity is an area of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> that is useful for several downstream applications, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, or <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> consists in assessing the extent to which two sentences express or do not express the same meaning. To do so, <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> with graded pairs of sentences are required. The <a href=https://en.wikipedia.org/wiki/Grading_in_education>grade</a> is positioned on a given scale, usually going from 0 (completely unrelated) to 5 (equivalent semantics). In this work, we introduce such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, the first that we know of. It is comprised of 1,010 sentence pairs with grades from five annotators. We describe the annotation process, analyse these <a href=https://en.wikipedia.org/wiki/Data>data</a>, and perform a few experiments for the automatic grading of semantic similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.854.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--854 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.854 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.854/>TIARA : A Tool for Annotating Discourse Relations and Sentence Reordering<span class=acl-fixed-case>TIARA</span>: A Tool for Annotating Discourse Relations and Sentence Reordering</a></strong><br><a href=/people/j/jan-wira-gotama-putra/>Jan Wira Gotama Putra</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a>
|
<a href=/people/k/kana-matsumura/>Kana Matsumura</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--854><div class="card-body p-3 small">This paper introduces <a href=https://en.wikipedia.org/wiki/TIARA>TIARA</a>, a new publicly available web-based annotation tool for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> and sentence reordering. Annotation tasks such as these, which are based on relations between large textual objects, are inherently hard to visualise without either cluttering the display and/or confusing the annotators. TIARA deals with the visual complexity during the annotation process by systematically simplifying the layout, and by offering interactive visualisation, including coloured links, <a href=https://en.wikipedia.org/wiki/Indentation_style>indentation</a>, and dual-view. TIARA&#8217;s text view allows annotators to focus on the analysis of logical sequencing between sentences. A separate <a href=https://en.wikipedia.org/wiki/Tree_view>tree view</a> allows them to review their analysis in terms of the overall discourse structure. The dual-view gives <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> an edge over other discourse annotation tools and makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> particularly attractive as an <a href=https://en.wikipedia.org/wiki/Educational_technology>educational tool</a> (e.g., for teaching students how to argue more effectively). As it is based on standard web technologies and can be easily customised to other annotation schemes, it can be easily used by anybody. Apart from the project it was originally designed for, in which hundreds of texts were annotated by three annotators, TIARA has already been adopted by a second discourse annotation study, which uses it in the teaching of argumentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.858.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--858 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.858 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.858" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.858/>ThaiLMCut : Unsupervised Pretraining for Thai Word Segmentation<span class=acl-fixed-case>T</span>hai<span class=acl-fixed-case>LMC</span>ut: Unsupervised Pretraining for <span class=acl-fixed-case>T</span>hai Word Segmentation</a></strong><br><a href=/people/s/suteera-seeha/>Suteera Seeha</a>
|
<a href=/people/i/ivan-bilan/>Ivan Bilan</a>
|
<a href=/people/l/liliana-mamani-sanchez/>Liliana Mamani Sanchez</a>
|
<a href=/people/j/johannes-huber/>Johannes Huber</a>
|
<a href=/people/m/michael-matuschek/>Michael Matuschek</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--858><div class="card-body p-3 small">We propose ThaiLMCut, a semi-supervised approach for Thai word segmentation which utilizes a bi-directional character language model (LM) as a way to leverage useful linguistic knowledge from unlabeled data. After the language model is trained on substantial unlabeled corpora, the weights of its embedding and recurrent layers are transferred to a supervised word segmentation model which continues fine-tuning them on a word segmentation task. Our experimental results demonstrate that applying the LM always leads to a performance gain, especially when the amount of labeled data is small. In such cases, the <a href=https://en.wikipedia.org/wiki/F1_Score>F1 Score</a> increased by up to 2.02 %. Even on abig labeled dataset, a small improvement gain can still be obtained. The approach has also shown to be very beneficial for out-of-domain settings with a gain in <a href=https://en.wikipedia.org/wiki/F1_score>F1 Score</a> of up to 3.13 %. Finally, we show that ThaiLMCut can outperform other open source state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieving an F1 Score of 98.78 % on the standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, InterBEST2009.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.859.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--859 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.859 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.859/>CCOHA : Clean Corpus of Historical American English<span class=acl-fixed-case>CCOHA</span>: Clean Corpus of Historical <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/r/reem-alatrash/>Reem Alatrash</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--859><div class="card-body p-3 small">Modelling language change is an increasingly important area of interest within the fields of <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. In recent years, there has been a growing number of publications whose main concern is studying changes that have occurred within the past centuries. The Corpus of Historical American English (COHA) is one of the most commonly used large corpora in diachronic studies in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This paper describes methods applied to the downloadable version of the COHA corpus in order to overcome its main limitations, such as inconsistent lemmas and malformed tokens, without compromising its qualitative and distributional properties. The resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> CCOHA contains a larger number of cleaned word tokens which can offer better insights into <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> and allow for a larger variety of tasks to be performed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.860.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--860 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.860 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.860/>Outbound Translation User Interface Ptakopt : A Pilot Study</a></strong><br><a href=/people/v/vilem-zouhar/>Vilém Zouhar</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--860><div class="card-body p-3 small">It is not uncommon for Internet users to have to produce a text in a foreign language they have very little knowledge of and are unable to verify the translation quality. We call the task outbound translation and explore it by introducing an open-source modular system Ptakopt. Its main purpose is to inspect <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human interaction</a> with MT systems enhanced with additional subsystems, such as backward translation and quality estimation. We follow up with an experiment on (Czech) human annotators tasked to produce questions in a language they do not speak (German), with the help of Ptakopt. We focus on three real-world use cases (communication with IT support, describing administrative issues and asking encyclopedic questions) from which we gain insight into different strategies users take when faced with outbound translation tasks. Round trip translation is known to be unreliable for evaluating MT systems but our experimental evaluation documents that it works very well for users, at least on MT systems of mid-range quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.861.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--861 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.861 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.861" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.861/>Seshat : a Tool for Managing and Verifying Annotation Campaigns of Audio Data<span class=acl-fixed-case>S</span>eshat: a Tool for Managing and Verifying Annotation Campaigns of Audio Data</a></strong><br><a href=/people/h/hadrien-titeux/>Hadrien Titeux</a>
|
<a href=/people/r/rachid-riad/>Rachid Riad</a>
|
<a href=/people/x/xuan-nga-cao/>Xuan-Nga Cao</a>
|
<a href=/people/n/nicolas-hamilakis/>Nicolas Hamilakis</a>
|
<a href=/people/k/kris-madden/>Kris Madden</a>
|
<a href=/people/a/alejandrina-cristia/>Alejandrina Cristia</a>
|
<a href=/people/a/anne-catherine-bachoud-levi/>Anne-Catherine Bachoud-Lévi</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--861><div class="card-body p-3 small">We introduce <a href=https://en.wikipedia.org/wiki/Seshat>Seshat</a>, a new, simple and open-source software to efficiently manage annotations of speech corpora. The Seshat software allows users to easily customise and manage annotations of large audio corpora while ensuring compliance with the formatting and naming conventions of the annotated output files. In addition, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> includes procedures for checking the content of <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> following specific rules that can be implemented in personalised parsers. Finally, we propose a double-annotation mode, for which Seshat computes automatically an associated inter-annotator agreement with the gamma measure taking into account the categorisation and segmentation discrepancies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.865.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--865 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.865 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.865/>KonText : Advanced and Flexible Corpus Query Interface<span class=acl-fixed-case>K</span>on<span class=acl-fixed-case>T</span>ext: Advanced and Flexible Corpus Query Interface</a></strong><br><a href=/people/t/tomas-machalek/>Tomáš Machálek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--865><div class="card-body p-3 small">We present an advanced, highly customizable corpus query interface KonText built on top of core libraries of the open-source corpus search engine NoSketch Engine (NoSkE). The aim is to overcome some limitations of the original NoSkE user interface and provide integration capabilities allowing connection of the basic search service with other language resources (LRs). The introduced features are based on long-term feedback given by the users and researchers of the Czech National Corpus (CNC) along with other LRs providers running KonText as a part of their services. KonText is a fully operational and mature software deployed at the CNC since 2014 that currently handles thousands user queries per day.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.867.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--867 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.867 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.867/>RKorAPClient : An R Package for Accessing the German Reference Corpus DeReKo via KorAP<span class=acl-fixed-case>RK</span>or<span class=acl-fixed-case>APC</span>lient: An <span class=acl-fixed-case>R</span> Package for Accessing the <span class=acl-fixed-case>G</span>erman Reference Corpus <span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>K</span>o via <span class=acl-fixed-case>K</span>or<span class=acl-fixed-case>AP</span></a></strong><br><a href=/people/m/marc-kupietz/>Marc Kupietz</a>
|
<a href=/people/n/nils-diewald/>Nils Diewald</a>
|
<a href=/people/e/eliza-margaretha/>Eliza Margaretha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--867><div class="card-body p-3 small">Making corpora accessible and usable for <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic research</a> is a huge challenge in view of (too) <a href=https://en.wikipedia.org/wiki/Big_data>big data</a>, legal issues and a rapidly evolving <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>. This does not only affect the design of user-friendly graphical interfaces to corpus analysis tools, but also the availability of <a href=https://en.wikipedia.org/wiki/Interface_(computing)>programming interfaces</a> supporting access to the functionality of these tools from various analysis and development environments. RKorAPClient is a new research tool in the form of an <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R package</a> that interacts with the Web API of the corpus analysis platform KorAP, which provides access to large annotated corpora, including the German reference corpus DeReKo with 45 billion tokens. In addition to optionally authenticated KorAP API access, RKorAPClient provides further processing and visualization features to simplify common corpus analysis tasks. This paper introduces the basic functionality of RKorAPClient and exemplifies various analysis tasks based on DeReKo, that are bundled within the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R package</a> and can serve as a basic framework for advanced analysis and visualization approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.871.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--871 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.871 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.871/>The xtsv Framework and the Twelve Virtues of Pipelines</a></strong><br><a href=/people/b/balazs-indig/>Balázs Indig</a>
|
<a href=/people/b/balint-sass/>Bálint Sass</a>
|
<a href=/people/i/ivan-mittelholcz/>Iván Mittelholcz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--871><div class="card-body p-3 small">We present xtsv, an abstract framework for building NLP pipelines. It covers several kinds of <a href=https://en.wikipedia.org/wiki/Functional_programming>functionalities</a> which can be implemented at an <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract level</a>. We survey these features and argue that all are desired in a modern <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a>. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> has a simple yet powerful internal communication format which is essentially tsv (tab separated values) with header plus some additional features. We put emphasis on the capabilities of the presented <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, for example its ability to allow new <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> to be easily integrated or replaced, or the variety of its usage options. When a <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> is put into xtsv, all functionalities of the system are immediately available for that <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a>, and the <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> can be be a part of an xtsv pipeline. The <a href=https://en.wikipedia.org/wiki/Design>design</a> also allows convenient investigation and manual correction of the data flow from one module to another. We demonstrate the power of our framework with a successful application : a concrete NLP pipeline for <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a> called e-magyar text processing system (emtsv) which integrates <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian NLP tools</a> in xtsv. All the advantages of the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> come from the inherent properties of the xtsv framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.873.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--873 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.873 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.873/>Data Query Language and Corpus Tools for Slot-Filling and Intent Classification Data</a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/e/eric-guldan/>Eric Guldan</a>
|
<a href=/people/k/kevin-leach/>Kevin Leach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--873><div class="card-body p-3 small">Typical machine learning approaches to developing task-oriented dialog systems require the collection and management of large amounts of training data, especially for the tasks of intent classification and slot-filling. Managing this <a href=https://en.wikipedia.org/wiki/Data>data</a> can be cumbersome without dedicated tools to help the <a href=https://en.wikipedia.org/wiki/Dialog_system>dialog system</a> designer understand the nature of the data. This paper presents a <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> for analyzing slot-filling and intent classification corpora. We present a toolkit that includes (1) a new lightweight and readable data and file format for intent classification and slot-filling corpora, (2) a new query language for searching intent classification and slot-filling corpora, and (3) tools for understanding the structure and makeup for such corpora. We apply our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> to several well-known NLU datasets, and demonstrate that our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> can be used to uncover interesting and surprising insights. By releasing our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> to the research community, we hope to enable others to develop more robust and intelligent slot-filling and intent classification models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.874.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--874 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.874 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lrec-1.874" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.874/>SHR++ : An Interface for Morpho-syntactic Annotation of Sanskrit Corpora<span class=acl-fixed-case>SHR</span>++: An Interface for Morpho-syntactic Annotation of <span class=acl-fixed-case>S</span>anskrit Corpora</a></strong><br><a href=/people/a/amrith-krishna/>Amrith Krishna</a>
|
<a href=/people/s/shiv-vidhyut/>Shiv Vidhyut</a>
|
<a href=/people/d/dilpreet-chawla/>Dilpreet Chawla</a>
|
<a href=/people/s/sruti-sambhavi/>Sruti Sambhavi</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--874><div class="card-body p-3 small">We propose a web-based annotation framework, SHR++, for morpho-syntactic annotation of corpora in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>. SHR++ is designed to generate annotations for the word-segmentation, morphological parsing and dependency analysis tasks in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>. It incorporates analyses and predictions from various tools designed for processing texts in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, and utilise them to ease the cognitive load of the human annotators. Specifically, SHR++ uses Sanskrit Heritage Reader, a lexicon driven shallow parser for enumerating all the phonetically and lexically valid word splits along with their morphological analyses for a given string. This would help the annotators in choosing the solutions, rather than performing the segmentations by themselves. Further, predictions from a word segmentation tool are added as suggestions that can aid the human annotators in their decision making. Our evaluation shows that enabling this segmentation suggestion component reduces the annotation time by 20.15 %. SHR++ can be accessed online at http://vidhyut97.pythonanywhere.com/ and the codebase, for the independent deployment of the system elsewhere, is hosted at https://github.com/iamdsc/smart-sanskrit-annotator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.876.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--876 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.876 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.876/>Gamification Platform for Collecting Task-oriented Dialogue Data</a></strong><br><a href=/people/h/haruna-ogawa/>Haruna Ogawa</a>
|
<a href=/people/h/hitoshi-nishikawa/>Hitoshi Nishikawa</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a>
|
<a href=/people/h/hikaru-yokono/>Hikaru Yokono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--876><div class="card-body p-3 small">Demand for massive language resources is increasing as the data-driven approach has established a leading position in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. However, creating dialogue corpora is still a difficult task due to the complexity of the human dialogue structure and the diversity of dialogue topics. Though <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> is majorly used to assemble such <a href=https://en.wikipedia.org/wiki/Data>data</a>, it presents problems such as less-motivated workers. We propose a <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> for collecting task-oriented situated dialogue data by using <a href=https://en.wikipedia.org/wiki/Gamification>gamification</a>. Combining a <a href=https://en.wikipedia.org/wiki/Video_game>video game</a> with data collection benefits such as motivating workers and <a href=https://en.wikipedia.org/wiki/Cost_reduction>cost reduction</a>. Our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> enables data collectors to create their original <a href=https://en.wikipedia.org/wiki/Video_game>video game</a> in which they can collect dialogue data of various types of tasks by using the logging function of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>. Also, the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> provides the <a href=https://en.wikipedia.org/wiki/Annotation>annotation function</a> that enables players to annotate their own utterances. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> can be gamified aswell. We aim at high-quality annotation by introducing such self-annotation method. We implemented a <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> of the proposed <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> and conducted a preliminary evaluation to obtain promising results in terms of both dialogue data collection and self-annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.878.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--878 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.878 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.878/>Improving Sentence Boundary Detection for Spoken Language Transcripts</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/t/thomas-schmidt/>Thomas Schmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--878><div class="card-body p-3 small">This paper presents experiments on sentence boundary detection in transcripts of spoken dialogues. Segmenting <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> into sentence-like units is a challenging task, due to disfluencies, ungrammatical or fragmented structures and the lack of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. In addition, one of the main bottlenecks for many NLP applications for <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> is the small size of the training data, as the transcription and annotation of spoken language is by far more time-consuming and labour-intensive than processing <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. We therefore investigate the benefits of data expansion and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and test different ML architectures for this task. Our results show that data expansion is not straightforward and even data from the same domain does not always improve results. They also highlight the importance of <a href=https://en.wikipedia.org/wiki/Model_(person)>modelling</a>, i.e. of finding the best <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> and <a href=https://en.wikipedia.org/wiki/Data_(computing)>data representation</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> at hand. For the detection of boundaries in spoken language transcripts, we achieve a substantial improvement when framing the boundary detection problem assentence pair classification task, as compared to a sequence tagging approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.888.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--888 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.888 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.888/>PyVallex : A Processing System for Valency Lexicon Data<span class=acl-fixed-case>P</span>y<span class=acl-fixed-case>V</span>allex: A Processing System for Valency Lexicon Data</a></strong><br><a href=/people/j/jonathan-verner/>Jonathan Verner</a>
|
<a href=/people/a/anna-vernerova/>Anna Vernerová</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--888><div class="card-body p-3 small">PyVallex is a Python-based system for presenting, searching / filtering, editing / extending and automatic processing of machine-readable lexicon data originally available in a <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based format</a>. The system consists of several components : a parser for the specific lexicon format used in several valency lexicons, a data-validation framework, a regular expression based search engine, a map-reduce style framework for querying the lexicon data and a web-based interface integrating complex search and some basic editing capabilities. PyVallex provides most of the typical functionalities of a Dictionary Writing System (DWS), such as multiple presentation modes for the underlying <a href=https://en.wikipedia.org/wiki/Lexical_database>lexical database</a>, automatic evaluation of consistency tests, and a mechanism of merging updates coming from multiple sources. The editing functionality is currently limited to the <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>client-side interface</a> and edits of existing lexical entries, but additional <a href=https://en.wikipedia.org/wiki/Scripting_language>script-based operations</a> on the <a href=https://en.wikipedia.org/wiki/Database>database</a> are also possible. The code is published under the open source MIT license and is also available in the form of a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python module</a> for integrating into other <a href=https://en.wikipedia.org/wiki/Software>software</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.891.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--891 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.891 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.891/>Fintan-Flexible, Integrated Transformation and Annotation eNgineering<span class=acl-fixed-case>N</span>gineering</a></strong><br><a href=/people/c/christian-fath/>Christian Fäth</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/b/bjorn-ebbrecht/>Björn Ebbrecht</a>
|
<a href=/people/m/maxim-ionov/>Maxim Ionov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--891><div class="card-body p-3 small">We introduce the Flexible and Integrated Transformation and Annotation eNgeneering (Fintan) platform for converting heterogeneous linguistic resources to <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a>. With its modular architecture, workflow management and visualization features, <a href=https://en.wikipedia.org/wiki/Fintan>Fintan</a> facilitates the development of complex transformation pipelines by integrating generic RDF converters and augmenting them with extended graph processing capabilities : Existing converters can be easily deployed to the system by means of an ontological data structure which renders their properties and the dependencies between transformation steps. Development of subsequent graph transformation steps for resource transformation, annotation engineering or entity linking is further facilitated by a novel visual rendering of SPARQL queries. A graphical workflow manager allows to easily manage the converter modules and combine them to new transformation pipelines. Employing the stream-based graph processing approach first implemented with CoNLL-RDF, we address common challenges and scalability issues when transforming resources and showcase the performance of Fintan by means of a purely graph-based transformation of the Universal Morphology data to <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.893.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--893 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.893 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.893/>Interchange Formats for Visualization : LIF and MMIF<span class=acl-fixed-case>LIF</span> and <span class=acl-fixed-case>MMIF</span></a></strong><br><a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/m/marc-verhagen/>Marc Verhagen</a>
|
<a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--893><div class="card-body p-3 small">Promoting interoperrable computational linguistics (CL) and natural language processing (NLP) application platforms and interchange-able data formats have contributed improving discoverabilty and accessbility of the openly available NLP software. In this paper, wediscuss the enhanced data visualization capabilities that are also enabled by inter-operating NLP pipelines and interchange formats. For adding openly available visualization tools and graphical annotation tools to the Language Applications Grid (LAPPS Grid) andComputational Linguistics Applications for Multimedia Services (CLAMS) toolboxes, we have developed interchange formats that cancarry annotations and metadata for text and audiovisual source data. We descibe those data formats and present case studies where wesuccessfully adopt open-source visualization tools and combine them with CL tools.</div></div></div><hr><div id=2020aespen-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.aespen-1/>Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.0/>Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/e/erdem-yoruk/>Erdem Yörük</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aespen-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aespen-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.1/>Automated Extraction of Socio-political Events from News (AESPEN): Workshop and Shared Task Report<span class=acl-fixed-case>AESPEN</span>): Workshop and Shared Task Report</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a>
|
<a href=/people/e/erdem-yoruk/>Erdem Yörük</a>
|
<a href=/people/a/ali-safaya/>Ali Safaya</a>
|
<a href=/people/o/osman-mutlu/>Osman Mutlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aespen-1--1><div class="card-body p-3 small">We describe our effort on automated extraction of socio-political events from <a href=https://en.wikipedia.org/wiki/News>news</a> in the scope of a workshop and a shared task we organized at Language Resources and Evaluation Conference (LREC 2020). We believe the event extraction studies in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and social and political sciences should further support each other in order to enable large scale socio-political event information collection across sources, countries, and languages. The event consists of regular research papers and a shared task, which is about event sentence coreference identification (ESCI), tracks. All submissions were reviewed by five members of the program committee. The workshop attracted research papers related to evaluation of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methodologies</a>, language resources, material conflict forecasting, and a shared task participation report in the scope of socio-political event information collection. It has shown us the volume and variety of both the data sources and event information collection approaches related to socio-political events and the need to fill the gap between automated text processing techniques and requirements of social and political sciences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aespen-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aespen-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.5/>Text Categorization for Conflict Event Annotation</a></strong><br><a href=/people/f/fredrik-olsson/>Fredrik Olsson</a>
|
<a href=/people/m/magnus-sahlgren/>Magnus Sahlgren</a>
|
<a href=/people/f/fehmi-ben-abdesslem/>Fehmi ben Abdesslem</a>
|
<a href=/people/a/ariel-ekgren/>Ariel Ekgren</a>
|
<a href=/people/k/kristine-eck/>Kristine Eck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aespen-1--5><div class="card-body p-3 small">We cast the problem of event annotation as one of <a href=https://en.wikipedia.org/wiki/Categorization>text categorization</a>, and compare state of the art <a href=https://en.wikipedia.org/wiki/Categorization>text categorization techniques</a> on event data produced within the Uppsala Conflict Data Program (UCDP). Annotating a single text involves assigning the labels pertaining to at least 17 distinct categorization tasks, e.g., who were the attacking organization, who was attacked, and where did the event take place. The text categorization techniques under scrutiny are a classical Bag-of-Words approach ; character-based contextualized embeddings produced by ELMo ; embeddings produced by the BERT base model, and a version of BERT base fine-tuned on UCDP data ; and a pre-trained and fine-tuned classifier based on ULMFiT. The categorization tasks are very diverse in terms of the number of classes to predict as well as the skeweness of the distribution of classes. The <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> results exhibit a large variability across tasks, ranging from 30.3 % to 99.8 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aespen-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aespen-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.7/>Seeing the Forest and the Trees : Detection and Cross-Document Coreference Resolution of Militarized Interstate Disputes</a></strong><br><a href=/people/b/benjamin-radford/>Benjamin Radford</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aespen-1--7><div class="card-body p-3 small">Previous efforts to automate the detection of social and political events in text have primarily focused on identifying events described within single sentences or documents. Within a corpus of documents, these automated systems are unable to link event referencesrecognize singular events across multiple sentences or documents. A separate literature in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> on event coreference resolution attempts to link known events to one another within (and across) documents. I provide a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> for evaluating methods to identify certain political events in text and to link related texts to one another based on shared events. The <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, Headlines of War, is built on the Militarized Interstate Disputes data set and offers headlines classified by dispute status and headline pairs labeled with coreference indicators. Additionally, I introduce a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> capable of accomplishing both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The multi-task convolutional neural network is shown to be capable of recognizing <a href=https://en.wikipedia.org/wiki/Event_(computing)>events</a> and event coreferences given the headlines&#8217; texts and publication dates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aespen-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aespen-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.9/>Supervised Event Coding from Text Written in Arabic : Introducing <a href=https://en.wikipedia.org/wiki/Hadath>Hadath</a><span class=acl-fixed-case>A</span>rabic: Introducing Hadath</a></strong><br><a href=/people/j/javier-osorio/>Javier Osorio</a>
|
<a href=/people/a/alejandro-reyes/>Alejandro Reyes</a>
|
<a href=/people/a/alejandro-beltran/>Alejandro Beltrán</a>
|
<a href=/people/a/atal-ahmadzai/>Atal Ahmadzai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aespen-1--9><div class="card-body p-3 small">This article introduces <a href=https://en.wikipedia.org/wiki/Hadath>Hadath</a>, a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised protocol</a> for coding event data from text written in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Hadath contributes to recent efforts in advancing multi-language event coding using <a href=https://en.wikipedia.org/wiki/Computer-aided_software_engineering>computer-based solutions</a>. In this <a href=https://en.wikipedia.org/wiki/Application_software>application</a>, we focus on extracting <a href=https://en.wikipedia.org/wiki/Event_(computing)>event data</a> about the conflict in Afghanistan from 2008 to 2018 using Arabic information sources. The <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> relies first on a <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning algorithm</a> to classify <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news stories</a> relevant to the <a href=https://en.wikipedia.org/wiki/War_in_Afghanistan_(2001&#8211;present)>Afghan conflict</a>. Then, using Hadath, we implement the Natural Language Processing component for <a href=https://en.wikipedia.org/wiki/Event-driven_programming>event coding</a> from <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic script</a>. The output <a href=https://en.wikipedia.org/wiki/Database>database</a> contains daily geo-referenced information at the district level on who did what to whom, when and where in the Afghan conflict. The <a href=https://en.wikipedia.org/wiki/Data>data</a> helps to identify trends in the dynamics of violence, the provision of <a href=https://en.wikipedia.org/wiki/Governance>governance</a>, and traditional <a href=https://en.wikipedia.org/wiki/Conflict_resolution>conflict resolution</a> in <a href=https://en.wikipedia.org/wiki/Afghanistan>Afghanistan</a> for different actors over time and across space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aespen-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aespen-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aespen-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aespen-1.10/>Protest Event Analysis : A Longitudinal Analysis for Greece<span class=acl-fixed-case>G</span>reece</a></strong><br><a href=/people/k/konstantina-papanikolaou/>Konstantina Papanikolaou</a>
|
<a href=/people/h/harris-papageorgiou/>Haris Papageorgiou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aespen-1--10><div class="card-body p-3 small">The advent of <a href=https://en.wikipedia.org/wiki/Big_data>Big Data</a> has shifted <a href=https://en.wikipedia.org/wiki/Social_science>social science research</a> towards <a href=https://en.wikipedia.org/wiki/Computational_science>computational methods</a>. The volume of data that is nowadays available has brought a radical change in traditional approaches due to the cost and effort needed for processing. Knowledge extraction from heterogeneous and ample data is not an easy task to tackle. Thus, <a href=https://en.wikipedia.org/wiki/Interdisciplinarity>interdisciplinary approaches</a> are necessary, combining experts of both social and computer science. This paper aims to present a work in the context of protest analysis, which falls into the scope of <a href=https://en.wikipedia.org/wiki/Computational_social_science>Computational Social Science</a>. More specifically, the contribution of this work is to describe a Computational Social Science methodology for Event Analysis. The presented <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is generic in the sense that it can be applied in every event typology and moreover, it is innovative and suitable for <a href=https://en.wikipedia.org/wiki/Interdisciplinarity>interdisciplinary tasks</a> as it incorporates the <a href=https://en.wikipedia.org/wiki/Human-in-the-loop>human-in-the-loop</a>. Additionally, a <a href=https://en.wikipedia.org/wiki/Case_study>case study</a> is presented concerning Protest Analysis in Greece over the last two decades. The conceptual foundation lies mainly upon claims analysis, and <a href=https://en.wikipedia.org/wiki/Newspaper>newspaper data</a> were used in order to map, document and discuss protests in Greece in a longitudinal perspective.</div></div></div><hr><div id=2020ai4hi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.ai4hi-1/>Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ai4hi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ai4hi-1.0/>Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access</a></strong><br><a href=/people/y/yalemisew-abgaz/>Yalemisew Abgaz</a>
|
<a href=/people/a/amelie-dorn/>Amelie Dorn</a>
|
<a href=/people/j/jose-luis-preza-diaz/>Jose Luis Preza Diaz</a>
|
<a href=/people/g/gerda-koch/>Gerda Koch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ai4hi-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ai4hi-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ai4hi-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ai4hi-1.3/>Toward the Automatic Retrieval and Annotation of Outsider Art images : A Preliminary Statement</a></strong><br><a href=/people/j/john-roberto/>John Roberto</a>
|
<a href=/people/d/diego-ortego/>Diego Ortego</a>
|
<a href=/people/b/brian-davis/>Brian Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ai4hi-1--3><div class="card-body p-3 small">The aim of this position paper is to establish an initial approach to the automatic classification of digital images about the <a href=https://en.wikipedia.org/wiki/Outsider_art>Outsider Art style of painting</a>. Specifically, we explore whether is it possible to classify non-traditional artistic styles by using the same features that are used for classifying traditional styles? Our research question is motivated by two facts. First, art historians state that non-traditional styles are influenced by factors outside of the world of art. Second, some studies have shown that several <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>artistic styles</a> confound certain classification techniques. Following current approaches to style prediction, this paper utilises Deep Learning methods to encode <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>image features</a>. Our preliminary experiments have provided motivation to think that, as is the case with traditional styles, <a href=https://en.wikipedia.org/wiki/Outsider_art>Outsider Art</a> can be computationally modelled with objective means by using <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training datasets</a> and CNN models. Nevertheless, our results are not conclusive due to the lack of a large available dataset on <a href=https://en.wikipedia.org/wiki/Outsider_art>Outsider Art</a>. Therefore, at the end of the paper, we have mapped future lines of action, which include the compilation of a large dataset of Outsider Art images and the creation of an ontology of Outsider Art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ai4hi-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ai4hi-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ai4hi-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ai4hi-1.5/>Towards a Comprehensive Assessment of the Quality and Richness of the Europeana Metadata of food-related Images</a></strong><br><a href=/people/y/yalemisew-abgaz/>Yalemisew Abgaz</a>
|
<a href=/people/a/amelie-dorn/>Amelie Dorn</a>
|
<a href=/people/j/jose-luis-preza-diaz/>Jose Luis Preza Diaz</a>
|
<a href=/people/g/gerda-koch/>Gerda Koch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ai4hi-1--5><div class="card-body p-3 small">Semantic enrichment of historical images to build interactive AI systems for the Digital Humanities domain has recently gained significant attention. However, before implementing any semantic enrichment tool for building <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>, it is also crucial to analyse the quality and richness of the existing datasets and understand the areas where semantic enrichment is most required. Here, we propose an approach to conducting a preliminary analysis of selected historical images from the Europeana platform using existing linked data quality assessment tools. The analysis targets food images by collecting <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> provided from curators such as Galleries, Libraries, Archives and Museums (GLAMs) and cultural aggregators such as <a href=https://en.wikipedia.org/wiki/Europeana>Europeana</a>. We identified <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to evaluate the quality of the <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> associated with food-related images which are harvested from the Europeana platform. In this paper, we present the food-image dataset, the associated <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> and our proposed method for the assessment. The results of our assessment will be used to guide the current effort to semantically enrich the <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> and build high-quality metadata using <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a>.</div></div></div><hr><div id=2020bucc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.bucc-1/>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.0/>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</a></strong><br><a href=/people/r/reinhard-rapp/>Reinhard Rapp</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/s/serge-sharoff/>Serge Sharoff</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.3/>Constructing a Bilingual Corpus of Parallel Tweets</a></strong><br><a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/s/sabit-hassan/>Sabit Hassan</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--3><div class="card-body p-3 small">In a bid to reach a larger and more diverse audience, Twitter users often post parallel tweetstweets that contain the same content but are written in different languages. Parallel tweets can be an important resource for developing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) systems</a> among other <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tasks</a>. In this paper, we introduce a generic <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for collecting parallel tweets. Using this method, we collect a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabictweets regularly. Since our method is generic, it can also be used for collecting parallel tweets that cover less-resourced languages such as <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a> and <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>. Additionally, we annotate a subset of Twitter accounts with their countries of origin and topic of interest, which provides insights about the population who post parallel tweets. This latter information can also be useful for author profiling tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.4/>Automatic Creation of Correspondence Table of Meaning Tags from Two Dictionaries in One Language Using Bilingual Word Embedding</a></strong><br><a href=/people/t/teruo-hirabayashi/>Teruo Hirabayashi</a>
|
<a href=/people/k/kanako-komiya/>Kanako Komiya</a>
|
<a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/h/hiroyuki-shinnou/>Hiroyuki Shinnou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--4><div class="card-body p-3 small">In this paper, we show how to use bilingual word embeddings (BWE) to automatically create a corresponding table of meaning tags from two dictionaries in one language and examine the effectiveness of the method. To do this, we had a problem : the meaning tags do not always correspond one-to-one because the granularities of the <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> and the <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> are different from each other. Therefore, we regarded the concept tag that corresponds to a <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a> the most as the correct concept tag corresponding the <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>. We used two BWE methods, a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation matrix</a> and VecMap. We evaluated the most frequent sense (MFS) method and the corpus concatenation method for comparison. The accuracies of the proposed methods were higher than the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the random baseline but lower than those of the MFS and corpus concatenation methods. However, because our method utilized the embedding vectors of the word senses, the relations of the sense tags corresponding to concept tags could be examined by mapping the sense embeddings to the vector space of the concept tags. Also, our methods could be performed when we have only concept or word sense embeddings whereas the MFS method requires a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> and the corpus concatenation method needs two tagged corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.bucc-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.6/>Benchmarking Multidomain English-Indonesian Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>I</span>ndonesian Machine Translation</a></strong><br><a href=/people/t/tri-wahyu-guntara/>Tri Wahyu Guntara</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/r/radityo-eko-prasojo/>Radityo Eko Prasojo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--6><div class="card-body p-3 small">In the context of Machine Translation (MT) from-and-to English, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Bahasa Indonesia</a> has been considered a low-resource language, and therefore applying Neural Machine Translation (NMT) which typically requires large training dataset proves to be problematic. In this paper, we show otherwise by collecting large, publicly-available datasets from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, which we split into several domains : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, general, and <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>, to train and benchmark some variants of transformer-based NMT models across the domains. We show using BLEU that our models perform well across them, outperform the baseline Statistical Machine Translation (SMT) models, and perform comparably with <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a>. Our datasets (with the standard split for training, validation, and testing), code, and models are available on<url>https://github.com/gunnxx/indonesian-mt-data</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.7/>Reducing the Search Space for Parallel Sentences in Comparable Corpora</a></strong><br><a href=/people/r/remi-cardon/>Rémi Cardon</a>
|
<a href=/people/n/natalia-grabar/>Natalia Grabar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--7><div class="card-body p-3 small">This paper describes and evaluates simple techniques for reducing the research space for parallel sentences in monolingual comparable corpora. Initially, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because even with a high performing <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, a lot of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We work on a manually annotated subset obtained from a French comparable corpus and show how we can drastically reduce the number of sentence pairs that have to be fed to a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> so that the results can be manually handled.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.9/>TALN / LS2N Participation at the BUCC Shared Task : Bilingual Dictionary Induction from Comparable Corpora<span class=acl-fixed-case>TALN</span>/<span class=acl-fixed-case>LS</span>2<span class=acl-fixed-case>N</span> Participation at the <span class=acl-fixed-case>BUCC</span> Shared Task: Bilingual Dictionary Induction from Comparable Corpora</a></strong><br><a href=/people/m/martin-laville/>Martin Laville</a>
|
<a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--9><div class="card-body p-3 small">This paper describes the TALN / LS2N system participation at the Building and Using Comparable Corpora (BUCC) shared task. We first introduce three strategies : (i) a word embedding approach based on fastText embeddings ; (ii) a concatenation approach using both character Skip-Gram and character CBOW models, and finally (iii) a cognates matching approach based on an exact match string similarity. Then, we present the applied <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> for the shared task which consists in the combination of the embeddings concatenation and the cognates matching approaches. The covered languages are <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Overall, our system mixing embeddings concatenation and perfect cognates matching obtained the best results while compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bucc-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bucc-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bucc-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bucc-1.11/>BUCC2020 : Bilingual Dictionary Induction using Cross-lingual Embedding<span class=acl-fixed-case>BUCC</span>2020: Bilingual Dictionary Induction using Cross-lingual Embedding</a></strong><br><a href=/people/s/sanjanasri-jp/>Sanjanasri JP</a>
|
<a href=/people/v/vijay-krishna-menon/>Vijay Krishna Menon</a>
|
<a href=/people/s/soman-kp/>Soman KP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bucc-1--11><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning system</a> for the BUCC 2020 shared task : Bilingual dictionary induction from comparable corpora. We have submitted two runs for this shared Task, German (de) and English (en) language pair for closed track and Tamil (ta) and English (en) for the open track. Our core approach focuses on quantifying the <a href=https://en.wikipedia.org/wiki/Semantics>semantics of the language pairs</a>, so that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of two different language pairs can be compared or transfer learned. With the advent of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, it is possible to quantify this. In this paper, we propose a deep learning approach which makes use of the supplied training data, to generate cross-lingual embedding. This is later used for inducting <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> from comparable corpora.</div></div></div><hr><div id=2020calcs-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.calcs-1/>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.calcs-1.0/>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/a/amitava-das/>Amitava Das</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.calcs-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--calcs-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.calcs-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.calcs-1.1/>An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines<span class=acl-fixed-case>S</span>panish Newspaper Headlines</a></strong><br><a href=/people/e/elena-alvarez-mellado/>Elena Alvarez-Mellado</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--calcs-1--1><div class="card-body p-3 small">The extraction of anglicisms (lexical borrowings from English) is relevant both for <a href=https://en.wikipedia.org/wiki/Lexicography>lexicographic purposes</a> and for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP downstream tasks</a>. We introduce a corpus of European Spanish newspaper headlines annotated with <a href=https://en.wikipedia.org/wiki/Anglicism>anglicisms</a> and a baseline model for anglicism extraction. In this paper we present : (1) a corpus of 21,570 newspaper headlines written in <a href=https://en.wikipedia.org/wiki/European_Spanish>European Spanish</a> annotated with emergent anglicisms and (2) a conditional random field baseline model with handcrafted features for anglicism extraction. We present the newspaper headlines corpus, describe the annotation tagset and guidelines and introduce a CRF model that can serve as baseline for the task of detecting anglicisms. The presented work is a first step towards the creation of an anglicism extractor for <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Spain>Spanish newswire</a>.</div></div></div><hr><div id=2020cllrd-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cllrd-1/>Proceedings of the LREC 2020 Workshop on "Citizen Linguistics in Language Resource Development"</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cllrd-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cllrd-1.0/>Proceedings of the LREC 2020 Workshop on "Citizen Linguistics in Language Resource Development"</a></strong><br><a href=/people/j/james-fiumara/>James Fiumara</a>
|
<a href=/people/c/christopher-cieri/>Christopher Cieri</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cllrd-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cllrd-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cllrd-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cllrd-1.4/>Speaking Outside the Box : Exploring the Benefits of Unconstrained Input in <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> and Citizen Science Platforms</a></strong><br><a href=/people/j/jon-chamberlain/>Jon Chamberlain</a>
|
<a href=/people/u/udo-kruschwitz/>Udo Kruschwitz</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cllrd-1--4><div class="card-body p-3 small">Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be <a href=https://en.wikipedia.org/wiki/Altruism>altruism</a>, social enhancement, <a href=https://en.wikipedia.org/wiki/Entertainment>entertainment</a> or money. This paper explores how <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> and <a href=https://en.wikipedia.org/wiki/Citizen_science>citizen science systems</a> collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The <a href=https://en.wikipedia.org/wiki/Game_(retailer)>game</a> was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> can be viewed as form of <a href=https://en.wikipedia.org/wiki/Citizen_science>citizen science platform</a> with both constrained and unconstrained inputs making for a highly complex dataset.</div></div></div><hr><div id=2020clssts-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.clssts-1/>Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clssts-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clssts-1.0/>Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020)</a></strong><br><a href=/people/k/kathleen-mckeown/>Kathy McKeown</a>
|
<a href=/people/d/douglas-w-oard/>Douglas W. Oard</a>
|
<a href=/people/e/elizabeth/>Elizabeth</a>
|
<a href=/people/r/richard-schwartz/>Richard Schwartz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clssts-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clssts-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clssts-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clssts-1.4/>SEARCHER : Shared Embedding Architecture for Effective Retrieval<span class=acl-fixed-case>SEARCHER</span>: Shared Embedding Architecture for Effective Retrieval</a></strong><br><a href=/people/j/joel-barry/>Joel Barry</a>
|
<a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/s/scott-miller/>Scott Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clssts-1--4><div class="card-body p-3 small">We describe an approach to <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>cross lingual information retrieval</a> that does not rely on explicit translation of either document or query terms. Instead, both queries and documents are mapped into a shared embedding space where <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> is performed. We discuss potential advantages of the approach in handling <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> and <a href=https://en.wikipedia.org/wiki/Synonym_(taxonomy)>synonymy</a>. We present a method for training the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, and give details of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> implementation. We present experimental results for two cases : Somali-English and Bulgarian-English CLIR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clssts-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clssts-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clssts-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clssts-1.5/>Cross-lingual Information Retrieval with BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/z/zhuolin-jiang/>Zhuolin Jiang</a>
|
<a href=/people/a/amro-el-jaroudi/>Amro El-Jaroudi</a>
|
<a href=/people/w/william-hartmann/>William Hartmann</a>
|
<a href=/people/d/damianos-karakos/>Damianos Karakos</a>
|
<a href=/people/l/lingjun-zhao/>Lingjun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clssts-1--5><div class="card-body p-3 small">Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Document_ranking>document ranking</a>. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> between English queries and foreign-language documents in the task of <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>cross-lingual information retrieval</a>. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective and outperforms the competitive baseline approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clssts-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clssts-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clssts-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clssts-1.6/>A Comparison of <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>Unsupervised Methods</a> for Ad hoc Cross-Lingual Document Retrieval</a></strong><br><a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/m/mark-granroth-wilding/>Mark Granroth-Wilding</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clssts-1--6><div class="card-body p-3 small">We address the problem of linking related documents across languages in a <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>multilingual collection</a>. We evaluate three diverse unsupervised methods to represent and compare documents : (1) multilingual topic model ; (2) cross-lingual document embeddings ; and (3) <a href=https://en.wikipedia.org/wiki/Wasserstein_distance>Wasserstein distance</a>. We test the performance of these methods in retrieving news articles in Swedish that are known to be related to a given Finnish article. The results show that ensembles of the methods outperform the stand-alone methods, suggesting that they capture complementary characteristics of the documents</div></div></div><hr><div id=2020cmlc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cmlc-1/>Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmlc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmlc-1.0/>Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora</a></strong><br><a href=/people/p/piotr-banski/>Piotr Bański</a>
|
<a href=/people/a/adrien-barbaresi/>Adrien Barbaresi</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/m/marc-kupietz/>Marc Kupietz</a>
|
<a href=/people/h/harald-lungen/>Harald Lüngen</a>
|
<a href=/people/i/ines-pisetta/>Ines Pisetta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmlc-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmlc-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmlc-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmlc-1.4/>Geoparsing the historical Gazetteers of Scotland : accurately computing location in mass digitised texts<span class=acl-fixed-case>S</span>cotland: accurately computing location in mass digitised texts</a></strong><br><a href=/people/r/rosa-filgueira/>Rosa Filgueira</a>
|
<a href=/people/c/claire-grover/>Claire Grover</a>
|
<a href=/people/m/melissa-terras/>Melissa Terras</a>
|
<a href=/people/b/beatrice-alex/>Beatrice Alex</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmlc-1--4><div class="card-body p-3 small">This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing (NLP) tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition (NER) techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmlc-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmlc-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmlc-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmlc-1.5/>The Corpus Query Middleware of Tomorrow A Proposal for a Hybrid Corpus Query Architecture</a></strong><br><a href=/people/m/markus-gartner/>Markus Gärtner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmlc-1--5><div class="card-body p-3 small">Development of dozens of specialized corpus query systems and languages over the past decades has let to a diverse but also fragmented landscape. Today we are faced with a plethora of query tools that each provide unique features, but which are also not interoperable and often rely on very specific database back-ends or formats for storage. This severely hampers usability both for end users that want to query different corpora and also for corpus designers that wish to provide users with an interface for querying and exploration. We propose a hybrid corpus query architecture as a first step to overcoming this issue. It takes the form of a <a href=https://en.wikipedia.org/wiki/Middleware>middleware system</a> between user front-ends and optional database or text indexing solutions as <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>back-ends</a>. At its core is a custom query evaluation engine for index-less processing of corpus queries. With a flexible JSON-LD query protocol the approach allows communication with <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>back-end systems</a> to partially solve queries and offset some of the performance penalties imposed by the custom evaluation engine. This paper outlines the details of our first draft of aforementioned <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmlc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmlc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmlc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmlc-1.6/>Using <a href=https://en.wikipedia.org/wiki/Full-text_search>full text indices</a> for querying spoken language data</a></strong><br><a href=/people/e/elena-frick/>Elena Frick</a>
|
<a href=/people/t/thomas-schmidt/>Thomas Schmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmlc-1--6><div class="card-body p-3 small">As a part of the ZuMult-project, we are currently modelling a backend architecture that should provide query access to corpora from the Archive of Spoken German (AGD) at the Leibniz-Institute for the German Language (IDS). We are exploring how to reuse existing search engine frameworks providing full text indices and allowing to query corpora by one of the corpus query languages (QLs) established and actively used in the corpus research community. For this purpose, we tested MTAS-an open source Lucene-based search engine for querying on text with multilevel annotations. We applied MTAS on three oral corpora stored in the TEI-based ISO standard for transcriptions of spoken language (ISO 24624:2016). These corpora differ from the corpus data that MTAS was developed for, because they include interactions with two and more speakers and are enriched, inter alia, with timeline-based annotations. In this contribution, we report our test results and address issues that arise when search frameworks originally developed for querying <a href=https://en.wikipedia.org/wiki/Text_corpus>written corpora</a> are being transferred into the field of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmlc-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmlc-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmlc-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmlc-1.8/>Czech National Corpus in 2020 : Recent Developments and Future Outlook<span class=acl-fixed-case>C</span>zech National Corpus in 2020: Recent Developments and Future Outlook</a></strong><br><a href=/people/m/michal-kren/>Michal Kren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmlc-1--8><div class="card-body p-3 small">The paper overviews the state of implementation of the Czech National Corpus (CNC) in all the main areas of its operation : <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus compilation</a>, <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, <a href=https://en.wikipedia.org/wiki/Application_software>application development</a> and user services. As the focus is on the recent development, some of the areas are described in more detail than the others. Close attention is paid to the <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> and, in particular, to the description of <a href=https://en.wikipedia.org/wiki/Web_application_development>web application development</a>. This is not only because <a href=https://en.wikipedia.org/wiki/Numerical_control>CNC</a> has recently seen a significant progress in this area, but also because we believe that end-user web applications shape the way linguists and other scholars think about the language data and about the range of possibilities they offer. This consideration is even more important given the variability of the <a href=https://en.wikipedia.org/wiki/Numerical_analysis>CNC corpora</a>.</div></div></div><hr><div id=2020computerm-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.computerm-1/>Proceedings of the 6th International Workshop on Computational Terminology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.computerm-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.computerm-1.0/>Proceedings of the 6th International Workshop on Computational Terminology</a></strong><br><a href=/people/b/beatrice-daille/>Béatrice Daille</a>
|
<a href=/people/k/kyo-kageura/>Kyo Kageura</a>
|
<a href=/people/a/ayla-rigouts-terryn/>Ayla Rigouts Terryn</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.computerm-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--computerm-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.computerm-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.computerm-1.7/>A study of semantic projection from single word terms to multi-word terms in the environment domain</a></strong><br><a href=/people/y/yizhe-wang/>Yizhe Wang</a>
|
<a href=/people/b/beatrice-daille/>Beatrice Daille</a>
|
<a href=/people/n/nabil-hathout/>Nabil Hathout</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--computerm-1--7><div class="card-body p-3 small">The semantic projection method is often used in <a href=https://en.wikipedia.org/wiki/Terminology>terminology structuring</a> to infer semantic relations between terms. Semantic projection relies upon the assumption of semantic compositionality : the relation that links simple term pairs remains valid in pairs of complex terms built from these simple terms. This paper proposes to investigate whether this assumption commonly adopted in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is actually valid. First, we describe the process of constructing a list of semantically linked multi-word terms (MWTs) related to the environmental field through the extraction of semantic variants. Second, we present our analysis of the results from the semantic projection. We find that contexts play an essential role in defining the relations between MWTs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.computerm-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--computerm-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.computerm-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.computerm-1.14/>TermEval 2020 : RACAI’s automatic term extraction system<span class=acl-fixed-case>T</span>erm<span class=acl-fixed-case>E</span>val 2020: <span class=acl-fixed-case>RACAI</span>’s automatic term extraction system</a></strong><br><a href=/people/v/vasile-pais/>Vasile Pais</a>
|
<a href=/people/r/radu-ion/>Radu Ion</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--computerm-1--14><div class="card-body p-3 small">This paper describes RACAI&#8217;s automatic term extraction system, which participated in the TermEval 2020 shared task on English monolingual term extraction. We discuss the <a href=https://en.wikipedia.org/wiki/Systems_architecture>system architecture</a>, some of the challenges that we faced as well as present our results in the English competition.</div></div></div><hr><div id=2020framenet-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.framenet-1/>Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.framenet-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.framenet-1.0/>Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet</a></strong><br><a href=/people/t/tiago-timponi-torrent/>Tiago T. Torrent</a>
|
<a href=/people/c/collin-f-baker/>Collin F. Baker</a>
|
<a href=/people/o/oliver-czulo/>Oliver Czulo</a>
|
<a href=/people/k/kyoko-ohara/>Kyoko Ohara</a>
|
<a href=/people/m/miriam-r-l-petruck/>Miriam R. L. Petruck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.framenet-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--framenet-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.framenet-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.framenet-1.2/>Finding Corresponding Constructions in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> in a TED Talk Parallel Corpus using Frames-and-Constructions Analysis<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>J</span>apanese in a <span class=acl-fixed-case>TED</span> Talk Parallel Corpus using Frames-and-Constructions Analysis</a></strong><br><a href=/people/k/kyoko-ohara/>Kyoko Ohara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--framenet-1--2><div class="card-body p-3 small">This paper reports on an effort to search for corresponding constructions in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> in a TED Talk parallel corpus, using frames-and-constructions analysis (Ohara, 2019 ; Ohara and Okubo, 2020 ; cf. Czulo, 2013, 2017). The purpose of the paper is two-fold : (1) to demonstrate the validity of frames-and-constructions analysis to search for corresponding constructions in typologically unrelated languages ; and (2) to assess whether the Do schools kill creativity? TED Talk parallel corpus, annotated in various languages for Multilingual FrameNet, is a good starting place for building a multilingual constructicon. The analysis showed that similar to our previous findings involving texts in a Japanese to English bilingual children&#8217;s book, the TED Talk bilingual transcripts include pairs of constructions that share similar pragmatic functions. While the TED Talk parallel corpus constitutes a good resource for frame semantic annotation in multiple languages, it may not be the ideal place to start aligning constructions among typologically unrelated languages. Finally, this work shows that the proposed method, which focuses on heads of sentences, seems valid for searching for corresponding constructions in transcripts of spoken data, as well as in written data of typologically-unrelated languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.framenet-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--framenet-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.framenet-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.framenet-1.7/>Greek within the Global FrameNet Initiative : Challenges and Conclusions so far<span class=acl-fixed-case>G</span>reek within the Global <span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Initiative: Challenges and Conclusions so far</a></strong><br><a href=/people/v/voula-giouli/>Voula Giouli</a>
|
<a href=/people/v/vera-pilitsidou/>Vera Pilitsidou</a>
|
<a href=/people/h/hephaestion-christopoulos/>Hephaestion Christopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--framenet-1--7><div class="card-body p-3 small">Large coverage lexical resources that bear deep linguistic information have always been considered useful for many natural language processing (NLP) applications including Machine Translation (MT). In this respect, Frame-based resources have been developed for many languages following Frame Semantics and the Berkeley FrameNet project. However, to a great extent, all those efforts have been kept fragmented. Consequentially, the Global FrameNet initiative has been conceived of as a joint effort to bring together <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNets</a> in different languages. The proposed paper is aimed at describing ongoing work towards developing the Greek (EL) counterpart of the Global FrameNet and our efforts to contribute to the Shared Annotation Task. In the paper, we will elaborate on the <a href=https://en.wikipedia.org/wiki/Annotation>annotation methodology</a> employed, the current status and progress made so far, as well as the problems raised during <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.framenet-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--framenet-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.framenet-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.framenet-1.11/>Exploring Crosslinguistic Frame Alignment</a></strong><br><a href=/people/c/collin-f-baker/>Collin F. Baker</a>
|
<a href=/people/a/arthur-lorenzi/>Arthur Lorenzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--framenet-1--11><div class="card-body p-3 small">The FrameNet (FN) project at the International Computer Science Institute in Berkeley (ICSI), which documents the core vocabulary of contemporary English, was the first lexical resource based on Fillmore&#8217;s theory of <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame Semantics</a>. Berkeley FrameNet has inspired related projects in roughly a dozen other languages, which have evolved somewhat independently ; the current Multilingual FrameNet project (MLFN) is an attempt to find alignments between all of them. The alignment problem is complicated by the fact that these projects have adhered to the Berkeley FrameNet model to varying degrees, and they were also founded at different times, when different versions of the Berkeley FrameNet data were available. We describe several new <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for finding relations of similarity between semantic frames across languages. We will demonstrate ViToXF, a new tool which provides interactive visualizations of these cross-lingual relations, between frames, lexical units, and frame elements, based on resources such as multilingual dictionaries and on shared distributional vector spaces, making clear the strengths and weaknesses of different alignment methods.</div></div></div><hr><div id=2020gamnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.gamnlp-1/>Workshop on Games and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.0/>Workshop on Games and Natural Language Processing</a></strong><br><a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.1/>Creating a Sentiment Lexicon with Game-Specific Words for Analyzing NPC Dialogue in The Elder Scrolls V : Skyrim<span class=acl-fixed-case>NPC</span> Dialogue in The Elder Scrolls <span class=acl-fixed-case>V</span>: Skyrim</a></strong><br><a href=/people/t/therese-bergsma/>Thérèse Bergsma</a>
|
<a href=/people/j/judith-van-stegeren/>Judith van Stegeren</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--1><div class="card-body p-3 small">A weak point of rule-based sentiment analysis systems is that the underlying sentiment lexicons are often not adapted to the domain of the text we want to analyze. We created a game-specific sentiment lexicon for video game Skyrim based on the E-ANEW word list and a dataset of Skyrim&#8217;s in-game documents. We calculated sentiment ratings for NPC dialogue using both our <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> and E-ANEW and compared the resulting sentiment ratings to those of human raters. Both lexicons perform comparably well on our evaluation dialogues, but the game-specific extension performs slightly better on the dominance dimension for dialogue segments and the arousal dimension for full dialogues. To our knowledge, this is the first time that a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis lexicon</a> has been adapted to the <a href=https://en.wikipedia.org/wiki/Video_game>video game domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.2/>ClueMeIn : Obtaining More Specific Image Labels Through a Game<span class=acl-fixed-case>C</span>lue<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>I</span>n: Obtaining More Specific Image Labels Through a Game</a></strong><br><a href=/people/c/christopher-harris/>Christopher Harris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--2><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/ESP_Game>ESP Game</a> (also known as the Google Image Labeler) demonstrated how the crowd could perform a task that is straightforward for humans but challenging for computers providing labels for images. The game facilitated the task of basic image labeling ; however, the labels generated were non-specific and limited the ability to distinguish similar images from one another, limiting its ability in search tasks, annotating images for the visually impaired, and training computer vision machine algorithms. In this paper, we describe ClueMeIn, an entertaining web-based game with a purpose that generates more detailed image labels than the <a href=https://en.wikipedia.org/wiki/ESP_Game>ESP Game</a>. We conduct experiments to generate specific image labels, show how the results can lead to improvements in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of image searches over image labels generated by the <a href=https://en.wikipedia.org/wiki/ESP_Game>ESP Game</a> when using the same public dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.3/>Cipher : A Prototype Game-with-a-Purpose for Detecting Errors in Text<span class=acl-fixed-case>C</span>ipher: A Prototype Game-with-a-Purpose for Detecting Errors in Text</a></strong><br><a href=/people/l/liang-xu/>Liang Xu</a>
|
<a href=/people/j/jon-chamberlain/>Jon Chamberlain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--3><div class="card-body p-3 small">Errors commonly exist in machine-generated documents and publication materials ; however, some correction algorithms do not perform well for complex errors and it is costly to employ humans to do the task. To solve the problem, a prototype <a href=https://en.wikipedia.org/wiki/PC_game>computer game</a> called Cipher was developed that encourages people to identify errors in text. Gamification is achieved by introducing the idea of <a href=https://en.wikipedia.org/wiki/Steganography>steganography</a> as the entertaining game element. People play the <a href=https://en.wikipedia.org/wiki/Game>game</a> for entertainment while they make valuable annotations to locate <a href=https://en.wikipedia.org/wiki/Typographical_error>text errors</a>. The <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> was tested by 35 players in a evaluation experiment, creating 4,764 annotations. After filtering the data, the <a href=https://en.wikipedia.org/wiki/System>system</a> detected manually introduced text errors and also genuine errors in the texts that were not noticed when they were introduced into the <a href=https://en.wikipedia.org/wiki/Game>game</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.4/>Game Design Evaluation of GWAPs for Collecting Word Associations<span class=acl-fixed-case>GWAP</span>s for Collecting Word Associations</a></strong><br><a href=/people/m/mathieu-lafourcade/>Mathieu Lafourcade</a>
|
<a href=/people/l/le-brun-nathalie/>Le Brun Nathalie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--4><div class="card-body p-3 small">GWAP design might have a tremendous effect on its popularity of course but also on the quality of the data collected. In this paper, a comparison is undertaken between two GWAPs for building term association lists, namely JeuxDeMots and Quicky Goose. After comparing both game designs, the <a href=https://en.wikipedia.org/wiki/Cohen_kappa>Cohen kappa</a> of associative lists in various configurations is computed in order to assess likeness and differences of the data they provide.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.5/>The Challenge of the <a href=https://en.wikipedia.org/wiki/Game_show>TV game</a> La Ghigliottina to NLP<span class=acl-fixed-case>TV</span> game La Ghigliottina to <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/f/federico-sangati/>Federico Sangati</a>
|
<a href=/people/a/antonio-pascucci/>Antonio Pascucci</a>
|
<a href=/people/j/johanna-monti/>Johanna Monti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--5><div class="card-body p-3 small">In this paper, we describe a Telegram bot, Mago della Ghigliottina (Ghigliottina Wizard), able to solve La Ghigliottina game (The Guillotine), the final game of the Italian TV quiz show L&#8217;Eredit. Our system relies on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>linguistic resources</a> and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> and achieves better results than human players (and competitors of L&#8217;Eredit too). In addition to solving a <a href=https://en.wikipedia.org/wiki/Game>game</a>, Mago della Ghigliottina can also generate new <a href=https://en.wikipedia.org/wiki/Game>game instances</a> and challenge the users to match the solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.gamnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--gamnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.gamnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.gamnlp-1.6/>A 3D Role-Playing Game for Abusive Language Annotation<span class=acl-fixed-case>D</span> Role-Playing Game for Abusive Language Annotation</a></strong><br><a href=/people/f/federico-bonetti/>Federico Bonetti</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--gamnlp-1--6><div class="card-body p-3 small">Gamification has been applied to many linguistic annotation tasks, as an alternative to <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platforms</a> to collect annotated data in an inexpensive way. However, we think that still much has to be explored. Games with a Purpose (GWAPs) tend to lack important elements that we commonly see in commercial games, such as 2D and 3D worlds or a <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>story</a>. Making GWAPs more similar to full-fledged video games in order to involve users more easily and increase dissemination is a demanding yet interesting ground to explore. In this paper we present a <a href=https://en.wikipedia.org/wiki/Role-playing_video_game>3D role-playing game</a> for abusive language annotation that is currently under development.</div></div></div><hr><div id=2020globalex-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.globalex-1/>Proceedings of the 2020 Globalex Workshop on Linked Lexicography</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.0/>Proceedings of the 2020 Globalex Workshop on Linked Lexicography</a></strong><br><a href=/people/i/ilan-kernerman/>Ilan Kernerman</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/j/jorge-gracia/>Jorge Gracia</a>
|
<a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/b/besim-kabashi/>Besim Kabashi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.1/>Modelling Frequency and Attestations for OntoLex-Lemon<span class=acl-fixed-case>O</span>nto<span class=acl-fixed-case>L</span>ex-Lemon</a></strong><br><a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/m/maxim-ionov/>Maxim Ionov</a>
|
<a href=/people/j/jesse-de-does/>Jesse de Does</a>
|
<a href=/people/k/katrien-depuydt/>Katrien Depuydt</a>
|
<a href=/people/f/fahad-khan/>Anas Fahad Khan</a>
|
<a href=/people/s/sander-stolk/>Sander Stolk</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--1><div class="card-body p-3 small">The OntoLex vocabulary enjoys increasing popularity as a means of publishing lexical resources with <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a> and as <a href=https://en.wikipedia.org/wiki/Linked_data>Linked Data</a>. The recent publication of a new OntoLex module for <a href=https://en.wikipedia.org/wiki/Lexicography>lexicography</a>, lexicog, reflects its increasing importance for digital lexicography. However, not all aspects of digital lexicography have been covered to the same extent. In particular, supplementary information drawn from <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> such as frequency information, links to attestations, and collocation data were considered to be beyond the scope of lexicog. Therefore, the OntoLex community has put forward the proposal for a novel module for frequency, attestation and corpus information (FrAC), that not only covers the requirements of digital lexicography, but also accommodates essential data structures for lexical information in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. This paper introduces the current state of the OntoLex-FrAC vocabulary, describes its structure, some selected use cases, elementary concepts and fundamental definitions, with a focus on frequency and attestations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.5/>Towards an Extension of the Linking of the Open Dutch WordNet with Dutch Lexicographic Resources<span class=acl-fixed-case>D</span>utch <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et with <span class=acl-fixed-case>D</span>utch Lexicographic Resources</a></strong><br><a href=/people/t/thierry-declerck/>Thierry Declerck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--5><div class="card-body p-3 small">This extended abstract presents on-going work consisting in interlinking and merging the Open Dutch WordNet and generic lexicographic resources for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, focusing for now on the Dutch and English versions of <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a> and using the Algemeen Nederlands Woordenboek as a quality checking instance. As the Open Dutch WordNet is already equipped with a relevant number of complex lexical units, we are aiming at expanding it and proposing a new representational framework for the encoding of the interlinked and integrated data. The longer term goal of the work is to investigate if and on how senses can be restricted to particular morphological variations of Dutch lexical entries, and how to represent this information in a Linguistic Linked Open Data compliant format.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.6/>Widening the Discussion on False Friends in Multilingual Wordnets<span class=acl-fixed-case>F</span>riends” in Multilingual Wordnets</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a>
|
<a href=/people/a/ana-luis/>Ana Luís</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--6><div class="card-body p-3 small">There are wordnets in many languages, many aligned with Princeton WordNet, some of which in a (semi-)automatic process, but we rarely see actual discussions on the role of false friends in this process. Having in mind known issues related to such words in <a href=https://en.wikipedia.org/wiki/Translation>language translation</a>, and further motivated by false friend-related issues on the alignment of a Portuguese wordnet with Princeton Wordnet, we aim to widen this discussion, while suggesting preliminary ideas of how wordnets could benefit from this kind of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.8/>Building Sense Representations in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> by Combining Word Embeddings with Lexical Resources<span class=acl-fixed-case>D</span>anish by Combining Word Embeddings with Lexical Resources</a></strong><br><a href=/people/i/ida-rormann-olsen/>Ida Rørmann Olsen</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--8><div class="card-body p-3 small">Our aim is to identify suitable sense representations for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. We investigate sense inventories that correlate with human interpretations of word meaning and <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> as typically described in dictionaries and wordnets and that are well reflected distributionally as expressed in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. To this end, we study a number of highly ambiguous Danish nouns and examine the effectiveness of sense representations constructed by combining <a href=https://en.wikipedia.org/wiki/Vector_space>vectors</a> from a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a> with the information from a <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a>. We establish <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> based on centroids obtained from wordnet synests and example sentences as well as <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> established via are tested in a word sense disambiguation task. We conclude that the more information extracted from the wordnet entries (example sentence, definition, semantic relations) the more successful the sense representation vector.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.globalex-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--globalex-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.globalex-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.globalex-1.16/>Translation Inference by Concept Propagation</a></strong><br><a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/c/christian-fath/>Christian Fäth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--globalex-1--16><div class="card-body p-3 small">This paper describes our contribution to the Third Shared Task on Translation Inference across Dictionaries (TIAD-2020). We describe an approach on translation inference based on <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic methods</a>, the propagation of concepts over a graph of interconnected dictionaries : Given a mapping from source language words to <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical concepts</a> (e.g., synsets) as a seed, we use <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionaries</a> to extrapolate a mapping of pivot and target language words to these <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical concepts</a>. Translation inference is then performed by looking up the lexical concept(s) of a source language word and returning the target language word(s) for which these lexical concepts have the respective highest score. We present two instantiations of this system : One using WordNet synsets as <a href=https://en.wikipedia.org/wiki/Concept>concepts</a>, and one using lexical entries (translations) as <a href=https://en.wikipedia.org/wiki/Concept>concepts</a>. With a threshold of 0, the latter <a href=https://en.wikipedia.org/wiki/Computer_configuration>configuration</a> is the second among participant systems in terms of <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. We also describe additional evaluation experiments on Apertium data, a comparison with an earlier approach based on embedding projection, and an approach for constrained projection that outperforms the TIAD-2020 vanilla system by a large margin.</div></div></div><hr><div id=2020isa-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.isa-1/>16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.isa-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.isa-1.0/>16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS</a></strong><br><a href=/people/h/harry-bunt/>Harry Bunt</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.isa-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--isa-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.isa-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.isa-1.5/>Annotation-based Semantics</a></strong><br><a href=/people/k/kiyong-lee/>Kiyong Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--isa-1--5><div class="card-body p-3 small">This paper proposes a semantics ABS for the model-theoretic interpretation of annotation structures. It provides a language ABSr, that represents semantic forms in a (possibly -free) type-theoretic first-order logic. For semantic compositionality, the representation language introduces two operators and with subtypes for the conjunctive or distributive composition of semantic forms. ABS also introduces a small set of <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>logical predicates</a> to represent semantic forms in a simplified format. The use of ABSr is illustrated with some annotation structures that conform to ISO 24617 standards on semantic annotation such as <a href=https://en.wikipedia.org/wiki/ISO-TimeML>ISO-TimeML</a> and ISO-Space.<i>semantic forms</i> in a (possibly <tex-math>\\lambda</tex-math>-free) type-theoretic first-order logic. For semantic compositionality, the representation language introduces two operators <tex-math>\\oplus</tex-math> and <tex-math>\\oslash</tex-math> with subtypes for the conjunctive or distributive composition of semantic forms. ABS also introduces a small set of logical predicates to represent semantic forms in a simplified format. The use of ABSr is illustrated with some annotation structures that conform to ISO 24617 standards on semantic annotation such as ISO-TimeML and ISO-Space.</div></div></div><hr><div id=2020iwltp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.iwltp-1/>Proceedings of the 1st International Workshop on Language Technology Platforms</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.0/>Proceedings of the 1st International Workshop on Language Technology Platforms</a></strong><br><a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/a/andrejs-vasiljevs/>Andrejs Vasiļjevs</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.5/>CLARIN : Distributed Language Resources and Technology in a European Infrastructure<span class=acl-fixed-case>CLARIN</span>: Distributed Language Resources and Technology in a <span class=acl-fixed-case>E</span>uropean Infrastructure</a></strong><br><a href=/people/m/maria-eskevich/>Maria Eskevich</a>
|
<a href=/people/f/franciska-de-jong/>Franciska de Jong</a>
|
<a href=/people/a/alexander-konig/>Alexander König</a>
|
<a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/d/dieter-van-uytvanck/>Dieter Van Uytvanck</a>
|
<a href=/people/t/tero-aalto/>Tero Aalto</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a>
|
<a href=/people/o/olga-gerassimenko/>Olga Gerassimenko</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/h/henk-van-den-heuvel/>Henk van den Heuvel</a>
|
<a href=/people/n/neeme-kahusk/>Neeme Kahusk</a>
|
<a href=/people/k/krista-liin/>Krista Liin</a>
|
<a href=/people/m/martin-matthiesen/>Martin Matthiesen</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/k/kadri-vider/>Kadri Vider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--5><div class="card-body p-3 small">CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on <a href=https://en.wikipedia.org/wiki/CLARIN>CLARIN</a> as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in <a href=https://en.wikipedia.org/wiki/CLARIN>CLARIN</a>. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.7/>Removing European Language Barriers with Innovative Machine Translation Technology<span class=acl-fixed-case>E</span>uropean Language Barriers with Innovative Machine Translation Technology</a></strong><br><a href=/people/d/dario-franceschini/>Dario Franceschini</a>
|
<a href=/people/c/chiara-canton/>Chiara Canton</a>
|
<a href=/people/i/ivan-simonini/>Ivan Simonini</a>
|
<a href=/people/a/armin-schweinfurth/>Armin Schweinfurth</a>
|
<a href=/people/a/adelheid-glott/>Adelheid Glott</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/t/thai-son-nguyen/>Thai-Son Nguyen</a>
|
<a href=/people/f/felix-schneider/>Felix Schneider</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/p/philip-williams/>Philip Williams</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/sangeet-sagar/>Sangeet Sagar</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/o/otakar-smrz/>Otakar Smrž</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--7><div class="card-body p-3 small">This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for <a href=https://en.wikipedia.org/wiki/Convention_(meeting)>conferences</a> and remote meetings live subtitling. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.9/>The Kairntech Sherpa An ML Platform and <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> for the Enrichment of (not only) Scientific Content<span class=acl-fixed-case>K</span>airntech <span class=acl-fixed-case>S</span>herpa – An <span class=acl-fixed-case>ML</span> Platform and <span class=acl-fixed-case>API</span> for the Enrichment of (not only) Scientific Content</a></strong><br><a href=/people/s/stefan-geissler/>Stefan Geißler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--9><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Computing_platform>software platform</a> and <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> that combines various ML and NLP approaches for the analysis and enrichment of textual content. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>&#8217;s design and implementation is guided by the goal to allow non-technical users to conduct their own experiments and training runs on their respective data, allowing to test, tune and deploy analysis models for production. Dedicated specific packages for subtasks such as document structure processing, document categorization, annotation with existing thesauri, disambiguation and linking, annotation with newly created entity recognizers and summarization available as open source components in isolation are combined into an end-user-facing, collaborative, scalable platform to support large-scale industrial document analysis document analysis. We see the Sherpa&#8217;s setup as an answer to the observation that ML has reached a level of maturity that allows to attain useful results in many analysis scenarios today, but that in-depth technical competencies in the required fields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> is often scarce ; a setup that focusses on non-technical domain-expert end-users can help to bring required analysis functionalities closer to the day-to-day reality in business contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.11/>NTeALan Dictionaries Platforms : An Example Of Collaboration-Based Model<span class=acl-fixed-case>NT</span>e<span class=acl-fixed-case>AL</span>an Dictionaries Platforms: An Example Of Collaboration-Based Model</a></strong><br><a href=/people/e/elvis-mboning/>Elvis Mboning</a>
|
<a href=/people/d/daniel-baleba/>Daniel Baleba</a>
|
<a href=/people/j/jean-marc-bassahak/>Jean Marc Bassahak</a>
|
<a href=/people/o/ornella-wandji/>Ornella Wandji</a>
|
<a href=/people/j/jules-assoumou/>Jules Assoumou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--11><div class="card-body p-3 small">Nowadays the scarcity and dispersion of open-source NLP resources and tools in and for <a href=https://en.wikipedia.org/wiki/Languages_of_Africa>African languages</a> make it difficult for researchers to truly fit these <a href=https://en.wikipedia.org/wiki/Language>languages</a> into current algorithms of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>, resulting in the stagnation of these numerous <a href=https://en.wikipedia.org/wiki/Language>languages</a>, as far as technological progress is concerned. Created in 2017, with the aim of building communities of voluntary contributors around African native and/or national languages, cultures, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP technologies</a> and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and manage their own lexicographic and linguistic resources. This paper aims at presenting the first versions of three lexicographic platforms that we developed in and for African languages : the REST / GraphQL API for saving lexicographic resources, the dictionary management platform and the collaborative dictionary platform. We also describe the <a href=https://en.wikipedia.org/wiki/Data_format>data representation format</a> used for these <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a>. After experimenting with a few dictionaries and looking at users feedback, we are convinced that only collaboration-based approaches and platforms can effectively respond to challenges of producing quality resources in and for African native and/or national languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwltp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwltp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwltp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwltp-1.12/>A Workflow Manager for Complex NLP and Content Curation Workflows<span class=acl-fixed-case>NLP</span> and Content Curation Workflows</a></strong><br><a href=/people/j/julian-moreno-schneider/>Julian Moreno-Schneider</a>
|
<a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/f/florian-kintzel/>Florian Kintzel</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwltp-1--12><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Workflow_management_system>workflow manager</a> for the flexible creation and customisation of NLP processing pipelines. The workflow manager addresses challenges in <a href=https://en.wikipedia.org/wiki/Interoperability>interoperability</a> across various different NLP tasks and hardware-based resource usage. Based on the four key principles of <a href=https://en.wikipedia.org/wiki/Generality>generality</a>, <a href=https://en.wikipedia.org/wiki/Flexibility_(engineering)>flexibility</a>, <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a> and <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>, we present the first version of the workflow manager by providing details on its custom definition language, explaining the communication components and the general system architecture and setup. We currently implement the <a href=https://en.wikipedia.org/wiki/System>system</a>, which is grounded and motivated by real-world industry use cases in several innovation and transfer projects.</div></div></div><hr><div id=2020ldl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.ldl-1/>Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ldl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ldl-1.0/>Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020)</a></strong><br><a href=/people/m/maxim-ionov/>Maxim Ionov</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/j/julia-bosque-gil/>Julia Bosque-Gil</a>
|
<a href=/people/j/jorge-gracia/>Jorge Gracia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ldl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ldl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ldl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ldl-1.2/>Transforming the Cologne Digital Sanskrit Dictionaries into OntoLex-Lemon<span class=acl-fixed-case>S</span>anskrit Dictionaries into <span class=acl-fixed-case>O</span>nto<span class=acl-fixed-case>L</span>ex-Lemon</a></strong><br><a href=/people/f/francisco-mondaca/>Francisco Mondaca</a>
|
<a href=/people/f/felix-rau/>Felix Rau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ldl-1--2><div class="card-body p-3 small">The Cologne Digital Sanskrit Dictionaries (CDSD) is a large collection of complex digitized Sanskrit dictionaries, consisting of over thirty-five works, and is the most prominent collection of Sanskrit dictionaries worldwide. In this paper we evaluate two methods for transforming the <a href=https://en.wikipedia.org/wiki/Conformational_isomerism>CDSD</a> into Ontolex-Lemon based on a modelling exercise. The first method that we evaluate consists of applying <a href=https://en.wikipedia.org/wiki/RDFa>RDFa</a> to the existent TEI-P5 files. The second method consists of transforming the TEI-encoded dictionaries into new files containing RDF triples modelled in OntoLex-Lemon. As a result of the modelling exercise we choose the second method : to transform TEI-encoded lexical data into Ontolex-Lemon by creating new files containing exclusively RDF triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ldl-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ldl-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ldl-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ldl-1.7/>Challenges of Word Sense Alignment : Portuguese Language Resources<span class=acl-fixed-case>P</span>ortuguese Language Resources</a></strong><br><a href=/people/a/ana-salgado/>Ana Salgado</a>
|
<a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/a/alberto-simoes/>Alberto Simões</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a>
|
<a href=/people/r/rute-costa/>Rute Costa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ldl-1--7><div class="card-body p-3 small">This paper reports on an ongoing task of monolingual word sense alignment in which a comparative study between the Portuguese Academy of Sciences Dictionary and the Dicionrio Aberto is carried out in the context of the ELEXIS (European Lexicographic Infrastructure) project. Word sense alignment involves searching for matching senses within dictionary entries of different lexical resources and linking them, which poses significant challenges. The lexicographic criteria are not always entirely consistent within individual dictionaries and even less so across different projects where different options may have been assumed in terms of structure and especially wording techniques of <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>lexicographic glosses</a>. This hinders the task of <a href=https://en.wikipedia.org/wiki/Sensory_nervous_system>matching senses</a>. We aim to present our annotation workflow in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> using the <a href=https://en.wikipedia.org/wiki/Semantic_Web>Semantic Web technologies</a>. The results obtained are useful for the discussion within the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.ldl-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--ldl-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.ldl-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.ldl-1.12/>Lexemes in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> : 2020 status<span class=acl-fixed-case>W</span>ikidata: 2020 status</a></strong><br><a href=/people/f/finn-nielsen/>Finn Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--ldl-1--12><div class="card-body p-3 small">Wikidata now records data about <a href=https://en.wikipedia.org/wiki/Lexeme>lexemes</a>, senses and lexical forms and exposes them as <a href=https://en.wikipedia.org/wiki/Linguistic_Linked_Open_Data>Linguistic Linked Open Data</a>. Since <a href=https://en.wikipedia.org/wiki/Lexeme>lexemes</a> in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> was first established in 2018, this <a href=https://en.wikipedia.org/wiki/Data>data</a> has grown considerable in size. Links between <a href=https://en.wikipedia.org/wiki/Lexeme>lexemes</a> in different languages can be made, e.g., through a <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivation property</a> or <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a>. We present some descriptive statistics about the lexemes of <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>, focusing on the multilingual aspects and show that there are still relatively few multilingual links.</div></div></div><hr><div id=2020lincr-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lincr-1/>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.0/>Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lincr-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lincr-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.1/>Extrapolating Binder Style Word Embeddings to New Words</a></strong><br><a href=/people/j/jacob-turton/>Jacob Turton</a>
|
<a href=/people/d/david-vinson/>David Vinson</a>
|
<a href=/people/r/robert-smith/>Robert Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lincr-1--1><div class="card-body p-3 small">Word embeddings such as Word2Vec not only uniquely identify words but also encode important semantic information about them. However, as single entities they are difficult to interpret and their individual dimensions do not have obvious meanings. A more intuitive and interpretable <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> based on neural representations of words was presented by Binder and colleagues (2016) but is only available for a very limited vocabulary. Previous research (Utsumi, 2018) indicates that Binder features can be predicted for words from their embedding vectors (such as Word2Vec), but only looked at the original Binder vocabulary. This paper aimed to demonstrate that Binder features can effectively be predicted for a large number of new words and that the predicted values are sensible. The results supported this, showing that correlations between predicted feature values were consistent with those in the original Binder dataset. Additionally, vectors of predicted values performed comparatively to established embedding models in tests of word-pair semantic similarity. Being able to predict Binder feature space vectors for any number of new words opens up many uses not possible with the original vocabulary size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lincr-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lincr-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.2/>Towards the First Dyslexic Font in Russian<span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/s/svetlana-alexeeva/>Svetlana Alexeeva</a>
|
<a href=/people/a/aleksandra-dobrego/>Aleksandra Dobrego</a>
|
<a href=/people/v/vladislav-zubov/>Vladislav Zubov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lincr-1--2><div class="card-body p-3 small">Texts comprise a large part of visual information that we process every day, so one of the tasks of <a href=https://en.wikipedia.org/wiki/Language_science>language science</a> is to make them more accessible. However, often the text design process is focused on the font size, but not on its type ; which might be crucial especially for the people with reading disabilities. The current paper represents a study on <a href=https://en.wikipedia.org/wiki/Accessibility>text accessibility</a> and the first attempt to create a research-based accessible font for <a href=https://en.wikipedia.org/wiki/Cyrillic_script>Cyrillic letters</a>. This resulted in the dyslexic-specific font, LexiaD. Its design rests on the reduction of inter-letter similarity of the <a href=https://en.wikipedia.org/wiki/Russian_alphabet>Russian alphabet</a>. In evaluation stage, dyslexic and non-dyslexic children were asked to read sentences from the Children version of the Russian Sentence Corpus. We tested the readability of LexiaD compared to PT Sans and PT Serif fonts. The results showed that all children had some advantage in letter feature extraction and information integration while reading in LexiaD, but <a href=https://en.wikipedia.org/wiki/Lexical_access>lexical access</a> was improved when sentences were rendered in <a href=https://en.wikipedia.org/wiki/PT_Sans>PT Sans</a> or <a href=https://en.wikipedia.org/wiki/PT_Serif>PT Serif</a>. Therefore, in several aspects, LexiaD proved to be faster to read and could be recommended to use by <a href=https://en.wikipedia.org/wiki/Dyslexia>dyslexics</a> who have visual deficiency or those who struggle with text understanding resulting in re-reading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lincr-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lincr-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.6/>The Little Prince in 26 Languages : Towards a Multilingual Neuro-Cognitive Corpus</a></strong><br><a href=/people/s/sabrina-stehwien/>Sabrina Stehwien</a>
|
<a href=/people/l/lena-henke/>Lena Henke</a>
|
<a href=/people/j/john-hale/>John Hale</a>
|
<a href=/people/j/jonathan-brennan/>Jonathan Brennan</a>
|
<a href=/people/l/lars-meyer/>Lars Meyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lincr-1--6><div class="card-body p-3 small">We present the Le Petit Prince Corpus (LPPC), a multi-lingual resource for research in (computational) psycho- and neurolinguistics. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of the children&#8217;s story The Little Prince in 26 languages. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is in the process of being built using state-of-the-art methods for <a href=https://en.wikipedia.org/wiki/Speech_processing>speech and language processing</a> and <a href=https://en.wikipedia.org/wiki/Electroencephalography>electroencephalography (EEG)</a>. The planned release of LPPC dataset will include raw text annotated with dependency graphs in the Universal Dependencies standard, a near-natural-sounding synthetic spoken subset as well as <a href=https://en.wikipedia.org/wiki/Electroencephalography>EEG recordings</a>. We will use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for conducting <a href=https://en.wikipedia.org/wiki/Neurolinguistics>neurolinguistic studies</a> that generalize across a wide range of languages, overcoming <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological constraints</a> to traditional approaches. The planned release of the LPPC combines linguistic and EEG data for many languages using fully automatic methods, and thus constitutes a readily extendable resource that supports cross-linguistic and cross-disciplinary research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lincr-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lincr-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lincr-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lincr-1.8/>Sensorimotor Norms for 506 Russian Nouns<span class=acl-fixed-case>R</span>ussian Nouns</a></strong><br><a href=/people/a/alex-miklashevsky/>Alex Miklashevsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lincr-1--8><div class="card-body p-3 small">Embodied cognitive science suggested a number of <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>variables</a> describing our sensorimotor experience associated with different concepts : modality experience rating (i.e., relationship between words and images of a particular perceptive modalityvisual, auditory, haptic etc.), manipulability (the necessity for an object to interact with human hands in order to perform its function), vertical spatial localization. According to the embodied cognition theory, these semantic variables capture our <a href=https://en.wikipedia.org/wiki/Mental_representation>mental representations</a> and thus should influence <a href=https://en.wikipedia.org/wiki/Word_formation>word learning</a>, processing and <a href=https://en.wikipedia.org/wiki/Word_formation>production</a>. However, it is not clear how these new <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>variables</a> are related to such traditional <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>variables</a> as imageability, age of acquisition (AoA) and <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a>. In the presented database, normative data on the modality (visual, auditory, haptic, olfactory, and gustatory) ratings, vertical spatial localization of the object, manipulability, imageability, age of acquisition, and subjective frequency for 506 Russian nouns are collected. Factor analysis revealed four factors : (1) visual and haptic modality ratings were combined with imageability, manipulability and AoA ; (2) word length, frequency and AoA ; (3) olfactory modality was united with gustatory ; (4) spatial localization only was included in the fourth factor. The database is available online together with a publication describing the method of data collection and data parameters (Miklashevsky, 2018).</div></div></div><hr><div id=2020lr4sshoc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lr4sshoc-1/>Proceedings of the Workshop about Language Resources for the SSH Cloud</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lr4sshoc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lr4sshoc-1.0/>Proceedings of the Workshop about Language Resources for the SSH Cloud</a></strong><br><a href=/people/d/daan-broeder/>Daan Broeder</a>
|
<a href=/people/m/maria-eskevich/>Maria Eskevich</a>
|
<a href=/people/m/monica-monachini/>Monica Monachini</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lr4sshoc-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lr4sshoc-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lr4sshoc-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lr4sshoc-1.5/>Mining Wages in Nineteenth-Century Job Advertisements. The Application of Language Resources and Language Technology to study Economic and Social Inequality</a></strong><br><a href=/people/r/ruben-ros/>Ruben Ros</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/a/auke-rijpma/>Auke Rijpma</a>
|
<a href=/people/r/richard-zijdeman/>Richard Zijdeman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lr4sshoc-1--5><div class="card-body p-3 small">For the analysis of historical wage development, no <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> is available. Job advertisements, as found in <a href=https://en.wikipedia.org/wiki/Newspaper>newspapers</a> can provide insights into what different types of jobs paid, but require <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a> to structure in a format conducive to <a href=https://en.wikipedia.org/wiki/Quantitative_analysis_(finance)>quantitative analysis</a>. In this paper, we report on our experiments to mine <a href=https://en.wikipedia.org/wiki/Wage>wages</a> from 19th century newspaper advertisements and detail the challenges that need to be overcome to perform a socio-economic analysis of textual data sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lr4sshoc-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lr4sshoc-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lr4sshoc-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lr4sshoc-1.7/>EOSC as a game-changer in the Social Sciences and Humanities research activities<span class=acl-fixed-case>EOSC</span> as a game-changer in the Social Sciences and Humanities research activities</a></strong><br><a href=/people/d/donatella-castelli/>Donatella Castelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lr4sshoc-1--7><div class="card-body p-3 small">This paper aims to give some insights on how the European Open Science Cloud (EOSC) will be able to influence the Social Sciences and Humanities (SSH) sector, thus paving the way towards innovation. Points of discussion on how the LRs and RIs community can contribute to the revolution in the practice of research areas are provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lr4sshoc-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lr4sshoc-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lr4sshoc-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lr4sshoc-1.9/>Crossing the SSH Bridge with Interview Data<span class=acl-fixed-case>SSH</span> Bridge with Interview Data</a></strong><br><a href=/people/h/henk-van-den-heuvel/>Henk van den Heuvel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lr4sshoc-1--9><div class="card-body p-3 small">Spoken audio data, such as interview data, is a scientific instrument used by researchers in various disciplines crossing the boundaries of <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> and humanities. In this paper, we will have a closer look at a portal designed to perform speech-to-text conversion on audio recordings through Automatic Speech Recognition (ASR) in the CLARIN infrastructure. Within the cluster cross-domain EU project SSHOC the potential value of such a linguistic tool kit for processing spoken language recording has found uptake in a <a href=https://en.wikipedia.org/wiki/Web_conferencing>webinar</a> about the topic, and in a task addressing audio analysis of panel survey data. The objective of this contribution is to show that the processing of <a href=https://en.wikipedia.org/wiki/Interview_(research)>interviews</a> as a research instrument has opened up a fascinating and fruitful area of collaboration between Social Sciences and Humanities (SSH).</div></div></div><hr><div id=2020lt4gov-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lt4gov-1/>Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4gov-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4gov-1.0/>Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov)</a></strong><br><a href=/people/d/doaa-samy/>Doaa Samy</a>
|
<a href=/people/d/david-perez-fernandez/>David Pérez-Fernández</a>
|
<a href=/people/j/jeronimo-arenas-garcia/>Jerónimo Arenas-García</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4gov-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4gov-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4gov-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4gov-1.1/>Development of Natural Language Processing Tools to Support Determination of Federal Disability Benefits in the U.S.<span class=acl-fixed-case>U</span>.<span class=acl-fixed-case>S</span>.</a></strong><br><a href=/people/b/bart-desmet/>Bart Desmet</a>
|
<a href=/people/j/julia-porcino/>Julia Porcino</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/g/guy-divita/>Guy Divita</a>
|
<a href=/people/e/elizabeth-rasch/>Elizabeth Rasch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4gov-1--1><div class="card-body p-3 small">The disability benefits programs administered by the US Social Security Administration (SSA) receive between 2 and 3 million new applications each year. Adjudicators manually review hundreds of evidence pages per case to determine eligibility based on financial, medical, and functional criteria. Natural Language Processing (NLP) technology is uniquely suited to support this adjudication work and is a critical component of an ongoing inter-agency collaboration between <a href=https://en.wikipedia.org/wiki/Social_Security_Administration>SSA</a> and the National Institutes of Health. This NLP work provides resources and models for <a href=https://en.wikipedia.org/wiki/Document_ranking>document ranking</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and <a href=https://en.wikipedia.org/wiki/Terminology_extraction>terminology extraction</a> in order to automatically identify documents and reports pertinent to a case, and to allow adjudicators to search for and locate desired information quickly. In this paper, we describe our vision for how <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> can impact <a href=https://en.wikipedia.org/wiki/Social_Security_Administration>SSA</a>&#8217;s adjudication process, present the resources and models that have been developed, and discuss some of the benefits and challenges in working with large-scale government data, and its specific properties in the functional domain.</div></div></div><hr><div id=2020lt4hala-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lt4hala-1/>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.0/>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/m/marco-passarotti/>Marco Passarotti</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.6/>Using LatInfLexi for an Entropy-Based Assessment of Predictability in Latin Inflection<span class=acl-fixed-case>L</span>at<span class=acl-fixed-case>I</span>nf<span class=acl-fixed-case>L</span>exi for an Entropy-Based Assessment of Predictability in <span class=acl-fixed-case>L</span>atin Inflection</a></strong><br><a href=/people/m/matteo-pellegrini/>Matteo Pellegrini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--6><div class="card-body p-3 small">This paper presents LatInfLexi, a large inflected lexicon of Latin providing information on all the <a href=https://en.wikipedia.org/wiki/List_of_Latin-script_digraphs>inflected wordforms</a> of 3,348 verbs and 1,038 nouns. After a description of the structure of the resource and some data on its size, the procedure followed to obtain the lexicon from the database of the Lemlat 3.0 morphological analyzer is detailed, as well as the choices made regarding overabundant and defective cells. The way in which the data of LatInfLexi can be exploited in order to perform a quantitative assessment of predictability in Latin verb inflection is then illustrated : results obtained by computing the conditional entropy of guessing the content of a paradigm cell assuming knowledge of one wordform or multiple wordforms are presented in turn, highlighting the descriptive and theoretical relevance of the analysis. Lastly, the paper envisages the advantages of an inclusion of LatInfLexi into the LiLa knowledge base, both for the presented resource and for the knowledge base itself.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lt4hala-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.7/>A Tool for Facilitating OCR Postediting in Historical Documents<span class=acl-fixed-case>OCR</span> Postediting in Historical Documents</a></strong><br><a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/m/mohammad-aboomar/>Mohammad Aboomar</a>
|
<a href=/people/j/jan-buts/>Jan Buts</a>
|
<a href=/people/j/james-hadley/>James Hadley</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--7><div class="card-body p-3 small">Optical character recognition (OCR) for <a href=https://en.wikipedia.org/wiki/Historical_document>historical documents</a> is a complex procedure subject to a unique set of material issues, including inconsistencies in <a href=https://en.wikipedia.org/wiki/Typeface>typefaces</a> and low quality scanning. Consequently, even the most sophisticated <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR engines</a> produce errors. This paper reports on a tool built for postediting the output of Tesseract, more specifically for correcting common errors in digitized historical documents. The proposed <a href=https://en.wikipedia.org/wiki/Tool>tool</a> suggests alternatives for <a href=https://en.wikipedia.org/wiki/Linguistic_description>word forms</a> not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary, 1719). As demonstrated below, the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> is successful in correcting a number of common errors. If sometimes unreliable, <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> is also transparent and subject to human intervention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.10/>A Thesaurus for Biblical Hebrew<span class=acl-fixed-case>H</span>ebrew</a></strong><br><a href=/people/m/miriam-azar/>Miriam Azar</a>
|
<a href=/people/a/aliza-pahmer/>Aliza Pahmer</a>
|
<a href=/people/j/joshua-waxman/>Joshua Waxman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--10><div class="card-body p-3 small">We built a thesaurus for <a href=https://en.wikipedia.org/wiki/Biblical_Hebrew>Biblical Hebrew</a>, with connections between roots based on phonetic, semantic, and distributional similarity. To this end, we apply established <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to find connections between <a href=https://en.wikipedia.org/wiki/Headword>headwords</a> based on existing lexicons and other digital resources. For semantic similarity, we utilize the cosine-similarity of tf-idf vectors of English gloss text of Hebrew headwords from Ernest Klein&#8217;s A Comprehensive Etymological Dictionary of the Hebrew Language for Readers of English as well as to Brown-Driver-Brigg&#8217;s Hebrew Lexicon. For phonetic similarity, we digitize part of Matityahu Clark&#8217;s Etymological Dictionary of Biblical Hebrew, grouping Hebrew roots into phonemic classes, and establish phonetic relationships between headwords in Klein&#8217;s Dictionary. For distributional similarity, we consider the cosine similarity of PPMI vectors of Hebrew roots and also, in a somewhat novel approach, apply Word2Vec to a Biblical corpus reduced to its lexemes. The resulting resource is helpful to those trying to understand <a href=https://en.wikipedia.org/wiki/Biblical_Hebrew>Biblical Hebrew</a>, and also stands as a good basis for programs trying to process the Biblical text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.lt4hala-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.12/>Comparing Statistical and Neural Models for Learning Sound Correspondences</a></strong><br><a href=/people/c/clementine-fourrier/>Clémentine Fourrier</a>
|
<a href=/people/b/benoit-sagot/>Benoît Sagot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--12><div class="card-body p-3 small">Cognate prediction and proto-form reconstruction are key tasks in computational historical linguistics that rely on the study of sound change regularity. Solving these tasks appears to be very similar to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, though methods from that field have barely been applied to <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. Therefore, in this paper, we investigate the learnability of sound correspondences between a proto-language and daughter languages for two machine-translation-inspired models, one statistical, the other neural. We first carry out our experiments on plausible artificial languages, without <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, in order to study the role of each parameter on the algorithms respective performance under almost perfect conditions. We then study real languages, namely <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, to see if those performances generalise well. We show that both model types manage to learn sound changes despite data scarcity, although the best performing model type depends on several parameters such as the size of the training data, the ambiguity, and the prediction direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.14/>Latin-Spanish Neural Machine Translation : from the Bible to Saint Augustine<span class=acl-fixed-case>L</span>atin-<span class=acl-fixed-case>S</span>panish Neural Machine Translation: from the <span class=acl-fixed-case>B</span>ible to Saint Augustine</a></strong><br><a href=/people/e/eva-martinez-garcia/>Eva Martínez Garcia</a>
|
<a href=/people/a/alvaro-garcia-tejedor/>Álvaro García Tejedor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--14><div class="card-body p-3 small">Although there are several sources where to find historical texts, they usually are available in the original language that makes them generally inaccessible. This paper presents the development of state-of-the-art Neural Machine Systems for the low-resourced Latin-Spanish language pair. First, we build a Transformer-based Machine Translation system on the Bible parallel corpus. Then, we build a comparable corpus from <a href=https://en.wikipedia.org/wiki/Augustine_of_Hippo>Saint Augustine texts</a> and their translations. We use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to study the domain adaptation case from the <a href=https://en.wikipedia.org/wiki/Bible>Bible texts</a> to Saint Augustine&#8217;s works. Results show the difficulties of handling a low-resourced language as Latin. First, we noticed the importance of having enough data, since the <a href=https://en.wikipedia.org/wiki/System>systems</a> do not achieve high BLEU scores. Regarding <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, results show how using in-domain data helps <a href=https://en.wikipedia.org/wiki/System>systems</a> to achieve a better quality translation. Also, we observed that it is needed a higher amount of data to perform an effective vocabulary extension that includes in-domain vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lt4hala-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lt4hala-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lt4hala-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lt4hala-1.19/>A Gradient Boosting-Seq2Seq System for Latin POS Tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a><span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq System for <span class=acl-fixed-case>L</span>atin <span class=acl-fixed-case>POS</span> Tagging and Lemmatization</a></strong><br><a href=/people/g/giuseppe-g-a-celano/>Giuseppe G. A. Celano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lt4hala-1--19><div class="card-body p-3 small">The paper presents the system used in the EvaLatin shared task to POS tag and lemmatize Latin. It consists of two components. A gradient boosting machine (LightGBM) is used for POS tagging, mainly fed with pre-computed word embeddings of a window of seven contiguous tokensthe token at hand plus the three preceding and following onesper target feature value. Word embeddings are trained on the texts of the <a href=https://en.wikipedia.org/wiki/Perseus_Digital_Library>Perseus Digital Library</a>, <a href=https://en.wikipedia.org/wiki/Patrologia_Latina>Patrologia Latina</a>, and Biblioteca Digitale di Testi Tardo Antichi, which together comprise a high number of texts of different genres from the Classical Age to Late Antiquity. Word forms plus the outputted POS labels are used to feed a seq2seq algorithm implemented in <a href=https://en.wikipedia.org/wiki/Keras>Keras</a> to predict lemmas. The final shared-task accuracies measured for Classical Latin texts are in line with state-of-the-art POS taggers (0.96) and lemmatizers (0.95).</div></div></div><hr><div id=2020mmw-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.mmw-1/>Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mmw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mmw-1.0/>Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020)</a></strong><br><a href=/people/t/thierry-declerk/>Thierry Declerk</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a>
|
<a href=/people/g/german-rigau/>German Rigau</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mmw-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mmw-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mmw-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mmw-1.3/>English WordNet 2020 : Improving and Extending a <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> for English using an Open-Source Methodology<span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et 2020: Improving and Extending a <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et for <span class=acl-fixed-case>E</span>nglish using an Open-Source Methodology</a></strong><br><a href=/people/j/john-philip-mccrae/>John Philip McCrae</a>
|
<a href=/people/a/alexandre-rademaker/>Alexandre Rademaker</a>
|
<a href=/people/e/ewa-rudnicka/>Ewa Rudnicka</a>
|
<a href=/people/f/francis-bond/>Francis Bond</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mmw-1--3><div class="card-body p-3 small">WordNet, while one of the most widely used resources for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, has not been updated for a long time, and as such a new project English WordNet has arisen to continue the development of the model under an open-source paradigm. In this paper, we detail the second release of this <a href=https://en.wikipedia.org/wiki/Resource>resource</a> entitled English WordNet 2020. The work has focused firstly, on the introduction of new synsets and senses and developing guidelines for this and secondly, on the integration of contributions from other projects. We present the changes in this edition, which total over 15,000 changes over the previous release.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.mmw-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--mmw-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.mmw-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.mmw-1.7/>Adding <a href=https://en.wikipedia.org/wiki/Pronunciation>Pronunciation Information</a> to Wordnets</a></strong><br><a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/l/lenka-bajcetic/>Lenka Bajcetic</a>
|
<a href=/people/m/melanie-siegel/>Melanie Siegel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--mmw-1--7><div class="card-body p-3 small">We describe on-going work consisting in adding pronunciation information to wordnets, as such <a href=https://en.wikipedia.org/wiki/Information>information</a> can indicate specific senses of a word. Many wordnets associate with their senses only a <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma form</a> and a <a href=https://en.wikipedia.org/wiki/Part-of-speech_tag>part-of-speech tag</a>. At the same time, we are aware that additional linguistic information can be useful for identifying a specific sense of a wordnet lemma when encountered in a corpus. While work already deals with the addition of grammatical number or grammatical gender information to wordnet lemmas, we are investigating the linking of wordnet lemmas to pronunciation information, adding thus a speech-related modality to wordnets</div></div></div><hr><div id=2020multilingualbio-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.multilingualbio-1/>Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.multilingualbio-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.multilingualbio-1.0/>Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)</a></strong><br><a href=/people/m/maite-melero/>Maite Melero</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.multilingualbio-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--multilingualbio-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.multilingualbio-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.multilingualbio-1.1/>Detecting Adverse Drug Events from Swedish Electronic Health Records using <a href=https://en.wikipedia.org/wiki/Text_mining>Text Mining</a><span class=acl-fixed-case>S</span>wedish Electronic Health Records using Text Mining</a></strong><br><a href=/people/m/maria-bampa/>Maria Bampa</a>
|
<a href=/people/h/hercules-dalianis/>Hercules Dalianis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--multilingualbio-1--1><div class="card-body p-3 small">Electronic Health Records are a valuable source of patient information which can be leveraged to detect Adverse Drug Events (ADEs) and aid post-mark drug-surveillance. The overall aim of this study is to scrutinize text written by clinicians in the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a> and build a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for ADE detection that produces medically relevant predictions. Natural Language Processing techniques will be exploited to create important <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>predictors</a> and incorporate them into the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a>. The study focuses on the 5 most frequent ADE cases found ina Swedish electronic patient record corpus. The results indicate that considering textual features, rather than the structured, can improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance by 15 % in some ADE cases. Additionally, variable patient history lengths are incorporated in the models, demonstrating the importance of the above decision rather than using an arbitrary number for a history length. The experimental findings suggest that the clinical text in <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a> includes information that can capture data beyond the ones that are found in a structured format.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.multilingualbio-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--multilingualbio-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.multilingualbio-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.multilingualbio-1.3/>Localising the Clinical Terminology SNOMED CT by Semi-automated Creation of a German Interface Vocabulary<span class=acl-fixed-case>SNOMED</span> <span class=acl-fixed-case>CT</span> by Semi-automated Creation of a <span class=acl-fixed-case>G</span>erman Interface Vocabulary</a></strong><br><a href=/people/s/stefan-schulz/>Stefan Schulz</a>
|
<a href=/people/l/larissa-hammer/>Larissa Hammer</a>
|
<a href=/people/d/david-hashemian-nik/>David Hashemian-Nik</a>
|
<a href=/people/m/markus-kreuzthaler/>Markus Kreuzthaler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--multilingualbio-1--3><div class="card-body p-3 small">Medical language exhibits great variations regarding users, institutions and language registers. With large parts of clinical documents in free text, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is playing a more and more important role in unlocking re-usable and interoperable meaning from <a href=https://en.wikipedia.org/wiki/Medical_record>medical records</a>. This study describes the architectural principles and the evolution of a German interface vocabulary, combining <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology <a href=https://en.wikipedia.org/wiki/SNOMED_CT>SNOMED CT</a>, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of medical jargon in order to optimise terminology grounding of <a href=https://en.wikipedia.org/wiki/Medicine>clinical texts</a> by <a href=https://en.wikipedia.org/wiki/Text_mining>text mining systems</a>. The core resource is a manually curated table of English-to-German word and chunk translations, supported by a set of language generation rules. The work describes a <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> consisting the enrichment and modification of this table with human and machine efforts, manually enriched by grammarspecific tags. Top-down and bottom-up methods for terminology population used in parallel. The final interface terms are produced by a term generator, which creates one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of more realistic terms or terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> constituted by text snippets from different clinical documents. With overall low retrieval parameters (with F-values around 30 %), the performance of the German language scenario reaches 80 90 % of the English one. Interestingly, annotations are slightly better with machine-translated (German English) texts, using the International SNOMED CT resource only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.multilingualbio-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--multilingualbio-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.multilingualbio-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.multilingualbio-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.multilingualbio-1.4/>Multilingual enrichment of disease biomedical ontologies</a></strong><br><a href=/people/l/leo-bouscarrat/>Léo Bouscarrat</a>
|
<a href=/people/a/antoine-bonnefoy/>Antoine Bonnefoy</a>
|
<a href=/people/c/cecile-capponi/>Cécile Capponi</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--multilingualbio-1--4><div class="card-body p-3 small">Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects : <a href=https://en.wikipedia.org/wiki/Coverage_(telecommunication)>coverage</a> and <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>. We look at the coverage of two biomedical ontologies focusing on diseases with respect to <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both, plus <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> for the second. We first use direct links between <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> and the studied <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> with a commercial machine translation tool, here Google Cloud Translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.multilingualbio-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--multilingualbio-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.multilingualbio-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.multilingualbio-1.6/>Automated Processing of Multilingual Online News for the Monitoring of Animal Infectious Diseases</a></strong><br><a href=/people/s/sarah-valentin/>Sarah Valentin</a>
|
<a href=/people/r/renaud-lancelot/>Renaud Lancelot</a>
|
<a href=/people/m/mathieu-roche/>Mathieu Roche</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--multilingualbio-1--6><div class="card-body p-3 small">The Platform for Automated extraction of animal Disease Information from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> (PADI-web) is an automated system which monitors the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> for monitoring and detecting emerging animal infectious diseases. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> automatically collects news via customised multilingual queries, classifies them and extracts <a href=https://en.wikipedia.org/wiki/Epidemiology>epidemiological information</a>. We detail the processing of multilingual online sources by PADI-web and analyse the translated outputs in a case study</div></div></div><hr><div id=2020onion-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.onion-1/>Proceedings of LREC2020 Workshop "People in language, vision and the mind" (ONION2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.onion-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.onion-1.0/>Proceedings of LREC2020 Workshop "People in language, vision and the mind" (ONION2020)</a></strong><br><a href=/people/p/patrizia-paggio/>Patrizia Paggio</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.onion-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--onion-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.onion-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.onion-1.2/>Analysis of Body Behaviours in Human-Human and Human-Robot Interactions</a></strong><br><a href=/people/t/taiga-mori/>Taiga Mori</a>
|
<a href=/people/k/kristiina-jokinen/>Kristiina Jokinen</a>
|
<a href=/people/y/yasuharu-den/>Yasuharu Den</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--onion-1--2><div class="card-body p-3 small">We conducted preliminary comparison of human-robot (HR) interaction with human-human (HH) interaction conducted in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. As the result, body gestures increased in <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>HR</a>, while hand and head gestures decreased in <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>HR</a>. Concerning <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gesture</a>, they were composed of more diverse and complex forms, trajectories and functions in HH than in HR. Moreover, <a href=https://en.wikipedia.org/wiki/English_language>English speakers</a> produced 6 times more <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gestures</a> than <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese speakers</a> in HH. Regarding head gesture, even though there was no difference in the frequency of head gestures between <a href=https://en.wikipedia.org/wiki/English_language>English speakers</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese speakers</a> in HH, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese speakers</a> produced slightly more nodding during the robot&#8217;s speaking than <a href=https://en.wikipedia.org/wiki/English_language>English speakers</a> in HR. Furthermore, positions of nod were different depending on the language. Concerning <a href=https://en.wikipedia.org/wiki/Gesture>body gesture</a>, participants produced <a href=https://en.wikipedia.org/wiki/Gesture>body gestures</a> mostly to regulate appropriate distance with the robot in <a href=https://en.wikipedia.org/wiki/Human_resources>HR</a>. Additionally, <a href=https://en.wikipedia.org/wiki/English_language>English speakers</a> produced slightly more <a href=https://en.wikipedia.org/wiki/List_of_gestures>body gestures</a> than <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese speakers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.onion-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--onion-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.onion-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.onion-1.5/>Improving <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> with Biofeedback Data</a></strong><br><a href=/people/d/daniel-schlor/>Daniel Schlör</a>
|
<a href=/people/a/albin-zehe/>Albin Zehe</a>
|
<a href=/people/k/konstantin-kobs/>Konstantin Kobs</a>
|
<a href=/people/b/blerta-veseli/>Blerta Veseli</a>
|
<a href=/people/f/franziska-westermeier/>Franziska Westermeier</a>
|
<a href=/people/l/larissa-brubach/>Larissa Brübach</a>
|
<a href=/people/d/daniel-roth/>Daniel Roth</a>
|
<a href=/people/m/marc-erich-latoschik/>Marc Erich Latoschik</a>
|
<a href=/people/a/andreas-hotho/>Andreas Hotho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--onion-1--5><div class="card-body p-3 small">Humans frequently are able to read and interpret emotions of others by directly taking verbal and non-verbal signals in human-to-human communication into account or to infer or even experience <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> from mediated stories. For <a href=https://en.wikipedia.org/wiki/Computer>computers</a>, however, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> is a complex problem : Thoughts and feelings are the roots of many <a href=https://en.wikipedia.org/wiki/Behavior>behavioural responses</a> and they are deeply entangled with <a href=https://en.wikipedia.org/wiki/Neurophysiology>neurophysiological changes</a> within humans. As such, <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text-based sentiment analysis often rely on incorporating sentiment lexicons or <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to capture the contextual meaning. This paper explores if and how we further can enhance <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> using <a href=https://en.wikipedia.org/wiki/Biofeedback>biofeedback</a> of humans which are experiencing <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> while reading texts. Specifically, we record the <a href=https://en.wikipedia.org/wiki/Heart_rate>heart rate</a> and <a href=https://en.wikipedia.org/wiki/Neural_oscillation>brain waves</a> of readers that are presented with short texts which have been annotated with the emotions they induce. We use these <a href=https://en.wikipedia.org/wiki/Physiology>physiological signals</a> to improve the performance of a lexicon-based sentiment classifier. We find that the combination of several <a href=https://en.wikipedia.org/wiki/Biosignal>biosignals</a> can improve the ability of a text-based classifier to detect the presence of a sentiment in a text on a per-sentence level.</div></div></div><hr><div id=2020osact-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.osact-1/>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.0/>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</a></strong><br><a href=/people/h/hend-al-khalifa/>Hend Al-Khalifa</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.osact-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.2/>AraBERT : Transformer-based Model for Arabic Language Understanding<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span>: Transformer-based Model for <span class=acl-fixed-case>A</span>rabic Language Understanding</a></strong><br><a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--2><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> is a morphologically rich language with relatively few resources and a less explored syntax compared to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, provided they are pre-trained on a very large corpus. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> in the pursuit of achieving the same success that BERT did for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. The performance of AraBERT is compared to multilingual BERT from <a href=https://en.wikipedia.org/wiki/Google>Google</a> and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.5/>From Arabic Sentiment Analysis to Sarcasm Detection : The ArSarcasm Dataset<span class=acl-fixed-case>A</span>rabic Sentiment Analysis to Sarcasm Detection: The <span class=acl-fixed-case>A</span>r<span class=acl-fixed-case>S</span>arcasm Dataset</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--5><div class="card-body p-3 small">Sarcasm is one of the main challenges for sentiment analysis systems. Its <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> comes from the <a href=https://en.wikipedia.org/wiki/Opinion>expression of opinion</a> using implicit indirect phrasing. In this paper, we present ArSarcasm, an Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains 10,547 tweets, 16 % of which are <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic</a>. In addition to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> the <a href=https://en.wikipedia.org/wiki/Data>data</a> was annotated for sentiment and dialects. Our analysis shows the highly subjective nature of these tasks, which is demonstrated by the shift in sentiment labels based on annotators&#8217; biases. Experiments show the degradation of state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysers</a> when faced with sarcastic content. Finally, we train a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> for sarcasm detection using BiLSTM. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.46, which shows the challenging nature of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and should act as a basic baseline for future research on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.9/>ALT Submission for OSACT Shared Task on Offensive Language Detection<span class=acl-fixed-case>ALT</span> Submission for <span class=acl-fixed-case>OSACT</span> Shared Task on Offensive Language Detection</a></strong><br><a href=/people/s/sabit-hassan/>Sabit Hassan</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/a/ammar-rashed/>Ammar Rashed</a>
|
<a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--9><div class="card-body p-3 small">In this paper, we describe our efforts at OSACT Shared Task on Offensive Language Detection. The shared <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> consists of two subtasks : offensive language detection (Subtask A) and hate speech detection (Subtask B). For offensive language detection, a system combination of Support Vector Machines (SVMs) and <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Networks (DNNs)</a> achieved the best results on development set, which ranked 1st in the official results for Subtask A with F1-score of 90.51 % on the test set. For hate speech detection, DNNs were less effective and a system combination of multiple SVMs with different parameters achieved the best results on development set, which ranked 4th in official results for Subtask B with F1-macro score of 80.63 % on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.10/>ASU_OPTO at OSACT4-Offensive Language Detection for Arabic text<span class=acl-fixed-case>ASU</span>_<span class=acl-fixed-case>OPTO</span> at <span class=acl-fixed-case>OSACT</span>4 - Offensive Language Detection for <span class=acl-fixed-case>A</span>rabic text</a></strong><br><a href=/people/a/amr-keleg/>Amr Keleg</a>
|
<a href=/people/s/samhaa-r-el-beltagy/>Samhaa R. El-Beltagy</a>
|
<a href=/people/m/mahmoud-khalil/>Mahmoud Khalil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--10><div class="card-body p-3 small">In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage. Having a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning based model</a> that is able to filter offensive Arabic content is of high need nowadays. In this paper, we describe the model that was submitted to the Shared Task on Offensive Language Detection that is organized by (The 4th Workshop on Open-Source Arabic Corpora and Processing Tools). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> makes use transformer based model (BERT) to detect offensive content. We came in the fourth place in subtask A (detecting Offensive Speech) and in the third place in subtask B (detecting Hate Speech).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.osact-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--osact-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.osact-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.osact-1.16/>Multi-Task Learning using AraBert for Offensive Language Detection<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>B</span>ert for Offensive Language Detection</a></strong><br><a href=/people/m/marc-djandji/>Marc Djandji</a>
|
<a href=/people/f/fady-baly/>Fady Baly</a>
|
<a href=/people/w/wissam-antoun/>Wissam Antoun</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--osact-1--16><div class="card-body p-3 small">The use of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> has become more prevalent, which has provided tremendous opportunities for people to connect but has also opened the door for misuse with the spread of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a>. This phenomenon has been driving more and more people to more extreme reactions and online aggression, sometimes causing physical harm to individuals or groups of people. There is a need to control and prevent such misuse of <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a> through automatic detection of profane language. The shared task on Offensive Language Detection at the OSACT4 has aimed at achieving state of art profane language detection methods for Arabic social media. Our team BERTologists tackled this problem by leveraging state of the art pretrained Arabic language model, AraBERT, that we augment with the addition of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task learning</a> to enable our model to learn efficiently from little data. Our Multitask AraBERT approach achieved the second place in both subtasks A & B, which shows that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs consistently across different tasks.</div></div></div><hr><div id=2020parlaclarin-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.parlaclarin-1/>Proceedings of the Second ParlaCLARIN Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.parlaclarin-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.parlaclarin-1.0/>Proceedings of the Second ParlaCLARIN Workshop</a></strong><br><a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/m/maria-eskevich/>Maria Eskevich</a>
|
<a href=/people/f/franciska-de-jong/>Franciska de Jong</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.parlaclarin-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--parlaclarin-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.parlaclarin-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.parlaclarin-1.4/>Compiling Czech Parliamentary Stenographic Protocols into a Corpus<span class=acl-fixed-case>C</span>zech Parliamentary Stenographic Protocols into a Corpus</a></strong><br><a href=/people/b/barbora-hladka/>Barbora Hladka</a>
|
<a href=/people/m/matyas-kopp/>Matyáš Kopp</a>
|
<a href=/people/p/pavel-stranak/>Pavel Straňák</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--parlaclarin-1--4><div class="card-body p-3 small">The Parliament of the Czech Republic consists of two chambers : the Chamber of Deputies (Lower House) and the Senate (Upper House). In our work, we focus on agenda and documents that relate to the Chamber of Deputies exclusively. We pay particular attention to <a href=https://en.wikipedia.org/wiki/Shorthand>stenographic protocols</a> that record the Chamber of Deputies&#8217; meetings. Our overall goal is to (1) compile the protocols into a ParlaCLARIN TEI encoded corpus, (2) make this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> accessible and searchable in the TEITOK web-based platform, (3) annotate the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> using the modules available in TEITOK, e.g. detect and recognize <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, and (4) highlight the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> in TEITOK. In addition, we add two more goals that we consider innovative : (5) update the corpus every time a new stenographic protocol is published online by the Chambers of Deputies and (6) expose the annotations as the <a href=https://en.wikipedia.org/wiki/Linked_open_data>linked open data</a> in order to improve the protocols&#8217; interoperability with other existing <a href=https://en.wikipedia.org/wiki/Linked_open_data>linked open data</a>. This paper is devoted to the goals (1) and (5).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.parlaclarin-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--parlaclarin-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.parlaclarin-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.parlaclarin-1.7/>Who mentions whom? Recognizing political actors in proceedings</a></strong><br><a href=/people/l/lennart-kerkvliet/>Lennart Kerkvliet</a>
|
<a href=/people/j/jaap-kamps/>Jaap Kamps</a>
|
<a href=/people/m/maarten-marx/>Maarten Marx</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--parlaclarin-1--7><div class="card-body p-3 small">We show that it is straightforward to train a state of the art named entity tagger (spaCy) to recognize political actors in Dutch parliamentary proceedings with high accuracy. The <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagger</a> was trained on 3.4 K manually labeled examples, which were created in a modest 2.5 days work. This resource is made available on github. Besides proper nouns of persons and <a href=https://en.wikipedia.org/wiki/Political_party>political parties</a>, the tagger can recognize quite complex definite descriptions referring to cabinet ministers, ministries, and parliamentary committees. We also provide a demo search engine which employs the tagged entities in its <a href=https://en.wikipedia.org/wiki/Search_engine_results_page>SERP</a> and result summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.parlaclarin-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--parlaclarin-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.parlaclarin-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.parlaclarin-1.13/>Querying a large annotated corpus of parliamentary debates</a></strong><br><a href=/people/s/sascha-diwersy/>Sascha Diwersy</a>
|
<a href=/people/g/giancarlo-luxardo/>Giancarlo Luxardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--parlaclarin-1--13><div class="card-body p-3 small">The TAPS corpus makes it possible to share a large volume of French parliamentary data. The TEI-compliant approach behind its design choices facilitates the publishing and the interoperability of data, but also the implementation of exploratory data analysis techniques in order to process institutional or political discourse. We demonstrate its application to the debates occurred in the context of a specific legislative process, which generated a strong opposition.</div></div></div><hr><div id=2020rail-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.rail-1/>Proceedings of the first workshop on Resources for African Indigenous Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.0/>Proceedings of the first workshop on Resources for African Indigenous Languages</a></strong><br><a href=/people/r/rooweither-mabuya/>Rooweither Mabuya</a>
|
<a href=/people/p/phathutshedzo-ramukhadi/>Phathutshedzo Ramukhadi</a>
|
<a href=/people/m/mmasibidi-setaka/>Mmasibidi Setaka</a>
|
<a href=/people/v/valencia-wagner/>Valencia Wagner</a>
|
<a href=/people/m/menno-van-zaanen/>Menno van Zaanen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rail-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rail-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.1/>Endangered African Languages Featured in a Digital Collection : The Case of the Khomani San, Hugh Brody Collection<span class=acl-fixed-case>A</span>frican Languages Featured in a Digital Collection: The Case of the ǂ<span class=acl-fixed-case>K</span>homani <span class=acl-fixed-case>S</span>an, <span class=acl-fixed-case>H</span>ugh <span class=acl-fixed-case>B</span>rody <span class=acl-fixed-case>C</span>ollection</a></strong><br><a href=/people/k/kerry-jones/>Kerry Jones</a>
|
<a href=/people/s/sanjin-muftic/>Sanjin Muftic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rail-1--1><div class="card-body p-3 small">The Khomani San, Hugh Brody Collection features the voices and history of indigenous hunter gatherer descendants in three endangered languages namely, N|uu, Kora and <a href=https://en.wikipedia.org/wiki/Khoekhoe_language>Khoekhoe</a> as well as a regional dialect of Afrikaans. A large component of this collection is audio-visual (legacy media) recordings of interviews conducted with members of the community by Hugh Brody and his colleagues between 1997 and 2012, referring as far back as the 1800s. The Digital Library Services team at the University of Cape Town aim to showcase the collection digitally on the UCT-wide Digital Collections platform, Ibali which runs on Omeka-S. In this paper we highlight the importance of such a collection in the context of <a href=https://en.wikipedia.org/wiki/South_Africa>South Africa</a>, and the ethical steps that were taken to ensure the respect of the Khomani San as their stories get uploaded onto a repository and become accessible to all. We will also feature some of the completed <a href=https://en.wikipedia.org/wiki/Collection_(publishing)>collection</a> on Ibali and guide the reader through the organisation of the <a href=https://en.wikipedia.org/wiki/Collection_(publishing)>collection</a> on the Omeka-S backend. Finally, we will outline our development process, from <a href=https://en.wikipedia.org/wiki/Digitization>digitisation</a> to repository publishing as well as present some of the challenges in data clean-up, the curation of legacy media, multi-lingual support, and site organisation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rail-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rail-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.4/>Complex Setswana Parts of Speech Tagging</a></strong><br><a href=/people/g/gabofetswe-malema/>Gabofetswe Malema</a>
|
<a href=/people/b/boago-okgetheng/>Boago Okgetheng</a>
|
<a href=/people/b/bopaki-tebalo/>Bopaki Tebalo</a>
|
<a href=/people/m/moffat-motlhanka/>Moffat Motlhanka</a>
|
<a href=/people/g/goaletsa-rammidi/>Goaletsa Rammidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rail-1--4><div class="card-body p-3 small">Setswana language is one of the <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu languages</a> written disjunctively. Some of its <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts of speech</a> such as qualificatives and some <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> are made up of multiple words. That is, the <a href=https://en.wikipedia.org/wiki/Part_of_speech>part of speech</a> is made up of a group of words. The disjunctive style of writing poses a challenge when a sentence is tokenized or when <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a>. A few studies have been done on identification of multi-word parts of speech. In this study we go further to tokenize complex parts of speech which are formed by extending basic forms of multi-word parts of speech. The parts of speech are extended by recursively concatenating more parts of speech to a basic form of parts of speech. We developed <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>rules</a> for building complex relative parts of speech. A morphological analyzer and Python NLTK are used to tag individual words and basic forms of multi-word parts of speech. Developed <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are then used to identify complex parts of speech. Results from a 300 sentence text files give a performance of 74 %. The <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> fails when it encounters expansion rules not implemented and when <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a> by the morphological analyzer is incorrect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rail-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rail-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.5/>Comparing Neural Network Parsers for a Less-resourced and Morphologically-rich Language : Amharic Dependency Parser<span class=acl-fixed-case>A</span>mharic Dependency Parser</a></strong><br><a href=/people/b/binyam-ephrem-seyoum/>Binyam Ephrem Seyoum</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a>
|
<a href=/people/b/baye-yimam-mekonnen/>Baye Yimam Mekonnen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rail-1--5><div class="card-body p-3 small">In this paper, we compare four state-of-the-art neural network dependency parsers for the Semitic language Amharic. As <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> is a morphologically-rich and less-resourced language, the out-of-vocabulary (OOV) problem will be higher when we develop data-driven models. This fact limits researchers to develop <a href=https://en.wikipedia.org/wiki/Neural_network>neural network parsers</a> because the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> requires large quantities of data to train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We empirically evaluate neural network parsers when a small Amharic treebank is used for training. Based on our experiment, we obtain an 83.79 LAS score using the UDPipe system. Better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is achieved when the neural parsing system uses external resources like <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. Using such <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a>, the LAS score for UDPipe improves to 85.26. Our experiment shows that the <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> can learn dependency relations better from limited data while segmentation and POS tagging require much data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rail-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rail-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.6/>Mobilizing Metadata : Open Data Kit (ODK) for Language Resource Development in East Africa<span class=acl-fixed-case>ODK</span>) for Language Resource Development in <span class=acl-fixed-case>E</span>ast <span class=acl-fixed-case>A</span>frica</a></strong><br><a href=/people/r/richard-griscom/>Richard Griscom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rail-1--6><div class="card-body p-3 small">Linguistic fieldworkers collect and archive metadata as part of the language resources (LRs) that they create, but they often work in resource-constrained environments that prevent them from using <a href=https://en.wikipedia.org/wiki/Computer>computers</a> for <a href=https://en.wikipedia.org/wiki/Data_entry_clerk>data entry</a>. In such situations, linguists must complete time-consuming and error-prone digitization tasks that limit the quantity and quality of the resources and metadata that they produce (Thieberger & Berez 2012 ; Margetts & Margetts 2012). This paper describes a method for entering <a href=https://en.wikipedia.org/wiki/Metadata>linguistic metadata</a> into <a href=https://en.wikipedia.org/wiki/Mobile_device>mobile devices</a> using the Open Data Kit (ODK) platform, a suite of open source tools designed for mobile data collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.rail-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--rail-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.rail-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.rail-1.7/>A Computational Grammar of Ga<span class=acl-fixed-case>G</span>a</a></strong><br><a href=/people/l/lars-hellan/>Lars Hellan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--rail-1--7><div class="card-body p-3 small">The paper describes aspects of an HPSG style computational grammar of the West African language Ga (a <a href=https://en.wikipedia.org/wiki/Kwa_languages>Kwa language</a> spoken in the Accra area of Ghana). As a Volta Basin Kwa language, Ga features many types of multiverb expressions and other particular constructional patterns in the verbal and nominal domain. The paper highlights theoretical and formal features of the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> motivated by these phenomena, some of them possibly innovative to the <a href=https://en.wikipedia.org/wiki/Formal_grammar>formal framework</a>. As a so-called deep grammar of the language, it hosts a rich lexical structure, and we describe ways in which the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> builds on previously available lexical resources. We outline an environment of current resources in which the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> is part, and lines of research and development in which it and its environment can be used.</div></div></div><hr><div id=2020readi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.readi-1/>Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.0/>Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)</a></strong><br><a href=/people/n/nuria-gala/>Núria Gala</a>
|
<a href=/people/r/rodrigo-wilkens/>Rodrigo Wilkens</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--readi-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.readi-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.4/>Automatically Assess Children’s Reading Skills</a></strong><br><a href=/people/o/ornella-mich/>Ornella Mich</a>
|
<a href=/people/n/nadia-mana/>Nadia Mana</a>
|
<a href=/people/r/roberto-gretter/>Roberto Gretter</a>
|
<a href=/people/m/marco-matassoni/>Marco Matassoni</a>
|
<a href=/people/d/daniele-falavigna/>Daniele Falavigna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--readi-1--4><div class="card-body p-3 small">Assessing reading skills is an important task teachers have to perform at the beginning of a new scholastic year to evaluate the starting level of the class and properly plan next learning activities. Digital tools based on automatic speech recognition (ASR) may be really useful to support teachers in this task, currently very time consuming and prone to human errors. This paper presents a <a href=https://en.wikipedia.org/wiki/Web_application>web application</a> for automatically assessing fluency and accuracy of oral reading in children attending Italian primary and lower secondary schools. Our system, based on ASR technology, implements the Cornoldi&#8217;s MT battery, which is a well-known Italian test to assess reading skills. The front-end of the <a href=https://en.wikipedia.org/wiki/System>system</a> has been designed following the participatory design approach by involving end users from the beginning of the creation process. Teachers may use our system to both test student&#8217;s reading skills and monitor their performance over time. In fact, the system offers an effective graphical visualization of the assessment results for both individual students and entire class. The paper also presents the results of a pilot study to evaluate the <a href=https://en.wikipedia.org/wiki/System>system</a> usability with teachers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--readi-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.readi-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.5/>Text Simplification to Help Individuals with Low Vision Read More Fluently</a></strong><br><a href=/people/l/lauren-sauvan/>Lauren Sauvan</a>
|
<a href=/people/n/natacha-stolowy/>Natacha Stolowy</a>
|
<a href=/people/c/carlos-aguilar/>Carlos Aguilar</a>
|
<a href=/people/t/thomas-francois/>Thomas François</a>
|
<a href=/people/n/nuria-gala/>Núria Gala</a>
|
<a href=/people/f/frederic-matonti/>Frédéric Matonti</a>
|
<a href=/people/e/eric-castet/>Eric Castet</a>
|
<a href=/people/a/aurelie-calabrese/>Aurélie Calabrèse</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--readi-1--5><div class="card-body p-3 small">The objective of this work is to introduce text simplification as a potential reading aid to help improve the poor reading performance experienced by visually impaired individuals. As a first step, we explore what makes a text especially complex when read with <a href=https://en.wikipedia.org/wiki/Visual_impairment>low vision</a>, by assessing the individual effect of three word properties (frequency, orthographic similarity and length) on reading speed in the presence of Central visual Field Loss (CFL). Individuals with bilateral CFL induced by <a href=https://en.wikipedia.org/wiki/Macular_degeneration>macular diseases</a> read pairs of French sentences displayed with the self-paced reading method. For each sentence pair, sentence n contained a target word matched with a synonym word of the same length included in sentence n+1. Reading time was recorded for each target word. Given the corpus we used, our results show that (1) word frequency has a significant effect on reading time (the more frequent the faster the reading speed) with larger amplitude (in the range of seconds) compared to normal vision ; (2) word neighborhood size has a significant effect on reading time (the more neighbors the slower the reading speed), this effect being rather small in amplitude, but interestingly reversed compared to normal vision ; (3) word length has no significant effect on reading time. Supporting the development of new and more effective <a href=https://en.wikipedia.org/wiki/Assistive_technology>assistive technology</a> to help <a href=https://en.wikipedia.org/wiki/Visual_impairment>low vision</a> is an important and timely issue, with massive potential implications for social and rehabilitation practices. The end goal of this project will be to use our findings to custom text simplification to this specific population and use it as an optimal and efficient reading aid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--readi-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.readi-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.10/>LagunTest : A NLP Based Application to Enhance Reading Comprehension<span class=acl-fixed-case>L</span>agun<span class=acl-fixed-case>T</span>est: A <span class=acl-fixed-case>NLP</span> Based Application to Enhance Reading Comprehension</a></strong><br><a href=/people/k/kepa-bengoetxea/>Kepa Bengoetxea</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a>
|
<a href=/people/a/amaia-aguirregoitia/>Amaia Aguirregoitia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--readi-1--10><div class="card-body p-3 small">The ability to read and understand written texts plays an important role in <a href=https://en.wikipedia.org/wiki/Education>education</a>, above all in the last years of primary education. This is especially pertinent in <a href=https://en.wikipedia.org/wiki/Language_immersion>language immersion educational programmes</a>, where some students have low linguistic competence in the languages of instruction. In this context, adapting the texts to the individual needs of each student requires a considerable effort by education professionals. However, <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> can facilitate the laborious adaptation of materials in order to enhance <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. In this paper, we present LagunTest, a NLP based application that takes as input a text in Basque or <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and offers synonyms, definitions, examples of the words in different contexts and presents some linguistic characteristics as well as visualizations. LagunTest is based on reusable and open multilingual and multimodal tools, and it is also distributed with an open license. LagunTest is intended to ease the burden of education professionals in the task of adapting materials, and the output should always be supervised by them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--readi-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.readi-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.11/>A Lexical Simplification Tool for Promoting Health Literacy</a></strong><br><a href=/people/l/leonardo-zilio/>Leonardo Zilio</a>
|
<a href=/people/l/liana-braga-paraguassu/>Liana Braga Paraguassu</a>
|
<a href=/people/l/luis-antonio-leiva-hercules/>Luis Antonio Leiva Hercules</a>
|
<a href=/people/g/gabriel-ponomarenko/>Gabriel Ponomarenko</a>
|
<a href=/people/l/laura-berwanger/>Laura Berwanger</a>
|
<a href=/people/m/maria-jose-bocorny-finatto/>Maria José Bocorny Finatto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--readi-1--11><div class="card-body p-3 small">This paper presents MedSimples, an authoring tool that combines <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>Corpus Linguistics</a> and <a href=https://en.wikipedia.org/wiki/Terminology>Terminology</a> to help writers to convert health-related information into a more accessible version for people with low literacy skills. MedSimples applies parsing methods associated with lexical resources to automatically evaluate a text and present simplification suggestions that are more suitable for the target audience. Using the suggestions provided by the <a href=https://en.wikipedia.org/wiki/Tool>tool</a>, the author can adapt the original text and make it more accessible. The focus of MedSimples lies on texts for special purposes, so that it not only deals with <a href=https://en.wikipedia.org/wiki/Vocabulary>general vocabulary</a>, but also with <a href=https://en.wikipedia.org/wiki/Specialization_(functional)>specialized terms</a>. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> is currently under development, but an online working prototype exists and can be tested freely. An assessment of MedSimples was carried out aiming at evaluating its current performance with some promising results, especially for informing the future developments that are planned for the tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.readi-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--readi-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.readi-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.readi-1.12/>A multi-lingual and cross-domain analysis of features for <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a></a></strong><br><a href=/people/r/regina-stodden/>Regina Stodden</a>
|
<a href=/people/l/laura-kallmeyer/>Laura Kallmeyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--readi-1--12><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, or proportion of POS tags. These <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are however mainly developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we investigate their relevance for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> within the web domain corpora. Overall, the negative statistical tests regarding the other <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.</div></div></div><hr><div id=2020restup-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.restup-1/>Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.restup-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.restup-1.0/>Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language</a></strong><br><a href=/people/j/johanna-monti/>Johanna Monti</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/m/maria-pia-di-buono/>Maria Pia Di Buono</a>
|
<a href=/people/r/raffaele-manna/>Raffaele Manna</a>
|
<a href=/people/a/antonio-pascucci/>Antonio Pascucci</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.restup-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--restup-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.restup-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.restup-1.1/>Profiling Bots, Fake News Spreaders and Haters</a></strong><br><a href=/people/p/paolo-rosso/>Paolo Rosso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--restup-1--1><div class="card-body p-3 small">Author profiling studies how language is shared by people. Stylometry techniques help in identifying aspects such as gender, age, <a href=https://en.wikipedia.org/wiki/First_language>native language</a>, or even <a href=https://en.wikipedia.org/wiki/Personality>personality</a>. Author profiling is a problem of growing importance, not only in marketing and forensics, but also in <a href=https://en.wikipedia.org/wiki/Computer_security>cybersecurity</a>. The aim is not only to identify users whose messages are potential threats from a terrorism viewpoint but also those whose messages are a threat from a social exclusion perspective because containing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> etc. Bots often play a key role in spreading <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, as well as <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, with the purpose of polarizing the public opinion with respect to controversial issues like <a href=https://en.wikipedia.org/wiki/Brexit>Brexit</a> or the <a href=https://en.wikipedia.org/wiki/2017_Catalan_independence_referendum>Catalan referendum</a>. For instance, the authors of a recent study about the 1 Oct 2017 Catalan referendum, showed that in a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 3.6 million tweets, about 23.6 % of tweets were produced by <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a>. The target of these <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> were pro-independence influencers that were sent negative, emotional and aggressive hateful tweets with hashtags such as # sonunesbesties (i.e. # theyareanimals). Since 2013 at the PAN Lab at CLEF (https://pan.webis.de/) we have addressed several aspects of <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In 2019 we investigated the feasibility of distinguishing whether the author of a <a href=https://en.wikipedia.org/wiki/Twitter>Twitter feed</a> is a bot, while this year we are addressing the problem of profiling those authors that are more likely to spread <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> because they did in the past. We aim at identifying possible <a href=https://en.wikipedia.org/wiki/Fake_news>fake news spreaders</a> as a first step towards preventing <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> from being propagated among online users (fake news aim to polarize the public opinion and may contain hate speech). In 2021 we specifically aim at addressing the challenging problem of profiling haters in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in order to monitor abusive language and prevent cases of <a href=https://en.wikipedia.org/wiki/Social_exclusion>social exclusion</a> in order to combat, for instance, <a href=https://en.wikipedia.org/wiki/Racism>racism</a>, <a href=https://en.wikipedia.org/wiki/Xenophobia>xenophobia</a> and <a href=https://en.wikipedia.org/wiki/Misogyny>misogyny</a>. Although we already started addressing the problem of detecting hate speech when targets are immigrants or women at the HatEval shared task in SemEval-2019, and when targets are women also in the Automatic Misogyny Identification tasks at IberEval-2018, Evalita-2018 and Evalita-2020, it was not done from an author profiling perspective. At the end of the keynote, I will present some insights in order to stress the importance of monitoring abusive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, for instance, in foreseeing sexual crimes. In fact, previous studies confirmed that a correlation might lay between the yearly per capita rate of rape and the misogynistic language used in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.restup-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--restup-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.restup-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.restup-1.2/>An Indian Language Social Media Collection for Hate and Offensive Speech<span class=acl-fixed-case>I</span>ndian Language Social Media Collection for Hate and Offensive Speech</a></strong><br><a href=/people/a/anita-saroj/>Anita Saroj</a>
|
<a href=/people/s/sukomal-pal/>Sukomal Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--restup-1--2><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, people express themselves every day on issues that affect their lives. During the <a href=https://en.wikipedia.org/wiki/Elections_in_Pakistan>parliamentary elections</a>, people&#8217;s interaction with the candidates in social media posts reflects a lot of social trends in a charged atmosphere. People&#8217;s likes and dislikes on leaders, <a href=https://en.wikipedia.org/wiki/Political_party>political parties</a> and their stands often become subject of hate and offensive posts. We collected social media posts in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> during the run-up to the parliamentary election 2019 of India (PEI data-2019). We created a dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> into three categories : <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, offensive and not hate, or not offensive. We report here the initial results of sentiment classification for the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>.</div></div></div><hr><div id=2020signlang-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.signlang-1/>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.0/>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a></strong><br><a href=/people/e/eleni-efthimiou/>Eleni Efthimiou</a>
|
<a href=/people/s/stavroula-evita-fotinea/>Stavroula-Evita Fotinea</a>
|
<a href=/people/t/thomas-hanke/>Thomas Hanke</a>
|
<a href=/people/j/julie-a-hochgesang/>Julie A. Hochgesang</a>
|
<a href=/people/j/jette-kristoffersen/>Jette Kristoffersen</a>
|
<a href=/people/j/johanna-mesch/>Johanna Mesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.1/>Back and Forth between Theory and Application : Shared Phonological Coding Between ASL Signbank and ASL-LEX<span class=acl-fixed-case>ASL</span> <span class=acl-fixed-case>S</span>ignbank and <span class=acl-fixed-case>ASL</span>-<span class=acl-fixed-case>LEX</span></a></strong><br><a href=/people/a/amelia-becker/>Amelia Becker</a>
|
<a href=/people/d/donovan-catt/>Donovan Catt</a>
|
<a href=/people/j/julie-a-hochgesang/>Julie A. Hochgesang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--1><div class="card-body p-3 small">The development of signed language lexical databases, digital organizations that describe different phonological features of and attempt to establish relationships between signs has resulted in a renewed interest in the phonological descriptions used to uniquely identify and organize the lexicons of respective sign languages (van der Kooij, 2002 ; Fenlon et al., 2016 ; Brentari et al., 2018). Throughout the mutually shared coding process involved in organizing two lexical databases, ASL Signbank (Hochgesang, Crasborn and Lillo-Martin, 2020) and ASL-LEX (Caselli et al., 2016), issues have arisen that require revisiting how phonological features and categories are to be applied and even decided upon, and which would adequately distinguish lexical contrast for respective sign languages. The paper concludes by exploring the inverse of the theory-to-database relationship. Examples are given of theoretical implications and research questions that arise from consequences of language resource building. These are presented as evidence that not only does <a href=https://en.wikipedia.org/wiki/Theory>theory</a> impact organization of databases but that the process of database creation can also inform our theories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.4/>Measuring <a href=https://en.wikipedia.org/wiki/Lexical_similarity>Lexical Similarity</a> across <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Languages</a> in Global Signbank<span class=acl-fixed-case>G</span>lobal <span class=acl-fixed-case>S</span>ignbank</a></strong><br><a href=/people/c/carl-borstell/>Carl Börstell</a>
|
<a href=/people/o/onno-crasborn/>Onno Crasborn</a>
|
<a href=/people/l/lori-whynot/>Lori Whynot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--4><div class="card-body p-3 small">Lexicostatistics is the main method used in previous work measuring linguistic distances between sign languages. As a method, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> disregards any possible structural / grammatical similarity, instead focusing exclusively on lexical items, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is time consuming as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires some comparable phonological coding (i.e. form description) as well as concept matching (i.e. meaning description) of signs across the <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> to be compared. In this paper, we present a novel approach for measuring <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> across any two sign languages using the Global Signbank platform, a <a href=https://en.wikipedia.org/wiki/Lexical_database>lexical database</a> of uniformly coded signs. The method involves a feature-by-feature comparison of all matched <a href=https://en.wikipedia.org/wiki/Phonological_feature>phonological features</a>. This method can be used in two distinct ways : 1) automatically comparing the amount of lexical overlap between two <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> (with a more detailed feature-description than previous lexicostatistical methods) ; 2) finding exact form-matches across languages that are either matched or mismatched in meaning (i.e. true or false friends). We show the feasability of this method by comparing three languages (datasets) in Global Signbank, and are currently expanding both the size of these three as well as the total number of datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.6/>PE2LGP Animator : A Tool To Animate A Portuguese Sign Language Avatar<span class=acl-fixed-case>PE</span>2<span class=acl-fixed-case>LGP</span> Animator: A Tool To Animate A <span class=acl-fixed-case>P</span>ortuguese <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Avatar</a></strong><br><a href=/people/p/pedro-cabral/>Pedro Cabral</a>
|
<a href=/people/m/matilde-goncalves/>Matilde Gonçalves</a>
|
<a href=/people/h/hugo-nicolau/>Hugo Nicolau</a>
|
<a href=/people/l/luisa-coheur/>Luísa Coheur</a>
|
<a href=/people/r/ruben-santos/>Ruben Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--6><div class="card-body p-3 small">Software for the production of sign languages is much less common than for <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a>. Such <a href=https://en.wikipedia.org/wiki/Software>software</a> usually relies on 3D humanoid avatars to produce signs which, inevitably, necessitates the use of <a href=https://en.wikipedia.org/wiki/Animation>animation</a>. One barrier to the use of popular animation tools is their complexity and steep learning curve, which can be hard to master for inexperienced users. Here, we present PE2LGP, an <a href=https://en.wikipedia.org/wiki/Authoring_system>authoring system</a> that features a <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>3D avatar</a> that signs <a href=https://en.wikipedia.org/wiki/Portuguese_Sign_Language>Portuguese Sign Language</a>. Our <a href=https://en.wikipedia.org/wiki/Animator>Animator</a> is designed specifically to craft sign language animations using a key frame method, and is meant to be easy to use and learn to users without animation skills. We conducted a preliminary evaluation of the <a href=https://en.wikipedia.org/wiki/Animator>Animator</a>, where we animated seven Portuguese Sign Language sentences and asked four sign language users to evaluate their quality. This evaluation revealed that the <a href=https://en.wikipedia.org/wiki/System>system</a>, in spite of its simplicity, is indeed capable of producing comprehensible messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.7/>Translating an <a href=https://en.wikipedia.org/wiki/Aesop&#8217;s_Fables>Aesop’s Fable</a> to <a href=https://en.wikipedia.org/wiki/Filipino_Sign_Language>Filipino Sign Language</a> through <a href=https://en.wikipedia.org/wiki/Animation>3D Animation</a><span class=acl-fixed-case>A</span>esop’s Fable to <span class=acl-fixed-case>F</span>ilipino <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage through 3<span class=acl-fixed-case>D</span> Animation</a></strong><br><a href=/people/m/mark-cueto/>Mark Cueto</a>
|
<a href=/people/w/winnie-he/>Winnie He</a>
|
<a href=/people/r/rei-untiveros/>Rei Untiveros</a>
|
<a href=/people/j/josh-zuniga/>Josh Zuñiga</a>
|
<a href=/people/j/joanna-pauline-rivera/>Joanna Pauline Rivera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--7><div class="card-body p-3 small">According to the National Statistics Office (2003) in the 2000 Population Census, the deaf community in the Philippines numbered to about 121,000 deaf and hard of hearing Filipinos. Deaf and hard of hearing Filipinos in these communities use the Filipino Sign Language (FSL) as the main method of manual communication. Deaf and hard of hearing children experience difficulty in developing reading and writing skills through traditional methods of teaching used primarily for hearing children. This study aims to translate an <a href=https://en.wikipedia.org/wiki/Aesop&#8217;s_Fables>Aesop&#8217;s fable</a> to <a href=https://en.wikipedia.org/wiki/Filipino_Sign_Language>Filipino Sign Language</a> with the use of <a href=https://en.wikipedia.org/wiki/Animation>3D animation</a> resulting to a video output. The video created contains a 3D animated avatar performing the sign translations to FSL (mainly focusing on hand gestures which includes hand shape, palm orientation, location, and movement) on screen beside their English text equivalent and related images. The final output was then evaluated by FSL deaf signers. Evaluation results showed that the final output can potentially be used as a learning material. In order to make it more effective as a learning material, it is very important to consider the animation&#8217;s appearance, <a href=https://en.wikipedia.org/wiki/Speed>speed</a>, naturalness, and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In this paper, the common action units were also listed for easier construction of animations of the signs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.9/>Elicitation and Corpus of Spontaneous Sign Language Discourse Representation Diagrams</a></strong><br><a href=/people/m/michael-filhol/>Michael Filhol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--9><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Languages</a> have no standard written form, many signers do capture their language in some form of spontaneous graphical form. We list a few <a href=https://en.wikipedia.org/wiki/Use_case>use cases</a> (discourse preparation, deverbalising for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, etc.) and give examples of <a href=https://en.wikipedia.org/wiki/Diagram>diagrams</a>. After hypothesising that they contain regular patterns of significant value, we propose to build a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of such <a href=https://en.wikipedia.org/wiki/Production_(economics)>productions</a>. The main contribution of this paper is the specification of the elicitation protocol, explaining the variables that are likely to affect the diagrams collected. We conclude with a report on the current state of a collection following this <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a>, and a few observations on the collected contents. A first prospect is the standardisation of a scheme to represent SL discourse in a way that would make them sharable. A subsequent longer-term prospect is for this scheme to be owned by users and with time be shaped into a script for their language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.11/>Signing as Input for a Dictionary Query : Matching Signs Based on Joint Positions of the Dominant Hand</a></strong><br><a href=/people/m/manolis-fragkiadakis/>Manolis Fragkiadakis</a>
|
<a href=/people/v/victoria-nyst/>Victoria Nyst</a>
|
<a href=/people/p/peter-van-der-putten/>Peter van der Putten</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--11><div class="card-body p-3 small">This study presents a new <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to search sign language lexica, using a full sign as input for a query. Thus, a dictionary user can look up information about a sign by signing the sign to a webcam. The recorded sign is then compared to potential matching signs in the lexicon. As such, it provides a new way of searching sign language dictionaries to complement existing methods based on (spoken language) glosses or phonological features, like <a href=https://en.wikipedia.org/wiki/Handshape>handshape</a> or <a href=https://en.wikipedia.org/wiki/Location>location</a>. The method utilizes OpenPose to extract the body and finger joint positions. Dynamic Time Warping (DTW) is used to quantify the variation of the trajectory of the dominant hand and the average trajectories of the fingers. Ten people with various degrees of sign language proficiency have participated in this study. Each subject viewed a set of 20 signs from the newly compiled Ghanaian sign language lexicon and was asked to replicate the signs. The results show that DTW can predict the matching sign with 87 % and 74 % accuracy at the Top-10 and Top-5 ranking level respectively by using only the trajectory of the dominant hand. Additionally, more proficient signers obtain 90 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> at the Top-10 ranking. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> has the potential to be used also as a variation measurement tool to quantify the difference in signing between different signers or sign languages in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.14/>An Isolated-Signing RGBD Dataset of 100 American Sign Language Signs Produced by Fluent ASL Signers<span class=acl-fixed-case>RGBD</span> Dataset of 100 <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Signs Produced by Fluent <span class=acl-fixed-case>ASL</span> Signers</a></strong><br><a href=/people/s/saad-hassan/>Saad Hassan</a>
|
<a href=/people/l/larwan-berke/>Larwan Berke</a>
|
<a href=/people/e/elahe-vahdani/>Elahe Vahdani</a>
|
<a href=/people/l/longlong-jing/>Longlong Jing</a>
|
<a href=/people/y/yingli-tian/>Yingli Tian</a>
|
<a href=/people/m/matt-huenerfauth/>Matt Huenerfauth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--14><div class="card-body p-3 small">We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor. This directed dataset had originally been collected as part of an ongoing collaborative project, to aid in the development of a sign-recognition system for identifying occurrences of these 100 signs in video. The set of words consist of vocabulary items that would commonly be learned in a first-year ASL course offered at a university, although the specific set of signs selected for inclusion in the dataset had been motivated by project-related factors. Given increasing interest among sign-recognition and other computer-vision researchers in red-green-blue-depth (RBGD) video, we release this dataset for use by the research community. In addition to the RGB video files, we share depth and HD face data as well as additional features of <a href=https://en.wikipedia.org/wiki/Face>face</a>, <a href=https://en.wikipedia.org/wiki/Hand>hands</a>, and <a href=https://en.wikipedia.org/wiki/Human_body>body</a> produced through post-processing of this data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.16/>Sign Language Motion Capture Dataset for Data-driven Synthesis</a></strong><br><a href=/people/p/pavel-jedlicka/>Pavel Jedlička</a>
|
<a href=/people/z/zdenek-krnoul/>Zdeněk Krňoul</a>
|
<a href=/people/j/jakub-kanis/>Jakub Kanis</a>
|
<a href=/people/m/milos-zelezny/>Miloš Železný</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--16><div class="card-body p-3 small">This paper presents a new 3D motion capture dataset of <a href=https://en.wikipedia.org/wiki/Czech_Sign_Language>Czech Sign Language (CSE)</a>. Its main purpose is to provide the data for further analysis and data-based automatic synthesis of CSE utterances. The content of the <a href=https://en.wikipedia.org/wiki/Data>data</a> in the given limited domain of weather forecasts was carefully selected by the CSE linguists to provide the necessary utterances needed to produce any new <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather forecast</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was recorded using the state-of-the-art motion capture (MoCap) technology to provide the most precise trajectories of the motion. In general, MoCap is a device capable of accurate recording of motion directly in <a href=https://en.wikipedia.org/wiki/Three-dimensional_space>3D space</a>. The <a href=https://en.wikipedia.org/wiki/Data>data</a> contains trajectories of body, arms, hands and face markers recorded at once to provide consistent data without the need for the time alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.20/>Recognition of Static Features in <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Language</a> Using Key-Points</a></strong><br><a href=/people/i/ioannis-koulierakis/>Ioannis Koulierakis</a>
|
<a href=/people/g/georgios-siolas/>Georgios Siolas</a>
|
<a href=/people/e/eleni-efthimiou/>Eleni Efthimiou</a>
|
<a href=/people/e/evita-fotinea/>Evita Fotinea</a>
|
<a href=/people/a/andreas-georgios-stafylopatis/>Andreas-Georgios Stafylopatis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--20><div class="card-body p-3 small">In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for <a href=https://en.wikipedia.org/wiki/Handshape>handshape</a>, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software. The <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> have been applied to a Danish and a Greek Sign Language dataset, providing results around 96 %. Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.22/>Machine Learning for Enhancing Dementia Screening in Ageing Deaf Signers of British Sign Language<span class=acl-fixed-case>B</span>ritish <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/x/xing-liang/>Xing Liang</a>
|
<a href=/people/b/bencie-woll/>Bencie Woll</a>
|
<a href=/people/k/kapetanios-epaminondas/>Kapetanios Epaminondas</a>
|
<a href=/people/a/anastasia-angelopoulou/>Anastasia Angelopoulou</a>
|
<a href=/people/r/reda-al-batat/>Reda Al-Batat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--22><div class="card-body p-3 small">Ageing trend in populations is correlated with increased prevalence of <a href=https://en.wikipedia.org/wiki/Cognitive_deficit>acquired cognitive impairments</a> such as <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>. Although there is no cure for <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>, a timely diagnosis helps in obtaining necessary support and appropriate medication. With this in mind, researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In this paper, we introduce an automatic dementia screening system for ageing Deaf signers of British Sign Language (BSL), using Convolutional Neural Networks (CNN), by analysing the sign space envelope and facial expression of BSL signers using normal 2D videos from BSL corpus. Our approach firstly establishes an accurate real-time hand trajectory tracking model together with a real-time landmark facial motion analysis model to identify differences in sign space envelope and facial movement as the keys to identifying language changes associated with <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>. Based on the differences in patterns obtained from facial and trajectory motion data, CNN models (ResNet50 / VGG16) are fine-tuned using Keras deep learning models to incrementally identify and improve dementia recognition rates. We report the results for two methods using different modalities (sign trajectory and facial motion), together with the performance comparisons between different deep learning CNN models in ResNet50 and VGG16. The experiments show the effectiveness of our deep learning based approach in terms of sign space tracking, facial motion tracking and early stage dementia performance assessment tasks. The results are validated against cognitive assessment scores as of our ground truth data with a test set performance of 87.88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.23/>Machine Translation from <a href=https://en.wikipedia.org/wiki/Spoken_language>Spoken Language</a> to <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Language</a> using Pre-trained Language Model as Encoder</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/y/yusuke-morita/>Yusuke Morita</a>
|
<a href=/people/m/masanori-sano/>Masanori Sano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--23><div class="card-body p-3 small">Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. To achieve flexible open-domain services with <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translations</a> into <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of the <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation model</a>. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>, which is a special feature used in <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. We also conducted trials to improve the translation quality for <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.27/>Automatic Classification of Handshapes in <a href=https://en.wikipedia.org/wiki/Russian_Sign_Language>Russian Sign Language</a><span class=acl-fixed-case>R</span>ussian <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/m/medet-mukushev/>Medet Mukushev</a>
|
<a href=/people/a/alfarabi-imashev/>Alfarabi Imashev</a>
|
<a href=/people/v/vadim-kimmelman/>Vadim Kimmelman</a>
|
<a href=/people/a/anara-sandygulova/>Anara Sandygulova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--27><div class="card-body p-3 small">Handshapes are one of the basic parameters of signs, and any <a href=https://en.wikipedia.org/wiki/Phonology>phonological or phonetic analysis</a> of a <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> must account for <a href=https://en.wikipedia.org/wiki/Handshapes>handshapes</a>. Many <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as it is important due to the need of generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, it is a very time-consuming process, thus only a handful of <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> have such inventories. This work proposes a process of automatically generating such inventories for sign languages by applying automatic hand detection, cropping, and clustering techniques. We applied our proposed method to a commonly used resource : the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verified the data to be able to perform <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Thus, the proposed <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> can serve as an alternative approach to manual annotation, and can help linguists in answering numerous research questions in relation to handshape frequencies in sign languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.30/>BosphorusSign22k Sign Language Recognition Dataset<span class=acl-fixed-case>B</span>osphorus<span class=acl-fixed-case>S</span>ign22k Sign Language Recognition Dataset</a></strong><br><a href=/people/o/ogulcan-ozdemir/>Oğulcan Özdemir</a>
|
<a href=/people/a/ahmet-alp-kindiroglu/>Ahmet Alp Kındıroğlu</a>
|
<a href=/people/n/necati-cihan-camgoz/>Necati Cihan Camgöz</a>
|
<a href=/people/l/lale-akarun/>Lale Akarun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--30><div class="card-body p-3 small">Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>video recognition</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning research communities</a>. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.34/>Video-to-HamNoSys Automated Annotation System<span class=acl-fixed-case>H</span>am<span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>S</span>ys Automated Annotation System</a></strong><br><a href=/people/v/victor-skobov/>Victor Skobov</a>
|
<a href=/people/y/yves-lepage/>Yves Lepage</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--34><div class="card-body p-3 small">The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use <a href=https://en.wikipedia.org/wiki/HamNoSys>HamNoSys</a>, i.e., <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings of characters</a>, as a representation of an SL corpus instead of <a href=https://en.wikipedia.org/wiki/Video>video material</a>. Processing <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings of characters</a> instead of images can significantly contribute to <a href=https://en.wikipedia.org/wiki/Sign_language>sign language research</a>. However, the complexity of <a href=https://en.wikipedia.org/wiki/HamNoSys>HamNoSys</a> makes it difficult to annotate without a lot of time and effort. Therefore <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> has to be automatized. This work proposes a conceptually new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as <a href=https://en.wikipedia.org/wiki/Camera_angle>camera angles</a>, <a href=https://en.wikipedia.org/wiki/Film_speed>speed</a>, and <a href=https://en.wikipedia.org/wiki/Gesture_recognition>gestures</a>. Our approach is conceptually different from other SL recognition solutions and offers a developed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.35/>Cross-Lingual Keyword Search for Sign Language</a></strong><br><a href=/people/n/nazif-can-tamer/>Nazif Can Tamer</a>
|
<a href=/people/m/murat-saraclar/>Murat Saraçlar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--35><div class="card-body p-3 small">Sign language research most often relies on exhaustively annotated and segmented data, which is scarce even for the most studied <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>. However, parallel corpora consisting of sign language interpreting are rarely explored. By utilizing such data for the task of <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a>, this work aims to enable <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> with the queries from the translated written language. With the written language translations as labels, we train a weakly supervised keyword search model for <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> and further improve the retrieval performance with two context modeling strategies. In our experiments, we compare the gloss retrieval and cross language retrieval performance on RWTH-PHOENIX-Weather 2014 T dataset.</div></div></div><hr><div id=2020sltu-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.sltu-1/>Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.0/>Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</a></strong><br><a href=/people/d/dorothee-beermann/>Dorothee Beermann</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/c/claudia-soria/>Claudia Soria</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.3/>Open-Source High Quality Speech Datasets for Basque, Catalan and Galician<span class=acl-fixed-case>B</span>asque, <span class=acl-fixed-case>C</span>atalan and <span class=acl-fixed-case>G</span>alician</a></strong><br><a href=/people/o/oddur-kjartansson/>Oddur Kjartansson</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a>
|
<a href=/people/a/alena-butryna/>Alena Butryna</a>
|
<a href=/people/i/isin-demirsahin/>Isin Demirsahin</a>
|
<a href=/people/c/clara-rivera/>Clara Rivera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--3><div class="card-body p-3 small">This paper introduces new open speech datasets for three of the languages of Spain : <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a> and <a href=https://en.wikipedia.org/wiki/Galician_language>Galician</a>. Catalan is furthermore the official language of the Principality of Andorra. The <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> consist of high-quality multi-speaker recordings of the three languages along with the associated transcriptions. The resulting <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> include over 33 hours of crowd-sourced recordings of 132 male and female native speakers. The recording scripts also include material for elicitation of global and local place names, personal and business names. The <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are released under a permissive license and are available for free download for commercial, academic and personal use. The high-quality annotated speech datasets described in this paper can be used to, among other things, build <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech systems</a>, serve as adaptation data in <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a> and provide useful phonetic and phonological insights in <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.5/>Morphological Disambiguation of South Smi with FSTs and <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a><span class=acl-fixed-case>S</span>outh <span class=acl-fixed-case>S</span>ámi with <span class=acl-fixed-case>FST</span>s and Neural Networks</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/l/linda-wiechetek/>Linda Wiechetek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--5><div class="card-body p-3 small">We present a method for conducting morphological disambiguation for South Smi, which is an <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered language</a>. Our method uses an FST-based morphological analyzer to produce an ambiguous set of morphological readings for each word in a sentence. These readings are disambiguated with a Bi-RNN model trained on the related North Smi UD Treebank and some synthetically generated South Smi data. The disambiguation is done on the level of morphological tags ignoring word forms and <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemmas</a> ; this makes it possible to use North Smi training data for South Smi without the need for a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> or aligned word embeddings. Our approach requires only minimal resources for South Smi, which makes it usable and applicable in the contexts of any other <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered language</a> as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.8/>Neural Text-to-Speech Synthesis for an Under-Resourced Language in a Diglossic Environment : the Case of Gascon Occitan<span class=acl-fixed-case>G</span>ascon <span class=acl-fixed-case>O</span>ccitan</a></strong><br><a href=/people/a/ander-corral/>Ander Corral</a>
|
<a href=/people/i/igor-leturia/>Igor Leturia</a>
|
<a href=/people/a/aure-seguier/>Aure Séguier</a>
|
<a href=/people/m/michael-barret/>Michäel Barret</a>
|
<a href=/people/b/benaset-dazeas/>Benaset Dazéas</a>
|
<a href=/people/p/philippe-boula-de-mareuil/>Philippe Boula de Mareüil</a>
|
<a href=/people/n/nicolas-quint/>Nicolas Quint</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--8><div class="card-body p-3 small">Occitan is a <a href=https://en.wikipedia.org/wiki/Minority_language>minority language</a> spoken in Southern France, some Alpine Valleys of Italy, and the Val d&#8217;Aran in Spain, which only very recently started developing language and speech technologies. This paper describes the first project for designing a Text-to-Speech synthesis system for one of its main <a href=https://en.wikipedia.org/wiki/Gascon_language>regional varieties</a>, namely <a href=https://en.wikipedia.org/wiki/Gascon_language>Gascon</a>. We used a state-of-the-art deep neural network approach, the Tacotron2-WaveGlow system. However, we faced two additional difficulties or challenges : on the one hand, we wanted to test if it was possible to obtain good quality results with fewer recording hours than is usually reported for such systems ; on the other hand, we needed to achieve a standard, non-Occitan pronunciation of French proper names, therefore we needed to record French words and test phoneme-based approaches. The evaluation carried out over the various developed <a href=https://en.wikipedia.org/wiki/System>systems</a> and <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> shows promising results with near production-ready quality. It has also allowed us to detect the phenomena for which some flaws or fall of quality occur, pointing at the direction of future work to improve the quality of the actual system and for new systems for other <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> and voices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.14/>Poio Text Prediction : Lessons on the Development and Sustainability of LTs for Endangered Languages<span class=acl-fixed-case>LT</span>s for Endangered Languages</a></strong><br><a href=/people/g/gema-zamora-fernandez/>Gema Zamora Fernández</a>
|
<a href=/people/v/vera-ferreira/>Vera Ferreira</a>
|
<a href=/people/p/pedro-manha/>Pedro Manha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--14><div class="card-body p-3 small">2019, the International Year of Indigenous Languages (IYIL), marked a crucial milestone for a diverse community united by a strong sense of urgency. In this presentation, we evaluate the impact of IYIL&#8217;s outcomes in the development of LTs for <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered languages</a>. We give a brief description of the field of <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a>, whose experts have led the research and data collection efforts surrounding <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered languages</a> for the past 30 years. We introduce the work of the Interdisciplinary Centre for Social and Language Documentation and we look at Poio as an example of an LT developed specifically with speakers of endangered languages in mind. This example illustrates how the deeper systemic causes of <a href=https://en.wikipedia.org/wiki/Endangered_language>language endangerment</a> are reflected in the development of <a href=https://en.wikipedia.org/wiki/Language_proficiency>LTs</a>. Additionally, we share some of the strategic decisions that have led the development of this project. Finally, we advocate the importance of bridging the divide between research and activism, pushing for the inclusion of threatened languages in the world of LTs, and doing so in close collaboration with the speaker community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.16/>Scaling Language Data Import / Export with a Data Transformer Interface</a></strong><br><a href=/people/n/nicholas-buckeridge/>Nicholas Buckeridge</a>
|
<a href=/people/b/ben-foley/>Ben Foley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--16><div class="card-body p-3 small">This paper focuses on the technical improvement of <a href=https://en.wikipedia.org/wiki/Elpis>Elpis</a>, a <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a> which assists people in the process of transcription, particularly for low-resource language documentation situations. To provide better support for the diversity of file formats encountered by people working to document the world&#8217;s languages, a Data Transformer interface has been developed to abstract the complexities of designing individual data import scripts. This work took place as part of a larger project of <a href=https://en.wikipedia.org/wiki/Code_quality>code quality improvement</a> and the publication of <a href=https://en.wikipedia.org/wiki/Template_processor>template code</a> that can be used for development of other <a href=https://en.wikipedia.org/wiki/Programming_language>language technologies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.21/>Natural Language Processing Chains Inside a Cross-lingual Event-Centric Knowledge Pipeline for European Union Under-resourced Languages<span class=acl-fixed-case>E</span>uropean <span class=acl-fixed-case>U</span>nion Under-resourced Languages</a></strong><br><a href=/people/d/diego-alves/>Diego Alves</a>
|
<a href=/people/g/gaurish-thakkar/>Gaurish Thakkar</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--21><div class="card-body p-3 small">This article presents the strategy for developing a platform containing Language Processing Chains for European Union languages, consisting of Tokenization to <a href=https://en.wikipedia.org/wiki/Parsing>Parsing</a>, also including Named Entity recognition and with addition of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>. These chains are part of the first step of an event-centric knowledge processing pipeline whose aim is to process multilingual media information about major events that can cause an impact in Europe and the rest of the world. Due to the differences in terms of availability of language resources for each language, we have built this strategy in three steps, starting with processing chains for the well-resourced languages and finishing with the development of new modules for the under-resourced ones. In order to classify all <a href=https://en.wikipedia.org/wiki/Languages_of_the_European_Union>European Union official languages</a> in terms of resources, we have analysed the size of annotated corpora as well as the existence of pre-trained models in mainstream Language Processing tools, and we have combined this information with the proposed classification published at META-NET whitepaper series.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.23/>Acoustic-Phonetic Approach for ASR of Less Resourced Languages Using Monolingual and Cross-Lingual Information<span class=acl-fixed-case>ASR</span> of Less Resourced Languages Using Monolingual and Cross-Lingual Information</a></strong><br><a href=/people/s/shweta-bansal/>Shweta Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--23><div class="card-body p-3 small">The exploration of <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing</a> for <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered languages</a> has substantially increased in the past epoch of time. In this paper, we present the acoustic-phonetic approach for automatic speech recognition (ASR) using monolingual and cross-lingual information with application to under-resourced Indian languages, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. The challenging task while developing the ASR was the collection of the acoustic corpus for under-resourced languages. We have described here, in brief, the strategies used for designing the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and also highlighted the issues pertaining while collecting data for these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. The bootstrap GMM-UBM based approach is used, which integrates pronunciation lexicon, <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and <a href=https://en.wikipedia.org/wiki/Acoustic_phonetics>acoustic-phonetic model</a>. Mel Frequency Cepstral Coefficients were used for extracting the acoustic signal features for training in monolingual and cross-lingual settings. The experimental result shows the overall performance of ASR for cross-lingual and monolingual. The phone substitution plays a key role in the cross-lingual as well as monolingual recognition. The result obtained by cross-lingual recognition compared with other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> and it has been found that the performance of the <a href=https://en.wikipedia.org/wiki/Speech_recognition>recognition system</a> is based on <a href=https://en.wikipedia.org/wiki/Phoneme>phonemic units</a>. The recognition rate of cross-lingual generally declines as compared with the monolingual.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sltu-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.25/>A Sentiment Analysis Dataset for Code-Mixed Malayalam-English<span class=acl-fixed-case>M</span>alayalam-<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/n/navya-jose/>Navya Jose</a>
|
<a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/e/elizabeth-sherly/>Elizabeth Sherly</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--25><div class="card-body p-3 small">There is an increasing demand for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of text from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> which are mostly code-mixed. Systems trained on monolingual data fail for code-mixed data due to the complexity of mixing at different levels of the text. However, very few resources are available for code-mixed data to create <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> specific for this <a href=https://en.wikipedia.org/wiki/Data>data</a>. Although much research in multilingual and cross-lingual sentiment analysis has used <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised or unsupervised methods</a>, <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> still performs better. Only a few datasets for popular languages such as English-Spanish, <a href=https://en.wikipedia.org/wiki/English_language_in_India>English-Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Standard_Chinese>English-Chinese</a> are available. There are no resources available for Malayalam-English code-mixed data. This paper presents a new gold standard corpus for sentiment analysis of code-mixed text in <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam-English</a> annotated by voluntary annotators. This gold standard <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> obtained a <a href=https://en.wikipedia.org/wiki/Krippendorff&#8217;s_alpha>Krippendorff&#8217;s alpha</a> above 0.8 for the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We use this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to provide the benchmark for sentiment analysis in Malayalam-English code-mixed texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sltu-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.27/>Macsen : A Voice Assistant for Speakers of a Lesser Resourced Language<span class=acl-fixed-case>M</span>acsen: A Voice Assistant for Speakers of a Lesser Resourced Language</a></strong><br><a href=/people/d/dewi-jones/>Dewi Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--27><div class="card-body p-3 small">This paper reports on the development of a voice assistant mobile app for speakers of a lesser resourced language Welsh. An assistant with a smaller set of effective but useful skills is both desirable and urgent for the wider Welsh speaking community. Descriptions of the <a href=https://en.wikipedia.org/wiki/Mobile_app>app</a>&#8217;s skills, architecture, design decisions and <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> is provided before elaborating on the most recent research and activities in open source speech technology for <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a>. The paper reports on the progress to date on crowdsourcing Welsh speech data in Mozilla Common Voice and of its suitability for training Mozilla&#8217;s DeepSpeech speech recognition for a voice assistant application according to conventional and transfer learning methods. We demonstrate that with smaller datasets of speech data, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and a domain specific language model, acceptable <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> is achievable that facilitates, as confirmed by beta users, a practical and useful voice assistant for Welsh speakers. We hope that this work informs and serves as a model to researchers and developers in other lesser-resourced linguistic communities and helps bring into being voice assistant apps for their languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.29/>Gender Detection from <a href=https://en.wikipedia.org/wiki/Human_voice>Human Voice</a> Using Tensor Analysis</a></strong><br><a href=/people/p/prasanta-roy/>Prasanta Roy</a>
|
<a href=/people/p/parabattina-bhagath/>Parabattina Bhagath</a>
|
<a href=/people/p/pradip-das/>Pradip Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--29><div class="card-body p-3 small">Speech-based communication is one of the most preferred modes of communication for humans. The <a href=https://en.wikipedia.org/wiki/Human_voice>human voice</a> contains several important information and clues that help in interpreting the voice message. The gender of the speaker can be accurately guessed by a person based on the received voice of a speaker. The knowledge of the speaker&#8217;s gender can be a great aid to design accurate <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition systems</a>. GMM based classifier is a popular choice used for gender detection. In this paper, we propose a Tensor-based approach for detecting the gender of a speaker and discuss its implementation details for low resourceful languages. Experiments were conducted using the TIMIT and SHRUTI dataset. An average gender detection accuracy of 91 % is recorded. Analysis of the results with the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.30/>Data-Driven Parametric Text Normalization : Rapidly Scaling Finite-State Transduction Verbalizers to New Languages</a></strong><br><a href=/people/s/sandy-ritchie/>Sandy Ritchie</a>
|
<a href=/people/e/eoin-mahon/>Eoin Mahon</a>
|
<a href=/people/k/kim-heiligenstein/>Kim Heiligenstein</a>
|
<a href=/people/n/nikos-bampounis/>Nikos Bampounis</a>
|
<a href=/people/d/daan-van-esch/>Daan van Esch</a>
|
<a href=/people/c/christian-schallhart/>Christian Schallhart</a>
|
<a href=/people/j/jonas-mortensen/>Jonas Mortensen</a>
|
<a href=/people/b/benoit-brard/>Benoit Brard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--30><div class="card-body p-3 small">This paper presents a methodology for rapidly generating FST-based verbalizers for ASR and TTS systems by efficiently sourcing language-specific data. We describe a questionnaire which collects the necessary data to bootstrap the number grammar induction system and parameterize the verbalizer templates described in Ritchie et al. (2019), and a machine-readable data store which allows the data collected through the questionnaire to be supplemented by additional data from other sources. This system allows us to rapidly scale technologies such as <a href=https://en.wikipedia.org/wiki/Autonomous_system_(Internet)>ASR</a> and <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>TTS</a> to more languages, including low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.32/>Adapting a Welsh Terminology Tool to Develop a Cornish Dictionary<span class=acl-fixed-case>W</span>elsh Terminology Tool to Develop a <span class=acl-fixed-case>C</span>ornish Dictionary</a></strong><br><a href=/people/d/delyth-prys/>Delyth Prys</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--32><div class="card-body p-3 small">Cornish and <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a> are closely related <a href=https://en.wikipedia.org/wiki/Celtic_languages>Celtic languages</a> and this paper provides a brief description of a recent project to publish an online bilingual English / Cornish dictionary, the Gerlyver Kernewek, based on similar work previously undertaken for <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a>. Both languages are endangered, Cornish critically so, but both can benefit from the use of <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a>. Welsh has previous experience of using language technologies for <a href=https://en.wikipedia.org/wiki/Language_revitalization>language revitalization</a>, and this is now being used to help the <a href=https://en.wikipedia.org/wiki/Cornish_language>Cornish language</a> create new tools and resources, including lexicographical ones, helping a dispersed team of language specialists and editors, many of them in a voluntary capacity, to work collaboratively online. Details are given of the Maes T dictionary writing and publication platform, originally developed for <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a>, and of some of the adaptations that had to be made to accommodate the specific needs of Cornish, including their use of Middle and Late varieties due to its development as a revived language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.40/>On the Exploration of English to Urdu Machine Translation<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>U</span>rdu Machine Translation</a></strong><br><a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/s/syeda-abida/>Syeda Abida</a>
|
<a href=/people/n/noor-e-hira/>Noor-e- Hira</a>
|
<a href=/people/s/syeda-zahra/>Syeda Zahra</a>
|
<a href=/people/d/dania-parvez/>Dania Parvez</a>
|
<a href=/people/j/javeria-bashir/>Javeria Bashir</a>
|
<a href=/people/q/qurat-ul-ain-majid/>Qurat-ul-ain Majid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--40><div class="card-body p-3 small">Machine Translation is the inevitable technology to reduce communication barriers in today&#8217;s world. It has made substantial progress in recent years and is being widely used in commercial as well as non-profit sectors. Such is only the case for European and other high resource languages. For English-Urdu language pair, the <a href=https://en.wikipedia.org/wiki/Technology>technology</a> is in its infancy stage due to scarcity of resources. Present research is an important milestone in English-Urdu machine translation, as we present results for four major domains including Biomedical, Religious, Technological and General using Statistical and Neural Machine Translation. We performed series of experiments in attempts to optimize the performance of each <a href=https://en.wikipedia.org/wiki/System>system</a> and also to study the impact of data sources on the <a href=https://en.wikipedia.org/wiki/System>systems</a>. Finally, we established a comparison of the data sources and the effect of language model size on <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.42/>Adapting Language Specific Components of Cross-Media Analysis Frameworks to Less-Resourced Languages : the Case of Amharic<span class=acl-fixed-case>A</span>mharic</a></strong><br><a href=/people/y/yonas-woldemariam/>Yonas Woldemariam</a>
|
<a href=/people/a/adam-dahlgren/>Adam Dahlgren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--42><div class="card-body p-3 small">We present an ASR based pipeline for <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> that orchestrates NLP components within a cross media analysis framework (CMAF). One of the major challenges that are inherently associated with CMAFs is effectively addressing multi-lingual issues. As a result, many languages remain under-resourced and fail to leverage out of available media analysis solutions. Although spoken natively by over 22 million people and there is an ever-increasing amount of Amharic multimedia content on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, querying them with simple text search is difficult. Searching for, especially audio / video content with simple key words, is even hard as they exist in their raw form. In this study, we introduce a spoken and textual content processing workflow into a CMAF for <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a>. We design an ASR-named entity recognition (NER) pipeline that includes three main components : ASR, a transliterator and NER. We explore various acoustic modeling techniques and develop an OpenNLP-based NER extractor along with a transliterator that interfaces between ASR and <a href=https://en.wikipedia.org/wiki/Near-infrared_spectroscopy>NER</a>. The designed ASR-NER pipeline for <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> promotes the multi-lingual support of CMAFs. Also, the state-of-the art design principles and techniques employed in this study shed light for other less-resourced languages, particularly the <a href=https://en.wikipedia.org/wiki/Semitic_languages>Semitic ones</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.45/>Owksape-An Online Language Learning Platform for Lakota<span class=acl-fixed-case>L</span>akota</a></strong><br><a href=/people/j/jan-ullrich/>Jan Ullrich</a>
|
<a href=/people/e/elliot-thornton/>Elliot Thornton</a>
|
<a href=/people/p/peter-vieira/>Peter Vieira</a>
|
<a href=/people/l/logan-swango/>Logan Swango</a>
|
<a href=/people/m/marek-kupiec/>Marek Kupiec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--45><div class="card-body p-3 small">This paper presents Owksape, an online language learning platform for the under-resourced language Lakota. The Lakota language (Laktiyapi) is a Siouan language native to the United States with fewer than 2000 fluent speakers. Owksape was developed by The Language Conservancy to support revitalization efforts, including reaching younger generations and providing a tool to complement traditional teaching methods. This project grew out of various multimedia resources in order to combine their most effective aspects into a single, self-paced learning tool. The first section of this paper discusses the motivation for and background of Owksape. Section two details the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic features</a> and language documentation principles that form the backbone of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>. Section three lays out the unique integration of cultural aspects of the Lakota people into the <a href=https://en.wikipedia.org/wiki/Graphic_design>visual design</a> of the <a href=https://en.wikipedia.org/wiki/Application_software>application</a>. Section four explains the pedagogical principles of Owksape. Application features and exercise types are then discussed in detail with visual examples, followed by an overview of the <a href=https://en.wikipedia.org/wiki/Software_design>software design</a>, as well as the effort required to develop the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>. Finally, a description of future features and considerations is presented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sltu-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sltu-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sltu-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sltu-1.51/>Speech Transcription Challenges for Resource Constrained Indigenous Language Cree<span class=acl-fixed-case>C</span>ree</a></strong><br><a href=/people/v/vishwa-gupta/>Vishwa Gupta</a>
|
<a href=/people/g/gilles-boulianne/>Gilles Boulianne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sltu-1--51><div class="card-body p-3 small">Cree is one of the most spoken Indigenous languages in Canada. From a speech recognition perspective, it is a low-resource language, since very little data is available for either acoustic or language modeling. This has prevented development of <a href=https://en.wikipedia.org/wiki/Speech_technology>speech technology</a> that could help revitalize the <a href=https://en.wikipedia.org/wiki/Language>language</a>. We describe our experiments with available Cree data to improve <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>automatic transcription</a> both in speaker- independent and dependent scenarios. While it was difficult to get low speaker-independent word error rates with only six speakers, we were able to get low word and phoneme error rates in the speaker-dependent scenario. We compare our phoneme recognition with two state-of-the-art open-source phoneme recognition toolkits, which use end-to-end training and sequence-to-sequence modeling. Our phoneme error rate (8.7 %) is significantly lower than that achieved by the best of these systems (15.1 %). With these systems and varying amounts of transcribed and text data, we show that pre-training on other languages is important for speaker-independent recognition, and even small amounts of additional text-only documents are useful. These results can guide practical language documentation work, when deciding how much transcribed and text data is needed to achieve useful phoneme accuracies.</div></div></div><hr><div id=2020stoc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.stoc-1/>Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.stoc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.stoc-1.0/>Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management</a></strong><br><a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.stoc-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--stoc-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.stoc-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.stoc-1.4/>A Privacy Preserving Data Publishing Middleware for Unstructured, Textual Social Media Data</a></strong><br><a href=/people/p/prasadi-abeywardana/>Prasadi Abeywardana</a>
|
<a href=/people/u/uthayasanker-thayasivam/>Uthayasanker Thayasivam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--stoc-1--4><div class="card-body p-3 small">Privacy is going to be an integral part of <a href=https://en.wikipedia.org/wiki/Data_science>data science and analytics</a> in the coming years. The next hype of data experimentation is going to be heavily dependent on privacy preserving techniques mainly as it&#8217;s going to be a legal responsibility rather than a mere social responsibility. Privacy preservation becomes more challenging specially in the context of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>. Social networks have become predominantly popular over the past couple of decades and they are creating a huge data lake at a high velocity. Social media profiles contain a wealth of personal and sensitive information, creating enormous opportunities for third parties to analyze them with different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, draw conclusions and use in <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation campaigns</a> and micro targeting based dark advertising. This study provides a mitigation mechanism for <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation campaigns</a> that are done based on the insights extracted from personal / sensitive data analysis. Specifically, this research is aimed at building a privacy preserving data publishing middleware for unstructured social media data without compromising the true analytical value of those <a href=https://en.wikipedia.org/wiki/Data>data</a>. A novel way is proposed to apply traditional structured privacy preserving techniques on <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>. Creating a comprehensive twitter corpus annotated with privacy attributes is another objective of this research, especially because the research community is lacking one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.stoc-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--stoc-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.stoc-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.stoc-1.5/>Information Space Dashboard</a></strong><br><a href=/people/t/theresa-krumbiegel/>Theresa Krumbiegel</a>
|
<a href=/people/a/albert-pritzkau/>Albert Pritzkau</a>
|
<a href=/people/h/hans-christian-schmitz/>Hans-Christian Schmitz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--stoc-1--5><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Information_space>information space</a>, where information is generated, stored, exchanged and discussed, is not idyllic but a space where <a href=https://en.wikipedia.org/wiki/Disinformation>campaigns of disinformation</a> and <a href=https://en.wikipedia.org/wiki/Destabilization>destabilization</a> are conducted. Such campaigns are subsumed under the terms <a href=https://en.wikipedia.org/wiki/Hybrid_warfare>hybrid warfare</a> and <a href=https://en.wikipedia.org/wiki/Information_warfare>information warfare</a> (Woolley and Howard, 2017). In order to enable awareness of them, we propose an information state dashboard comprising various components / apps for <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, analysis and <a href=https://en.wikipedia.org/wiki/Data_visualization>visualization</a>. The aim of the dashboard is to support an analyst in generating a common operational picture of the information space, link it with an operational picture of the physical space and, thus, contribute to overarching situational awareness. The <a href=https://en.wikipedia.org/wiki/Dashboard>dashboard</a> is work in progress. However, a first prototype with components for exploiting elementary language statistics, keyword and metadata analysis, <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Network_theory>network analysis</a> has been implemented. Further components, in particular, for <a href=https://en.wikipedia.org/wiki/Event_(computing)>event extraction</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> are under development. As a demonstration case, we briefly discuss the analysis of historical data regarding <a href=https://en.wikipedia.org/wiki/2018_Chemnitz_protests>violent anti-migrant protests</a> and respective <a href=https://en.wikipedia.org/wiki/Counter-protest>counter-protests</a> that took place in <a href=https://en.wikipedia.org/wiki/Chemnitz>Chemnitz</a> in 2018.</div></div></div><hr><div id=2020trac-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.trac-1/>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.0/>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/b/bornini-lahiri/>Bornini Lahiri</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/v/vanessa-murdock/>Vanessa Murdock</a>
|
<a href=/people/d/daniel-kadar/>Daniel Kadar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.5/>Aggression Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : a Transfer Learning Based Approach</a></strong><br><a href=/people/f/faneva-ramiandrisoa/>Faneva Ramiandrisoa</a>
|
<a href=/people/j/josiane-mothe/>Josiane Mothe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--5><div class="card-body p-3 small">The way people communicate have changed in many ways with the outbreak of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. One of the aspects of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is the ability for their information producers to hide, fully or partially, their identity during a discussion ; leading to <a href=https://en.wikipedia.org/wiki/Cyber-aggression>cyber-aggression</a> and interpersonal aggression. Automatically monitoring <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> in order to help moderating it is thus a very hot topic. In this paper, we propose to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) (Devlin et al., 2019) to identify aggressive content. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also used to predict the level of <a href=https://en.wikipedia.org/wiki/Aggression>aggressiveness</a>. The evaluation part of this paper is based on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provided by the TRAC shared task (Kumar et al., 2018a). When compared to the other participants of this shared task, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the third best performance according to the weighted F1 measure on both Facebook and Twitter collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.7/>A Comparative Study of Different State-of-the-Art Hate Speech Detection Methods in Hindi-English Code-Mixed Data<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Data</a></strong><br><a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--7><div class="card-body p-3 small">Hate speech detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media communication</a> has become one of the primary concerns to avoid conflicts and curb undesired activities. In an environment where multilingual speakers switch among multiple languages, hate speech detection becomes a challenging task using methods that are designed for monolingual corpora. In our work, we attempt to analyze, detect and provide a comparative study of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> in a code-mixed social media text. We also provide a Hindi-English code-mixed data set consisting of Facebook and Twitter posts and comments. Our experiments show that <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> trained on this code-mixed corpus perform better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.9/>Bagging BERT Models for Robust Aggression Identification<span class=acl-fixed-case>BERT</span> Models for Robust Aggression Identification</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--9><div class="card-body p-3 small">Modern transformer-based models with hundreds of millions of parameters, such as BERT, achieve impressive results at text classification tasks. This also holds for aggression identification and offensive language detection, where deep learning approaches consistently outperform less complex models, such as <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision trees</a>. While the complex models fit training data well (low bias), they also come with an unwanted high variance. Especially when fine-tuning them on small datasets, the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance varies significantly for slightly different training data. To overcome the high variance and provide more robust predictions, we propose an ensemble of multiple fine-tuned BERT models based on bootstrap aggregating (bagging). In this paper, we describe such an ensemble system and present our submission to the shared tasks on aggression identification 2020 (team name : Julian). Our submission is the best-performing <a href=https://en.wikipedia.org/wiki/System>system</a> for five out of six subtasks. For example, we achieve a weighted F1-score of 80.3 % for task A on the test dataset of English social media posts. In our experiments, we compare different model configurations and vary the number of models used in the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. We find that the F1-score drastically increases when ensembling up to 15 models, but the returns diminish for more models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.10/>Scmhl5 at TRAC-2 Shared Task on Aggression Identification : Bert Based Ensemble Learning Approach<span class=acl-fixed-case>TRAC</span>-2 Shared Task on Aggression Identification: Bert Based Ensemble Learning Approach</a></strong><br><a href=/people/h/han-liu/>Han Liu</a>
|
<a href=/people/p/pete-burnap/>Pete Burnap</a>
|
<a href=/people/w/wafa-alorainy/>Wafa Alorainy</a>
|
<a href=/people/m/matthew-williams/>Matthew Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--10><div class="card-body p-3 small">This paper presents a system developed during our participation (team name : scmhl5) in the TRAC-2 Shared Task on aggression identification. In particular, we participated in English Sub-task A on three-class classification (&#8216;Overtly Aggressive&#8217;, &#8216;Covertly Aggressive&#8217; and &#8216;Non-aggressive&#8217;) and English Sub-task B on binary classification for Misogynistic Aggression (&#8216;gendered&#8217; or &#8216;non-gendered&#8217;). For both sub-tasks, our method involves using the pre-trained Bert model for extracting the text of each instance into a 768-dimensional vector of embeddings, and then training an ensemble of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on the embedding features. Our method obtained <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.703 and <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F-measure</a> of 0.664 for Sub-task A, whereas for Sub-task B the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> was 0.869 and <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F-measure</a> was 0.851. In terms of the rankings, the weighted F-measure obtained using our method for Sub-task A is ranked in the 10th out of 16 teams, whereas for Sub-task B the weighted F-measure is ranked in the 8th out of 15 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.14/>Spyder : Aggression Detection on Multilingual Tweets<span class=acl-fixed-case>S</span>pyder: Aggression Detection on Multilingual Tweets</a></strong><br><a href=/people/a/anisha-datta/>Anisha Datta</a>
|
<a href=/people/s/shukrity-si/>Shukrity Si</a>
|
<a href=/people/u/urbi-chakraborty/>Urbi Chakraborty</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--14><div class="card-body p-3 small">In the last few years, <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and aggressive comments have covered almost all the social media platforms like <a href=https://en.wikipedia.org/wiki/Facebook>facebook</a>, <a href=https://en.wikipedia.org/wiki/Twitter>twitter</a> etc. As a result hatred is increasing. This paper describes our (Team name : Spyder) participation in the Shared Task on <a href=https://en.wikipedia.org/wiki/Aggression>Aggression Detection</a> organised by TRAC-2, Second Workshop on <a href=https://en.wikipedia.org/wiki/Internet_troll>Trolling</a>, <a href=https://en.wikipedia.org/wiki/Aggression>Aggression</a> and <a href=https://en.wikipedia.org/wiki/Cyberbullying>Cyberbullying</a>. The Organizers provided datasets in three languages <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. The task was to classify each instance of the test sets into three categories Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-Aggressive (NAG). In this paper, we propose three different models using Tf-Idf, sentiment polarity and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning based classifiers</a>. We obtained <a href=https://en.wikipedia.org/wiki/F-number>f1 score</a> of 43.10 %, 59.45 % and 44.84 % respectively for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>.<b>Team name:</b>\n <b>Spyder</b>) participation in the Shared Task on Aggression Detection organised by TRAC-2, Second Workshop on Trolling, Aggression and Cyberbullying. The Organizers provided datasets in three languages &#8211; English, Hindi and Bengali. The task was to classify each instance of the test sets into three categories &#8211; &#8220;Overtly Aggressive&#8221; (OAG), &#8220;Covertly Aggressive&#8221; (CAG) and &#8220;Non-Aggressive&#8221; (NAG). In this paper, we propose three different models using Tf-Idf, sentiment polarity and machine learning based classifiers. We obtained f1 score of 43.10%, 59.45% and 44.84% respectively for English, Hindi and Bengali.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.15/>BERT of all trades, master of some<span class=acl-fixed-case>BERT</span> of all trades, master of some</a></strong><br><a href=/people/d/denis-gordeev/>Denis Gordeev</a>
|
<a href=/people/o/olga-lykova/>Olga Lykova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--15><div class="card-body p-3 small">This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants&#8217; task was to classify aggression in short texts from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> placed first in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.17/>FlorUniTo@TRAC-2 : Retrofitting Word Embeddings on an Abusive Lexicon for Aggressive Language Detection<span class=acl-fixed-case>F</span>lor<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>T</span>o@<span class=acl-fixed-case>TRAC</span>-2: Retrofitting Word Embeddings on an Abusive Lexicon for Aggressive Language Detection</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--17><div class="card-body p-3 small">This paper describes our participation to the TRAC-2 Shared Tasks on Aggression Identification. Our team, FlorUniTo, investigated the applicability of using an abusive lexicon to enhance word embeddings towards improving detection of aggressive language. The embeddings used in our paper are word-aligned pre-trained vectors for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, to reflect the languages in the shared task data sets. The embeddings are retrofitted to a multilingual abusive lexicon, HurtLex. We experimented with an LSTM model using the original as well as the transformed embeddings and different language and setting variations. Overall, our <a href=https://en.wikipedia.org/wiki/System>systems</a> placed toward the middle of the <a href=https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers&#8217;_Champions>official rankings</a> based on <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1 score</a>. However, the results on the development and test sets show promising improvements across languages, especially on the misogynistic aggression sub-task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.19/>Multilingual Joint Fine-tuning of Transformer models for identifying Trolling, Aggression and Cyberbullying at TRAC 2020<span class=acl-fixed-case>TRAC</span> 2020</a></strong><br><a href=/people/s/sudhanshu-mishra/>Sudhanshu Mishra</a>
|
<a href=/people/s/shivangi-prasad/>Shivangi Prasad</a>
|
<a href=/people/s/shubhanshu-mishra/>Shubhanshu Mishra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--19><div class="card-body p-3 small">We present our team &#8216;3Idiots&#8217; (referred as &#8216;sdhanshu&#8217; in the official rankings) approach for the Trolling, Aggression and Cyberbullying (TRAC) 2020 shared tasks. Our approach relies on fine-tuning various Transformer models on the different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We also investigated the utility of task label marginalization, joint label classification, and joint training on multilingual datasets as possible improvements to our models. Our team came second in English sub-task A, a close fourth in the English sub-task B and third in the remaining 4 sub-tasks. We find the multilingual joint training approach to be the best trade-off between computational efficiency of model deployment and <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s evaluation performance. We open source our approach at https://github.com/socialmediaie/TRAC2020.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.20/>Aggression and Misogyny Detection using <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> : A Multi-Task Approach<span class=acl-fixed-case>BERT</span>: A Multi-Task Approach</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/p/parth-patwa/>Parth Patwa</a>
|
<a href=/people/s/srinivas-pykl/>Srinivas PYKL</a>
|
<a href=/people/p/prerana-mukherjee/>Prerana Mukherjee</a>
|
<a href=/people/a/amitava-das/>Amitava Das</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--20><div class="card-body p-3 small">In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection. This paper presents our system for TRAC-2 shared task on Aggression Identification (sub-task A) and Misogynistic Aggression Identification (sub-task B). The data for this shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is provided in three different languages-English, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. Each data instance is annotated into one of the three aggression classes-Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes-Gendered and Non-Gendered. We propose an end-to-end neural model using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> on top of BERT that incorporates a multi-task learning paradigm to address both the sub-tasks simultaneously. Our team, na14, scored 0.8579 weighted F1-measure on the English sub-task B and secured 3rd rank out of 15 teams for the task. The code and the model weights are publicly available at https://github.com/NiloofarSafi/TRAC-2. Keywords : <a href=https://en.wikipedia.org/wiki/Aggression>Aggression</a>, <a href=https://en.wikipedia.org/wiki/Misogyny>Misogyny</a>, <a href=https://en.wikipedia.org/wiki/Abusive_language>Abusive Language</a>, Hate-Speech Detection, BERT, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>, <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.24/>Lexicon-Enhancement of Embedding-based Approaches Towards the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/j/jason-scott/>Jason Scott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--24><div class="card-body p-3 small">Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focuses on detecting personal attacks in <a href=https://en.wikipedia.org/wiki/Online_chat>online conversations</a>. As previous research on this task has largely used <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> based on <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, we explore the use of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to enhance embedding-based methods in an effort to see how these methods apply in the particular task of detecting personal attacks. The methods implemented and experimented with in this paper are quite different from each other, not only in the type of lexicons they use (sentiment or semantic), but also in the way they use the knowledge from the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, in order to construct or to change embeddings that are ultimately fed into the learning model. The sentiment lexicon approaches focus on integrating <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> (in the form of sentiment embeddings) into the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning model</a>. The <a href=https://en.wikipedia.org/wiki/Semantic_lexicon>semantic lexicon approaches</a> focus on transforming the original <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> so that they better represent relationships extracted from a <a href=https://en.wikipedia.org/wiki/Semantic_lexicon>semantic lexicon</a>. Based on our experimental results, semantic lexicon methods are superior to the rest of the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> in this paper, with at least 4 % macro-averaged F1 improvement over the baseline.</div></div></div><hr><div id=2020wac-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wac-1/>Proceedings of the 12th Web as Corpus Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wac-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wac-1.0/>Proceedings of the 12th Web as Corpus Workshop</a></strong><br><a href=/people/a/adrien-barbaresi/>Adrien Barbaresi</a>
|
<a href=/people/f/felix-bildhauer/>Felix Bildhauer</a>
|
<a href=/people/r/roland-schafer/>Roland Schäfer</a>
|
<a href=/people/e/egon-stemle/>Egon Stemle</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wac-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wac-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wac-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wac-1.1/>Current Challenges in Web Corpus Building</a></strong><br><a href=/people/m/milos-jakubicek/>Miloš Jakubíček</a>
|
<a href=/people/v/vojtech-kovar/>Vojtěch Kovář</a>
|
<a href=/people/p/pavel-rychly/>Pavel Rychlý</a>
|
<a href=/people/v/vit-suchomel/>Vit Suchomel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wac-1--1><div class="card-body p-3 small">In this paper we discuss some of the current challenges in web corpus building that we faced in the recent years when expanding the corpora in <a href=https://en.wikipedia.org/wiki/Sketch_Engine>Sketch Engine</a>. The purpose of the paper is to provide an overview and raise discussion on possible solutions, rather than bringing ready solutions to the readers. For every issue we try to assess its severity and briefly discuss possible mitigation options.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wac-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wac-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wac-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wac-1.5/>The ELTE.DH Pilot Corpus Creating a Handcrafted Gigaword Web Corpus with Metadata<span class=acl-fixed-case>ELTE</span>.<span class=acl-fixed-case>DH</span> Pilot Corpus – Creating a Handcrafted <span class=acl-fixed-case>G</span>igaword Web Corpus with Metadata</a></strong><br><a href=/people/b/balazs-indig/>Balázs Indig</a>
|
<a href=/people/a/arpad-knap/>Árpád Knap</a>
|
<a href=/people/z/zsofia-sarkozi-lindner/>Zsófia Sárközi-Lindner</a>
|
<a href=/people/m/maria-timari/>Mária Timári</a>
|
<a href=/people/g/gabor-palko/>Gábor Palkó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wac-1--5><div class="card-body p-3 small">In this article, we present the method we used to create a middle-sized corpus using targeted web crawling. Our corpus contains news portal articles along with their metadata, that can be useful for diverse audiences, ranging from digital humanists to NLP users. The method presented in this paper applies rule-based components that allow the curation of the text and the <a href=https://en.wikipedia.org/wiki/Metadata>metadata content</a>. The curated data can thereon serve as a reference for various tasks and measurements. We designed our <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> to encourage modification and customisation. Our <a href=https://en.wikipedia.org/wiki/Concept>concept</a> can also be applied to other genres of <a href=https://en.wikipedia.org/wiki/Portal_(architecture)>portals</a> by using the discovered <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> in the architecture of the portals. We found that for a systematic creation or extension of a similar corpus, our method provides superior <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and ease of use compared to The <a href=https://en.wikipedia.org/wiki/Wayback_Machine>Wayback Machine</a>, while requiring minimal manpower and computational resources. Reproducing the corpus is possible if changes are introduced to the text-extraction process. The standard <a href=https://en.wikipedia.org/wiki/Text_Encoding_Initiative>TEI format</a> and Schema.org encoded metadata is used for the output format, but we stress that placing the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> in a digital repository system is recommended in order to be able to define semantic relations between the segments and to add rich annotation.</div></div></div><hr><div id=2020wildre-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wildre-1/>Proceedings of the WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wildre-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wildre-1.0/>Proceedings of the WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation</a></strong><br><a href=/people/g/girish-nath-jha/>Girish Nath Jha</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/s/sobha-l/>Sobha L.</a>
|
<a href=/people/s/s-s-agrawal/>S. S. Agrawal</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wildre-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wildre-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wildre-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wildre-1.4/>Handling Noun-Noun Coreference in Tamil<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/v/vijay-sundar-ram/>Vijay Sundar Ram</a>
|
<a href=/people/s/sobha-lalitha-devi/>Sobha Lalitha Devi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wildre-1--4><div class="card-body p-3 small">Natural language understanding by automatic tools is the vital requirement for document processing tools. To achieve it, automatic system has to understand the coherence in the text. Co-reference chains bring coherence to the text. The commonly occurring reference markers which bring cohesiveness are Pronominal, Reflexives, Reciprocals, Distributives, One-anaphors, Nounnoun reference. Here in this paper, we deal with noun-noun reference in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>. We present the methodology to resolve these noun-noun anaphors and also present the challenges in handling the noun-noun anaphoric relations in <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wildre-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wildre-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wildre-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wildre-1.9/>Determination of Idiomatic Sentences in Paragraphs Using Statement Classification and Generalization of Grammar Rules</a></strong><br><a href=/people/n/naziya-shaikh/>Naziya Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wildre-1--9><div class="card-body p-3 small">The translation systems are often not able to determine the presence of an <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom</a> in a given paragraph. Due to this many systems tend to return the word-for-word translation of such statements leading to loss in the flavor of the <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> in the paragraph. This paper suggests a novel approach to efficiently determine probability of any statement in a given English paragraph to be an <a href=https://en.wikipedia.org/wiki/Idiom>idiom</a>. This approach combines the rule-based generalization of idioms in <a href=https://en.wikipedia.org/wiki/English_language>English language</a> and classification of statements based on the context to determine the <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> in the sentence. The context based classification method can be used further for determination of idioms in regional Indian languages such as <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Konkani_language>Konkani</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> as the difference in the semantic context of the proverb as compared to the context in a paragraph is also evident in these other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wildre-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wildre-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wildre-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wildre-1.12/>A Deeper Study on <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>Features</a> for Named Entity Recognition</a></strong><br><a href=/people/m/malarkodi-c-s/>Malarkodi C S</a>
|
<a href=/people/s/sobha-lalitha-devi/>Sobha Lalitha Devi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wildre-1--12><div class="card-body p-3 small">This paper deals with the various <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> used for the identification of named entities. The performance of the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning system</a> heavily depends on the feature selection criteria. The intention to trace the essential features required for the development of named entity system across languages motivated us to conduct this study. The <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a> was done to find out the part of speech patterns surrounding the context of named entities and from the observation linguistic oriented features are identified for both Indian and European languages. The Indian languages belongs to <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian language family</a> such as <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan language family</a> such as <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> and <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, European languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a> are used in this work. The machine learning technique CRFs was used for the system development. The experiments were conducted using the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> and the results obtained for each languages are comparable with <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-art systems</a>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>